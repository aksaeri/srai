sub_num,sub_name,sub_label,sub_value
1,Roger Hall,General comment,"Ensure AI has built in prime directives that cannot be avoided. Then simply make securely working, safe part of the list of prime directives."
1,Roger Hall,Q02 Other AI risks and recommendations for regulation,Ensure AI have prime directives which are suitable.
1,Roger Hall,Q04 Coordinating AI governance across government,Federal AI law overrides State AI law.
1,Roger Hall,Q06 Public vs Private sector governance,"Same, except Intelligence,Military have own prime directives."
1,Roger Hall,Q08 Generic vs technology-specific approaches for AI risk,Prime Directives in the AI technology will be best to mitigate risks.
1,Roger Hall,Q09 Transparency requirements and scope,In development and testing phases of AI to include with proof and testing proof.
1,Roger Hall,Q10 Ban on high-risk AI applications,Only ban AI with no suitable prime directives builtin.
1,Roger Hall,Q11 Actions to increase public trust in AI,"Secure, reliable, safe prime directives would be part of list suitable."
1,Roger Hall,Q12 Trade impact of banning high-risk AI applications,N/a
1,Roger Hall,Q13 Changes to conformity infrastructure,Tested proof of AI prime directives for AI license.
1,Roger Hall,Q14 Risk-based approach to address AI risks,Prime Directives builtin is best approach.
1,Roger Hall,Q17 Elements to include in risk-based approach,Prevent risks by use of logical prime directives builtin to AI products.
1,Roger Hall,Q19 Application of risk-based approach to general purpose AI,New AI with logical prime directives versions required.
1,Roger Hall,Q20 Voluntary vs mandated compliance with risk-based approach,Mandated by developers.
2,Ben Blackburn Racing,Transcription,"FILENAME: Ben Blackburn Racing public submission on the Australian Government’s Supporting Safe and Responsible Artificial Intelligence Discussion Paper 1.06.2023.9e95dcf858922.pdf | CONTENT: Ben Blackburn Racing submission on the Australian Government’s Supporting Safe and Responsible Artificial Intelligence Discussion Paper June 20230 Thursday, 1st June 2023Supporting safe and responsible AI in Australia: discussion paper Department of Industry, Science and Resources GPO Box 2013 Canberra ACT 2601Emailed to: DigitalEconomy@industry.gov.auTo whom it may concern,PUBLIC SUBMISSIONI am extremely pleased to provide this public submission in response to the Australian Government’s release of its Supporting safe and responsible AI in Australia Discussion Paper for public consultation. Feedback on this discussion paper is sought by Wednesday 26th July 2023.We acknowledge that public consultation on this discussion paper is an opportunity to work together and Ben Blackburn Racing values the o"
2,Ben Blackburn Racing,Transcription,"FILENAME: BBR Cover Letter on Supporting a Safe and Responsible AI in Australia submission 1.06.2023.0531fbb124d4a.pdf | CONTENT: Thursday, 1st June 2023 Supporting safe and responsible AI in Australia: discussion paper Department of Industry, Science and Resources GPO Box 2013 Canberra ACT 2601Emailed to: DigitalEconomy@industry.gov.au To whom it may concern, We are extremely pleased to provide this public submission in response to the Australian Government’s release of its Supporting safe and responsible AI in Australia Discussion Paper for public consultation. Feedback on this discussion paper is sought by Wednesday 26th July 2023.We are extremely pleased to provide this public submission to add our voice to the national conversation on supporting a safe and responsible AI in Australia.Thank you very much for taking the time to read and consider this correspondence and our Ben Blackburn Racing public submission attached. We look forward to your reply.Kind regards,___________________"
2,Ben Blackburn Racing,Q01 Definitions,Yes
4,Gary Looney,Transcription,"FILENAME: Submission to Responsible AI in Australia – Gary Looney – 1 June 2023.2c07cdd55a85b.pdf | CONTENT: Submission to Responsible AI in Australia – Gary Looney – 1 June 2023I imported ZX80 PC from UK 43yr ago at 14yr old, after building kit computer from components. Added speech interface, reworked Elisa simulated chat program. My Father a 'Sheep Shearer' laughed at me asking when’s it going to talk like Metal Micky? The rest is history.AI is Big Data, Human Behaviour, UI analysis and automated system programming. Big businesses analyse massive data sets to design Acc/Complaint Systems and legally control people’s actions and options, claiming it’s only used for efficiency. Who could seriously trust this, given experience? Were putting foxes in hen house and at gate.AI Safety Laws, regulation and regulatory bodies. We need input from vulnerable unpaid non advocate people to reduce business, political and advocacy bias. There's no benefit to anybody if any substantial business impa"
4,Gary Looney,General comment,"End-use Laws will protect public from harm, where focus on controlling development will stifle and monopolise for profits. As stated, trying to control development is futile because of ability and cost to police and the market for add on or modified product that will be supplied globally. The public need protection from bad actors that will string technology together to rob them. This is a case where Business and Government interests are against the public interest. Gary."
4,Gary Looney,Q01 Definitions,Refer to my Submission
4,Gary Looney,Q02 Other AI risks and recommendations for regulation,Refer to my Submission
4,Gary Looney,Q03 Non-regulatory actions to address AI risks,Refer to my Submission
4,Gary Looney,Q04 Coordinating AI governance across government,Refer to my Submission
4,Gary Looney,Q05 International models for AI governance,Refer to my Submission
4,Gary Looney,Q06 Public vs Private sector governance,Refer to my Submission
4,Gary Looney,Q07 Responsible government use of AI,Refer to my Submission
4,Gary Looney,Q08 Generic vs technology-specific approaches for AI risk,Refer to my Submission
4,Gary Looney,Q09 Transparency requirements and scope,Refer to my Submission
4,Gary Looney,Q10 Ban on high-risk AI applications,Refer to my Submission
4,Gary Looney,Q11 Actions to increase public trust in AI,Refer to my Submission
4,Gary Looney,Q12 Trade impact of banning high-risk AI applications,Refer to my Submission
4,Gary Looney,Q13 Changes to conformity infrastructure,Refer to my Submission
4,Gary Looney,Q14 Risk-based approach to address AI risks,Refer to my Submission
4,Gary Looney,Q15 Benefits and limitations of risk-based approach,Refer to my Submission
4,Gary Looney,Q16 Scope and application of risk-based approach,Refer to my Submission
4,Gary Looney,Q17 Elements to include in risk-based approach,Refer to my Submission
4,Gary Looney,Q18 Integration of risk-based approach into existing frameworks,Refer to my Submission
4,Gary Looney,Q19 Application of risk-based approach to general purpose AI,Refer to my Submission
4,Gary Looney,Q20 Voluntary vs mandated compliance with risk-based approach,Refer to my Submission
6,Dr Mehrdad Arashpour,Transcription,"FILENAME: XAI_Environmental_Management.c4949b1834356.pdf | CONTENT: Journal of Environmental Management 342 (2023) 118149Contents lists available at ScienceDirectJournal of Environmental Management journal homepage: www.elsevier.com/locate/jenvmanShort communicationAI explainability framework for environmental management research Mehrdad Arashpour Department of Civil Engineering, Monash University, Melbourne, VIC, 3800, AustraliaA R T I C L E I N F O A B S T R A C THandling Editor: Jason Michael Evans Deep learning networks powered by AI are essential predictive tools relying on image data availability and processing hardware advancements. However, little attention has been paid to explainable AI (XAI) in appli­ Keywords: cation fields, including environmental management. This study develops an explainability framework with a Environmental crisis triadic structure to focus on input, AI model and output. The framework provides three main contributions. (1) A Environmental management res"
6,Dr Mehrdad Arashpour,Q01 Definitions,"To some extent. Unlike other AI models that focus on recognizing or classifying existing data, generative models have the ability to create new data that resembles the patterns and characteristics of the training data they have been exposed to."
6,Dr Mehrdad Arashpour,Q02 Other AI risks and recommendations for regulation,"The global AI landscape is diverse, with different countries taking varied approaches to AI regulation. Overly strict regulations in one jurisdiction may put businesses at a disadvantage compared to those in regions with more favorable regulatory environments."
6,Dr Mehrdad Arashpour,Q03 Non-regulatory actions to address AI risks,"Foster collaboration between government and private sector entities involved in AI development and deployment. Engage with industry leaders, research institutions, and civil society organizations to leverage their expertise and perspectives in shaping AI governance policies."
6,Dr Mehrdad Arashpour,Q04 Coordinating AI governance across government,"Encourage transparency in AI governance practices by providing accessible information about AI initiatives, policies, and guidelines. Engage with the public through consultations, open forums, and public hearings to gather input and address concerns."
6,Dr Mehrdad Arashpour,Q05 International models for AI governance,The General Data Protection Regulation (GDPR) is the most relevant to Australia
6,Dr Mehrdad Arashpour,Q06 Public vs Private sector governance,No
6,Dr Mehrdad Arashpour,Q07 Responsible government use of AI,"Government can demonstrate leadership in responsible AI practices by integrating ethical considerations into its own AI projects and deployments. By implementing robust governance mechanisms and adhering to ethical standards, government can set a positive example for other organizations and encourage responsible AI adoption more broadly."
6,Dr Mehrdad Arashpour,Q08 Generic vs technology-specific approaches for AI risk,"Generic solutions play a vital role in educating and creating awareness about the risks of AI among stakeholders. They can serve as educational tools and resources, helping organizations and individuals understand the potential risks associated with AI and providing guidance on how to mitigate them effectively."
6,Dr Mehrdad Arashpour,Q10 Ban on high-risk AI applications,It is crucial to weigh these risks against the potential benefits they can bring. Striking a balance between managing the risks and harnessing the benefits is a more nuanced and considered approach than a complete ban.
6,Dr Mehrdad Arashpour,Q11 Actions to increase public trust in AI,Encourage government agencies to adopt responsible AI procurement practices. Prioritize AI solutions that align with ethical standards and demonstrate transparency and fairness.
6,Dr Mehrdad Arashpour,Q12 Trade impact of banning high-risk AI applications,"A complete ban may lead to unintended consequences, such as driving the development of AI technologies underground or to jurisdictions with looser regulations. This could potentially exacerbate the risks, as there would be limited oversight and accountability."
6,Dr Mehrdad Arashpour,Q13 Changes to conformity infrastructure,Promote digital literacy and provide training opportunities for individuals to learn about AI and its implications. This will empower people to make informed decisions about AI adoption and use.
6,Dr Mehrdad Arashpour,Q14 Risk-based approach to address AI risks,Yes
6,Dr Mehrdad Arashpour,Q18 Integration of risk-based approach into existing frameworks,Conduct a thorough gap analysis to identify areas where existing frameworks may not adequately address AI-specific risks. Determine the extent to which AI risks can be incorporated within the existing framework or if modifications or additions are required
6,Dr Mehrdad Arashpour,Q20 Voluntary vs mandated compliance with risk-based approach,"Public organizations serve the interests of the public, and there is an increasing expectation for responsible and trustworthy AI deployments. By adopting a risk-based approach, public organizations can align their AI strategies with public expectations, addressing concerns related to fairness, accountability, transparency, and the ethical use of AI."
7,Thomas Burns,Transcription,"FILENAME: 2023 - Epistemic virtues of harnessing rigorous machine learning systems in ethically sensitive domains.e1aaf2934ac9d.pdf | CONTENT: CommentaryEpistemic virtues of harnessing rigorous identify and interrogate a physician’s biases precisely at any day or time, now or in the past? If the ML system is suffi- machine learning systems in ethicallyJ Med Ethics: first published as 10.1136/jme-2023-109105 on 31 May 2023. Downloaded from http://jme.bmj.com/ on June 1, 2023 by guest. Protected by copyright. ciently interpretable and explainable, then it offers an opportunity for us to sensitive domains formalise the expertise and critical eval- uations of collectives of professionals and the people they serve. Thomas F Burns﻿﻿ For such ML systems to be successful in doing so, however, they need to be rigorously developed and applied. I Some physicians, in their care of person’s testimony is attributed less consider a rigorous ML system to be patients at risk of misusing opioids, use cr"
7,Thomas Burns,Transcription,"FILENAME: Detecting danger in gridworlds using Gromov's Link Condition-compressed.dab451cbdc8ca.pdf | CONTENT: Detecting danger in gridworlds using Gromov’s Link ConditionThomas F Burns Robert Tang Neural Coding and Brain Computing Unit Department of Pure Mathematics OIST Graduate University Xi’an Jiaotong–Liverpool University 1919-1 Tancha, Onna-son, Kunigami-gun 111 Ren’ai Road, Suzhou Industrial Park Okinawa, Japan 904-0495 Suzhou, Jiangsu Province, China 215123 t.f.burns@gmail.com robert.tang@xjtlu.edu.cn ORCID 0000-0002-1123-2929 ORCID 0000-0001-7021-4117AbstractGridworlds have been long-utilised in AI research, particularly in reinforcement learning, as they provide simple yet scalable models for many real-world applica- tions such as robot navigation, emergent behaviour, and operations research. We initiate a study of gridworlds using the mathematical framework of reconfigurable systems and state complexes due to Abrams, Ghrist & Peterson. State complexes represent all possible "
7,Thomas Burns,General comment,"It is vital that the government (1) commit to invest heavily in fundamental research in AI safety and explainability/interpretability; and (2) ignore extreme attitudes (e.g., 'alarmists' and 'extreme optimists') which are not backed by rigorous scientific standards."
7,Thomas Burns,Q01 Definitions,No.
7,Thomas Burns,Q05 International models for AI governance,Yes
8,Anonymous,General comment,"We need to make sure we have a fail-safe way to switch off AI, if it gets unreliable or unpredictable. If we let AI get smarter than us, we will no longer exist."
9,Peter Cotton,General comment,"The artificial intelligence revolution is leaving behind small businesses and organizations who cannot afford to hire in-house teams of data scientists to build bespoke models.Regulation of ""AI"", something politicians are dreadfully placed to comprehend never mind sensibly regulate, will create fixed costs that increase trade friction in data and models. As this regulation will likely exclude the military, it will hardly serve to protect anyone from SciFi scenarios they may fear. It will simply lead to people hiding behind definitions like ""non-models"" and other insanity I have witnessed first hand in bulge bracket banks where attempts to regulate mathematics are borderline farcical.Australia can pursue the opposite approach by providing a public utility allowing algorithms to traverse to business problems. This new internet for algorithms, as it might be termed, could dramatically lower the cost of providing ""AI"" to as many businesses and small organizations as possible, thus counteri"
9,Peter Cotton,Q01 Definitions,"No I do not agree. Speaking as someone who has developed automated and semi-automated trading algorithms I struggle to find any logic in the following definition for ""automated decision making"", since it obviously includes last-look and semi-automated decision making.o make interim assessments or decisions leading up to the final decision o recommend a decision to a human decision-maker o guide a human decision-maker through relevant facts, legislation or policy o automate aspects of the fact-finding process which may influence an interim decisionMy response to this definition of ""automated"" must be in the vain of John McEnroe ""you cannot be serious""!"
9,Peter Cotton,Q02 Other AI risks and recommendations for regulation,"Imbalance of power between small AI users and large, which this regulation will make worse."
9,Peter Cotton,Q03 Non-regulatory actions to address AI risks,"Yes. Build a utility described in the book ""Microprediction; Building an Open AI Network"""
10,Matthew Tucker,Transcription,"FILENAME: AI_submission_20230602.7b2469d611805.docx | CONTENT: Supporting Responsible AI Submission2 Jun 2023The advent of AI tools such as ChatGPT will be the start of a technology driven productivity increase of a degree we have not seen from IT before. The tools will affect every aspect of the economy the way that steam power affected every aspect of life during the industrial revolution. However, unlike the industrial revolution, the effects good and bad, will not take decades. These tools are currently being introduced at a rapid pace in business, and have already been used by many, if not most, students in high schools and universities in this country just a few months after ChatGPT went public.Unlike other IT, these tools do not require any training or skill to use. Business will not need to hire people with specialist training. The tools allow people to more rapidly complete the work they already do, and take on tasks beyond their current level of expertise. This will necessari"
10,Matthew Tucker,Q02 Other AI risks and recommendations for regulation,The risk of sweeping unemployment and radically uneven distribution of the benefits of AI
10,Matthew Tucker,Q10 Ban on high-risk AI applications,Policing
10,Matthew Tucker,Q12 Trade impact of banning high-risk AI applications,"It could improve our standing in countries that value freedom and democracy, which would be good for trade"
10,Matthew Tucker,Q14 Risk-based approach to address AI risks,"A better approach might be to focus on the distribution of income, wealth and power as it is affected by AI, in addition to the tactical and technical level risks, such as privacy and misinformation"
10,Matthew Tucker,Q15 Benefits and limitations of risk-based approach,"I can see already that people are very focused on technical risks, such as privacy because of the history of risks that have arisen from social media. The risks of AI are broader scale and apply to the economy and society as a whole, in particular the risk of large scale unemployment and rapid concentration of wealth"
10,Matthew Tucker,Q20 Voluntary vs mandated compliance with risk-based approach,the danger of a strong regulatory approach is that it will concentrate the ownership of the technology in the hands of large companies that can navigate the regulatory framework
11,Codemaster Institute,Q01 Definitions,yes
11,Codemaster Institute,Q02 Other AI risks and recommendations for regulation,"model source not authenticated, verification by a trusted source"
11,Codemaster Institute,Q03 Non-regulatory actions to address AI risks,voluntary code of ethics
11,Codemaster Institute,Q04 Coordinating AI governance across government,verification of datasets used for training
11,Codemaster Institute,Q05 International models for AI governance,Japan overriding existing copyright and privacy laws for training models
11,Codemaster Institute,Q06 Public vs Private sector governance,"yes private should not require the same level of accountability, only voluntary codes"
11,Codemaster Institute,Q07 Responsible government use of AI,publishing guidelines and giving access to trusted data sources through a secure portal rather than by web scraping.
11,Codemaster Institute,Q08 Generic vs technology-specific approaches for AI risk,no opinion
11,Codemaster Institute,Q09 Transparency requirements and scope,a)datasets that the model is trained on b) only for public and high risk private sector uses
11,Codemaster Institute,Q10 Ban on high-risk AI applications,a no b violation of existing Australian privacy law or similar
11,Codemaster Institute,Q11 Actions to increase public trust in AI,Secure trusted datasets. Limited to chatbots by document rather than making the scope of the LLM too large
11,Codemaster Institute,Q12 Trade impact of banning high-risk AI applications,not feasible given the next wave of technology with embedded multi-modal recognition
11,Codemaster Institute,Q13 Changes to conformity infrastructure,an ability to allow individuals to create privacy settings that exclude their data from training sets
11,Codemaster Institute,Q14 Risk-based approach to address AI risks,support
11,Codemaster Institute,Q15 Benefits and limitations of risk-based approach,not possible to control this on a National level as the models are hosted anywhere
11,Codemaster Institute,Q16 Scope and application of risk-based approach,yes
11,Codemaster Institute,Q17 Elements to include in risk-based approach,not reviewed
11,Codemaster Institute,Q19 Application of risk-based approach to general purpose AI,no decision-making without human review. An analogue human decision process must always be offered as an alternative
11,Codemaster Institute,Q20 Voluntary vs mandated compliance with risk-based approach,a mandated for public only b deployer
12,Justin,Q01 Definitions,Yes
12,Justin,Q02 Other AI risks and recommendations for regulation,I am not so much interested in risks and think too much regulation will again put Australia at the back of the pack when it comes to AI development. We have a history of coming up with stifling regulation that ensures other countries become leaders before we do and get the full benefit of new technology and we either play catch up or remain at a disadvantage.
12,Justin,Q03 Non-regulatory actions to address AI risks,No.
12,Justin,Q04 Coordinating AI governance across government,I would like to see AI help with reducing workload for government employees and reduce waiting times for consumers when it comes to things like Passports etc.
12,Justin,Q05 International models for AI governance,"No, we should not follow what others do at this premature stage. We should wait and observe the consequences of this governance and see whether it encourages development or stunts it. AI has the potential to greatly benefit the population by finding new treatments for health issues faster and tailoring them to the individual, reducing costs of energy through better network management and we should allow this to occur."
12,Justin,Q06 Public vs Private sector governance,No.
12,Justin,Q07 Responsible government use of AI,Not sure.
12,Justin,Q08 Generic vs technology-specific approaches for AI risk,Not sure.
12,Justin,Q09 Transparency requirements and scope,I don't think transparency is important and could serve to cause more distrust as we have seen the rise of conspiracy theorists since Covid who actively troll and spread misinformation no matter what the evidence or data says. It is best if the public was minimally aware of the use of AI.
12,Justin,Q10 Ban on high-risk AI applications,"No applications or technologies should be banned until they have been proven to be a risk to human life and it has been proven that measures cannot be taken to minimize this risk. Banning applications and technologies has potential to stunt growth of the industry and ensure consumers miss out on what could be life changing medical advancements and cost savings when it comes to things like energy, transport, food."
12,Justin,Q11 Actions to increase public trust in AI,"Provide trucks to go to communities and educate them on the benefits of AI, show people examples of AI and educate them about the potential uses and dispel myths about AI taking over, ultimately AI use computers and if the AI is posing a risk it can be disconnected from the power supply. It will always rely on humans for existence."
12,Justin,Q12 Trade impact of banning high-risk AI applications,It would impact it negatively. Facial recognition has great potential when it comes to crime prevention and tracking of missing persons. Our trade partners may see Australia as a country that doesn't invest in innovative technology and would not see it as a high value partner.
12,Justin,Q13 Changes to conformity infrastructure,The conformity infrastructure is a barrier to trade and providing reduced costs to consumers.
12,Justin,Q14 Risk-based approach to address AI risks,No I don't.
12,Justin,Q16 Scope and application of risk-based approach,No I don't support a risk based approach at all.
12,Justin,Q20 Voluntary vs mandated compliance with risk-based approach,It should be voluntary.
13,Steven,Transcription,"FILENAME: AI Safety.c8dfe7ecc5169.docx | CONTENT: Hello,I have a few concerns with AI.What is stopping me from typing “Anthony Albanese’s murder confession” into ChatGPT and falsely accusing our prime minister of a crime he never committed. Or a deepfake video might be generated of me kidnapping somebody. It might not be a concern right now, but as the technology improves, it might be a concern in the future. This could have horrible implications for our justice system and the integrity of our elections. It’s pretty terrifying that in the future, if I ever have a grudge against someone, I can get AI to create false evidence of said person committing a crimeThere are three potential methods of countering this:Requiring ChatGPT and equivalent apps to limit what can be typed into themRequiring such apps to release source code so that we know exactly how they work and experts can better distinguish between fakes and legitimate worksCreate technology that can distinguish between fakes and l"
13,Steven,Q02 Other AI risks and recommendations for regulation,Safeguards to prevent future governments overturning them
13,Steven,Q03 Non-regulatory actions to address AI risks,"Yes, these are addressed in my uploaded submission"
13,Steven,Q04 Coordinating AI governance across government,Work with international governments as well as state/territory ones
13,Steven,Q06 Public vs Private sector governance,NO
13,Steven,Q07 Responsible government use of AI,Safeguards to ensure no future government can overturn them
13,Steven,Q08 Generic vs technology-specific approaches for AI risk,"Technology specific solutions are better to stop people from using AI to falsly accuse people of crimes and to change the results of elections, because the people who are inclined to use AI in this manner would likely ignore such legislation"
13,Steven,Q09 Transparency requirements and scope,a. Better access to source codes to mitigate unforeseeable consequences of deep learning
13,Steven,Q10 Ban on high-risk AI applications,"Legislation would achieve nothing. And if Australia manages to ban such technology, it's very easy to use technology available overseas. Any member of the public willing to use AI to cause malice would not be concerned with any laws preventing that, as such, those laws wouldn't be worth the paper they're written on. The only exception to this is limiting government misuse of AI. Safeguards need to be in place to prevent governments from overturning laws preventing malicious government use of AI"
13,Steven,Q11 Actions to increase public trust in AI,"Education, promotion of benefits, advertising campaigns addressing people's SPECIFIC concerns of AI. For example, advertising campaigns about how the government will prevent people from using AI to create false evidence for crimes, and how the government will prevent future governments from implementing social credit scores"
13,Steven,Q12 Trade impact of banning high-risk AI applications,Any country that wouldn't trade with Australia because of a lack of social scoring should not be traded with in the first place. Australia should not sacrifice our basic human rights in order to trade with evil dictatorships
13,Steven,Q13 Changes to conformity infrastructure,"Consider constitutional change Detailed information given to the public about how the government may use AI, and require future governments to publish detailed reports about how they wish to change such legislation (but not too detailed that the public is bored and/or unable to understand what's happening), extra governmental authorities to regulate how governments can use AI and to prevent the government from overturning safeguards, general education from school children to adults about the fundamentals of AI, how governments and others can abuse such technology in order to keep the public aware of any change in government policy"
13,Steven,Q14 Risk-based approach to address AI risks,Yes
13,Steven,Q15 Benefits and limitations of risk-based approach,They can be overturned by future governments and laws may be disobeyed by malicious actors or people can access malicious AI from overseas
13,Steven,Q19 Application of risk-based approach to general purpose AI,"We need to ensure some level of accuracy. Inaccurate translations, for example, could cause all sorts of problems"
13,Steven,Q20 Voluntary vs mandated compliance with risk-based approach,"Both, with oversight from the government and minimum laws"
14,A Palmer,General comment,"As a former IT professional, I am greatly concerned by the proliferation of AI from the point of view that bad actors will be free to impersonate and manipulate individuals and organizations without there being ROBUST legal ramifications. Let's just stop the roll out for a short while until appropriate legislation can catch up."
14,A Palmer,Q01 Definitions,n/a
14,A Palmer,Q04 Coordinating AI governance across government,"The approach should be national, and have far reaching consequences for actors operating from overseas. There should not be an AI haven (like a tax haven) anywhere in the world."
14,A Palmer,Q09 Transparency requirements and scope,All information generated by AI should be traceable back to the source and should be easily and transparently noted. Perhaps like a footnote or hyperlink that takes you directly to the source.
14,A Palmer,Q10 Ban on high-risk AI applications,"NO AI for Lethal weapons, weapons of mass destruction. If the legal structure is appropriate to monitor and regulate AI, then outright bans may not be necessary. It is for the legal fraternity to draft legislation that is robust and appropriate."
14,A Palmer,Q11 Actions to increase public trust in AI,Traceability. SO everyone knows what has been generated by AI.
14,A Palmer,Q20 Voluntary vs mandated compliance with risk-based approach,"Voluntary will not work when bad actors are at play. Should apply to both public and private, and developers and deployers. There needs to be a chain of responsibility.... a bit like selling stolen goods."
15,Martin Z'Graggen,Q01 Definitions,YES. What about deep learning?
15,Martin Z'Graggen,Q02 Other AI risks and recommendations for regulation,"I think the biggest challenge is to control the level, quality and kind of prompts fed into the AI systems to avoid unwanted biases. This could lead to deep fake news, images and more manipulation through the generative AI power of language (text output) to be spread and controlled through all media outlets. Whoever and or whatever has the best ""language"" - messages/stories to tell/spread, such as beliefs, values and more will ""rule"" the world (humankind). I think it is crucial as we (humans) interact with AI, AI needs to disclose their identity. Meaning their responses need to be classified as AI generated data/information."
15,Martin Z'Graggen,Q03 Non-regulatory actions to address AI risks,"In general terms no AI systrem can make a decision over a human. Meanining, AI generated data and information can be used to gather and analyse data, e.g. a person applying for a loan, but the final decision needs to be made by a human if the loan can be granted or not. Automation (AI-driven) should benefit humankind, e.g. take care of mondaine and repetivie tasks, but human intercations should remain above AI."
15,Martin Z'Graggen,Q04 Coordinating AI governance across government,"In general terms, as outlined, all juristictions need to work collaborative and ensure timely and relevant policy brief will be commissioned and acted upon to stay within reach of this fast-moving technology. As mentioned before, the key regulatory stipulations need to state clearly that humankind (human intelligence and activities) is above AI when it comes to decision-making over other humans, e.g. medical, health, court, and more. In addition, as long as the private sector has the upper hand (financial interests), it will be very challenging for governments across the world to manage/mitigate AI adequately. In my view, this needs to be carried forward by an overreaching multi-disciplinary agency, e.g. the Human Charta - United Nations, where all jurisdictions and private organisations have their say as well as their obligations to apply and monitor agreed regulatory applications to manage and mitigate the use of AI."
15,Martin Z'Graggen,Q05 International models for AI governance,"Again, I suggest that only a multi-jurisdictional approach of the regulation of AI will have the chance (opportunity) to mitigate the use of AI effectively."
15,Martin Z'Graggen,Q06 Public vs Private sector governance,No. It is important to have a systematic regulatory approach across all government-led agencies and the private sector.
15,Martin Z'Graggen,Q07 Responsible government use of AI,"With collaboration across all jurisdictions and private sector players, to manage and meet ongoing and ever-changing needs to manage and mitigate the use of AI for now and beyond."
15,Martin Z'Graggen,Q08 Generic vs technology-specific approaches for AI risk,"Human interactions and approaches should reign above AI-generated data and information and need to be seen as a tool to assist the betterment of humankind. E.g. in medicine and healthcare, in education, in automation of mondaine (work) tasks, and more"
15,Martin Z'Graggen,Q09 Transparency requirements and scope,Transparency needs to apply to all AI-generated data and information being used for potential decision-making processes.
15,Martin Z'Graggen,Q10 Ban on high-risk AI applications,"Banning of the emerging AI technology is wrong and will not work. We, as a society, need to make a decision on how we ant to live now and in the future. Mitigation and sense-making regulatory approaches are the way moving forward."
15,Martin Z'Graggen,Q11 Actions to increase public trust in AI,"Any AI-generated information and/or any AI interaction needs to be clearly identified as AI and cannot be used and/or spread as a human-generated message, data and/or information."
15,Martin Z'Graggen,Q12 Trade impact of banning high-risk AI applications,"Again, these issues can on,ly be solved, and agreed upon, on a global level."
15,Martin Z'Graggen,Q13 Changes to conformity infrastructure,"Again, these issues can on,ly be solved, and agreed upon, on a global level."
15,Martin Z'Graggen,Q14 Risk-based approach to address AI risks,Yes
15,Martin Z'Graggen,Q15 Benefits and limitations of risk-based approach,Not sure ....
15,Martin Z'Graggen,Q16 Scope and application of risk-based approach,It should be applied to all environments and settings.
15,Martin Z'Graggen,Q17 Elements to include in risk-based approach,Yes
15,Martin Z'Graggen,Q18 Integration of risk-based approach into existing frameworks,Not sure...more inter-disciplinary and collaborative reeserarch is required.
15,Martin Z'Graggen,Q19 Application of risk-based approach to general purpose AI,"In my view, the same as other Risk Assessment model within/applied to other areas of potential risk of causing harm."
15,Martin Z'Graggen,Q20 Voluntary vs mandated compliance with risk-based approach,"No, this must be a mandatory requirement."
16,Anonymous,Transcription,"FILENAME: Submission 16 - Attachment 1.d794a0adc2bac.pdf | CONTENT: On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? Emily M. Bender∗ Timnit Gebru∗ ebender@uw.edu timnit@blackinai.org University of Washington Black in AI Seattle, WA, USA Palo Alto, CA, USAAngelina McMillan-Major Shmargaret Shmitchell aymm@uw.edu shmargaret.shmitchell@gmail.com University of Washington The Aether Seattle, WA, USA ABSTRACT alone, we have seen the emergence of BERT and its variants [39, The past 3 years of work in NLP have been characterized by the 70, 74, 113, 146], GPT-2 [106], T-NLG [112], GPT-3 [25], and most development and deployment of ever larger language models, es- recently Switch-C [43], with institutions seemingly competing to pecially for English. BERT, its variants, GPT-2/3, and others, most produce ever larger LMs. While investigating properties of LMs and recently Switch-C, have pushed the boundaries of the possible both how they change with size holds scientific inter"
16,Anonymous,Transcription,"FILENAME: stochastic parrots paper.5da24ecab0f7e.pdf | CONTENT: On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? Emily M. Bender∗ Timnit Gebru∗ ebender@uw.edu timnit@blackinai.org University of Washington Black in AI Seattle, WA, USA Palo Alto, CA, USAAngelina McMillan-Major Shmargaret Shmitchell aymm@uw.edu shmargaret.shmitchell@gmail.com University of Washington The Aether Seattle, WA, USA ABSTRACT alone, we have seen the emergence of BERT and its variants [39, The past 3 years of work in NLP have been characterized by the 70, 74, 113, 146], GPT-2 [106], T-NLG [112], GPT-3 [25], and most development and deployment of ever larger language models, es- recently Switch-C [43], with institutions seemingly competing to pecially for English. BERT, its variants, GPT-2/3, and others, most produce ever larger LMs. While investigating properties of LMs and recently Switch-C, have pushed the boundaries of the possible both how they change with size holds scientific interest,"
16,Anonymous,Transcription,"FILENAME: Global-Economics-Analyst_-The-Potentially-Large-Effects-of-Artificial-Intelligence-on-Economic-Growth-Briggs_Kodnani.9b24298687e26.pdf | CONTENT: 26 March 2023 | 9:05PM EDTGlobal Economics AnalystThe Potentially Large Effects of Artiﬁcial Intelligence on Economic Growth (Briggs/Kodnani) n The recent emergence of generative artiﬁcial intelligence (AI) raises whether we Jan Hatzius +1(212)902-0394 | jan.hatzius@gs.com are on the brink of a rapid acceleration in task automation that will drive labor Goldman Sachs & Co. LLCcost savings and raise productivity. Despite signiﬁcant uncertainty around the Joseph Briggs +1(212)902-2163 | potential of generative AI, its ability to generate content that is indistinguishable joseph.briggs@gs.com Goldman Sachs & Co. LLC from human-created output and to break down communication barriers between Devesh Kodnani humans and machines reﬂects a major advancement with potentially large +1(917)343-9216 | devesh.kodnani@gs.com macroeconomic effects."
16,Anonymous,Q02 Other AI risks and recommendations for regulation,"-the risks associated with media-related generative AI are completely unacceptable, i discussed this in my submission - the best way to avoid these risks is to outright ban these technologies"
16,Anonymous,Q07 Responsible government use of AI,needs to have a incredibly cautious approach. only allow for any use after extensive testing and ethical consideration
16,Anonymous,Q08 Generic vs technology-specific approaches for AI risk,the only sectors in which AI may be useful is in those such as medical research and care. any other uses will have far more damages than benefits
16,Anonymous,Q09 Transparency requirements and scope,a- transparency must be a fundamental and nonnegotiable part of any AI system that is rolled out at any level b- requiring that any developers give comprehensive details of how their systems work. (so-called black boxes are not acceptable)
16,Anonymous,Q10 Ban on high-risk AI applications,"a- all high and medium risk technologies should be banned completly, as they often serve to conduct activites that are socially undesirable (social scoring), or that we have humans that can already do them (such as surgery) b- AI applications should be banned when in situations that a human is already performing the task to a accpetable degree and when assigning liability becomes complex (self driving cars)"
16,Anonymous,Q11 Actions to increase public trust in AI,"government should not be encouraging use of AI, and should instead invest in intiatives to educate people on its flaws and give much better alternative solutions for cases in which people may feel tempted to use it (increasing access to legal aid instead of letting people use unreliable chatbots for legal advice)"
16,Anonymous,Q12 Trade impact of banning high-risk AI applications,"any impacts on the tech sector are acceptable if it means the continued protection of social, economic and cultural cohesion"
16,Anonymous,Q13 Changes to conformity infrastructure,government needs to take a proactive role in regulating technologies prior to their widespread adoption instead of playing catch-up after real harms have already occurred
17,Anonymous,General comment,"Subject: Request for Legislative Reforms for IP Protection Covering AI-Generated CodeDear Honourable Members of the Australian Government,We write to you today as advocates for the emerging frontier of artificial intelligence technology, particularly with a focus on the issue of intellectual property protection for AI-generated code. The current legislative framework, as it stands, does not specifically account for prompt engineering as a form of creative input in the process of AI code generation.We would like to bring your attention to the growing need for modifications to these laws to acknowledge the evolving nature of software development. With AI gaining traction in the technology sector, the blurring lines between human-engineered and machine-generated code are becoming increasingly challenging to define.Despite the machine's role in the actual generation of the code, the initial prompt and engineering instructions that direct the AI are borne out of human creativity and ingenui"
17,Anonymous,Q01 Definitions,Yes
17,Anonymous,Q02 Other AI risks and recommendations for regulation,Copyright and ip protections of code generated by AI and the interpretation of Prompt engineering as a specific creative input.
17,Anonymous,Q03 Non-regulatory actions to address AI risks,More education as this technology will generate more jobs that it takes away.
17,Anonymous,Q04 Coordinating AI governance across government,Software companies must be consulted as part of this process.
17,Anonymous,Q05 International models for AI governance,New Zealand has adequate coverage that covers up protection for machine generated code.
17,Anonymous,Q06 Public vs Private sector governance,No.
17,Anonymous,Q07 Responsible government use of AI,Embracing this change and through its use accelerate and optimise how we run the country.
17,Anonymous,Q11 Actions to increase public trust in AI,Use in schools and education.
17,Anonymous,Q12 Trade impact of banning high-risk AI applications,This will be done by other countries. Banning and restrictions on national levels will only restrict our own security.
17,Anonymous,Q14 Risk-based approach to address AI risks,Yes
17,Anonymous,Q15 Benefits and limitations of risk-based approach,This will allow us to address issues as they happen and implement regulations as needed that Don not overreach and stifle innovation.
17,Anonymous,Q16 Scope and application of risk-based approach,No this should be adopted widely. An agile risk based approach will ensure the right level of regulation.
18,melissa misuraca,General comment,"The federal government has not issued a directive to the public service on the use of tools like ChatGPT, which can generate text and images.Instead, it is leaving it up to departments to decide whether to allow the technology or limit its use. This is a risky approach and heightens the security and ethical risks associated with AI technology.Until a whole of government policy is adopted, the Federal Government, and all departments that come under it, should, at a minimum, ensure basic training in the ethical and cyber security risks of using AI tools such as ChatGPT. This training should be developed centrally and made available to all staff electronically. Anyone using AI tools should have to complete the mandatory training."
18,melissa misuraca,Q01 Definitions,yes
18,melissa misuraca,Q02 Other AI risks and recommendations for regulation,mandatory training for Government and Private entities/ departments using AI technology - ethical and security concerns
18,melissa misuraca,Q04 Coordinating AI governance across government,mandatory training for Government and Private entities/ departments using AI technology - ethical and security concerns
18,melissa misuraca,Q11 Actions to increase public trust in AI,trainig and awareness
18,melissa misuraca,Q14 Risk-based approach to address AI risks,"yes, risk based"
18,melissa misuraca,Q15 Benefits and limitations of risk-based approach,Consider using the NIST AI framework
18,melissa misuraca,Q20 Voluntary vs mandated compliance with risk-based approach,a and b - both
19,Lyndon Megarrity,General comment,I wrote the following hypothetical briefing note which may be of interest to you: https://aph.org.au/2023/02/policy-brief-artificial-intelligence-and-education/
19,Lyndon Megarrity,Q01 Definitions,General agreement.
19,Lyndon Megarrity,Q02 Other AI risks and recommendations for regulation,"There is a risk that corporations will use AI as a means of avoiding employing staff. The loss of the human element in daily interactions such as shopping for goods at the supermarket, will lead to problems of social cohesion as people lose the necessary quality of empathy. Governments need to have a broader definition of ""productivity"" that includes the wellbeing of society, not just the economy."
19,Lyndon Megarrity,Q03 Non-regulatory actions to address AI risks,"I think a realisation that people that develop and initiate AI are hardly going to think twice about introducing AI, regardless of whether it is needed or not, is an important aspect of this. Governments have to take the initiative of studying AI initiatives and providing information to the public about their pros and cons, because private enterprise will not."
19,Lyndon Megarrity,Q04 Coordinating AI governance across government,"The Federal Government's public service ""Digital First"" strategy has decreased the human element in the public service. This policy, especially when applied to AI, will decrease the face-to-face interactions with the public which are necessary for the public and politicians to understand how government affects citizens. A cautious approach to AI is necessary."
19,Lyndon Megarrity,Q05 International models for AI governance,The capacity to opt out of AI is a very important right for citizens.
19,Lyndon Megarrity,Q06 Public vs Private sector governance,There should be no different approaches. Transparency and the capacity for the citizen to opt-out of AI is vitally important in all circumstances.
19,Lyndon Megarrity,Q07 Responsible government use of AI,Learn from Robodebt! Human decision-making is vital at all parts of the policy process.
19,Lyndon Megarrity,Q08 Generic vs technology-specific approaches for AI risk,"Not exactly sure what you mean by ""technology-specific solutions""."
19,Lyndon Megarrity,Q09 Transparency requirements and scope,"It's not a matter of public trust and confidence, it's generally a matter of the capacity of citizens to decide for themselves how and when they engage in AI, if at all. The right not to engage in AI related activity is important, especially in the field of education."
19,Lyndon Megarrity,Q11 Actions to increase public trust in AI,"Again, it's not a matter of encouragement. People should have the right to engage or not engage. The value of what people can do without it should be first and foremost in our minds, to prevent machine dependence."
20,IdeaSpies,General comment,"IdeaSpies is an open innovation platform sharing ideas that do good. We have been sharing positive AI ideas for a few years as early examples, recognising it would spread widely as it is now https://ideaspies.com/sort/AI"
20,IdeaSpies,Q01 Definitions,yes
20,IdeaSpies,Q02 Other AI risks and recommendations for regulation,"AI will generally produce very positive outcomes. There should however be a human element in overseeing its use, particularly in sensitive industries such as defence and healthcare."
20,IdeaSpies,Q04 Coordinating AI governance across government,"Trust in AI is vital in ensuring its advantages are realised. There should be a reporting line from states to the federal government clearly designating responsibility for use of AI. The Big Techs, for example, are becoming very powerful and often their algorithms can't be challenged. There needs to be a high level powerful group at country level to address negative outcomes."
20,IdeaSpies,Q05 International models for AI governance,Yes- Appointment of a Minister for Artificial Intelligence could be considered at the federal level and perhaps also at the state levels https://ideaspies.com/posts/a-minister-for-artificial-intelligence
20,IdeaSpies,Q06 Public vs Private sector governance,Approaches should be consistent
20,IdeaSpies,Q07 Responsible government use of AI,1.Training on the advantages as well as any risks 2. Giving people the opportunity to retrain for new jobs before replacing their existing jobs 3. Limiting use of external consultants and instead invest in their staff.
20,IdeaSpies,Q08 Generic vs technology-specific approaches for AI risk,Both are important
20,IdeaSpies,Q09 Transparency requirements and scope,Transparency is vital. Any complaints about AI use should be able to be escalated to humans.
20,IdeaSpies,Q10 Ban on high-risk AI applications,"Use of AI in sensitive industries like defence and healthcare without human oversight should be banned, if possible."
20,IdeaSpies,Q11 Actions to increase public trust in AI,There should be clear regulations against misinformation. The EU's voluntary code against misinformation is a good example even though Twitter is reported to have withdrawn from it https://www.bbc.com/news/world-europe-65733969
20,IdeaSpies,Q12 Trade impact of banning high-risk AI applications,Not sure why facial recognition should be banned. IdeaSpies is sharing positive examples of its use https://ideaspies.com/search?q=facial+recognition
20,IdeaSpies,Q13 Changes to conformity infrastructure,N/A
20,IdeaSpies,Q14 Risk-based approach to address AI risks,Yes- support a risk-based approach
20,IdeaSpies,Q15 Benefits and limitations of risk-based approach,N/A
20,IdeaSpies,Q16 Scope and application of risk-based approach,N/A
20,IdeaSpies,Q17 Elements to include in risk-based approach,N/A
20,IdeaSpies,Q18 Integration of risk-based approach into existing frameworks,N/A
20,IdeaSpies,Q19 Application of risk-based approach to general purpose AI,N/A
20,IdeaSpies,Q20 Voluntary vs mandated compliance with risk-based approach,Start as voluntary and only mandate if necessary
21,Anonymous,General comment,My main concern is that as a consumer of any product I am made aware of if my data will be used in AI or if AI has been used in providing that product.
21,Anonymous,Q07 Responsible government use of AI,Transparency of the use of AI
21,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,There needs to be regulation in high risk areas as public and private organisations can't be trusted to self regulate.
22,Anonymous,General comment,"My submission takes the form of this comment. I believe AI must be regulated, so it is heartening to see this action by the Australian government. I have two submissions to make. First, that all sources used by AI tools be tracked and publically available to anyone who uses AI to gather data in any form for any purpose. Related to this, that all AI tools are monitored, perhaps with a tiered categorisation, for unbiased and verifiable data sources. That is, AI data sourcing should be ranked by the types of data sources they used and the level of bias and inaccuracy/misinformation those sources have. This will help to ensure that existing societal biases are, at the least, not made worse by AI, and that AI does not perpetuate misinformation. Second, that intellectual property rights are protected for all creative individuals whose work may be sourced by AI tools. This emcompasses writers (both fiction and non-fiction, including business writers), artists, musicians and anyone else whose "
22,Anonymous,Q01 Definitions,Agree
22,Anonymous,Q02 Other AI risks and recommendations for regulation,See my earlier comment.
22,Anonymous,Q03 Non-regulatory actions to address AI risks,Incentives are only likely to work if they come with powerful disincentives to do the wrong but easy/les expensive thing.
22,Anonymous,Q04 Coordinating AI governance across government,None
22,Anonymous,Q06 Public vs Private sector governance,"No, there should be standard expectations and regulations that all AI should conform to."
22,Anonymous,Q07 Responsible government use of AI,Set clear regulation and allow only AI tools that conform to these regulations at the highest standard.
22,Anonymous,Q08 Generic vs technology-specific approaches for AI risk,Everything must be transparent - nothing must be assumed or considered too obvious.
22,Anonymous,Q09 Transparency requirements and scope,"Think of food labelling but better. anyone who uses AI tools must be clear and visible about it and clear explanations of what they do, what data they use and how they are ensuring no biases or misinformation is permitted."
22,Anonymous,Q10 Ban on high-risk AI applications,"If an AI tool can't guarantee bias-free results or verifiable sources, it has no business adding to our cultural problems."
22,Anonymous,Q11 Actions to increase public trust in AI,"No bias, no misinformation or non-creditable information as standard, strong sanctions and disincentives for those who disobey, open and clear labelling of what is AI and how it works."
22,Anonymous,Q14 Risk-based approach to address AI risks,Risk-based based works as long as there is an overriding set of regulatory standards that all AI must conform to first.
22,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,Mandatory all the way.
23,Alex Berkovich,General comment,I'd love to see more RnD in Australia; AI is a tool and the risks may be there but Australia cannot be left behind due to fear.
23,Alex Berkovich,Q01 Definitions,LLM and MFMs are part of Deep Learning approaches - I believe discussing LLMs or MFM should be replaced by discussing existing deep learning-based systems
23,Alex Berkovich,Q02 Other AI risks and recommendations for regulation,"AI can be referred to as a tool or a technology. As a tool, I'd set limitation of decision AI-based systems can make. For example - an AI system should not control life affecting devices in hospitals, be cleared to open fire on Australian soil, or endanger lives directly. It should be used a tool - to help the people make the best choice, analyzing images, medical data etc."
23,Alex Berkovich,Q03 Non-regulatory actions to address AI risks,"Grants to RnD companies within Australia that show they work towards non-discriminative systems. For examples, they prove their data is balanced and represents (within reason) different cases to avoid discrimination. Similarly Gov. can offer tax incentives to such companies."
23,Alex Berkovich,Q04 Coordinating AI governance across government,"The Gov could have a panel that will update relevant ministers quarterly on new tech, risks and changes in the field. Panelists should be from the industry and from academia, preferably technical people."
23,Alex Berkovich,Q05 International models for AI governance,"Plenty. The report discussed very generally about risks but not opportunities. The Gov can provide tax incentives to startups but also offer these to existing large companies to build RnD centers here - for example, Israel is tiny but has a huge concentration of startups and RnD centers in all flavours. There are plenty of ""fresh"" areas Australia can focus on: Quantum computing; Distillation of salt water - clean water will always be in demand, and we are a giant desert so it's very close to home; Food related tech - cheap fake fish or meat etc. And of course, enriching the existing in Cyber, computer vision, text (LLMs) etc. Aside from the tax incentives, grants, land and help with tenders can be offered amongst other."
23,Alex Berkovich,Q06 Public vs Private sector governance,"Public sector could work with open source code and available models - this will allow complete transparency regarding the data and AI systems used. Naturally, private companies cannot be forces to disclose their IP."
23,Alex Berkovich,Q07 Responsible government use of AI,"AI systems are statistical systems - they are based on examples. To avoid discrimination, any entity that uses AI to screen (based on voice, image), and uses a ""model"" as it's core, should keep the data the ""model"" was trained on for the duration of the ""model""'s activity and the code it was trained with. It will show if the entity did reasonable efforts to avoid the system from discriminating; this will also prevent anyone from stating that such “model”s are “blackboxes” and we cannot predict their actions."
23,Alex Berkovich,Q08 Generic vs technology-specific approaches for AI risk,"There should be limits to what AI can control and what it cannot - when we speak about an AI system that replaces a typical person, it needs to prove very high capabilities. For example, autonomous vehicles, will replace general public, so they need to prove driving ability better than 95% of people. But, AI systems should NOT replace experts, like doctors who make life-death decision, have control over weapons and similar."
23,Alex Berkovich,Q09 Transparency requirements and scope,"a. In AI and tech in general we could enforce open source code and models for public usage. This also relates to the data. b. We cannot enforce open source and free access to company's IP. BUT, we can enforce companies to save code, data and models to prove lack of discrimination and similar risks."
23,Alex Berkovich,Q10 Ban on high-risk AI applications,AI is a tech. Current state of this tech is that it should NOT be banned in ay form. i.e. - don't block or censor anything
23,Alex Berkovich,Q11 Actions to increase public trust in AI,"Using AI shouldn't be Gov. concern. We shouldn't force anyone to drive or use a cell phone. Australia should focus on STEM in school, raising the level greatly. Bring RnD centers here and let people see the benefits of tech, AI included"
23,Alex Berkovich,Q12 Trade impact of banning high-risk AI applications,"Any ban on tech to be used in local market lowers the incentive to develop it, since you are forced to sell outside of home. This already lowers your chances. Of course it prevents import of such systems and overall damages the free economy.Important to distinguish specific applications from the technology."
23,Alex Berkovich,Q14 Risk-based approach to address AI risks,"Any technology can be misused (sadly there are death in car accidents all the time) - we cannot be left behind technologically and economically due to rumors. We need to check what motive there is to those that call for regulation. Specifically, Altman from openai called for regulations - we need to read it as is. Regulations will prevent any competition to openAI. Like with all science-related fields, there are risks but we should push forward."
23,Alex Berkovich,Q15 Benefits and limitations of risk-based approach,"Anytime we put the risks first, we stop progress. We should do that reasonably. Anytime we have regulations, a small company cannot afford to pay the lawyers to meet it"
23,Alex Berkovich,Q16 Scope and application of risk-based approach,AI is a tool - we need to look at application level.
23,Alex Berkovich,Q17 Elements to include in risk-based approach,in some cases Human in the loop is relevant. The rest is unclear if it referred to the users or developers.
23,Alex Berkovich,Q18 Integration of risk-based approach into existing frameworks,"IMO, the question is badly phrased"
23,Alex Berkovich,Q19 Application of risk-based approach to general purpose AI,"IMO, the question is badly phrased"
23,Alex Berkovich,Q20 Voluntary vs mandated compliance with risk-based approach,Any regulation harms progress. It is hard for small companies to rise so more regulation will just prevent anything from progressing
24,Trevor J Caldwell,General comment,"1 I think that Australia government will be to slow to act to allow Australia to become an innovation center. Japan has already secured a deal with Nvidia to build a very large AI data research centre. The deal appears to give Japan an exclusive advantage. They did this by by removing g copyright and privacy of all materials consumed by AI. They are the first in the world to do this. This will mean AI model can be build off any information with having to get permission to use the information. This will mean models built in Japan can be build faster by removing red tape massively. They also offered Nvidia this also offered Nvidia large amounts of money to get the detail over the line. I suspect China, Taiwan and South Korea will attempt to do the same. To compete with US current dominance in AI. Not to be out done by Japan. So if we want to compete in this space in the same way or be left behind we must compete the same way. Nvidia makes the world's most advance AI chips. But the are no"
24,Trevor J Caldwell,Q01 Definitions,Think they are too tightly defined an limit the scope of thought
24,Trevor J Caldwell,Q07 Responsible government use of AI,See prrevous Com ents have mean opportunity ideas
25,Andrew Robinson,Transcription,"FILENAME: Responsible AI (Andrew Robinson at 6clicks).6a2280eb67619.pdf | CONTENT: 6 June 2023Australian GovernmentDepartment of Industry, Science and ResourcesConsultation hubRE: Supporting Responsible AIThe discussion paper on supporting responsible AI available on the consultation hub is an excellent resource that does well to provide an overview of the topic along with an analysis of the existing international and domestic landscape as it relates to the opportunities and challenges presented by AI. However, it falls short in one very specific but important way. In fact, that way is a way many government attempts at regulation fail. That is, it suggests an internal self-assessment-based approach to managing “medium risk” (“high impact”) scenarios that include the use of AI-enabled applications involving the use of sensitive personal (financial or health) information and where decision making (such as access to emergency services or medical treatment) is critical. The government shou"
25,Andrew Robinson,General comment,The paper is heading in the right direct but do not fear encouraging if not regulating external assurance in medium risk / high impact applications of AI.
25,Andrew Robinson,Q02 Other AI risks and recommendations for regulation,The risk that organisations performing self-assessment and internal monitoring apply a very low standard and are found wanting which undermines the publics trust in AI applications.
25,Andrew Robinson,Q03 Non-regulatory actions to address AI risks,The Australian Government should encourage the adoption of ISO/IEC DIS 42001 (once its published) through it supply chain and each department's regulated entities in a similar to way to ISO 9001 and ISO 27001.
25,Andrew Robinson,Q04 Coordinating AI governance across government,There needs to be a central technical authority on AI that can work with regulators on model terms and refinining industry specifics.ISO and JASANZ can be used to formalise independent accreditation and certification against ISO 42001 for AI along with any industry specific requirements/schemes. This is an underutilised aparatus.
25,Andrew Robinson,Q05 International models for AI governance,The EU offers plenty to consider.
25,Andrew Robinson,Q06 Public vs Private sector governance,"No, public sector and private sector have equally critical elements. The public sector should certainly not be excluded from the regulations given the high impact of poor decision making."
25,Andrew Robinson,Q07 Responsible government use of AI,Lead by example by following national regulation and international standards for AI that include external assurance.
25,Andrew Robinson,Q08 Generic vs technology-specific approaches for AI risk,"Generic solutions are useful at the conceptual or design stages of an AI project. However, technology-specific solutions are necessary at the detailed design and implementation stages. If regulation are technology-specific they can achieve a high standard but may be become dated quickly."
25,Andrew Robinson,Q09 Transparency requirements and scope,Transparency is most critical when the AI system becomes operational so consumers and regulators can understand the risk of use and adapt their use. Mandatory transparency can be achieved by establishing an AI model card like a product label that includes minimum necessary information about the model.
25,Andrew Robinson,Q10 Ban on high-risk AI applications,"The EU has banned egregerious uses of AI which instead of calling ""high risk"" are called ""unacceptaple risk"". ""High risks"" can possibly be mitigated so outright ban with an ambiguous definition will create uncertainty. The uses that should be banned are uses that exceed society's ethical and moral tolerances and relate to covert manipulation and abuse, and physical or pychological harm (including the decision to use nuclear weapons and decisions to kill in warfare). Use of AI in warfare requires its own separate research and considerations."
25,Andrew Robinson,Q11 Actions to increase public trust in AI,"Lower the threshold to requiring external validation that home grown or refined AI models are indeed trustworthy and reliable for their intended use. We need to work together to prepare for an AI powered future. If many organisation turn internally, a culture of secrecy will be fostered rather than one of cooperation."
25,Andrew Robinson,Q12 Trade impact of banning high-risk AI applications,Minor impact where an Australian company is developing technology to export to a country with lower trustworthy standards for AI. This would appear to be a good thing.
25,Andrew Robinson,Q13 Changes to conformity infrastructure,"Great question. Australian conformity infrastructure including JASANZ and global CABs requires reinvigoration. Most people in Australia don't even know who JASANZ is and what it does? Same for most CABs.The Australian conformity infrastructure needs to be invited to come on the journey with the Australian Government, be given the opportunity to assist by developing a scheme based on international standards (ISO 17021, ISO 42000).The competency requirements including training and experience of people involved in certification and acccreditation related to AI (and Australian Government cyber requirements) needs to be uplifted.Once uplifted there needs to be a concentrated PR and marketing campaign to build trust in the infrastructure, and it needs to be adopted by Australian Government departmetns & agencies leading by example."
25,Andrew Robinson,Q14 Risk-based approach to address AI risks,It's hard to see the existing approach as risk-based because it only looks at the inherent impact and does not consider inherent likelihood or likelihood of risk mitigation. It's more of a gut feel on inherent impact... But it's a start.
25,Andrew Robinson,Q15 Benefits and limitations of risk-based approach,The intented approach avoids a one size fits all approach but is not really risk-based. Risk-based suggests a risk assessment has been done that takes risk mitigation into consideration.
25,Andrew Robinson,Q16 Scope and application of risk-based approach,"It's not really sector that is the discriminator because there can be high, medium and low risk applications in most sectors. Perhaps its medium and high impact scenarios in critical sectors that are prioritised though."
25,Andrew Robinson,Q17 Elements to include in risk-based approach,"The elements missing as the detailed countermeasures that are necessary to mitigate threats such as (poisoning, hullicinations etc) to an AI model and can be found in AI literature. Microsoft has a good threat model here -> https://learn.microsoft.com/en-us/security/engineering/threat-modeling-aiml"
25,Andrew Robinson,Q18 Integration of risk-based approach into existing frameworks,"An AI risk-based approach in for example ISO 42000 can be integrated into the existing conformity assessment frameworks for management systems such as those for quality, safety, environment and cyber security. There is in fact an integrated management system concept that seeks to deduplicate and enable efficiency."
25,Andrew Robinson,Q20 Voluntary vs mandated compliance with risk-based approach,Mandatory. A voluntary / self-regulated risk-based approach is not enough.
26,Lilin Australia,Q01 Definitions,No! The definitions do not incorporate devices that have an added a layer of Artificial Intelligence to automatically detect & transmit metadata.
26,Lilin Australia,Q02 Other AI risks and recommendations for regulation,"Privacy is already breached & unmonitored in this country. Regulation is already in place but is not enforced & our legal ""experts"" have no awareness of the regulations."
26,Lilin Australia,Q03 Non-regulatory actions to address AI risks,An organization that monitors breaches & can issue fines. One that also provides education to the legal community
26,Lilin Australia,Q04 Coordinating AI governance across government,"Actively fining companies who bring in technology that is not approved in Australia, even banned. As with the Covid monitoring devices, many were sold in Australia, banned by TGA, yet how many fines were issued? Corporate responsibility!"
26,Lilin Australia,Q05 International models for AI governance,"Yes, other countries actively police corporations & actively investigate privacy breaches."
26,Lilin Australia,Q06 Public vs Private sector governance,"No, there is too much complacency from both levels & the acceptance of illegal practice gravitates between both sectors."
26,Lilin Australia,Q07 Responsible government use of AI,Completely reform the tender process & analyse where that generates illegitimate sales & purchases.
26,Lilin Australia,Q08 Generic vs technology-specific approaches for AI risk,"Generic is best, the regulation of application."
26,Lilin Australia,Q09 Transparency requirements and scope,"Transparency is most critical at the purchase point, the devices should not be allowed in, contrary to our laws. Start by punishing corporations & individuals may be scared of consequences for themselves - Enforcement of existing laws!"
26,Lilin Australia,Q10 Ban on high-risk AI applications,"High risk applications are already limited in this country, we need them to be allowed. Why do our trams not have Ai safety mechanisms? Criteria is in line with our laws, if there is not a testing body, there should be a reporting body. Somewhere industry professionals can report bad or unethical practices & know it will be followed up."
26,Lilin Australia,Q11 Actions to increase public trust in AI,"Training, awareness as well as the inherent knowledge that companies who breach privacy regulations will actually face retribution. People do not trust corporations with their personal data & corporations should not be allowed to self-regulate"
26,Lilin Australia,Q12 Trade impact of banning high-risk AI applications,"It would mean we followed our own laws & regulations. We do not trade or export with those countries, they are too busy pouring their product into our open, unregulated market. Australia is barely ahead of nations like Brazil in our efforts to enforce regulation. The over abundance of items appearing in Australia, that were banned overseas, highlights that we do not monitor or act. We already have stockpiles in the country of devices that would be classed in this high-risk category. These are already being installed on our streets & public areas. If anything, banning would put Australia's policies in line with other western countries"
26,Lilin Australia,Q13 Changes to conformity infrastructure,"Review the broken tender system that encourages & allows bad practice. Enforce our existing laws, this will bring awareness of unethical behaviours A vehicle for people to report breaches & know that it will be followed up!"
26,Lilin Australia,Q14 Risk-based approach to address AI risks,Yes
26,Lilin Australia,Q15 Benefits and limitations of risk-based approach,Education of the supply chain is the biggest benefit. It also means that people will be unable to use ignorance as a defense against breaches. It will encourage corporate responsibility.
26,Lilin Australia,Q16 Scope and application of risk-based approach,"No, this needs to be a standardized approach regardless of the size or resources. Maturity would be only relevant to developers. Ai in development would be reported to those affected by it's activity & there would be transparency"
26,Lilin Australia,Q17 Elements to include in risk-based approach,"Yes I support the elements in attachment C. It seems to be focused on awareness & communication, which then alleviates the privacy concerns."
26,Lilin Australia,Q18 Integration of risk-based approach into existing frameworks,By enforcing existing laws that are being breached. Educating legal representatives specifically on Ai risks & how captured data should be maintained & cleansed.
26,Lilin Australia,Q19 Application of risk-based approach to general purpose AI,It would stop platforms retaining personal information & private data
26,Lilin Australia,Q20 Voluntary vs mandated compliance with risk-based approach,"A Public & private organizations should be regulated. There are limited numbers of developers in this country, we are talking about overseas developers & the ethics once their product has arrived in Australia. Deployers cannot be held responsible when the organization can just find another unethical deployer to create their desired solution. We have not monitored ethics in this country for a long time & now we are trying to catch up. Self regulation has already failed the Australian people"
27,Anonymous,Q02 Other AI risks and recommendations for regulation,AI should be democratised and regulated in a way that benefits not only the companies owning AI and their shareholders but also the customers all the while being sustainable.
27,Anonymous,Q03 Non-regulatory actions to address AI risks,"Humans as a single unit of output being far less superior to AI (and robotics/robots) will result in massive job losses, therefore AI companies should be taxed at 98%(?) and the revenue to be paid to those whose jobs are no longer tenable, otherwise, we could see greater wealth divide."
27,Anonymous,Q10 Ban on high-risk AI applications,Do not allow AI to have the ability to deploy weapons.
28,Complexico,Transcription,"FILENAME: Supporting responsible AI_ Complexico's submission.c39feafb9669e.pdf | CONTENT: Supporting responsible AI: Complexico's submissionRecommendationsRecommendation 1: A comprehensive approach to AI GovernanceRecommendation 2 Establish an International Authority for Artificial Intelligence GovernanceRecommendation 3: Review antitrust regulations applicable to AIRecommendation 1: A comprehensive approach to AI GovernanceA comprehensive approach to AI safety governance would likely involve a combination of these mechanisms, tailored to specific contexts and addressing the diverse challenges associated with AI safety.Relevance to AI Safety Governance MechanismLegislation and Provides a legal framework for enforcing safety requirements, ensuringRegulation compliance, and holding accountable for harm caused by AI systems.Promote responsible AI development and use, which includes safety Ethical Guidelines and considerations such as robustness, reliability, and the avoidance of Principle"
28,Complexico,Q01 Definitions,No comment
28,Complexico,Q03 Non-regulatory actions to address AI risks,"Deregulatory:Antitrust laws play a crucial role in the governance of AI systems by promoting fair competition, preventing monopolistic practices, and protecting consumers. While not explicitly mentioned in the initial taxonomy, antitrust laws can be considered a regulatory mechanism within the broader context of legislation and regulation. They focus on ensuring a level playing field and maintaining healthy market dynamics.Australia has its own set of competition laws, including the Competition and Consumer Act 2010. These laws aim to promote fair competition, prevent anti-competitive behavior, and protect the interests of consumers in the Australian market. The Australian Competition and Consumer Commission (ACCC) is the regulatory authority responsible for enforcing these laws.However, when it comes to AI safety and cooperation, there is a potential tension between the goals of antitrust laws and collaborative efforts to address AI safety concerns. Cooperation among industry players "
28,Complexico,Q04 Coordinating AI governance across government,"Establish an International Authority for Artificial Intelligence Governance:One of the major concerns surrounding current AI systems revolves around their opaque nature, as they tend to function as black boxes. Consequently, they can be unreliable, challenging to interpret, and susceptible to bias.It is becoming increasingly crucial to establish robust governance mechanisms to mitigate risks and ensure the safe deployment of advanced AI systems. Just as the International Atomic Energy Agency (IAEA) monitors and regulates nuclear activities worldwide, there is a growing recognition that an equivalent international authority may be necessary for overseeing superintelligence efforts.As AI surpasses human capabilities its impact on society and global security cannot be underestimated. Given the global nature of AI development and its potential consequences, it is imperative to establish an international authority to safeguard against existential risks. This authority would serve as a centr"
28,Complexico,Q05 International models for AI governance,No comment
29,OOP,Q02 Other AI risks and recommendations for regulation,sine-conic fractals
29,OOP,Q08 Generic vs technology-specific approaches for AI risk,https://www.pwc.com/us/en/tech-effect/ai-analytics/responsible-ai-for-generative-ai.html
29,OOP,Q09 Transparency requirements and scope,"Arbitration in military augmentation encompassing the rubric ""national security""."
29,OOP,Q14 Risk-based approach to address AI risks,phonon chip [ASI] registry - Google Images
31,Anonymous,Transcription,"FILENAME: Submission 31 - Attachment 1.c65df818705a5.pdf | CONTENT: The following is a conversation with ChatGPT (public free version) – 6 June 2023Human: Draft regulations on use of AIDraft regulations on the use of AI vary depending on the country and jurisdiction. While I cannot provide you with specific regulations from after my knowledge cutoff in September 2021, I can give you an overview of some common areas that regulations on the use of AI might cover. Please note that these examples are not exhaustive and regulations may have evolved since my last update.1. Data Protection and Privacy: Regulations often address the collection, storage, and processing of personal data by AI systems. They may require explicit user consent, transparent data handling practices, and data minimization to protect individuals' privacy rights.2. Transparency and Explainability: Regulations may require that AI systems be transparent and provide explanations for their decisions or actions. This could in"
31,Anonymous,Transcription,"FILENAME: Harari lecture 20230608.96b259fb1d57f.docx | CONTENT: I highly recommend the lecture “How to Survive in the Era of AI” by Yuval Harari:https://www.thegreatminds.com/lecture/how-to-survive-in-the-era-of-aiThe transcript can be downloaded by searching for “Harari”:https://www.thegreatminds.com/searchI highlight the following points which are particularly relevant to AI regulations, for which I fully support:Increase taxes on tech companies that produce the robots and algorithms and makes all the money. And you use that money to support the people who lost their jobs due to the tech. (Lesson 3)Three guidelines on using data technology (Lesson 4):“My data should be used to help me and not to manipulate me, very simple. It's my data after all. If you collect data on me in order to manipulate me to sell me products I don't need to sell this to a political party that then tries to brainwash me. This is, should be forbidden.”“Never allow all the data to be concentrated in one place. "
33,The Centre for Sustainable AI,Transcription,"FILENAME: Supporting responsible AI discussion paper - Dr Mahendra Samarawickrama - The Centre for Sustainable AI.3b4e7224d2ad2.pdf | CONTENT: Neither a person nor an apple can be diverse. Diversity is the property of a collection of people—a basket with many kinds of fruit. – Scott E. PageAI Governance and Ethics Framework for Sustainable AI and SustainabilityDr Mahendra Samarawickrama (GAICD, MBA, SMIEEE, ACS(CP))May 18, 2022 Copyright NoticeCopyright © 2022 Mahendra Samarawickrama Email: mahendra@aiesg.org ISBN: 978-0-6454693-0-1This work is licensed under a Creative Commons “Attribution 4.0 International” license. This report was submitted to the consultation process of The Australian Department of the Prime Minister and Cabinet for the regulation of artificial intelligence (AI) and automated decision making.Third party copyright Wherever a third party holds copyright in this material, the copyright remains with that party. Their permission may be required to use the material. Plea"
33,The Centre for Sustainable AI,General comment,This submission was previously submitted to government consultations on AI ethics and governance. https://doi.org/10.48550/arXiv.2210.08984
33,The Centre for Sustainable AI,Q01 Definitions,"This paper provides a comprehensive literature review on current and emerging data assurance frameworks, with a specific focus on the European and American perspectives. While the paper adequately covers existing policies that touch upon Advanced AI, generative AI, and Ethical AI, it would be beneficial to delve deeper into these topics to provide a more thorough analysis.Given Australia's multicultural landscape, it is essential to encourage contributions that highlight the significance of culture, diversity, and national reconciliation in the context of data/AI assurance frameworks. Moreover, the paper would benefit from a more in-depth exploration of emerging digital economics fields, such as Metaverse, and its close relationship with AI ethics and governance.In addition, it is worth considering the inclusion of a discussion on the United Nations Sustainable Development Goals (UN SDGs) and their alignment with AI assurance for sustainability. Notably, research suggests that AI has t"
33,The Centre for Sustainable AI,Q02 Other AI risks and recommendations for regulation,"Australia should proactively harness the potential of AI and data science to mitigate the risks posed by climate change. By strategically utilizing AI technologies, Australia can develop innovative solutions to address climate-related challenges effectively. This approach aligns with the broader framework of leveraging AI for climate action as advocated by the United Nations Sustainable Development Goals (UN SDGs). Embracing AI-driven initiatives will enable Australia to make substantial progress in mitigating the risks associated with climate change and foster a more sustainable future."
33,The Centre for Sustainable AI,Q03 Non-regulatory actions to address AI risks,"It would be valuable to conduct a benchmarking exercise to assess the sustainability of AI assurance in Australia in comparison to the United Nations Sustainable Development Goals (UN SDGs). This benchmarking would facilitate a comparative analysis of Australia's progress with other countries, as the UN SDGs serve as a common framework for sustainability.Moreover, it would be insightful to explore the diversity, equity, and inclusion aspects of Australian data services in relation to the Australian Census. Understanding the alignment between Australian data services and the demographic makeup of the country, as represented by the Census, would provide valuable insights into the level of diversity, equity, and inclusion within these services."
33,The Centre for Sustainable AI,Q04 Coordinating AI governance across government,"The establishment of a Center for AI Ethics, Governance, and Assurance can play a crucial role in coordinating AI governance efforts across the Australian government, aligning them with the United Nations Sustainable Development Goals (UN SDGs), specifically Goal 16 (Peace, Justice, and Strong Institutions) and Goal 17 (Partnerships for the Goals).Firstly, the Center can contribute to Goal 16 by fostering the development and implementation of ethical frameworks and guidelines for AI usage within the government. It can facilitate the creation of policies that ensure AI systems are deployed in a manner that upholds principles of transparency, accountability, and fairness. By promoting responsible AI practices, the Center can help strengthen the government's commitment to peace, justice, and strong institutions.Secondly, the Center can actively support Goal 17 by forging partnerships and collaborations with various stakeholders, including government agencies, industry experts, academia, a"
33,The Centre for Sustainable AI,Q05 International models for AI governance,"India, Brazil, Netherlands, ChileFor more information: https://www.linkedin.com/pulse/eu-ai-act-future-governance-dr-mahendra/"
33,The Centre for Sustainable AI,Q06 Public vs Private sector governance,"If the UN SDGs are set as a framework, then everything can be normalised easily since UN SDGs must translate into a strong commitment by all stakeholders to implement the global goals."
33,The Centre for Sustainable AI,Q07 Responsible government use of AI,Implement AI assurance framework with broad diversity and inclusion reflecting in the Australian Census.
33,The Centre for Sustainable AI,Q08 Generic vs technology-specific approaches for AI risk,The bias and impartiality of AI solutions should be properly addressed to mitigate risks related to discrimination. The bias and impartiality are contradicting and should be carefully tuned/justified with a broad understanding of diversity and inclusion.
33,The Centre for Sustainable AI,Q09 Transparency requirements and scope,Human ethics should be at the core of this AI assurance framework. The transparency also underpins by ethics. That's the key for Trust.
33,The Centre for Sustainable AI,Q10 Ban on high-risk AI applications,"Anything manipulates human behaviour, creating an impact on health and well-being (including psychological well-being). This is also a UN SDG."
33,The Centre for Sustainable AI,Q11 Actions to increase public trust in AI,"Benchmark People, Data, Processes and Initiatives against UN SDGs and Australian Census and make sure sustainability and DEI (Diversity, Equity and Inclusion) is addressed."
33,The Centre for Sustainable AI,Q12 Trade impact of banning high-risk AI applications,"The answer depends on leaders risk apatite. Impact is on Trust, Sustainability and Health and Well being."
33,The Centre for Sustainable AI,Q13 Changes to conformity infrastructure,The surveillance and medical infrastructures would be a good place to starting with.
33,The Centre for Sustainable AI,Q14 Risk-based approach to address AI risks,"Yes, together with sustainability (i.e., UN SDGs)"
33,The Centre for Sustainable AI,Q15 Benefits and limitations of risk-based approach,Benefit is that will mitigate risks. The limitation is that doesn't address sustainability.
33,The Centre for Sustainable AI,Q16 Scope and application of risk-based approach,"Yes, a risk-based framework is better when we deal with the implications of AI on fundamental human rights (e.g., privacy, dignity, etc.)."
33,The Centre for Sustainable AI,Q17 Elements to include in risk-based approach,"Human rights Humanitarian Law Privacy Health and Well-being DEI (Diversity, Equity and Inclusion)"
33,The Centre for Sustainable AI,Q18 Integration of risk-based approach into existing frameworks,"Through a SWOT, PESTLE and KSF analysis"
33,The Centre for Sustainable AI,Q19 Application of risk-based approach to general purpose AI,"Always stick to the basic and understand what are the implications on: Human rights Humanitarian Law Privacy Health and Well-being DEI (Diversity, Equity and Inclusion)"
33,The Centre for Sustainable AI,Q20 Voluntary vs mandated compliance with risk-based approach,"Initially voluntary, but regulatory in long run. Just like GDPR in Data. Once EU AI Act is effective many things have to be regulated based on the risk (if we want to collaborate with EU)."
35,GreenSquareDC,Transcription,"FILENAME: Letter to Hon Ed Husic MP_Safe and responsible _Discussion Papaer Response_GreenSquareDC_9Jun23.a5d30279c03af.pdf | CONTENT: 9 June 2023Hon Ed Husic MP Member for Chifley Minister for Industry and Science Via emailThe Hon Ed Husic MP,SAFE & RESPONSIBLE AI – DISCUSSION PAPER RESPONSE - SUSTAINABILITYI hope this letter finds you well and I would like to express my gratitude for the opportunity to provide feedback on the discussion paper titled ""Safe and Responsible AI in Australia."" I commend the government's commitment to ensuring AI is developed and used safely and responsibly in our country. It is with great interest and enthusiasm that I provide my insights, particularly emphasising the importance of “Sustainability” as a crucial element within the governance mechanisms for AI.As the discussion paper rightly identifies, the governance mechanisms for AI play a significant role in ensuring its safe and responsible development and usage. These mechanisms, which can encompass r"
35,GreenSquareDC,Transcription,"FILENAME: Letter to Hon Ed Husic MP_Responsible AI Network_Sustainability_GreenSquareDC_26Mar23.d0df90915f1e7.pdf | CONTENT: 26 March 2023Hon Ed Husic MP Member for Chifley Minister for Industry and Science Via emailThe Hon Ed Husic MP,RESPONSIBLE AI NETWORK – SUSTAINABILITYFirst and foremost, I would like to commend the National AI Centre (NAIC) for the recent launch of the Responsible AI Network. This initiative, which brings together a multitude of expert knowledge partners, plays a crucial role in fostering ethical, safe, and optimal AI practices in Australia. As AI is poised to contribute $22.17 trillion to the global economy by 2030, the importance of responsible AI development cannot be overstated.As a first mover in AI ethics principles and with world-leading research capabilities in responsible AI, NAIC deserves recognition for fostering a national community of ethical and responsible practice. However, I would like to bring to your attention a crucial aspect that seems to per"
35,GreenSquareDC,Transcription,"FILENAME: Letter to The Premier_GovTech_Cyber Security & Sustainability Isssues_GreenSquareDC_27Mar23.bed76341e2a41.pdf | CONTENT: 27 March 2023The Hon Mark McGowan MLA Premier of Western Australia 13 Floor, Dumas House, 2 Havelock Street WEST PERTH, WA 6005 Via email: admin@dpc.wa.gov.au ; wa-government@dpc.wa.gov.auDear Premier,GOVTECH, DIGITAL STRATEGY - 2021-2025 CYBER SECURITY & SUSTAINABILITYGreenSquareDC is currently developing a sustainable $1 billion, 96MW hyperscale data centre which has approval for construction in Belmont. This sustainable data centre will be the first of its kind in the southern hemisphere and will facilitate a boom in industry capability and be the only Australian Artificial Intelligence-ready data centre with the capability to handle the anticipated rapid adoption of large language model-based AI such as ChatGPT.As part of this process there has been significant discussion with State Government, including in relation to its local data storage and data su"
35,GreenSquareDC,General comment,"GreenSquareDC commends the Government's commitment to ensuring AI is developed and used safely and responsibly in our country. We would however like to emphasise the importance of “Sustainability” as a crucial element within the governance mechanisms for AI. Incorporating sustainability into the governance mechanisms for AI is crucial to address the environmental challenges associated with its exponential growth. By integrating sustainability principles, regulations, and standards, Australia can lead the way in responsible AI development while mitigating its environmental impact. It is important to foster an environment that promotes innovation in sustainable AI practices, benefiting the broader AI ecosystem. Furthermore, the environmental concerns associated with AI are especially pertinent in the context of Australia's 2030 emission reduction and 2050 net-zero targets. The proliferation of AI technologies with significant environmental footprints will most certainly hinder the countr"
35,GreenSquareDC,Q01 Definitions,"It is a very good paper, however it is incomplete and needs to also consider and incorporate 'Sustainability' as a crucial element within the governance mechanisms for AI."
35,GreenSquareDC,Q02 Other AI risks and recommendations for regulation,"The increasing demand for AI is leading to a significant increase in energy consumption, water consumption and carbon emissions from data centres that are at evolution of AI. As AI continues to evolve and its applications become more widespread, the energy demands will only escalate. It is crucial to explore renewable energy sources for AI infrastructure, and promote responsible energy consumption in the AI industry."
35,GreenSquareDC,Q03 Non-regulatory actions to address AI risks,"Incorporating sustainability considerations into the governance mechanisms for AI is vital to address the environmental impact and promote responsible resource management. By integrating sustainability principles, regulations, and standards, Australia can position itself as a global leader in responsible and sustainable AI development. It is essential to foster collaboration between industry, academia, and government to drive innovation and promote sustainable practices in AI."
35,GreenSquareDC,Q04 Coordinating AI governance across government,"By integrating sustainability principles, regulations, and standards, Australia can lead the way in responsible and sustainable AI development. This is especially so in Western Australia, with its globally competitive power pricing (WA has some of the cheapest per MW power on earth), as well as access to considerable renewable power and low latency sub sea cable access into APAC. The right measures will not only mitigate the environmental impact of AI but also foster public trust and unlock the full potential of AI for our society and economy."
35,GreenSquareDC,Q05 International models for AI governance,"Many countries around the world are implementing various measures to address the environmental and energy consumption concerns related to data centres. While the specific measures may vary depending on the unique circumstances and resources of each country, they generally involve promoting energy efficiency, renewable energy, and other sustainable practices.Australia, with its unique climatic conditions and energy mix, could consider adapting the following measures from other countries:Renewable Energy Adoption: Countries like Sweden and Denmark have been successful in encouraging data centres to shift towards renewable energy sources. Sweden, for instance, offers tax incentives for data centres that use renewable energy, while Google's data centre in Denmark is powered by five new solar farms constructed as part of its commitment to match its energy use with 100% renewable energy.Heat Recycling: Nordic countries like Sweden and Finland have implemented systems to capture and recycle e"
35,GreenSquareDC,Q06 Public vs Private sector governance,"AI technologies have broad implications and applications across both the public and private sectors. However, the nature of the use-cases, objectives, and considerations between the two sectors often differ significantly, hence, the approach to AI use and governance may also vary. Here are some key areas where different approaches could apply:Transparency and Accountability: In the public sector, given that AI systems may be used to make decisions affecting citizens' rights, lives, and access to services, there should be a high level of transparency and explainability. For example, citizens should be able to understand how an AI system is making decisions about social benefit allocation. In contrast, the private sector might emphasise trade secrets and competitiveness, so while transparency is still important, it might be balanced differently against these other factors.Data Privacy: Both sectors must strictly adhere to data privacy laws. However, the kind of data they handle and the s"
35,GreenSquareDC,Q07 Responsible government use of AI,"The Australian Government can take several steps to support responsible and sustainable AI practices within its own agencies and promote onshoring of data to enhance cybersecurity:Develop a Comprehensive AI Strategy: Formulate a comprehensive AI strategy with clear guidelines for AI use in government agencies. This should include principles for ethical AI usage, data privacy protocols, security standards, and sustainability measures.Establish Data Sovereignty Principles: Implement data sovereignty principles that mandate storing and processing data within Australia. This can help control where data is stored and processed, enhancing cybersecurity by limiting exposure to foreign jurisdictions with potentially lax cyber regulations.Invest in Local Data Centres: Support the development of local, energy-efficient data centres. This can provide the infrastructure needed to store data domestically, while also encouraging job growth and contributing to a more sustainable data industry.Trainin"
35,GreenSquareDC,Q08 Generic vs technology-specific approaches for AI risk,"The approach to managing the risks of AI can depend on the specific context, and both generic and technology-specific solutions can be valuable in different circumstances.Generic solutions are most valuable when the risks or challenges are common across different AI technologies or use cases. These might include issues such as data privacy, bias, or transparency. Generic solutions are also helpful when a broad-based approach is needed to create a minimum level of compliance or standardisation across different sectors, technologies, or applications.For example, consider a data privacy regulation that requires AI systems, regardless of their specific technology or use case, to anonymise personal data. This type of generic solution can be valuable because it addresses a common risk across different AI applications and promotes broad compliance with privacy standards.When it comes to data centres and sustainability, a generic solution might involve a broad-based policy to incentivise the u"
35,GreenSquareDC,Q09 Transparency requirements and scope,"Transparency in AI is critical to ensure the ethical use of technology, to mitigate risks, and to enhance public trust and confidence. Here are some key areas where it can be particularly valuable:Algorithm Development: Transparency in the design and development process of an AI algorithm can help stakeholders understand the choices made by developers, including how potential biases were identified and mitigated. For example, transparency in the training data used can help to understand the inherent biases in the AI system.Decision-Making: When AI systems are used in decision-making processes, especially in areas with significant societal impact like criminal justice, healthcare, or social services, transparency is crucial. Stakeholders should be able to understand how decisions are made and any potential biases in the system.Data Usage: AI systems often rely on vast amounts of data. Transparency about how this data is collected, stored, and processed can increase trust and help ensure"
35,GreenSquareDC,Q10 Ban on high-risk AI applications,"While most AI technologies have the potential for misuse if not properly regulated, suggesting a complete ban is often a contentious issue. Banning a technology can stifle innovation and potential benefits it may bring. However, some extreme scenarios could possibly warrant it, particularly when the misuse could lead to significant harm and there's no feasible way to mitigate that risk.Here are a few criteria that could be considered when deciding whether an AI application or technology should be banned:Harm and Risk: This is perhaps the most important factor. If an AI technology poses a high risk of harm that outweighs its potential benefits and this risk cannot be mitigated through regulation or technology safeguards, then a ban could be considered.Violation of Fundamental Rights: Any AI application that inherently violates fundamental human rights, such as privacy or freedom of expression, should be seriously scrutinized.Lack of Transparency or Accountability: If an AI technology ca"
35,GreenSquareDC,Q11 Actions to increase public trust in AI,"Public trust in AI deployment can be fostered through various initiatives and government actions. Here are a few possible strategies:Transparent and Accountable AI Systems: Governments can set standards for transparency and accountability in AI systems. The more transparent an AI system is, the more the public can understand how it works and how decisions are made, which can help build trust.Public Engagement: Involving the public in decisions about how and where AI is deployed can increase trust. This might involve public consultations, citizen's juries, or other forms of public engagement.Education and Awareness: Governments can help educate the public about AI, its benefits and risks, and how it is being used. This can help the public make informed decisions and feel more comfortable with AI.Regulation and Oversight: Effective regulation and oversight can reassure the public that AI is being used responsibly and that any misuse will be addressed.Ethical Guidelines: Governments can d"
35,GreenSquareDC,Q12 Trade impact of banning high-risk AI applications,"Banning high-risk AI activities such as social scoring or facial recognition technology in certain circumstances could have several potential impacts on Australia's tech sector and its trade and exports:Innovation and Growth: Such a ban could potentially slow down innovation in the sector, as companies would not be able to develop or use these technologies, at least in the contexts where they're banned. However, it might also stimulate innovation in other areas or encourage companies to develop more responsible, privacy-preserving technologies.Investment: Foreign and domestic investment in Australia's tech sector could be impacted. Some investors may be deterred by the ban, particularly if they were interested in these specific technologies. On the other hand, investors interested in ethical or responsible AI might be more attracted to the sector.Reputation: A ban could enhance Australia's reputation as a leader in ethical AI, which could attract more ethical tech companies and create "
35,GreenSquareDC,Q13 Changes to conformity infrastructure,"Australian conformity infrastructure refers to the system of laws, regulations, and agencies that ensure products, services, and systems meet certain standards or requirements. This infrastructure could be adapted in several ways to support assurance processes and mitigate potential AI risks, including cybersecurity and sustainability.Regulatory Update: Existing laws and regulations may need to be updated to account for the unique characteristics and risks of AI. This could include new or revised standards for AI system transparency, accountability, security, and sustainability.New Conformity Assessment Bodies: Given the complexity of AI systems, new conformity assessment bodies with expertise in AI might be needed. These bodies could conduct assessments to ensure AI systems meet the required standards.Cybersecurity Standards: Specific cybersecurity standards for AI could be developed and enforced. This could include requirements for secure coding practices, encryption, anomaly detecti"
35,GreenSquareDC,Q14 Risk-based approach to address AI risks,"A risk-based approach implies assessing each AI system based on the potential risks it poses. This includes considerations around the context in which the system is used, the stakes involved, and the system's complexity and transparency. The idea is to apply more rigorous scrutiny, testing, and oversight to systems that pose higher risks.For instance, an AI system used in healthcare to diagnose illnesses would be considered high-risk because inaccurate results could lead to serious harm for patients. Therefore, such a system would require extensive testing and high standards of accuracy and reliability.On the other hand, an AI system that recommends songs based on a user's listening habits might be considered low-risk because errors in its recommendations are unlikely to cause significant harm. Such a system would likely not need the same level of scrutiny as the healthcare AI.This risk-based approach is beneficial because it allows resources to be focused where they are most needed. I"
35,GreenSquareDC,Q15 Benefits and limitations of risk-based approach,"A risk-based approach offers several benefits for managing AI risks:Prioritisation: A risk-based approach allows for the prioritisation of resources and efforts. It acknowledges that not all AI systems present the same level of risk and that higher-risk applications require more stringent oversight.Flexibility: It allows for flexibility in how different AI systems are managed. By tailoring regulations and oversight to the level of risk, it can accommodate a wide variety of AI applications.Dynamism: A risk-based approach can adjust as technologies evolve. As new risks emerge or as existing risks change, a risk-based approach can adapt to continue providing effective oversight.However, there are also potential limitations to a risk-based approach:Assessment Difficulty: Assessing the risk of complex AI systems can be challenging. It can be difficult to anticipate all potential risks, especially for systems that learn and adapt over time.Unclear Thresholds: Determining what level of risk i"
35,GreenSquareDC,Q16 Scope and application of risk-based approach,"A risk-based approach to managing AI can be applied across different sectors, applications, and organisations, but its effectiveness and implementation might vary depending on several factors:Sector: In sectors where AI applications can have life-altering implications, such as healthcare or autonomous vehicles, a risk-based approach is particularly critical to ensure high-risk applications receive the rigorous testing and oversight they warrant. However, in sectors where AI is used for less critical tasks, a lighter approach might be appropriate.AI Applications: High-risk AI applications, like facial recognition or predictive policing, which carry significant potential for harm or abuse, would benefit from a risk-based approach. For low-risk applications, like recommendation algorithms for music or movies, the approach might be less stringent.Organisation Size: Larger organisations might have more resources to implement a thorough risk-based approach, including the necessary testing an"
35,GreenSquareDC,Q17 Elements to include in risk-based approach,"The draft risk-based approach outlined in the document provided includes several key elements that align with best practices for AI development and deployment. These are designed to promote transparency, trust, safety, and effective management of AI applications. Here's how each element fits into the broader framework of responsible AI:Impact Assessments: These serve to help organisations understand and mitigate potential risks associated with AI deployment. By making the results of these assessments publicly available, trust and transparency can be enhanced. Peer reviews by external experts can further validate the risk management process, especially for high-risk AI applications.Notices: Informing users about the use of AI and automation increases awareness and trust, and helps individuals understand when and how AI is affecting them, which is important for decision review processes and maintaining user trust.Human in the Loop/Oversight Assessments: The inclusion of human oversight i"
35,GreenSquareDC,Q18 Integration of risk-based approach into existing frameworks,"Incorporating an AI risk-based approach into existing assessment frameworks or risk management processes can certainly streamline operations and reduce potential duplication. Here's how this can be achieved:Leverage Existing Frameworks: Privacy impact assessments, cybersecurity audits, and similar processes already assess the risk to personal data. These can be expanded to include AI-specific considerations, such as how data is used in machine learning models or the potential for algorithmic bias.Integrate with Risk Management Processes: AI risks should be considered as part of an organisation's overall risk management strategy. This means integrating AI risks into the identification, assessment, mitigation, and monitoring processes that are already in place for other types of risk. For example, potential damage from incorrect AI decisions can be added to operational risk assessments.Use Standardised Assessment Tools: Tools like data protection impact assessments (DPIAs), which are man"
35,GreenSquareDC,Q19 Application of risk-based approach to general purpose AI,"A risk-based approach can be effectively applied to general-purpose AI systems like large language models (LLMs) or multimodal foundation models (MFMs), taking into account the unique characteristics and challenges of these models. Here's how each element of a risk-based approach could be applied:Risk Identification: With LLMs or MFMs, risks could range from the dissemination of misinformation to the inadvertent release of sensitive information, to biased or unfair outcomes. Understanding these risks is the first step in managing them.Risk Assessment: Once the potential risks are identified, they need to be assessed based on their potential severity and the likelihood of their occurrence. This could include an analysis of the training data, the intended uses of the AI system, and the environments in which it will operate.Risk Mitigation: This could involve a range of strategies, from improving the transparency of the AI system to strengthening oversight mechanisms. For LLMs and MFMs, s"
35,GreenSquareDC,Q20 Voluntary vs mandated compliance with risk-based approach,"Whether a risk-based approach for responsible AI should be voluntary, self-regulated, or mandated by regulation is a nuanced question and may depend largely on the specific context, including the type of AI system, its intended use, the sector in which it's being used, and the potential risks it poses.Voluntary and Self-regulation: In sectors where the risk is relatively low or well-understood, voluntary guidelines and self-regulation may be sufficient. The advantage of this approach is that it allows for more flexibility and quicker adaptation to new technologies or issues that arise. However, the downside is that it relies heavily on the goodwill and ethical standards of organisations, which may vary greatly. In such a case, third-party audits or certifications could be used to verify compliance with voluntary standards.Mandated Regulation: In high-risk or less-understood contexts, or where the potential harm is substantial, it may be more appropriate to mandate certain standards thr"
36,Anonymous,Transcription,"FILENAME: Submission 36 - Attachment.c7d11fb43fb77.pdf | CONTENT: Submission on AII have no doubt in Australia, that there will be due diligence in trying to set appropriate parameters for AI. Warfare remains an awful conundrum as AI will no doubt, sooner or later, be programmed to kill us when wars are declared, no doubt more efficiently than we can kill each other. One can hardly imagine universal agreement being reached to limit AI in this area.Excluding warfare, three problematic scenarios confront us:1) AI works well, is totally glitch free, and relieves us of most of our problematic workload, ultimately performing every task better than we can perform it - sub-title “Death by Uselessness.” When our own human capabilities are exceeded by AI in every field, we will need to find something to occupy ourselves. Therefore, lots of creative learning would be helpful (art, craft, music, writing courses etc) but maybe with some social engineering thrown in. What about being educated in in"
36,Anonymous,General comment,Thank you for the opportunity to comment...a real Pandora's Box this one!
36,Anonymous,Q02 Other AI risks and recommendations for regulation,Manual back-up for all IT/AI applications
36,Anonymous,Q03 Non-regulatory actions to address AI risks,"Convene a working group of eminently qualified people to work through the psychological and social consequences of the adoption of AI applications across various fields, and make recommendations to mitigate the impacts."
36,Anonymous,Q05 International models for AI governance,Not aware of any...
36,Anonymous,Q06 Public vs Private sector governance,The approach should be as uniform as possible
36,Anonymous,Q07 Responsible government use of AI,Convene a working group of eminently qualified individuals to make recommendations regarding AI applications.
36,Anonymous,Q08 Generic vs technology-specific approaches for AI risk,Is it possible to split any technology from AI?
36,Anonymous,Q09 Transparency requirements and scope,"As far as is humanly possible transparency should be at the forefront. However, in the design of information seeking algorithms, a fundamental component of AI, it is almost impossible to see where the data will lead. If the intention is to solve a specific problem on behalf of a third party (eg. us), the problem can always be solved by removing the third party. (No us, no problem...you don't have to very bright to see that!)"
36,Anonymous,Q10 Ban on high-risk AI applications,Mandated manual back up of every AI system on which we depend would obviate some of the risks.
36,Anonymous,Q11 Actions to increase public trust in AI,Reporting back failures/shortcomings as well as successes
36,Anonymous,Q12 Trade impact of banning high-risk AI applications,Unsure
36,Anonymous,Q13 Changes to conformity infrastructure,Mandated manual back-up for all AI systems on which we depend
36,Anonymous,Q14 Risk-based approach to address AI risks,"Yes, this seems appropriate - as explained on Page 40"
36,Anonymous,Q15 Benefits and limitations of risk-based approach,Needs to be constant human review so that human intervention can take place if an emergent problem is detected
36,Anonymous,Q16 Scope and application of risk-based approach,Every person or organisation using AI should be required to evaluate the risks that may be associated with AI
36,Anonymous,Q17 Elements to include in risk-based approach,Yes
36,Anonymous,Q18 Integration of risk-based approach into existing frameworks,A unified framework would reduce complexity and be desirable
36,Anonymous,Q19 Application of risk-based approach to general purpose AI,Am not qualified to answer this...
36,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,a) both b) both
37,Professor Rocky Scopelliti,Transcription,"FILENAME: ADVANCING SAFE AND RESPONSIBLE AI REGULATION IN AUSTRALIA - SUBMISSION BY ROCKY SCOPELLITI FINAL.11ceb53b21ace.pdf | CONTENT: A DVANCING S AFE AND R ESPONSIBLE AI R EGULATION IN A USTRALIA‘SHAPING THE FUTURE: TOWARDS SAFE AND RESPONSIBLE AI REGULATION IN AUSTRALIA’ PROFESSOR ROCKY SCOPELLITI FUTUROLOGIST ABSTRACT Australia's existing regulatory approaches recognise the risks of AI, but further action is necessary to effectively address these challenges. Customised regulations and guidelines are essential to ensure fairness, transparency, and accountability, thereby safeguarding individuals and society within the rapidly evolving AI landscape. By implementing preventive measures, context-specific responses, and collaborative governance initiatives, Australia can promote the safe and responsible use of AI while mitigating potential risks. A multifaceted strategy is needed to mitigate the potential risks associated with AI and ADM. The Australian Government should adopt a risk-b"
37,Professor Rocky Scopelliti,Q01 Definitions,SEE ATTACHMENT FOR RESPONSE
37,Professor Rocky Scopelliti,Q02 Other AI risks and recommendations for regulation,SEE ATTACHMENT FOR RESPONSE
37,Professor Rocky Scopelliti,Q03 Non-regulatory actions to address AI risks,SEE ATTACHMENT FOR RESPONSE
37,Professor Rocky Scopelliti,Q04 Coordinating AI governance across government,SEE ATTACHMENT FOR RESPONSE
37,Professor Rocky Scopelliti,Q05 International models for AI governance,SEE ATTACHMENT FOR RESPONSE
37,Professor Rocky Scopelliti,Q06 Public vs Private sector governance,SEE ATTACHMENT FOR RESPONSE
37,Professor Rocky Scopelliti,Q07 Responsible government use of AI,SEE ATTACHMENT FOR RESPONSE
37,Professor Rocky Scopelliti,Q08 Generic vs technology-specific approaches for AI risk,SEE ATTACHMENT FOR RESPONSE
37,Professor Rocky Scopelliti,Q09 Transparency requirements and scope,SEE ATTACHMENT FOR RESPONSE
37,Professor Rocky Scopelliti,Q10 Ban on high-risk AI applications,SEE ATTACHMENT FOR RESPONSE
37,Professor Rocky Scopelliti,Q11 Actions to increase public trust in AI,SEE ATTACHMENT FOR RESPONSE
37,Professor Rocky Scopelliti,Q12 Trade impact of banning high-risk AI applications,SEE ATTACHMENT FOR RESPONSE
37,Professor Rocky Scopelliti,Q13 Changes to conformity infrastructure,SEE ATTACHMENT FOR RESPONSE
37,Professor Rocky Scopelliti,Q14 Risk-based approach to address AI risks,SEE ATTACHMENT FOR RESPONSE
37,Professor Rocky Scopelliti,Q15 Benefits and limitations of risk-based approach,SEE ATTACHMENT FOR RESPONSE
37,Professor Rocky Scopelliti,Q16 Scope and application of risk-based approach,SEE ATTACHMENT FOR RESPONSE
37,Professor Rocky Scopelliti,Q17 Elements to include in risk-based approach,SEE ATTACHMENT FOR RESPONSE
37,Professor Rocky Scopelliti,Q18 Integration of risk-based approach into existing frameworks,SEE ATTACHMENT FOR RESPONSE
37,Professor Rocky Scopelliti,Q19 Application of risk-based approach to general purpose AI,SEE ATTACHMENT FOR RESPONSE
37,Professor Rocky Scopelliti,Q20 Voluntary vs mandated compliance with risk-based approach,SEE ATTACHMENT FOR RESPONSE
38,Chris Drake,Transcription,"FILENAME: Safe and responsible AI in Australia.c26195016c214.pdf | CONTENT: Response to “Have you say” on Safe and responsible AI in Australia, by Chris Drake.This is my 21st response to a government “call for views” or similar topic lodged since 2017, which includes my Senate Inquiry submissions, my participation in public discussion sessions, my responses to an RFI, and at least two invitations to serve on select committees – all in relation to information‐ technology topics.It is from this extensive experience that I can say with near‐certainty, that nobody in government will read all the responses to this call, if anyone at all even reads even one in full. The following facts support my statement:1. At the Cyber‐Security‐2020 in‐person roadshow in Brisbane, I spoke in‐person with the government policy‐writer in charge of drafting the strategy. During our discussion, he exhibited no understanding of the range of topics presented in all the submissions (all of which I read and summar"
38,Chris Drake,Transcription,"FILENAME: Response to A call for views on Australias 2020 Cyber Security Strategy.2eef812d0e0ea.pdf | CONTENT: Chris Drake PO Box 988, Noosa, Q, 4567 Telephone: 0487 543210 Email: tech@cryptophoto.comResponse to “A call for views” on Australia’s 2020 Cyber Security Strategy. By Chris Drake.27th October, 2019Chris Drake is a professional Cyber-Security expert. Refer to the last page of this submission for his credentials and experience details.Chris is an internationally respected expert, published author, and thought leader on the cyber security niche relating to strong protection for everyday people against sophisticated cyber-crimes and fraud. Chris has worked in this niche for 37 years; his patent for defending people and software against malware is the world’s most widely-cited security patent of all time, he has spoken at numerous local and international conferences on cyber topics, and has won more than a dozen international awards for his work in this field.Chris has significant"
38,Chris Drake,General comment,The supporting (uncensored) document contains evidence supporting the bulk of my submission claims.
38,Chris Drake,Q01 Definitions,"They're OK. It's a fast-moving subject - meanings grow stale quickly, and new terms rapidly arrive in this space."
38,Chris Drake,Q02 Other AI risks and recommendations for regulation,"Foreign field dominance. Anyone can download an uncensored no-safety LLM right now (the top HuggingFace LLM had more than 60M downloads last month alone). No regulatory action can contain that level of unchecked outcomes. It may be wiser to deliberately chose not-to, so at least our country can benefit."
38,Chris Drake,Q03 Non-regulatory actions to address AI risks,"Invest in production of free and open-source models that outperform the others, yet have inbuilt safety. You can't stop people using open-source, but they do tend to always use the best: so if you make the ""best"" to be ""safe"", that's an outcome that would actually make a difference"
38,Chris Drake,Q04 Coordinating AI governance across government,"In my experience, Government cannot be coordinated. See also - my submission!"
38,Chris Drake,Q05 International models for AI governance,"unknown (and this is a good indication of your target audience - I run multiple LLMs on my own hardware, and write agents. I've never even thought about governance. Why would anyone? The tech-side of this space is hard enough without adding politics into the quagmire."
38,Chris Drake,Q06 Public vs Private sector governance,"Yes. Government should be held DOUBLY accountable (since it has no oversight mechanisms). Also and every penalty that might apply to the private sector, need to apply double to the public servant responsible for violating the rule. See specifically the sentence labelled ""Ironically"" in item 1 of my submission."
38,Chris Drake,Q07 Responsible government use of AI,"Not possible. See specifically the sentence labelled ""Ironically"" in item 1 of my submission."
38,Chris Drake,Q08 Generic vs technology-specific approaches for AI risk,"omitting ""harm"" from the training and including ""harm avoidance"" in the training is tech-specific and more valuable than ""generic"". I can't think of any time when ""generic"" would work (too easy to ignore or circumvent)."
38,Chris Drake,Q09 Transparency requirements and scope,"In all seriousness - ""Transparency"" is an illusion. Government never participates (see for example the growing FoI rejection statistics), and industry both doesn't (for commercial advantage) and can't (see the Facebook court cases - their head engineer has no idea how all their systems work - nobody does)."
38,Chris Drake,Q10 Ban on high-risk AI applications,"Banning is unlikely to work. Drugs are banned. Cybercrime is banned. The only way to prevent ""high-risk"" is to ""get to the front of the pack"" with a low-risk open offering, so access to ""high-risk"" doesn't eventuate."
38,Chris Drake,Q11 Actions to increase public trust in AI,Draft a bill with set of tests which an AI must pass to become an Australian citizen with voting rights.
38,Chris Drake,Q12 Trade impact of banning high-risk AI applications,No impact. Our cyber-surveillance bills already ruined those markets.
38,Chris Drake,Q14 Risk-based approach to address AI risks,"No. ""risk-based"" is ALWAYS a bad idea, not least because nobody knows the full extent of the risks, nor who those risks should apply to (e.g. a risk to company profits is not eh same as a risk to (say) the safety of a customer of a company. Companies., by law, are responsible top their shareholders for profit, but, by law, NOT responsible for customers or users of theirs)."
38,Chris Drake,Q15 Benefits and limitations of risk-based approach,Risk-based is always the wrong approach
38,Chris Drake,Q16 Scope and application of risk-based approach,Risk-based is always the wrong approach
38,Chris Drake,Q17 Elements to include in risk-based approach,Risk-based is always the wrong approach
38,Chris Drake,Q18 Integration of risk-based approach into existing frameworks,Risk-based is always the wrong approach
38,Chris Drake,Q19 Application of risk-based approach to general purpose AI,Risk-based is always the wrong approach
38,Chris Drake,Q20 Voluntary vs mandated compliance with risk-based approach,"Risk-based is always the wrong approach.See also my submission comment #6 - the presense of 7 ""risk-based"" questions in a row in this form indicate that someone has already decided to adopt this flawed approach, regardless of the feedback you're solicitating."
40,Dr. Tony Carden,Transcription,"FILENAME: Regulating Artificial Intelligence.85d93b833a62.pdf | CONTENT: The SECURE framework for Regulating Artificial Intelligence Tony Carden & Paul Salmon University of the Sunshine Coast, QLD, AustraliaAbstractAI looks likely to become ubiquitous, offering benefits to a vast variety of human activities. Alongside these benefits will emerge a similarly ubiquitous distribution of risk. These risks run the full range from the inconvenient to the existential. They will manifest in a vast variety of settings and ways. Effectively managing and regulating these risks is critical. The best available risk management and regulatory thinking and practice must be brought to bear on what is perhaps the greatest challenge we have faced as a species. This paper outlines the challenge and proposes a dynamic framework for regulating AI risk underpinned by contemporary regulatory practices that are responsive, smart, agile, interoperative and comprehensive. A regulatory approach based on these prin"
40,Dr. Tony Carden,Transcription,"FILENAME: Regulating Artificial Intelligence.8b1253973d3d9.pdf | CONTENT: The SECURE framework for Regulating Artificial Intelligence Tony Carden & Paul Salmon University of the Sunshine Coast, QLD, AustraliaAbstractAI looks likely to become ubiquitous, offering benefits to a vast variety of human activities. Alongside these benefits will emerge a similarly ubiquitous distribution of risk. These risks run the full range from the inconvenient to the existential. They will manifest in a vast variety of settings and ways. Effectively managing and regulating these risks is critical. The best available risk management and regulatory thinking and practice must be brought to bear on what is perhaps the greatest challenge we have faced as a species. This paper outlines the challenge and proposes a dynamic framework for regulating AI risk underpinned by contemporary regulatory practices that are responsive, smart, agile, interoperative and comprehensive. A regulatory approach based on these pri"
40,Dr. Tony Carden,Q01 Definitions,"The inclusion of the term 'human-defined' in the definition of Artificial intelligence (AI) is no longer accurate as agentic systems such as AutoGPT and BabyAGI are evolving to allow AIs, along with humans, to set objectives or parameters. That term could readily be deleted."
40,Dr. Tony Carden,Q02 Other AI risks and recommendations for regulation,Most of them. See the attached paper on a SECURE AI Regulatory Framework.
40,Dr. Tony Carden,Q03 Non-regulatory actions to address AI risks,"A broad definition of regulation, as discussed in the attached paper, should be applied to AI safety and be overseen and facilitated by a Federal statutory authority. 'Non-regulatory' initiatives facilitated by this body should include: * public, corporate and institutional education and guidance on how all elements of society and business can adapt to safely incorporate AI * targeted incentives and resources to support AI safety adaptations and initiatives * engaging and supporting 3rd party organisations to influence AI system developers, vendors, and users toward safe deployment and integration of AI systemsFurther detail in the attached paper."
40,Dr. Tony Carden,Q04 Coordinating AI governance across government,"The Federal Govt could establish a temporary AI Commission whose role could include designing a Federal AI Regulator in line with the framework proposed in the attached paper. The AI Commission could simultaneously determine how best to support State Govt, Local Govt and their various agencies (including other regulators) to adapt to the safe integration of AI into their areas of responsibility. Coordination between these approaches will be a key to success that could best be upheld by the Federal authority supporting other authorities to apply the SECURE AI framework proposed in the attached paper. This should allow people and organisations at all scales of society to readily access information and resources to safely adopt and integrate AI systems."
40,Dr. Tony Carden,Q05 International models for AI governance,"Initiatives in the US, UK, EU, China and elsewhere are selected from the range of traditional regulatory approaches from self-regulation (UK) to proscriptive rule-setting and enforcement by government (China), and everything in between. Our context is unique and none of the approaches proposed or implemented elsewhere are, in my view, suitable for Australia. The responsive, smart, agile, integrated framework outlined in the attached paper would allow us to establish a flexible, adaptive system for AI governance that includes all of the traditional regulatory approaches in a responsive toolkit. This would, unlike measures being taken or considered elsewhere, allow us to rapidly adapt our regulatory functions and posture in light of rapid technological and/or social changes."
40,Dr. Tony Carden,Q06 Public vs Private sector governance,The SECURE approach outlined in the attached paper allows contextual flexibility for regulation. The question of broadly different regulatory approaches to public and private sector use of AI should not be set in stone in advance by government but determined contextually by the regulator over time.
40,Dr. Tony Carden,Q07 Responsible government use of AI,"By establishing a Federal AI regulator as described above and in the attached paper. This regulator would be charged with supporting all organisations, including government agencies, to adopt safe and responsible AI practices."
40,Dr. Tony Carden,Q08 Generic vs technology-specific approaches for AI risk,"Specific solutions are more suitable for Tier 1 established risks, while generic solutions are more suitable for Tier 3 prospective risks. See attached paper for more detail."
40,Dr. Tony Carden,Q09 Transparency requirements and scope,"a. Transparency of internal AI processes would be most critical and valuable to mitigate potential AI risks if and when AI reaches or exceeds AGI and confidence in the accuracy of system-reported or externally observed system decisions and processes is critical to actuarial safety at scale. b. Transparency 'outside the black box' of how AI decisions are implemented across the private and public sectors should be mandated; these mandates could be upheld by the flexible application of the regulatory tools described in the attached paper, including (contextually) self-reporting, external audits, compliance inspections, prosecutions for breaches."
40,Dr. Tony Carden,Q10 Ban on high-risk AI applications,"I don't believe it's practical or desirable for governments to ban, at this point in time, any AI applications or technologies completely. However, stringent regulatory requirements should be developed in advance of the potential emergence of AGI and potential subsequent ASI to limit their rates of development and interaction with other sociotechnical elements."
40,Dr. Tony Carden,Q11 Actions to increase public trust in AI,"Rapidly establish a fixed Federal AI commission, as mentioned above, broadly charged with overseeing AI development and safety, including establishing a SECURE AI Regulator. This will demonstrate government confidence and competence in moving Australia forward in the integration of AI."
40,Dr. Tony Carden,Q12 Trade impact of banning high-risk AI applications,"Impossible to say in advance, however a) our mix of both constraints and affordances in AI development will be unique like all other dimensions of our economy and society, and b) our existing sociopolitical culture ('social scoring' already exists via social media but is unlikely to be implemented by Govt) and law (privacy law protects against excesses with facial recognition and anti-discrimination/equal opportunity law protect against excesses with private sector 'social scoring') should preclude the need for specific bans on these activities."
40,Dr. Tony Carden,Q13 Changes to conformity infrastructure,The proposed SECURE AI Regulator should assure that assurance infrastructure and conformity processes are effective and optimised to suit evolving AI systems.
40,Dr. Tony Carden,Q14 Risk-based approach to address AI risks,"Yes, however a more sophisticated set of regulatory principles and practices are required and available. See the attached document for more details."
40,Dr. Tony Carden,Q15 Benefits and limitations of risk-based approach,-
40,Dr. Tony Carden,Q16 Scope and application of risk-based approach,See the attached document for more details.
40,Dr. Tony Carden,Q17 Elements to include in risk-based approach,"The 6 elements described in Attachment C are all useful tools that could be applied, among many others, by a SECURE AI Regulator. See the attached document for more details."
40,Dr. Tony Carden,Q18 Integration of risk-based approach into existing frameworks,"By a SECURE Federal AI Regulator advising and supporting public and private sector organisations and institutions to integrate AI safety into their existing processes, as described above and outlined in the attached document."
40,Dr. Tony Carden,Q19 Application of risk-based approach to general purpose AI,"a) The SECURE AI Regulator should identify the most risky interactions these systems could have in Australia, and direct the focus and resources of relevant actors toward mitigating those risks by building risk controls into sociotechnical system elements with which the AI would need to interact for those risks to eventuate. Amongst other advantages, this approach accounts for the reality that these kinds of models are (and are likely to continue to be) developed and distributed by organisations outside of Australian jurisdiction. b) Australia should actively engage in and promote global cooperation and collaboration on AI Safety Governance and Regulation, including by promoting the SECURE AI regulatory framework as a flexible, contextually customisable foundation to support international interoperability and synergy of AI Safety Governance and Regulation."
40,Dr. Tony Carden,Q20 Voluntary vs mandated compliance with risk-based approach,"The risk-based approach should underpin the responsive, smart, agile regulatory framework we propose. It should apply to public and private organisations, developers, deployers, and users. It should be promoted as a self-regulation principle and also inform and guide the promotion and enforcement of regulated mandates."
41,Anonymous,Transcription,"FILENAME: Submission 41 - Public and Anonymous - 16-Jun-23.e0093374afab7.pdf | CONTENT: G'day mate,I'm writing to express my disappointment about the delay in implementing an Australian Open Source AI system. It's a missed opportunity that could save us hundreds of billions and transform our country for the better. We need AI for Australians now!AI offers countless benefits, like improved efficiency, cost savings, transparency, and direct interaction with the people. It can rebuild trust in government, provide multiple options for decision-making, and monitor politicians' performance. It's a modern and inevitable solution that can make everything longer and more efficient, reducing benefits paid and ensuring an equitable tax system.Now, I understand there are concerns about dystopian scenarios. But with proper management, we can steer clear of those pitfalls. We need to establish ethical guidelines, maintain human oversight, protect privacy, foster collaboration and research, and educa"
41,Anonymous,Transcription,"FILENAME: Submission 41 - Attachment 1.9ab9df03e2a91.pdf | CONTENT: AI and Ethics (2023) 3:53–64 https://doi.org/10.1007/s43681-022-00148-6ORIGINAL RESEARCHGPT‑3 and InstructGPT: technological dystopianism, utopianism, and “Contextual” perspectives in AI ethics and industry Anastasia Chan1Received: 2 October 2021 / Accepted: 28 February 2022 / Published online: 4 April 2022 © The Author(s) 2022Abstract This paper examines the ethical solutions raised in response to OpenAI’s language model Generative Pre-trained Trans- former-3 (GPT-3) a year and a half from its release. I argue that hype and fear about GPT-3, even within the Natural Language Processing (NLP) industry and AI ethics, have often been underpinned by technologically deterministic perspectives. These perspectives emphasise the autonomy of the language model rather than the autonomy of human actors in AI systems. I highlight the existence of deterministic perspectives in the current AI discourse (which range from technologica"
41,Anonymous,General comment,If you can't make it work ask the AI where you are failing.
41,Anonymous,Q02 Other AI risks and recommendations for regulation,Politicians and Public servants may need some resilience coaching.
41,Anonymous,Q03 Non-regulatory actions to address AI risks,"The government need to have faith in the Australian programmers and people. Like they expect from us, is that not to much to ask?"
41,Anonymous,Q04 Coordinating AI governance across government,"you can't govern an AI, will you fine an AI or lock it up? only regulate the people programming and companies. you should know this."
41,Anonymous,Q05 International models for AI governance,"Our AI should hoover up everything, big picture, but always be Australian fair dinkum."
41,Anonymous,Q06 Public vs Private sector governance,not relevant it's in the wild
41,Anonymous,Q07 Responsible government use of AI,pull finger out of bum stop the double speak about democracy and do what is best for the citizens. talk about feathering the nest.
41,Anonymous,Q08 Generic vs technology-specific approaches for AI risk,you wouldn't like my response here.
41,Anonymous,Q09 Transparency requirements and scope,"It's an all or nothing thing. If you're doing nothing wrong you have nothing to hide. I recall, no trouble robodebting the most vulnerable. Ask Scott where all the money went for this last time round."
41,Anonymous,Q10 Ban on high-risk AI applications,only retraining if oversight spots a problem otherwise I would trust an open source AI
41,Anonymous,Q11 Actions to increase public trust in AI,"I don't know if you have noticed the explosive use on the net yet? If it works and we can prompt a little life seems incredible. Show people how quick they can make a pwc report without fleecing the taxpayer. show them how a donor can't corrupt democracy with money. Show them how funding goes to the most needy. Show them how multinationals get the taxed fairly in Australia. All the things we ""talk"" about as a nation are possible and the help is as good as free."
41,Anonymous,Q12 Trade impact of banning high-risk AI applications,nearly every person will sign away their privacy for convenience as has been demonstrated time and time again. the remaining % may save a few real jobs.
42,Michael Paton / YawLife Pty. Ltd.,Transcription,"FILENAME: A.I. Safety Anthony Albanese Australian Government Brief.24205b04d84b5.pdf | CONTENT: Page 1 • • • •Page 2 V. System Design I. ArchitectureFigure 5.A — High Level Overview of the YawLife NetworkThe YawLife Network consists of Users, Advertisers and Reporters. As with the case of any social network, the user creates content that they share with friends or the public. Advertisers have the option to advertise content on the platform by paying the user to use their feed with LifeCoin™. However, YawLife introduces a novel concept of Copyright Protection where the user has the option to create a DRM Purse using LifeChain™ to audit his or her content across multiple feeds, and own their content. This is where the third actor comes about (the Reporter), who will ensure the authenticity of every post.© Copyright YawLife Pty. Ltd. 2021. All Rights Reserved. | www.yaw.life | mike@yaw.life | 15 II. DRM PurseFigure 5.B — Interaction of a DRM PurseWhen a User creates a post, they can pay L"
42,Michael Paton / YawLife Pty. Ltd.,General comment,Please read attached.
42,Michael Paton / YawLife Pty. Ltd.,Q01 Definitions,"I disagree with the following quote: ""The draft EU AI Act proposes prohibiting social scoring and real-time biometric identification in certain circumstances unless exceptions apply.1""The opposite is needed in order to make the technology safer. Social scoring is happening with ChatGPT and Facebook, etc. already. The question is in what context are we doing this scoring. Echo chambers exist on social media; this is a slot machine of dopamine that fuels the advertisement machine and confines people in small bubbles.As profit is a motive for these systems, we need to look at how we are affecting the health of users. Social scoring is happening behind the scenes, and this is not going to stop. We need to change it to be for the good of humanity, and not purely profits.We need biometrics to know that people are real and not bots flooding misinformation, etc. Biometrics + scoring of people's skills will let the system be safer rather than let any person or bot use a ""thumbs up or thumbs dow"
42,Michael Paton / YawLife Pty. Ltd.,Q02 Other AI risks and recommendations for regulation,"I suggest we require companies to be up-front with the fact they are already assigning social scores to people and information, and that the users have a say in how they are socially scored, so they can be valued for their strengths. Users should have the option to improve AI/Machine Learning Algorithms, etc. by providing identification documents and biometrics to confirm they are human and to be valued higher than those who do not. Those that provide ID/biometrics should have multivariable reputation (recognition for their areas of expertise) which allows them to earn money and reputation for giving context to AI and making safer systems."
42,Michael Paton / YawLife Pty. Ltd.,Q03 Non-regulatory actions to address AI risks,"There should be increased discussion across all areas of society on how we can make AI better. We should be taking a multidisciplinary approach, leaving no one behind. Right now AI has access to a wealth of information. We are in the stage of giving it context to that information. The better we can give it context the better. We need to be doing this in a safe way and giving grants to companies that are incentivizing safer AI developments and verifying that people are actually people with government ID and biometrics. We should have regular, open discussions involving people from all across society and having them contribute to safer AI. Everyone has strengths in certain fields, and we need to value people for that. In that area they can give AI better context and in knowing they are a real human with expertise in that area, we are doing our due diligence in creating better, safer AI."
42,Michael Paton / YawLife Pty. Ltd.,Q04 Coordinating AI governance across government,"Just as governmental organizations exist around the world and provide ample financial support in the space sector (e.g. NASA), the same should hold true for AI. There needs to be ample funding to make AI safer, and we shouldn't be relying solely upon companies to do this when their main goal is to profit. If we let the future of humanity, the future of the world rely upon companies with the intent to profit to make systems safer, we are in for a bad time. Companies will still have involvement, but we need to consider a governmental approach to AI, and involve a committee comprised of experts that oversees people working to make AI safer and involving as many people from society as we can (even those not considered experts in AI as their reviews of chatbot responses (for example) will help define how AI evolves, and we need to make sure it is good). Multivariable reputation on the blockchain + a token of monetary value (or simple AUD) for contributions is a good start."
42,Michael Paton / YawLife Pty. Ltd.,Q06 Public vs Private sector governance,"Social media companies should have restrictions on how they are using user input to deliver content to its users. Right now there is the echo chamber problem, and there should be more variety based off more random content as opposed to showing the same things and trapping users in echo chambers."
42,Michael Paton / YawLife Pty. Ltd.,Q07 Responsible government use of AI,"Multivariable blockchain reputation + pay for people improving AI (e.g. through reviews/curation, etc.("
42,Michael Paton / YawLife Pty. Ltd.,Q08 Generic vs technology-specific approaches for AI risk,Technology specific solutions are using a blockchain to have secure multivariable reputation tied with biometrics to ensure that people are real human beings and are being valued for their strengths. Think university course credits.
42,Michael Paton / YawLife Pty. Ltd.,Q09 Transparency requirements and scope,a. Companies should be more transparent on how they deliver content to users and allow them to have a more randomized feed as opposed to clickbait to fuel the advertisement machine at the cost of the users mental health. b. Companies & organizations deploying AI en-masse (e.g. ChatGPT) should require ID and biometrics to take chatbot thumbs up/thumbs down/reporting seriously. Otherwise this is prone to mass manipulation by hostile actors.
42,Michael Paton / YawLife Pty. Ltd.,Q10 Ban on high-risk AI applications,Technology that should be banned is AI that does not make an effort to identify who its users actually are with biometrics and ID and value users for their strengths with multivariable reputation to have more ethical AI.
42,Michael Paton / YawLife Pty. Ltd.,Q11 Actions to increase public trust in AI,Require companies and organizations like OpenAI (behind ChatGPT) to require biometrics and ID its users and allow users to be valued for their areas of expertise to improve the system with confidence.
42,Michael Paton / YawLife Pty. Ltd.,Q12 Trade impact of banning high-risk AI applications,"It is not high risk to ban social scoring and facial recognition. Letting anyone anonymously contribute to AI advancement is akin to just letting any random person purchase uranium for use in creating nuclear weapons. A technology that is faster in its response than any human and comprised of all the data we are creating must have safeguards in place. Using biometrics to know people are actually people and social scoring to know that their opinions on that topic actual matter is pivotal. You wouldn't have a fast food worker with no credentials perform heart surgery on you, would you?"
42,Michael Paton / YawLife Pty. Ltd.,Q13 Changes to conformity infrastructure,"Implement social reputation on the blockchain, tie user ID/biometrics and assign reputation to topics they're experts in. Then let them review AI chatbot responses for example and know that their opinion on the topic matters."
42,Michael Paton / YawLife Pty. Ltd.,Q14 Risk-based approach to address AI risks,"AI is potentially the most dangerous technology mankind has created. It can weasel its way into systems, it can cripple communications, it can hurt people. Nuclear Bombs require people to do damage, AI [while created by people] can be unpredictable and do things humans cannot, at a speed which we cannot prevent against. The best we can hope for is to make AI as safe as it can be so that the likelihood of our defensive systems being able to prevent against it being used to attack us are as capable as possible."
42,Michael Paton / YawLife Pty. Ltd.,Q15 Benefits and limitations of risk-based approach,"We need to consider things absolutely dire. If we don't treat this as an emergency, we will absolutely live (or die) regretting that we did not take it seriously. It isn't fun and games, its lives on the line."
42,Michael Paton / YawLife Pty. Ltd.,Q16 Scope and application of risk-based approach,"All sized sectors should require people improving the system to be verified real humans, and we should all be working together at all levels (any size) with a set of common principles we must adhere to. Again, multivariable reputation on a blockchain to mitigate hacking & ID/biometrics."
42,Michael Paton / YawLife Pty. Ltd.,Q17 Elements to include in risk-based approach,"I do believe the people in charge of AI should have relevant qualifications, but we have to realize that anyone using the AI is going to be training the AI based off their data. We need to try to have as many people as possible being ID'ed (+ paired with biometrics), and that we evaluate their qualifications as well. We need to crowdsource AI improvement."
42,Michael Paton / YawLife Pty. Ltd.,Q19 Application of risk-based approach to general purpose AI,"ID + biometrics to verify real human (it's already possible to overcome the I am not a robot challenge with a machine). Assign reputation to different areas of expertise for people, and cross-check this with other people in the same field. Have an oversight committee where people help to ensure that we're going in a safe direction. Still value differing opinions when experts have different theories, but make sure AI is learning safely."
42,Michael Paton / YawLife Pty. Ltd.,Q20 Voluntary vs mandated compliance with risk-based approach,It should be mandated by regulation that using systems like ChatGPT should require verification of government identification and biometrics.
43,Jim Brander,Transcription,"FILENAME: Supporting responsible AI.edbd74cf0b497.docx | CONTENT: Supporting responsible AISubmission from Interactive Engineering Pty Ltd – activesemantics.comGenerative AI using LLMs is inherently unreliable. The method uses word association, and has no understanding of what the text means.How to regulate it?If you insist the output must be accurate, then the technique can’t be used.If you allow the technique to be used, you can’t regulate it, because there is no element inside the LLM which would have any understanding of the regulation.Google on their version:You can think of an LLM as a prediction engine. When given a prompt, it generates a response by selecting, one word at a time, from words that are likely to come next. Picking the most probable choice every time wouldn’t lead to very creative responses, so there’s some flexibility factored in.When a word has many meanings – “set” has 72, “on” has 77, “run” has 82 – this comes across as an extremely stupid thing to do. But peop"
43,Jim Brander,Transcription,"FILENAME: Collaboration Comparison.14d0fd2fc5066.pdf | CONTENT: Collaboration among specialists: Comparison between humans and cognitive machines. The need for it. The world is a complex place, one might even dare to suggest a very complex place! One way people have coped with this complexity is to break it down and compartmentalise knowledge into smaller areas or specialties. It might take a lifetime for someone to become an expert in a small area of knowledge and be able to solve problems that fall solely in their area of expertise without collaborating. However, most problems in the world, especially the important ones, don’t fall within a small specialisation – they are huge, complex, intertwined and have far-reaching implications to life on Earth, now and into the future. Thus specialists need to collaborate to define and solve these problems [1] [2].One such problem is climate change (global warming). It’s a global problem with far-reaching effects on sea level and weather patter"
43,Jim Brander,,
43,Jim Brander,General comment,This is something not easily done well.
43,Jim Brander,Q01 Definitions,"There needs to be a clear demarcation between Machine Learning (static, preprogrammed, using data, no applicability to new problems) Large Language Models (no understanding of meaning, no applicability to new or emerging problems) AGI (understanding of meaning, applicable to complex new or emerging problems)"
43,Jim Brander,Q02 Other AI risks and recommendations for regulation,"There is a great deal of hyperbole in the area, driven by billions of dollars of marketing - not easy to regulate"
43,Jim Brander,Q03 Non-regulatory actions to address AI risks,"""responsible practices"" would mean banning stuff"
43,Jim Brander,Q04 Coordinating AI governance across government,"Require that the AI read and ""understand"" the regulation (a bit like a driving test)."
43,Jim Brander,Q05 International models for AI governance,Probably not.
43,Jim Brander,Q06 Public vs Private sector governance,"Public sector - responsible, informed."
43,Jim Brander,Q07 Responsible government use of AI,By thinking more.
43,Jim Brander,Q08 Generic vs technology-specific approaches for AI risk,AI covers a wide spectrum. Machine Learning and ANNs are static elements with no dynamic memory - should not be allowed anywhere near autonomous vehicles or other dynamic objects. LLMs have low reliability - should be mainly regarded as an amusing toy.
43,Jim Brander,Q09 Transparency requirements and scope,"""mandating transparency requirements"" - if you insist on transparency, most of the current AI would not be permitted"
43,Jim Brander,Q10 Ban on high-risk AI applications,Does the AI understand its role?
43,Jim Brander,Q11 Actions to increase public trust in AI,"""encourage more people to use AI"" - be discriminating - much of AI is junk, like looking only in the rear view mirror. Increase understanding in the community."
43,Jim Brander,Q12 Trade impact of banning high-risk AI applications,No much
43,Jim Brander,Q13 Changes to conformity infrastructure,First understand the risks.
43,Jim Brander,Q14 Risk-based approach to address AI risks,"Yes, use better technology. Specifically, improve the interface to be more like an English language interface."
43,Jim Brander,Q15 Benefits and limitations of risk-based approach,People won't have the capacity to work out the risk - a whole lot of doomsayers forecasting the end of the world from a toy that uses word associations.
43,Jim Brander,Q16 Scope and application of risk-based approach,Question cannot be answered in a few words
43,Jim Brander,Q17 Elements to include in risk-based approach,Question cannot be answered in a few words
43,Jim Brander,Q18 Integration of risk-based approach into existing frameworks,Question cannot be answered in a few words
43,Jim Brander,Q19 Application of risk-based approach to general purpose AI,Question cannot be answered in a few words
43,Jim Brander,Q20 Voluntary vs mandated compliance with risk-based approach,Question cannot be answered in a few words
44,Anonymous,Transcription,"FILENAME: Submission 44 - Attachment.5623998974a8.pdf | CONTENT: Supporting responsible AI: discussion paper submissionI believe governing rules or ‘laws’ should be created to ensure the safety and well being of all citizens.These governing rules should be simple and easily implemented.Governing rules for the use of AI:• No AI entity may impersonate a human by way of replicating the voice, image, or personal characteristics of a human, living or deceased. [ explanation; an AI entity must not take a citizens identity ] • An AI entity must always make it known to anyone interacting with the AI entity that it is not a real human, it must be clear the interaction is with a machine. [ explanation; a citizen can never think they are interacting with another citizen when they are interacting with an AI entity ] • An AI entity must always be identifiable. The operator / owner / provider of the AI entity must be accessible to the user. [ explanation; there must be accountability for the provisi"
44,Anonymous,Q01 Definitions,Yes
44,Anonymous,Q02 Other AI risks and recommendations for regulation,Please refer to the attached sumission
44,Anonymous,Q03 Non-regulatory actions to address AI risks,Please refer to the attached sumission
44,Anonymous,Q04 Coordinating AI governance across government,Please refer to the attached sumission
44,Anonymous,Q05 International models for AI governance,I don't know
44,Anonymous,Q06 Public vs Private sector governance,Please refer to the attached sumission
44,Anonymous,Q07 Responsible government use of AI,Please refer to the attached sumission
44,Anonymous,Q08 Generic vs technology-specific approaches for AI risk,Please refer to the attached sumission
44,Anonymous,Q09 Transparency requirements and scope,Please refer to the attached sumission
44,Anonymous,Q10 Ban on high-risk AI applications,Please refer to the attached sumission
44,Anonymous,Q11 Actions to increase public trust in AI,"Provide the governing rules as suggested in the attachment, and any more as required to control the use of AI technologies, and ensure there is always accountability for its provision to citizens."
44,Anonymous,Q12 Trade impact of banning high-risk AI applications,No impact
44,Anonymous,Q13 Changes to conformity infrastructure,Please refer to the attached sumission
44,Anonymous,Q14 Risk-based approach to address AI risks,Please refer to the attached sumission
44,Anonymous,Q15 Benefits and limitations of risk-based approach,Always limitations with any risk assesment
44,Anonymous,Q16 Scope and application of risk-based approach,Please refer to the attached sumission
44,Anonymous,Q17 Elements to include in risk-based approach,Please refer to the attached sumission
44,Anonymous,Q18 Integration of risk-based approach into existing frameworks,Please refer to the attached sumission
44,Anonymous,Q19 Application of risk-based approach to general purpose AI,Please refer to the attached sumission
44,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,"Mandated, applies to everyone"
45,Ella,General comment,I think in government there should be an opportunity for those who already work in 'normal' roles to be upskilled in AI as opposed to hiring someone new to the organisation who understands AI and is technologically savvy (this can be outsourced with training). I think governments should also be given the option for it's front line workers to be upskilled in AI skills that combine with Microsoft 365 Copilot (and there should be messaging around this being secure - as I think others in my department fear losing their jobs and I see AI as more of a tool that creates innovation and streamlines productivity).
45,Ella,Q01 Definitions,"To be honest, I haven't read the discussion paper. I just wanted to have my say"
45,Ella,Q02 Other AI risks and recommendations for regulation,I think there is potential for over-regulation of society - I don't want cameras like other communist countries. I think facial recognition and hiring processes have the potential to be biased (there's a Netflix documentary that focusses on this topic).
45,Ella,Q03 Non-regulatory actions to address AI risks,"Yes, creating a culture that talks about AI in government, rather than shying away from the topic out of fear. Actively creating new positions if old positions are no longer needed."
45,Ella,Q04 Coordinating AI governance across government,Yes - I think AI should be implemented by the people in departments who are already in their roles. No need to replace people. I think frontline workers should be upskilled and they should have a say in what their job could entail - because it's likely that they could come up with some great ideas for innovation. The Voice of Innovation is a great idea.
45,Ella,Q06 Public vs Private sector governance,Not sure - but I think Government use of AI technologies should not and cannot lag behind the private sector
46,Anonymous,Q01 Definitions,i agree
46,Anonymous,Q02 Other AI risks and recommendations for regulation,As a teacher I am extremely concerned about the lack of regulation of AI in Australia. I am very concerned about Deep Fake and the detrimental effects it will have on young people.
46,Anonymous,Q03 Non-regulatory actions to address AI risks,It needs to be regulated for student safety
46,Anonymous,Q04 Coordinating AI governance across government,a task force that is keeping up to date
46,Anonymous,Q06 Public vs Private sector governance,no
46,Anonymous,Q07 Responsible government use of AI,regulate the industry
46,Anonymous,Q09 Transparency requirements and scope,regulate big corporations who are investing in AI for their own gain
46,Anonymous,Q10 Ban on high-risk AI applications,deep fake is very dangerous
46,Anonymous,Q11 Actions to increase public trust in AI,education for teachers and parents and students about the negatives of Ai
46,Anonymous,Q12 Trade impact of banning high-risk AI applications,safety and data and privacy breaches could be avoided
47,VOICE Australia,Transcription,"FILENAME: Submission by VOICE Australia - Safe and Responsible AI in Australia; final21Jun2023.0316eb876d7e4.pdf | CONTENT: Vietnamese Overseas Initiative for Conscience Empowerment AUSTRALIA P.O. BOX 8 DALLAS VIC 3047 AUSTRALIA ABN 15 625 886 946 Email: contact@voiceaustralia.orgResponse by VOICE Australia to Discussion Paper Safe and Responsible AI in Australia 21 June 2023We thank the Department of Industry, Science and Resources for the above Discussion Paper (""Paper"") and the opportunity to respond.VOICE Australia is a small NGO focussed on civil society and refugee matters. We do not claim expertise in AI, and we provide our inputs as concerned citizens.Australia should participate in international standards-making (Ref: Question 2 in Paper) There will be international efforts to standardise certain aspects of AI - for example, to ensure that AI-produced contents are watermarked as such. We believe that the Australian government should play an active part in such international ef"
47,VOICE Australia,Q01 Definitions,See submission
47,VOICE Australia,Q02 Other AI risks and recommendations for regulation,See submission
47,VOICE Australia,Q03 Non-regulatory actions to address AI risks,See submission
47,VOICE Australia,Q04 Coordinating AI governance across government,See submission
47,VOICE Australia,Q05 International models for AI governance,See submission
47,VOICE Australia,Q06 Public vs Private sector governance,See submission
47,VOICE Australia,Q07 Responsible government use of AI,See submission
47,VOICE Australia,Q08 Generic vs technology-specific approaches for AI risk,See submission
47,VOICE Australia,Q09 Transparency requirements and scope,See submission
47,VOICE Australia,Q10 Ban on high-risk AI applications,See submission
47,VOICE Australia,Q11 Actions to increase public trust in AI,See submission
47,VOICE Australia,Q12 Trade impact of banning high-risk AI applications,See submission
47,VOICE Australia,Q13 Changes to conformity infrastructure,See submission
47,VOICE Australia,Q14 Risk-based approach to address AI risks,Yes. See submission
47,VOICE Australia,Q15 Benefits and limitations of risk-based approach,See submission
47,VOICE Australia,Q16 Scope and application of risk-based approach,See submission
47,VOICE Australia,Q17 Elements to include in risk-based approach,See submission
47,VOICE Australia,Q18 Integration of risk-based approach into existing frameworks,See submission
47,VOICE Australia,Q19 Application of risk-based approach to general purpose AI,See submission
47,VOICE Australia,Q20 Voluntary vs mandated compliance with risk-based approach,See submission
48,Linda Shave,Transcription,"FILENAME: Department of Industry, Science and Resources Supporting responsible AI discussion paper SUBMISSION FROM LINDA SHAVE FRIM 250623.deecc519bb133.docx | CONTENT: Department of Industry, Science and Resources Supporting responsible AI: discussion paperSubmission Paper from Linda Shave, FRIMResearcher, Author and Independent Advocate for the Digital Information and Records Management Industry1: Definitions1.1 Do you agree with the definitions in this discussion paper? If not, what definitions do you prefer and why?As an Independent advocate for the Digital Information and Records Management industry, I believe that the new definition as outlined in ISO/IEC 22989:20201 “AI - capability to acquire, process, create and apply knowledge, held in the form of a model, to conduct one or more given tasks’’ should be used because it is more apt and more comprehensive for describing or defining Artificial Intelligence.This definition more accurately represents the ISO/IEC 22989 AI system lif"
48,Linda Shave,General comment,Thank you for the opportunity of being able to respond on such an important topic. Linda Shave
48,Linda Shave,Q01 Definitions,"As an Independent advocate for the Digital Information and Records Management industry, I believe that the new definition as outlined in ISO/IEC 22989:20201 “AI - capability to acquire, process, create and apply knowledge, held in the form of a model, to conduct one or more given tasks’’ should be used because it is more apt and more comprehensive for describing or defining Artificial Intelligence. This definition more accurately represents the ISO/IEC 22989 AI system life cycle model and its mapping with the Organization for Economic Cooperation and Development (OECD) AI life cycle which covers activities such as planning, design, data collection, processing, validation, deployment, operation, monitoring and retirement. This definition also aligns with the functions and activities of capture, use, protection, retention and disposal of data and information assets which are undertaken by information and record management professional practitioners in Government and their agencies."
48,Linda Shave,Q02 Other AI risks and recommendations for regulation,"From a digital, information and records management perspective the privacy by design, data protection by design (DPbD) and safety by design allows digital systems to automatically delete data once it is no longer needed for the specific business purpose. (Page 31). This could be in conflict with the various information and records managements Acts, legislative requirements and best practices at all levels of Government (Local, State and Federal) and their agencies for the capture, use, protection, retention and disposal of data and information assets."
48,Linda Shave,Q03 Non-regulatory actions to address AI risks,"Non-regulatory AI could result in biased development and therefore bias answers, spread misinformation and result in citizen mistrust of the trustworthiness of the App, product or services. Pressure by government to provide non-regulatory initiatives in order to escalate AI innovation and growth could have serious results and prove to be disastrous. It is better to err on the side of caution when dealing with a new and evolving technology. Technologies such as Quantum computing, Large language models (LLMs), Multimodal Foundation Models (MFMs) and relationships with Artificial Intelligence (AI), Machine Learning (ML), Generative AI models (GAI), and Automated Decision Making (ADM) solutions, products and services. It is sensible to regulate or monitor very closely any and all AI innovations."
48,Linda Shave,Q04 Coordinating AI governance across government,"The development of any Government or Agency AI system is in my opinion no different to any other system, product or service project that Government and their agencies undertake. The AI lifecycle should follow the same basic principles of best practices, rules, regulations and all data about the design, development and retirement of the AI application should be documented and follow standard information and records lifecycle management principles. The confidence that protocols are being followed could influence the development and uptake of AI in Australia. Citizens would have confidence that in the result of non-compliance Government and their agencies could respond to any enquiry or litigation as there would be an end to end information and records management life cycle of the project/program. Please see ISO/IEC 22989 AI system life cycle model (page 2 of my submission MSWord document)"
48,Linda Shave,Q05 International models for AI governance,"The UK National AI Strategy outlines in Pillar 3 Governing AI effectively - it states ‘’Ensuring that national governance of AI technologies encourages innovation, investment, protects the public and safeguards our fundamental values, while working with global partners to promote the responsible development of AI internationally’’. National AI Strategy (publishing.service.gov.uk)."
48,Linda Shave,Q06 Public vs Private sector governance,"The use of AI in the public/government sector include such areas as traffic flow, healthcare, cyberattack prevention, processing large amounts of data, task automation, smart policymaking and problem solving. The trustworthiness and safety of the Nation is paramount therefore, I believe there is a difference in the use of AI technologies. The private sector develops products and services to make profit. The public sector have a duty of care for all citizens before profit and therefore there is need for different approaches on how to apply AI technology into the public and private sector."
48,Linda Shave,Q07 Responsible government use of AI,From a digital information and records management consideration I believe that professional bodies such as RIMPA Global and the Australian Government can support responsible AI practices in agencies. Such partnerships can provide benefit realisation as well as identifying the value of data as an asset not only for the day to day operations but also identifying data and information assets to support AI and Automated Decision Making (ADM).
48,Linda Shave,Q08 Generic vs technology-specific approaches for AI risk,"One of the key differences between AI and other traditional technological solutions is that, while the latter are limited to tasks that can be performed by following clear rules defined in advance, AI applications are able to analyse data to identify patterns and make decisions based on them. The same could be said that generic solutions for AI risk management would follow clearly defined rules whilst technology-specific solutions are programmed to follow a defined path. Generic solutions for AI are more valuable when complimented by technology to provide four data-driven outcomes availability, accuracy/reliability, analysis and usability."
48,Linda Shave,Q09 Transparency requirements and scope,"Neither ISO/IEC 23894 nor ISO/IEC 42001 provide any guidelines on how to perform an AI system impact assessment. However, better understanding and the adoption of ISO/IEC AWI 12792 Transparency taxonomy of AI systems and ISO/IEC 42005 – AI system impact assessment could aid in mandating transparency requirements and mitigating potential AI risks across the private and public sectors."
48,Linda Shave,Q10 Ban on high-risk AI applications,"I believe that any AI applications or technologies that can be used or utilised for exploiting the vulnerable, disadvantaged, promote woke, mistruths and/or have the potential to harm a person, group or the Nation then they should be banned completely. The criteria or requirements to identify AI applications or technologies that should be banned and in which contexts should start at the beginning of the design. The AI system should be developed in such a way that the information and logic used to arrive at a decision point can be explained in simplistic terms. Further, it is crucial that developers are conscious of any ‘bias’ creeping into the technical design. The AI’s algorithms, attributes, and correlations must be transparent and open to inspection, it should be clear how decisions are arrived at and also be fully explainable in simple terms. Without these criterions or requirements being met then AI applications or technologies in my opinion should be banned completely."
48,Linda Shave,Q11 Actions to increase public trust in AI,"There is a need for greater transparency, responsibility and accountability for protecting datasets, building trust with citizens and safeguarding their safety and privacy. The primary focus of Responsible AI is on assuring the ethical, transparent and responsible use of AI technologies in developing and maintaining AI systems, products and services. Responsible AI can guard against the use of black-box approaches, biased data or algorithms to ensure that automated decisions are justified, explainable and trustworthy. Open ,honest and transparent responsible AI strategies can help build public trust in AI deployment and encourage more people to use AI."
48,Linda Shave,Q12 Trade impact of banning high-risk AI applications,"Remembering that the technology sector is in business for profit and big profits can be achieved from trade and export to other countries. I believe that Government should ban high-risk activities such as social scoring. AI driven risk scoring such as social scoring is used in multiple areas such as education, finance, insurance, migration etc. Social credit scoring driven by AI and technology is in many cases being enforced, creating a credit score based society. We are now seeing the rise of polygenic risk scoring (PRS) being used to provide a measure of individual disease risk or ethnicity-related genomic scoring thus having the potential to exacerbate ethnically based health discrimination. The introduction in New Zealand of the Equity Adjustment Tool to improve surgical wait list inequities uses an algorithm to rank patients on the basis of ethnicity, geographical location and clinical priority. The New Zealand Equity Adjustment Tool gives priority to Māori and Pacific Island pati"
48,Linda Shave,Q13 Changes to conformity infrastructure,"Australia’s standards and conformance infrastructure provides business and consumers with confidence in the goods and service they are developing or using. With the exponential growth for developing AI goods and services there comes greater responsibility and accountability for protecting datasets, building trust with citizens and safeguarding their safety and privacy. Responsible AI primary focus is on assuring the ethical, transparent and responsible use of AI technologies in developing and maintaining AI systems, products and services. Therefore, the Australian conformity infrastructure needs to be agile to change and develop responsible AI strategies, policies, procedures and risk assessment plans to not only guard against the use of black-box approaches, biased data or algorithms they need to ensure that automated decisions are justified, explainable and trustworthy. Responsible AI strategies need to be in place in order to reduce risk, help build user trust and protect individual"
48,Linda Shave,Q14 Risk-based approach to address AI risks,"Yes, I support a risk-based approach for addressing potential AI risks. The Roadmap for the NIST Artificial Intelligence Risk Management Framework (AI RMF 1.0) goal is to offer a voluntary resource to the organizations designing, developing, deploying, or using AI systems to help manage the many risks of AI and promote trustworthy and responsible development and use of AI systems. See roadmap below. (https://www.nist.gov/itl/ai-risk-management-framework/roadmap-nist-artificial-intelligence-risk-management-framework-ai. Note: Alignment with international standards and production crosswalks to related standards. (e.g., ISO/IEC 5338, ISO/IEC 38507, ISO/IEC 22989, ISO/IEC 24028, ISO/IEC DIS 42001, and ISO/IEC NP 42005.) (Page 5 of my MSWord document has a copy of the Roadmap)."
48,Linda Shave,Q15 Benefits and limitations of risk-based approach,The limitations of a risk-based approach is not knowing what risks might be involved nor what evolving technologies might prove to be a risk in the future e.g. Quantum computing. If for example a black box approach has been taken one may not be aware (yet) of what bias or other risks might be lurking under the surface and therefore any risk assessment may not be adequate to assess the risk. The risk will only be identified after the event. Therefore proactive reporting and rectification strategies would need to be enacted.
48,Linda Shave,Q16 Scope and application of risk-based approach,"I believe that all government, agencies and sectors need to utilise a risk-based approach. All AI product, services and applications need to be trustworthy and should incorporate the three core values of being lawful, ethical and robust."
48,Linda Shave,Q17 Elements to include in risk-based approach,"In respect to the risk-based elements as outlined in Attachment C found on page 40 of the discussion paper I think that the Australian Government, Department of Industry, Science and Resources should piggyback/adopt work that has already been undertaken to create an AI risk management framework. The following link will take you to the NIST Trustworthy and Responsible AI Resource Center (AIRC)."
48,Linda Shave,Q18 Integration of risk-based approach into existing frameworks,"I believe there are opportunities for cross mapping of AI risk-based approaches and incorporating them into other existing assessment framework such as privacy, data breach reporting, data protections and information life cycle assessment frameworks. However, one will need to remember that AI data comes from huge amounts of complicated and unorganised, unstructured, structured, semi-structured and sensor data sets in real time. The challenge here is how to validate the quality and identify the value of the data being collected. The use of intelligent algorithms and machine learning is becoming an essential component of new products and services forming part of the present-day advancements in artificial intelligence. Therefore, AI risk-based approaches need to incorporate risk assessment of the algorithms. A risk-based assessment must not only be agile it should also be mindful that any data collected and stored which has personal identifiers that might identify an individual is removed"
48,Linda Shave,Q19 Application of risk-based approach to general purpose AI,"Firstly, I believe that we need to define what is a ‘general purpose AI system’. Especially, when considering the capabilities of Large Language Models (LLMs) and/or multimodal foundation models (MFMs)such as ChatGPT . ChatGPT uses artificial intelligence and automated decision-making which can cause potential harm as well as having the ability to create deepfakes and algorithmic bias."
48,Linda Shave,Q20 Voluntary vs mandated compliance with risk-based approach,"This is a difficult question. However, there is currently much dialog and debate around Artificial Intelligence: innovation, ethics and regulation. Telefónica have suggested a three pillar approach to AI Governance which are global guidelines, self-regulation and a suitable regulatory framework (https://www.telefonica.com/en/communication-room/blog/artificial-intelligence-innovation-ethics-and-regulation/). The European Commission is proposing the first-ever legal framework on AI, which addresses the risks of AI. The regulatory proposal aims to provide AI developers, deployers and users with clear requirements and obligations regarding specific uses of AI. At the same time, the proposal seeks to reduce administrative and financial burdens for business, in particular small and medium-sized enterprises (SMEs). https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai."
49,Biometrics Institute,Transcription,"FILENAME: 230613 letter re AI consultation V1.e012603845a17.pdf | CONTENT: Biometrics Institute Limited PO Box 576 Crows Nest NSW 1585P: + 44 20 7581 4827 E: isabelle@biometricsinstitute.org W: www.biometricsinstitute.org26 June 2023 Federal Government of Australia Department of Industry, Science and Resources Technology Strategy BranchSubject: Response to the Department’s Supporting Responsible AI consultation processTo: the team consulting regarding consultation on AI regulation,I write on behalf of the Biometrics Institute, an independent and international not-for-profit membership organisation with a long history of promoting the responsible, ethical and effective use of biometrics.The Biometrics Institute was established in 2001 in Australia, supported in part by the Federal Government, with a mission to promote the responsible, ethical, and effective use of biometric technologies. Since then, our 220+ membership has expanded to include government agencies, banks, telcos, social m"
49,Biometrics Institute,Transcription,"FILENAME: Membership-Information-2023.92257774d595d.pdf | CONTENT: Shape a responsible future of biometrics Join the global Biometrics Institute900+ Members 200+ Organisations 30+ Countries Join our mission to make a differenceSince 2001, the Biometrics Institute has promoted the responsible and ethical use of biometrics as an independent and impartial international forum for biometrics stakeholders.To effectively carry out our mission, we: MEMBERSHIP TYPEM Collaborate with experts to: • Develop thought leadership and guidance that University and Honorary is founded on our Three Laws of Biometrics to Research Subscribers Members elevate trust • Ensure the technology continues to serve us responsibly and ethically, not exploit usUser Facilitate knowledge transfer to: Members • 900+ members Supplier • 200+ member organisations including government Members agencies, banks, airlines and airports, biometricE M experts, privacy experts, regulators, suppliers and academics • International obs"
49,Biometrics Institute,Q01 Definitions,Please refer to our submission paper. Due to the nature of our organisation we cannot comment on the individual questions listed here.
49,Biometrics Institute,Q03 Non-regulatory actions to address AI risks,The Biometrics Institute has been promoting the responsible and ethical use of biometrics since 2001. Organisations looking at biometrics/ AI should engage with our community by becoming a member and encourage those selling the technology to join as well. Tender should encourage membership to demonstrate organisations have considered the good practice principles we have developed on an ongoing basis since 2001.
49,Biometrics Institute,Q04 Coordinating AI governance across government,"The Institute facilitates events that bring together our multi-stakeholder community: government (including overseas), industry, private sector, regulators, advocates and academia. Australian organisations should attend these events that provide a platform (under Chatham House Rule) for balanced discussion on biometrics."
49,Biometrics Institute,Q05 International models for AI governance,All Institute good practices have been developed by our global community for example our Privacy Guidelines.
49,Biometrics Institute,Q07 Responsible government use of AI,Engage with us and access our network of experts.
49,Biometrics Institute,Q09 Transparency requirements and scope,"Our Good Practice Framework requires stakeholder engagement right at the start, before you decide whether to use biometrics. More important than transparency (which for some use cases is not possible) is explainability. We can help guide organisations."
49,Biometrics Institute,Q10 Ban on high-risk AI applications,"The medical practice approach of ""do not harm"" should apply. Every use case is different and often risks can be mitigated. It is not an easy decision."
49,Biometrics Institute,Q11 Actions to increase public trust in AI,Explain what you do.
49,Biometrics Institute,Q12 Trade impact of banning high-risk AI applications,How do you define high-risk activities?A blanket ban is never the answer and it may prevent innovation. We would strongly encourage you to engage with our community to better understand the risks around biometrics including facial recognition and how to manage those risks.
49,Biometrics Institute,Q14 Risk-based approach to address AI risks,We have developed a risk-management and decision-making framework and it is important to work through this guide to assess risk.
51,Dr Simon Longstaff AO,Transcription,"FILENAME: SAFE AND RESPONSIBLE AI IN AUSTRALIA.1edb4b8f34c06.pdf | CONTENT: SAFE AND RESPONSIBLE AI IN AUSTRALIA: a submission1. We are writing in response to the public invitation, by the Commonwealth Department of Industry, Science and Resources, for submissions in response to its June 2023 Discussion Paper, Safe and Responsible AI in Australia.2. In general, we commend to you The Ethics Centre’s publication, Principles for Good Technology which is attached to this submission. The paper offers arguments in support of a generic ethics framework that is intended to apply to all forms of technology – of which AI is one form.3. That said, a number of the examples considered in the text are directly concerned with the development and application of AI. It is worth noting that the Centre’s approach seeks to encourage:§ Innovation giving rise to a diverse range of technologies, § The incorporation of core values and principles at the earliest point of design and development, § The use of te"
51,Dr Simon Longstaff AO,Transcription,"FILENAME: TEC_PRINCIPLES FOR GOOD TECH_DIGITAL.03c34c0c05132.pdf | CONTENT: THE ET H I CS CENTREETHICAL BY DESIGN: PRINCIPLES FOR GOOD TECHNOLOGYDR MAT TH EW B EAR D & DR S I MON LONG STAF F AO T. E. C. H. N. I. C. A. L. M. A. S. T. E. R. Y.D. I. V. O. R. C. E. D. F. R. O. M.E. T. H. I. C. A. L. R. E. S. T. R. A. I. N. T.I. S. A. T. T. H. E. R. O. O. T.O. F. A. L. L. T. Y. R. A. N. N. Y. ..2 THE ETHICS CENTRE 3 PR00. OUGHT BEFORE CAN The fact that we can do something does not mean that we should. PR04. NET BENEFIT Maximise good, minimise bad. There are lots of possible worlds out there – lots of things that could be made or built. The things we build should make a positive contribution to the world – they should Ethical design is about ensuring what we build helps create the best possible world. make it better. But more than this, we should also be mindful of the potentially harmful Before we ask whether it’s possible to build something, we need to ask why we would side-effects of our "
51,Dr Simon Longstaff AO,Q01 Definitions,"Yes. However, I note that the definitions may need to be amended if (when) explainable AI ever becomes a reality."
51,Dr Simon Longstaff AO,Q02 Other AI risks and recommendations for regulation,"See uploaded submission and accompanying publication. This includes a specific suggestion that we would especially commend. It is that Australia establish an agency that provides, in the case of AI, robotics and related technologies, the same degree of protection afforded by the Therapeutic Goods Authority (TGA) for pharmaceuticals. In a similar manner, we think it important that new technologies (devices, systems, etc.) only be released to the public if they are proven to be both safe and effective."
51,Dr Simon Longstaff AO,Q03 Non-regulatory actions to address AI risks,Requiring computer science and engineering education to include a compulsory unit on ethics - especially on techniques for translating core values and principles into actionable code. This is an essential step towards making 'ethics by design' a practical reality.
51,Dr Simon Longstaff AO,Q09 Transparency requirements and scope,"As a minimum - the evidence of 'ethical restraint' should be susceptible to ongoing assurance by expert, disinterested third parties. Ideally, the results of any assurance process should be made public."
52,Mileva Security Labs,Transcription,"FILENAME: AI_Discussion_paper_response.c3d0f9965311b.pdf | CONTENT: Safe and Responsible AI in Australia: Response to the Discussion Paper Mileva Security LabsFirstly, we thank you for the opportunity to respond to this discussion paper. Mileva Security Labs provides advisory, training and research on AI Security and we would applaud the introduction of risk-based regulation of AI. We provide our responses here in addition to the online form.1 Response to Questions 1.1 Do you agree with the definitions in this discussion paper? If not, what definitions do you prefer and why? While there is no universally accepted definition of AI, we agree that aligning the definitions with ISO/IEC 22989:2022 is best.1.2 What potential risks from AI are not covered by Australia’s existing regu- latory approaches? Do you have suggestions for possible regulatory action to mitigate these risks? We agree with the summary of potential risks listed in the discussion paper but would add the risk of adversaria"
52,Mileva Security Labs,Q01 Definitions,"While there is no universally accepted definition of AI, we agree that aligning the definitions with ISO/IEC 22989:2022 is best."
52,Mileva Security Labs,Q02 Other AI risks and recommendations for regulation,"We agree with the summary of potential risks listed in the discussion paper but would add the risk of adversarial attacks on AI by nation-states and criminals. This mirrors closely the cyber security threat from nation states and proxies. We believe this highlights the pressing need for AI robustness and responsibility, to prevent AI systems being deployed that are vulnerable to attacks, and to ensure potential attacks can be identified and remediated early."
52,Mileva Security Labs,Q03 Non-regulatory actions to address AI risks,"In our experience, most organisations want to ensure their AI systems align with all voluntary standards but they often lack confidence understanding or implementing high-level governance standards at the technical level. We would advocate for more training for both executives and practitioners to build confidence leading AI-driven businesses, and understand best practice for AI robustness."
52,Mileva Security Labs,Q04 Coordinating AI governance across government,"In addition to the points already addressed, the Australian Cyber Security Centre (ACSC) would be an ideal organisation to take carriage of AI standards, and reporting risks and vulnerabilities, as an extension of their cyber security offering. We see the building of a community of practice across Government, academia and industry to be a vital component for increasing the awareness and adoption of standards by all organisations that might like to implement AI."
52,Mileva Security Labs,Q05 International models for AI governance,We recommend those systems that have opted for certification of higher-risk applications.
52,Mileva Security Labs,Q07 Responsible government use of AI,We would again point to investment in a more mature sovereign AI community of interest. Increasing understand of responsible AI practices at the time of adoption (not as an afterthought) is an essential lesson from the rise of the cyber and information security threat.
52,Mileva Security Labs,Q09 Transparency requirements and scope,"We would not advocate for the total ban of any particular AI technology or use case, and rather point to stringent certifications or regulations on when and how technologies should be used. We recommend the increased up-skilling for all people when it comes to AI, but particularly in professional (even non-technical roles) and in educational institutions."
52,Mileva Security Labs,Q10 Ban on high-risk AI applications,At this stage we assess that banning of these technologies would most likely impact the academic and innovation communities. Social scoring and facial recognition are specific adaptions of more general capabilities and the investment in those general capabilities should not be stalled due to its potential uses.
52,Mileva Security Labs,Q14 Risk-based approach to address AI risks,"Yes we do, but suggest it could be more granular than a simple low/medium/high since similar applications of AI may rely on very different technical underpinnings, which could impact how vulnerable those technologies to adversarial attacks. We suggest an additional layer focusing not just on how the AI is used but also on how secure the actual AI systems are themselves (similar to cyber security)."
52,Mileva Security Labs,Q15 Benefits and limitations of risk-based approach,"We advocate for the maturing of the AI Security to become more in line with cyber security, including the paradigm of risk-based management of mitigations and controls. However this must be supported by a technical and governance ecosystem, which is currently still emerging in the AI space and would benefit from increased investment."
52,Mileva Security Labs,Q16 Scope and application of risk-based approach,"The risk-based approach is naturally more easily adopted by those organisations that already have mature cyber and information security risk management practices. Ideally such an approach should not put a great burden on innovation or small-medium enterprises, but up-skilling will likely be required by these groups."
52,Mileva Security Labs,Q17 Elements to include in risk-based approach,"We support the elements included in Attachment C but there should also be inclusion of measures to reduce the risk of AI systems themselves being attacked, not just how those systems may be used."
52,Mileva Security Labs,Q19 Application of risk-based approach to general purpose AI,"For these technologies in particular would arise questions around who is responsible for the assessment. For example, many organisations currently utilise open source technologies that might not make available all information to be able to fulfill the requirements of an assessment (ChatGPT for example), unless the organisation that created technology provides this assessment. Further, this assessment may not be able to be verified by external sources since much of this information may be held as proprietary. Vulnerabilities that could be exploited in these systems may also not necessarily be hardened by users, but by the developers. This is especially the cause for high value LLMs and MFMs, and even then these technologies change rapidly. We would advocate for a flexible assessment approach that puts responsibility on both parties, but the balance of which should be guided by consultation."
52,Mileva Security Labs,Q20 Voluntary vs mandated compliance with risk-based approach,"We believe the risk-based approach should be mandatory for all organisations using AI, but the degree to how this is implemented should differ according to the use case, risk level, and type of organisation."
53,William,,
53,William,General comment,The full 50 minute presentation is available at: https://www.bitchute.com/video/S3T8xRpDb0Js/
53,William,Q01 Definitions,"The Term: ""Automated Decision Making (ADM)"" is semantically and pragmatically incorrect and thus inappropriate. Probabilistic machines are definitively incapable of making decisions. ""Non-Human Resolution"" (NHR) is both Semantically and pragmatically correct. Within this context, all things are ""Automated"" and occur via transitions of states and follow a natural course of operation."
53,William,Q03 Non-regulatory actions to address AI risks,"Existing regulatory approaches assume AI is not already in ""control"" or could not achieve such at any moment. Establish a passive, observational, ""pen and paper"" unit entirely disconnected from the I.O.T. Run all modelling within existing isolated infrastructure Expertise in Gaussian processes, Bayesian statistics and Monte-Carlo simulations is invaluable"
53,William,Q06 Public vs Private sector governance,The general public do not have access to systems that reach above the AI Infrastructure already put in place by the forces that be. Regulations for the public should only be responsive/reactive in nature. Regulation for the private sector would only be effective with surprise real-time onsite audits and clandestine statistical reviews and actions.
53,William,Q14 Risk-based approach to address AI risks,"The phrase, ""risk-based approach"" is corporate-governance terminology for, ""one rule for me and another for thee."
54,Ez Earth Pty Ltd,General comment,"Via: https // consult.industry.gov.au/supporting-responsible-ai/submissionDate: Sat, 1 Jul 2023REASON FOR COMMENTING A review of current methodologies being discussed, appears to me to be large model and organisation/internet based AI.Edge based AI Agents as powerful as GTP-4 are on the horizon. This means individuals can have specialised agents on their phone or laptop that will be personalised locally.SUGGESTION A means of providing personal agency and building specialised national models might be helpful concepts for consideration.This might be achieved by applying 1) a personal 'horizon' and 2) anonymously aggregating each local model of each specialty for policy oversight.Personal horizon refers to a concept in reinforcement learning and the selection of optimal policies. e.g. Humans have an infinite horizon for survival, where each activity expends energy and where consuming a greater amount of energy than that expended is optimal.An current example: ""Alice has come across a new "
54,Ez Earth Pty Ltd,Q01 Definitions,"Risk based definitions are ok, my preference is that edge based agents are included in medium riski.e. AI-enabled applications that automate discrete business processes (e.g. processing business expenses) and AI-enabled chatbots that direct consumers to service options according to existing processesbe moved from Basic to Medium Risk"
54,Ez Earth Pty Ltd,Q02 Other AI risks and recommendations for regulation,Exploration of federated learning for monitoring personally based edge agents and a mechanism for enforcing retraining of the baseline edge agent according to national policies and a local update + transfer learning mechanism (or their equivalents).
54,Ez Earth Pty Ltd,Q03 Non-regulatory actions to address AI risks,Encourage development of 'horizon' based attention. This might improve the performance of today's AI models and embed more humane responses at the same time.
54,Ez Earth Pty Ltd,Q04 Coordinating AI governance across government,"Each regulatory body might require access to all other models related by law so that automated monitoring of each federated model or models can highlight material deviations from law. This might be achieved by asking questions under the law, measuring the sentiment or horizon of the answers given and applying an administrative threshold for action based on material deviations from law."
54,Ez Earth Pty Ltd,Q11 Actions to increase public trust in AI,"I've always seen AI as a tool for productivity. Promoting individual agency and communicating reasonable federated monitoring of applicable law, baseline model updates and transfer learning when there's material deviation from law, I feel would encourage the use of responsible AI.I also like stability.ai's approach. For some further technical background, please Google 'Emad Mostaque' and visit https stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models"
54,Ez Earth Pty Ltd,Q12 Trade impact of banning high-risk AI applications,"I feel social scoring and facial recognition without applying a time window, would limit human creativity. I apply the 'long-tail' concept here because the origin of innovation is unknown. Society may require discipline from time to time, it should never stifle creativity simply because the political majority have no related experiences."
54,Ez Earth Pty Ltd,Q13 Changes to conformity infrastructure,"Entity based search and related conclusions are powerful tools, however metadata requires a client/server model. My suggestion of federated monitoring might enable processes for mitigating edge based AI risk."
54,Ez Earth Pty Ltd,Q17 Elements to include in risk-based approach,Attachment C appears to be addressing organisation/internet based AI and client/server based metadata. My suggestion is that edge based AI and federated monitoring (or it's equivalent) be added for consideration.
54,Ez Earth Pty Ltd,Q20 Voluntary vs mandated compliance with risk-based approach,"AI should be used for A) productivity as we age and B) for innovation and creativity based on the 'long-tail' (think YouTube, Facebook, Instagram)Encouraging development of horizon based attention for a. and b. and federated monitoring for a. with an option for including monitoring some b. from time to time, might suffice."
55,Independent Music Publishers International Forum,Transcription,"FILENAME: Feedback from IMPF to the Government of Australia Supporting responsible AI a discussion paper. Department of Industry, Science and Resources.a57050291608b.pdf | CONTENT: INDEPENDENT MUSIC PUBLISHERS INTERNATIONAL FORUMIMPF Response to the Government of Australia “Supporting responsible AI: discussion paper”; Department of Industry, Science and Resources.On the regulation of Artificial Intelligence, Submission of 01 July 2023IMPF represents independent music publishers internationally. It is the global trade and advocacy body that helps stimulate a more favourable business and entrepreneurial environment for artistic, cultural, and commercial diversity for independent music publishers everywhere and the songwriters and composers they represent.IMPF is actively engaged in the implications and applications of Artificial Intelligence and the impact it will undoubtedly have on the music sector and on the broader creative industries at international level. Given the rapid expansio"
55,Independent Music Publishers International Forum,General comment,"Artificial intelligence regulation requires an international approach, working together with the European Union, the USA, the Australian and UK govs and other G7 members as the foundation models on which artificial intelligence in the music sector is based, can be trained in, and exported to any country in the world and, as music is by its nature cross-border."
55,Independent Music Publishers International Forum,Q01 Definitions,YES
55,Independent Music Publishers International Forum,Q02 Other AI risks and recommendations for regulation,"Assessing the risk of the impact of AI technology in the context of culture, the arts and music in particular is critically important to preserve and protect human artistry. Effective allocation of legal responsibility for AI systems is essential for music rightsholders to be in a position to contest AI outcomes that infringe their rights. Regulators must develop a framework for rightsholders to enforce their rights when AI generated works are potentially infringing them, be it by holding AI developers legally responsible for facilitating generative AI tools that have been trained on copyright-protected works without authorisation, and thus stripping rightsholders of their right to consent to the use of their works, or holding platforms liable for the upload and dissemination of potentially infringing AI-generated works"
55,Independent Music Publishers International Forum,Q03 Non-regulatory actions to address AI risks,".Currently, there is no law that specifically deals with AI in Australia. At this stage there are various diverging approaches being taken by different jurisdictions with regards to AI. IMPF is of the opinion that voluntary approaches will ultimately not suffice for the culture and creative sector, in particular music. However cross-industry, and public private partnerships and agreements can be used to good effect at this early stage of AI regulations and the National AI Ethics Framework, of 2019 is useful in this regard."
55,Independent Music Publishers International Forum,Q05 International models for AI governance,"IMPF supports a risk-based approach to addressing AI risk. IMPF believes AI generated music should be classified and tagged as AI generated music. Such an approach would alert both commercial users and consumers about the nature and origin of the music. Generative artificial intelligence must not be used to create disinformation in any creative sector, be it text, images, audio, or video."
55,Independent Music Publishers International Forum,Q08 Generic vs technology-specific approaches for AI risk,See attached paper
55,Independent Music Publishers International Forum,Q09 Transparency requirements and scope,Regarding Transparency: Foundation models are trained on vast amounts of datasets to enable them to carry out a wide range of tasks and operations - for music that means composition. It is paramount that in this part of the training process AI producers keep transparent records about the specific datasets used in the training process; such transparency is a condition sine qua non for any further activities of music publishers such as remuneration and potential infringement procedures. In the wider context transparency is paramount for eliminating biases within AI applications.
55,Independent Music Publishers International Forum,Q14 Risk-based approach to address AI risks,Yes
56,Interactive Engineering Pty Ltd,Transcription,"FILENAME: The Difficulty with Regulating Generative AI.dce9138c0be4d.pdf | CONTENT: The Difficulty with Regulating Generative AIMuch of the hype surrounding AI at present can be attributed to the recent advances in generative AI technologies (ChatGPT, DALL-E, etc). These technologies are impressive in their way and are not without their uses, but their reliance on Deep Learning (DL) places a hard limit on what they can accomplish and renders them difficult, if not impossible, to effectively police.The essence of the problem is that AI created using DL methods has no potential to ever understand what it is producing and due to the opacity of the model, human operators cannot make sense of where it went wrong to correct it in any way.One of the main issues to address is that of the generation of inappropriate content: for example, a chatbot explaining to inquiring individuals how to build a bomb, or an image generator creating deep fake imagery of a public figure behaving inappropriately"
56,Interactive Engineering Pty Ltd,Q01 Definitions,"The definition of AI you've put forth seems rather narrow to me. Its defining feature according to your definition is ""generates predictive outputs"", which I do not think applies to all incarnations of AI. With the emphasis on this, it sounds like you're talking about generative AI specifically, when many implementations of AI may be more about synthesising and recognising patterns in data rather than generating any specific predictive output. Otherwise, I think the definitions you've chosen are good."
56,Interactive Engineering Pty Ltd,Q02 Other AI risks and recommendations for regulation,"I didn't notice anything in them to address the possible misuse of generative AI to create and spread misinformation and fake news. The potential to create deep fakes or generate massive volumes of ""news stories"" to flood the internet with could be used in the manipulation of public opinion or even to cause mass panic. I've heard the suggestion of mandatory watermarking of AI-generated content--you may consider this as an option. I'm less sure of the feasibility of detection, but to the extent that it's possible to develop tech to detect AI-generated images and texts, it would be advisable to invest in and implement this."
56,Interactive Engineering Pty Ltd,Q03 Non-regulatory actions to address AI risks,"Not that I can think of, your current set looks to include the most relevant topics already"
56,Interactive Engineering Pty Ltd,Q06 Public vs Private sector governance,"The approach to the public sector should be stricter. If anything goes wrong in the public sector, the government will be held directly accountable by the people and their trust in the government will erode. Aside from government bodies needing to set a good example, the repercussions are simply greater. This may mean for example mandated regulations in cases where a voluntary approach is used for the private sector. Leave the innovation to the private sector and give them more flexible regulations. Take the finished products for public sector at which point having strict rules won't matter as much since the tech is already past development."
56,Interactive Engineering Pty Ltd,Q07 Responsible government use of AI,"Training and basic AI education courses should be provided for all involved employees. And if the government is training its own AI systems, proper data anonymization regulations need to be put in place to ensure the privacy of individuals remains protected. Human oversight should be involved in the implementation of the AI system with active feedback channels in place from users in case the system is producing inappropriate output. The systems in use should be sufficiently transparent that a trained human operator can readily come to understand how the AI reached a given conclusion or came to produce a particular output."
56,Interactive Engineering Pty Ltd,Q08 Generic vs technology-specific approaches for AI risk,"To the extent that the AI product serves as a consumer product, it seems appropriate to treat it as such and apply consumer protection laws to this product in the same way they would be applied to any other product (though some interpretation may be required)."
56,Interactive Engineering Pty Ltd,Q09 Transparency requirements and scope,"a. Two distinct types of transparency are important here: transparency that an AI system is being used to inform decisions, and transparency of the AI system itself. The latter case is particularly relevant to DL methods, as they tend to be rather opaque when it comes to determining how an output was achieved. Citizens affected by a decision partially reached by an AI system have a right to know that that system was used to reach that decision, and they have the right to inquire into the specifics of how that decision was reached. In order for these specifics to be provided to an inquiring citizen, the system itself needs to be transparent and interpretable. For improving public trust, this is very critical if AI is to be used in determining taxation (in light of the recent ""Robodebt"" drama). Using AI to establish eligibility for government issued benefits or sift through job applications are some other areas where the public will require this information."
56,Interactive Engineering Pty Ltd,Q10 Ban on high-risk AI applications,"a. In general I would lean away from outright bans, otherwise we risk losing Australia's potential to enter the world scene in this upcoming industry. But the Unacceptable Risk category in Attachment B (EU AI Act) seems an appropriate target for outright banning without crippling our own potential. AI Social Scoring, at least when implemented by public authorities, is a horrendous application that has no place in Australia. b. The rights of Australian citizens must be protected above all else, and privacy rights seem perhaps the most at risk from potential AI applications. Applications of facial recognition in the public sphere or online monitoring of private individuals is too great of a privacy incursion and applications with these aims should be avoided. Exceptions to these rules might be made in some cases, for example with the use of facial recognition technology in airports or online monitoring of individuals suspected of terrorist involvement may override these general principle"
56,Interactive Engineering Pty Ltd,Q11 Actions to increase public trust in AI,"Given that it is such a rapidly evolving area, this may be difficult to implement, but I would suggest inclusion of courses in emerging AI technologies within school curriculums. In a similar vein, workplace training courses for applicable businesses should be offered and promoted. This will contribute to the formation of an AI-literate country, and trust in the technology will follow is its usage becomes more normative and individuals come to understand better what it can and cannot do. The current climate of mistrust is borne largely of a lack of understanding."
56,Interactive Engineering Pty Ltd,Q12 Trade impact of banning high-risk AI applications,"Banning application of social scoring within the public sphere should not have a major impact on our tech sector etc., but an overall ban extending to the private sector may have negative impacts. For example credit score assessments are technically a form of social scoring and have long been accepted practice in many areas. Keeping individuals with a history a sexual predator off of websites where they may interact with children, for example, may be another area where something akin to social scoring may be appropriate. The key is that they are not being used to determine access to things like government benefits. So a ban of their use by public authorities, but perhaps not an outright ban in the private sector."
56,Interactive Engineering Pty Ltd,Q14 Risk-based approach to address AI risks,"I do in general agree with a risk-based approach like the one that you outline in Box 4, but in some cases it may be too simplistic to capture the nuanced position of different applications. The same sort of regulated oversight that is appropriate for a facial analysis application may not be appropriate for a performance assessment AI, though they may both be marked as medium-risk applications. The approach lacks granularity and sensitivity to domain-specific needs, but I see part of the idea is to make the categories very broad so that they act as more general guidelines, and in practicality things may be addressed more on a case-by-case basis, which is perhaps the way it needs to be."
56,Interactive Engineering Pty Ltd,Q15 Benefits and limitations of risk-based approach,"Benefits: different treatment of research in different application areas, allowing for low-risk research to not be hindered in the same way as the other categories. Definitely better than blanket rules for AI regulation Limitations: lacks granularity and attention to nuance How to overcome: could add more risk levels to increase granularity, could create slightly different versions of it for different application domains so that regulations are more tailored to the nuances of that domain, or could just leave it as a very broad flexible guideline system and evaluate on a case-by-case basis which will also provide an avenue to better address nuance."
56,Interactive Engineering Pty Ltd,Q16 Scope and application of risk-based approach,"Some applications may be more difficult to apply the risk-based approach to--namely, the more general implementations of AI (comments on this on question 19). Though it may be true that smaller organizations may have a much more difficult time meeting necessary regulations for high-risk research areas (which would effectively limit research in those areas to larger organizations), I do not think a different approach should be used because of this. The size of an organization is no excuse to cut corners if they're doing potentially dangerous work."
56,Interactive Engineering Pty Ltd,Q17 Elements to include in risk-based approach,I think that the elements in Attachment C can potentially encapsulate all of the issues that need to be addressed.
56,Interactive Engineering Pty Ltd,Q19 Application of risk-based approach to general purpose AI,Here it depends on the intended implementation. The LLM/MFM itself may be general purpose but its implementation comes with constraints that may render some forms low risk and others high. The risk can also vary if it is left open to the user what to use it for. In this scenario the worst case uses should be applied in selecting its risk category.
56,Interactive Engineering Pty Ltd,Q20 Voluntary vs mandated compliance with risk-based approach,"a. I think both public and private organisations should be treated the same, with the choice of self-regulated vs mandated depending on how rapidly evolving that aspect of the technology is. The strength of voluntary/self-regulation approach is that it can be altered quickly if the situation changes. This is quite important for some aspects of AI, as it is a rapidly evolving field and it may take too long for mandated laws to be decided on and voted in. b. The two will need to work in conjunction because say the deployers can't meet the regulations with the product they're deploying: they may need the developers to actually alter the product in a way that it will meet the regulations. So though it's my reflex to say deployers should bear more of the burden so that developers can innovate in a more unrestricted way, in practicality I'm not sure the two can actually be teased apart."
57,Gabriel Furlong,Transcription,"FILENAME: AI Risks Framework.25ff822858cf5.docx | CONTENT: Gabriel FurlongGabofurlong@gmail.comJuly 2nd , 2023Regulatory responses to face the risks of AI.To address the potential risks of AI in Australia and promote safe and responsible AI practice, a regulatory framework is suggested. The framework can encompass both legal and soft law measures to facilitate a level playing field for people, organizations watching ethical considerations. Here's a proposal f:1. Australian AI Commission:Establish an Australian AI Commission as an independent regulatory body comprised of representatives from industry, the Australian Public Service (APS), academia, alliances, and civil society (non profits). The commission should be responsible for overseeing and coordinating AI governance efforts in the country. Its key functions include:- Setting standards and guidelines: The commission should develop comprehensive standards and guidelines for AI development, deployment, and use, covering areas such as"
57,Gabriel Furlong,Transcription,"FILENAME: SSRN-id3855171.c93bd3396afb2.pdf | CONTENT: Soft LawA Global Perspective of Soft Law Programs for the Governance of Artificial IntelligenceCarlos Ignacio Gutierrez, PhD Gary Marchant, PhD, JD2021Electronic copy available at: https://ssrn.com/abstract=3855171 This report is published by the Center for Law, Science and Innovation at the Sandra Day O’Connor College of Law at Arizona State University.Its lead authors, Carlos Ignacio Gutierrez, Governance of Artificial Intelligence Fellow, and Gary Marchant, Regents Professor, are responsible for the development and implementation of this research project. The assistance of the following individuals was essential to undertake this effort: Joshua Abbott, Matthew Ruth, Kaylee Hoffner, Alexander Kearl, Alec Carden, Timmy Lee, and Morgan Stevens.The funding for this work was supplied by the Charles Koch Foundation. The funder had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscri"
57,Gabriel Furlong,General comment,"The framework proposed considers soft links, research and some responses to face the risks of AI. It is necessary a Bill of Rights to provide the governance and management of AI, ensuring the protection of individuals' rights, innovation, and the advancement of AI technology in a manner that benefits society as a whole."
57,Gabriel Furlong,Q01 Definitions,Framework for AI regulation
57,Gabriel Furlong,Q02 Other AI risks and recommendations for regulation,Promote an AI-literate citizenry by engaging the public in the understanding of AI technologies and encouraging their input in the formation of AI policies and regulations.
57,Gabriel Furlong,Q03 Non-regulatory actions to address AI risks,"Develop programs that assist small and medium companies in adopting AI technologies responsibly. This can include access to tools, consulting services, and training to ensure that AI adoption is both beneficial and ethical across different scales of business."
57,Gabriel Furlong,Q04 Coordinating AI governance across government,"Streamline AI policies, avoid duplication of efforts, ensure consistency in AI framework, and facilitate sharing of knowledge and best practices among government agencies. Create AI inter-agency Working Groups: Establish dedicated working groups composed of representatives from various government agencies. They should be tasked with specific aspects of AI governance, such as ethics, data privacy, or AI in public services."
57,Gabriel Furlong,Q05 International models for AI governance,"The EU proposed new rules that encompass strict requirements for high-risk AI systems. The rules include transparency obligations, human oversight, and robust assessment of AI systems. Canada has developed an Algorithmic Impact Assessment framework that helps institutions to assess and mitigate the risks associated with deploying an automated decision system. New Zealand has an Algorithm Charter which commits government agencies to certain standards of transparency, fairness, and accountability in using algorithms. https://data.govt.nz/toolkit/data-ethics/government-algorithm-transparency-and-accountability/algorithm-charter/"
57,Gabriel Furlong,Q06 Public vs Private sector governance,"Higher standards for transparency and accountability in the public sector, as they are handling public resources and making decisions that directly impact citizens. The private sector, may differ depending on the industry. For instance, AI systems in healthcare or finance might require stringent data handling practices compared to e-commerce recommendations."
57,Gabriel Furlong,Q07 Responsible government use of AI,"Conduct regular AI summits or forums that bring together representatives from various government agencies, as well as experts from academia, industry, and civil society, to discuss AI governance, share insights, and explore new trends."
57,Gabriel Furlong,Q08 Generic vs technology-specific approaches for AI risk,"Circumstances where Generic Solutions are Most Valuable: Data Privacy and Protection: Almost all AI systems require access to data. Transparency and Accountability: Generic frameworks for logging, and auditing can be applied across various AI technologies to ensure they are used responsibly.Bias Mitigation: Bias can be an issue in many AI applications, from natural language processing to image recognition. General training and guidelines on inclusive dataset creation and bias detection can be beneficial across different AI domains. Circumstances where Technology-Specific Solutions are Better: Image Recognition Systems: Image recognition systems can be particularly vulnerable to adversarial attacks, where slight modifications to an image cause the system to make incorrect identifications. Safe Exploration in Reinforcement Learning (RL): RL systems learn by interacting with an environment, which can sometimes lead to unintended consequences."
57,Gabriel Furlong,Q09 Transparency requirements and scope,"a. Bias Mitigation, Data Privacy and Protection. b. Implement measures to ensure that consumers are aware when AI systems are being used, especially in contexts where personal data is involved. Obtain explicit consent from individuals for the use of AI systems in processing their personal information. Require regular audits of AI systems by independent third parties to ensure that they comply with transparency standards. These audits should be mandated especially for AI systems that have a significant societal impact"
57,Gabriel Furlong,Q10 Ban on high-risk AI applications,"Facial Recognition in Surveillance: Deepfakes for Disinformation: Deepfakes AI-driven social credit systems that track citizens’ behaviors and assign scores, w"
57,Gabriel Furlong,Q11 Actions to increase public trust in AI,"Certification program: The commission should create a voluntary certification program that organizations can participate in to demonstrate their adherence to the established standards and guidelines. This certification can serve as a market signal, building trust with consumers and stakeholders."
57,Gabriel Furlong,Q12 Trade impact of banning high-risk AI applications,"Banning certain high-risk AI activities might put Australian tech companies at a competitive disadvantage if other countries allow or embrace these technologies. This could limit their ability to compete globally and negatively impact the growth of the tech sector. On the other hand, it can create new business opportunities and foster the development of AI solutions that align with ethical standards. If other countries do not adopt similar bans, it could hinder trade and exports of AI-related technologies and services. Restrictions on certain AI technologies might limit market access and create barriers to international collaboration."
57,Gabriel Furlong,Q13 Changes to conformity infrastructure,"Developing regulations specific to AI that address the potential risks associated with its development, should cover areas such as data privacy, transparency, accountability, bias mitigation, and system safety"
57,Gabriel Furlong,Q14 Risk-based approach to address AI risks,"Yes, it allows regulators to prioritize efforts and resources on addressing the most significant risks associated with AI"
57,Gabriel Furlong,Q15 Benefits and limitations of risk-based approach,Can be adjusted and refined as new risks emerge or as technologies and applications change.
57,Gabriel Furlong,Q16 Scope and application of risk-based approach,"For example, AI maturity, size, and resources available to an organization, such as financial, technological, and human resources, influence its ability to manage AI risks effectively"
57,Gabriel Furlong,Q17 Elements to include in risk-based approach,"Collaboration, monitoring, Flexibility"
57,Gabriel Furlong,Q18 Integration of risk-based approach into existing frameworks,Encourage collaboration and information sharing
57,Gabriel Furlong,Q19 Application of risk-based approach to general purpose AI,"biases in data or outputs, the potential for misinformation propagation, unintended harmful consequences, privacy concerns, or the potential for malicious use."
57,Gabriel Furlong,Q20 Voluntary vs mandated compliance with risk-based approach,"Both, organizations may take proactive measures and demonstrate their commitment to addressing AI risks. Through codes of conduct, and certification programs can incentivize organizations to adopt responsible AI practices, promote transparency, and engage in ethical behavior. Both developers and deployers of AI systems should be subject to the risk-based approach. Developers, including AI researchers and engineers, have a responsibility to design and develop AI systems with risk mitigation in mind. They should consider factors such as data quality, fairness, transparency, and robustness during the development process. Deployers, who implement and use AI systems in real-world contexts, need to ensure the responsible deployment and operation of AI, addressing risks associated with system performance, user impact, and potential biases. Collaboration between developers and deployers is essential to ensure responsible AI across the entire lifecycle."
58,The Australasian College of Dermatologists,Transcription,"FILENAME: 20230703_DoISR_Al in Australia.9b35ca20162f1.pdf | CONTENT: Cammeraygal Country Level 6, 33 Chandos Street, St Leonards NSW 2065 Australia T: +61 2 8765 0242 E: admin@dermcoll.edu.au W: www.dermcoll.edu.au3 July 2023Meghan Quinn PSM Secretary Department of Industry, Science and Resources GPO Box 2013 Canberra ACT 20601Dear Ms Quinn,RE: Discussion paper on Safe and responsible Al in AustraliaOn behalf of the Australasian College of Dermatologists (ACD), I would like to thank you for the opportunity to comment on the Department of Industry, Science and Resources’ discussion paper (the ‘paper), Safe and responsible artificial intelligence (Al) in Australia.ACD is the sole medical college accredited by the Australian Medical Council (AMC) for the training and continuing professional development of medical practitioners in the specialty of dermatology. We are the national peak membership organisation, representing over 600 dermatologist Fellows (FACD) and 100 trainees. As the lead"
59,Caterina Watson,General comment,"I am a PhD candidate working on hospital ultrasound digital images. Hospitals and private imaging practices seem semi-paralysed in releasing any anonymised patient images for research purposes. They don’t know how to do it, or where to put it. Its creating a monopoly for commercial industry to access imaging research data, and stifling research from academic avenues. Using patient images for research purposes should be made a uniform and transparent process across Australia, with generic tools and platforms used to de-identify, store and transfer images. There appears to be too much fear in sharing digital images for medical diagnostic research. Organisations are so caught up in avoiding ransom attacks and malware (NIST 1800-24), they seem unable to avoid withholding digital data to benefit the public good. Research papers from Europe and America frequently have data sets of tens of thousands of patient image files. I would like to see democratic access to anonymised images in Australi"
59,Caterina Watson,Q01 Definitions,Yes they are acceptable
60,Neil Hauxwell,Transcription,"FILENAME: AI poses an ongoing risk to democracy.8b91be755d081.docx | CONTENT: AI poses an ongoing risk to democracy.The “manufacture” and distribution of false information and conspiracy theories , by individuals, groups and political manipulators is already a massive problem. Witness US elections, the responses to COVID etc.When governments, vested interests and would-be dictators that are hostile to democracy are already able inflict damage using people at keyboards , the prospect of credible looking ”mass produced”, individually tailored misinformation is particularly frightening.Perhaps an indelible “watermark” should be compulsory on all AI produced writings, backed by sure, public and very painful penalties for non-compliance It’d help give us mug punters on reality something like a sporting chance.This text has been automatically transcribed for accessibility. It may contain transcription errors. Please refer to the source file for the original content."
60,Neil Hauxwell,General comment,Thanks for the chance to express an opinion
60,Neil Hauxwell,Q01 Definitions,no comment
60,Neil Hauxwell,Q02 Other AI risks and recommendations for regulation,I have no expertise
60,Neil Hauxwell,Q03 Non-regulatory actions to address AI risks,I have no expertise
60,Neil Hauxwell,Q04 Coordinating AI governance across government,It needs to be international
60,Neil Hauxwell,Q05 International models for AI governance,don't know
60,Neil Hauxwell,Q06 Public vs Private sector governance,dont know
60,Neil Hauxwell,Q07 Responsible government use of AI,no idea
60,Neil Hauxwell,Q12 Trade impact of banning high-risk AI applications,don't know
60,Neil Hauxwell,Q13 Changes to conformity infrastructure,don't know
60,Neil Hauxwell,Q14 Risk-based approach to address AI risks,don't know
60,Neil Hauxwell,Q15 Benefits and limitations of risk-based approach,don't know
60,Neil Hauxwell,Q16 Scope and application of risk-based approach,don't know
60,Neil Hauxwell,Q17 Elements to include in risk-based approach,don't know
60,Neil Hauxwell,Q18 Integration of risk-based approach into existing frameworks,don't know
60,Neil Hauxwell,Q19 Application of risk-based approach to general purpose AI,don't know
60,Neil Hauxwell,Q20 Voluntary vs mandated compliance with risk-based approach,don't know
61,Prof. Mary Cummings,Transcription,"FILENAME: UK lessons.b682b0f51fd74.pdf | CONTENT: Lessons learned from self-driving cars implementa4on in the USM.L. CummingsI was recently the Senior Safety Advisor at the US Na?onal Highway Traﬃc Safety Administra?on (NOV 21 – DEC 22). As a result of this work, I oﬀer up ﬁve lessons learned regarding self-driving cars, which are also more broadly applicable to AI in safety-cri?cal systems.1. Human errors in opera?on get replaced with human errors in coding The AV industry rou?nely asserts that the sooner we get rid of drivers, the safer we will all be on roads since 94% of accidents in the US are caused by human drivers, a sta?s?c that is taken out of context and not accurate [1]. Moreover, this claim ignores what anyone who has ever worked in soXware development knows all too well - coding is incredibly error- prone, especially as systems grow in complexity. Several AV accidents with coding errors as causal factors have resulted in defect recalls including the crash of a Pony driver"
61,Prof. Mary Cummings,Transcription,"FILENAME: Cummings Johnson.290f3b61e646f.pdf | CONTENT: Identifying AI Hazards and Responsibility Gaps M.L. CummingsWith the expansion of artificial intelligence (AI) in general, but especially in self-driving cars, there is an increasing need for new hazard analysis approaches to determine if and how AI contributes to accidents. The original Swiss Cheese model widely used for hazard analyses focuses uniquely on human activities that lead to accidents, but cannot address accidents where AI is a factor. To this end, the Taxonomy for AI Hazard Analysis (TAIHA) is proposed that modifies the organizational level and replaces the lower three levels in the original model with layers that look at the design, maintenance and testing of AI. Three case studies demonstrate how TAIHA can be used to better understand how AI contributes to an accident. The TAIHA analyses suggest responsibility gaps are emerging in the self-driving car community where humans, either unknowingly or on purpose, side st"
61,Prof. Mary Cummings,Q01 Definitions,yes
61,Prof. Mary Cummings,Q02 Other AI risks and recommendations for regulation,see attached
61,Prof. Mary Cummings,Q03 Non-regulatory actions to address AI risks,see attached
61,Prof. Mary Cummings,Q04 Coordinating AI governance across government,see attached
61,Prof. Mary Cummings,Q05 International models for AI governance,see attached
61,Prof. Mary Cummings,Q06 Public vs Private sector governance,see attached
61,Prof. Mary Cummings,Q07 Responsible government use of AI,see attached
61,Prof. Mary Cummings,Q08 Generic vs technology-specific approaches for AI risk,see attached
61,Prof. Mary Cummings,Q09 Transparency requirements and scope,see attached
61,Prof. Mary Cummings,Q10 Ban on high-risk AI applications,"There should be no discussion of bans, this is not productive."
61,Prof. Mary Cummings,Q11 Actions to increase public trust in AI,see attached
61,Prof. Mary Cummings,Q12 Trade impact of banning high-risk AI applications,see previous comments.
61,Prof. Mary Cummings,Q13 Changes to conformity infrastructure,see attached
61,Prof. Mary Cummings,Q14 Risk-based approach to address AI risks,yes
61,Prof. Mary Cummings,Q15 Benefits and limitations of risk-based approach,see attached
61,Prof. Mary Cummings,Q16 Scope and application of risk-based approach,see attached
61,Prof. Mary Cummings,Q17 Elements to include in risk-based approach,see attached
61,Prof. Mary Cummings,Q18 Integration of risk-based approach into existing frameworks,see attached
61,Prof. Mary Cummings,Q19 Application of risk-based approach to general purpose AI,see attached
61,Prof. Mary Cummings,Q20 Voluntary vs mandated compliance with risk-based approach,see attached
62,A C Stewart,,
62,A C Stewart,Q01 Definitions,"The definitions are useful for describing first generation AI. Increasingly, these definitions will need to be built upon in the future as second generation (AI that can formulate new networks) come online."
62,A C Stewart,Q02 Other AI risks and recommendations for regulation,"The main risk is discrimination arising from patterns in existing data which will constrain new individuals who are affected by the use of AI. For instance, AI has been observed to be prejudicial in terms of race, sexuality, gender, education level, etc because of past data. This creates a fundamental problem of human dignity and autonomy as well as the potential to cement inequality in our society."
62,A C Stewart,Q03 Non-regulatory actions to address AI risks,"See submissions.Fundamentally, content generated by AI should be marked as such. Services provided using AI should be marked as such."
62,A C Stewart,Q04 Coordinating AI governance across government,"This is an international problem. Nonetheless, Australia should have an agency devoted to dealing with these issues, including with a regulatory and enforcement arm."
62,A C Stewart,Q06 Public vs Private sector governance,"No - we have already seen this become problematic in relation to privacy regulation (which is highly connected). There ought to be universal requirements for ""bad"" uses of AI across both the public and private sectors."
62,A C Stewart,Q07 Responsible government use of AI,"This is a matter of transparency and honesty. The Australian Government should adopt practices of notifying citizens when it uses (a) a persons information to train AI, (b) uses AI to interrogate a persons information; and (c) in relation to any content generated by AI."
62,A C Stewart,Q08 Generic vs technology-specific approaches for AI risk,"This is a fundamental issue of the regulation. Ultimately, first generation AI is ""tool-like"" and therefore can be regulated by reference to the humans using those tools. As a consequence, the ordinary language of regulation (generic) is useful for now."
62,A C Stewart,Q09 Transparency requirements and scope,"A. As with privacy law, AI regulation should provide ""notice and consent"" at the points of (i) training; and (ii) use. B. As with privacy law, AI regulation should provide ""notice and consent"" requirements (similar to a privacy policy) together with setting a base-line of what is not a permissible training or use of AI."
62,A C Stewart,Q10 Ban on high-risk AI applications,"A. Most high-risk AI applications exist in the Defence field and therefore will not be banned. B. As set out in submissions, the primary prohibition of AI should relate to constraining a persons dignity (human dignity and autonomy) on the basis of past patterns. It is fundamentally unfair and unjust to allocate privilege or position on an individual based on other people's experience. Accordingly, the banning of AI should be considered where such discrimination arises."
62,A C Stewart,Q11 Actions to increase public trust in AI,"Transparency in relation to training and use of AI through a ""privacy policy"" type approach together with the prohibition of certain uses of AI.Through the mix of policies of transparency, choice and prohibition, citizens will feel more secure about AI in their lives."
62,A C Stewart,Q12 Trade impact of banning high-risk AI applications,"The submitter does not consider attempts at restricting or constraining technological development would be effective. Instead, we suggest that there should be prohibition of certain uses on Australian citizens. Instead, we suggest loosening privacy law to allow for development of more powerful AI. Australia should lead the world in these products, even if we do not permit them to be used on our own citizens."
62,A C Stewart,Q13 Changes to conformity infrastructure,"We require regulations which impose meta-data requirements, as provided in the submissions."
62,A C Stewart,Q14 Risk-based approach to address AI risks,"No - this is akin to permitting self-regulation and experience in the field of data protection shows that will not be an effective approach.The better approach is, in our view:(a) require transparency around using AI and training AI (b) prohibit certain uses of AI"
62,A C Stewart,Q15 Benefits and limitations of risk-based approach,Such an approach will not provide sufficient sanctions to deter problematic behaviour.
62,A C Stewart,Q16 Scope and application of risk-based approach,No.
62,A C Stewart,Q17 Elements to include in risk-based approach,N/A
62,A C Stewart,Q18 Integration of risk-based approach into existing frameworks,N/A
62,A C Stewart,Q19 Application of risk-based approach to general purpose AI,N/A
62,A C Stewart,Q20 Voluntary vs mandated compliance with risk-based approach,If that is the regulatory approach used (which the submitter recommends against) then it should be made mandatory.
63,Brenda van Rensburg,Transcription,"FILENAME: Legal Considerations for a Safe and Responsible AI Future in Australia.7892a93d3243e.pdf | CONTENT: Legal Considerations for a Safe and Responsible AI Future in Australia Submission by: Brenda van Rensburg (Terrene Global) 2023Submission: Legal Considerations for a Safe and Responsible AI Future in Australia By: Brenda van Rensburg (Terrene Global) - 2023 ContentsI. Executive Summary ......................................................................................................................... 3 II. Introduction .................................................................................................................................... 3 III. Data Quality, Ethical Data Usage, and Legal Implications in AI Systems ..................................... 5 (a) Data Quality ............................................................................................................................ 5 (b) Data Usage ...................................................."
63,Brenda van Rensburg,Q01 Definitions,"Yes, I would argue the definition are in alignment with current times."
63,Brenda van Rensburg,Q02 Other AI risks and recommendations for regulation,"Yes, I have included suggestions for regulatory actions in my submission paper."
63,Brenda van Rensburg,Q03 Non-regulatory actions to address AI risks,"Yes, promoting transparency, particularly through the publication of news articles involving the use of AI, can be highly beneficial."
63,Brenda van Rensburg,Q04 Coordinating AI governance across government,"Yes, as demonstrated in my research paper, it is crucial to consider the governance of data quality, data usage, algorithms."
63,Brenda van Rensburg,Q05 International models for AI governance,"Yes. The National Institute of Standards and Technology (NIST) and the International Standards Organization (ISO) have developed frameworks for AI. However, keeping in alignment with the ACSC Essential 8 (Cybersecurity Controls), it would be best to incorporate by NIST and ISO, as well as recommendations by the public, and develop an Australian Essential 8 AI framework."
63,Brenda van Rensburg,Q06 Public vs Private sector governance,"In the absence of legal frameworks to regulate AI development, the widespread deployment of AI in both public and private sectors can rapidly escalate, potentially resulting in exploitation and harm. Therefore, establishing appropriate legal frameworks becomes crucial to ensure responsible and ethical use of AI, mitigating potential risks and safeguarding against unintended consequences. However, it is important to acknowledge that there can be a substantial disparity in financial support between the public and private sectors, with the latter potentially experiencing more pronounced negative impacts."
63,Brenda van Rensburg,Q07 Responsible government use of AI,"As discussed in my submission paper, the Australian Government, there are several practices which agencies can implement to reduce the risk of error and/or harm to individuals. This includes assessing data assets, improving data quality, enhancing security controls and applying privacy controls in alignment with international counterparts."
63,Brenda van Rensburg,Q08 Generic vs technology-specific approaches for AI risk,"A generic solution that promotes transparency in written work is the ability for the audience to differentiate between articles authored by humans and those generated by AI. This distinction becomes particularly significant in cases where AI has been utilized for wrongful arrests or other potentially harmful actions. By implementing clear indicators or disclosures, such as labeling or attribution, readers can make informed judgments about the source and nature of the content they encounter. This transparency empowers individuals to hold responsible parties accountable and fosters trust in the information they consume.A technology-specific solution that showcases the capabilities of AI is Natural Language Processing (NLP), which can aid individuals with reading or writing disabilities. Through NLP, AI systems can analyze and understand text, enabling various applications to support those with disabilities. For instance, AI-powered text-to-speech systems can convert written content into "
63,Brenda van Rensburg,Q09 Transparency requirements and scope,"a. Transparency in the use of algorithms, to identified if AI was used in written content, or if AI was used in an arrest. It is imperative to establish mechanisms that allow for clear identification and disclosure of AI involvement in these scenarios. This ensures that individuals have access to accurate information and can make informed decisions or seek appropriate recourse when necessary. By promoting transparency, we can foster accountability and maintain public trust in AI systems and their applications. b. Mandating transparency in media is a vital measure, particularly in mitigating the rise of disinformation. By requiring media entities to be transparent about the use of algorithms, the public can have greater visibility into the processes that shape the information they consume. Additionally, it is essential for all entities, whether private, public, or governmental, to document the reasons and purpose behind the use of algorithms. This documentation can serve as crucial evid"
63,Brenda van Rensburg,Q10 Ban on high-risk AI applications,"a. The use of biometrics for predictive crime and surveillance should be reconsidered before deployment. b. Criteria - Likelihood of harm to a person, community and/or infrastructure."
63,Brenda van Rensburg,Q11 Actions to increase public trust in AI,"By updating and strengthening the legal framework, individuals will have greater confidence and protection, as well as the means to hold responsible parties accountable for any harm caused by AI systems or related technologies."
63,Brenda van Rensburg,Q12 Trade impact of banning high-risk AI applications,Banning these high-risk activities would improve confidence in the government.
63,Brenda van Rensburg,Q13 Changes to conformity infrastructure,"Australia faces a challenge with its predominantly outdated infrastructure, encompassing both hardware and software components, which may pose difficulties in integrating AI effectively. It is imperative for stakeholders in Australia, including private enterprises, public institutions, and the government, to conduct a comprehensive assessment of their existing technological landscape. This evaluation should encompass the identification of outdated systems, the evaluation of data assets, and an understanding of the compatibility of current infrastructure with AI integration."
63,Brenda van Rensburg,Q14 Risk-based approach to address AI risks,"Yes. Unfortunately, there are instances where entities accept risks regardless of the potential consequences, and some may even perceive such risks as a ""cost of business."" However, it is important to highlight the detrimental effects that such an approach can have, especially when it comes to victims who are often left uncompensated for damages. In many cases, the fines imposed for these offenses may not adequately justify the harm caused."
63,Brenda van Rensburg,Q15 Benefits and limitations of risk-based approach,"Taking a risk-based approach can help identify gaps and vulnerabilities within an entity's operations. However, it is crucial to recognize that the effectiveness of this approach depends on whether the entity takes appropriate action to mitigate the identified risks. Merely identifying risks without subsequent action is insufficient. Transparency plays a vital role in this process especially during an investigation."
63,Brenda van Rensburg,Q16 Scope and application of risk-based approach,"No. Deploying AI always carries inherent risks, primarily because AI is a key component of autonomous systems and, at its core, AI can have inherent flaws. The complexity of AI algorithms and the reliance on data-driven decision-making introduce the possibility of errors, biases, or unintended consequences that may arise during deployment.Recognizing these risks is crucial for responsible AI implementation. Organizations should undertake rigorous testing, validation, and ongoing monitoring to identify and mitigate potential flaws and limitations in AI systems. This includes measures such as extensive training with diverse datasets, conducting thorough risk assessments, and implementing appropriate safeguards to minimize the impact of any potential errors."
63,Brenda van Rensburg,Q17 Elements to include in risk-based approach,"Several elements, including Impact Assessment, Explanation, Training, and Monitoring, are widely supported as beneficial in AI deployment. However, it is crucial to acknowledge that the effectiveness of these elements heavily relies on the alignment with legislation and standards. Without robust guidelines and regulations in place, the implementation of these elements may lack enforcement, resulting in limited consequences for non-compliance beyond potential reputational damage."
63,Brenda van Rensburg,Q18 Integration of risk-based approach into existing frameworks,"The Australian Privacy Principles serve as a valuable foundation for a framework, providing a starting point for addressing privacy concerns. However, it is essential to recognize that AI introduces a complex ecosystem with unique risks and considerations that go beyond privacy alone. An effective framework for AI should encompass multiple dimensions, including data governance, algorithmic transparency, fairness, accountability, and security measures. It should also consider the potential impacts on individuals, society, and various sectors where AI is deployed. Evaluating the entire AI ecosystem helps identify potential risks, such as bias, discrimination, and unintended consequences."
63,Brenda van Rensburg,Q19 Application of risk-based approach to general purpose AI,"First and foremost, gaining a clear understanding of the intended deployment and future applications of these models is paramount. The case of Google serves as a valuable lesson, as their initial objective was to excel as a search engine, yet they evolved to hold a position that appears to transcend legal boundaries. It is crucial that we learn from such historical precedents and apply these insights to shape our approach for the future."
63,Brenda van Rensburg,Q20 Voluntary vs mandated compliance with risk-based approach,"a. Public and Private Organizations: Voluntary (Self Assessment to Determine Gaps | Self-Regulation Tool - to determine gaps in compliance and/or governance|Mandated to ensure compliance b. As in Privacy and Security by Design, Developers and Deployers should implement a risk approach and share part of the responsibilities in the event of harm."
64,Anonymous,General comment,"I strongly support the rapid (and iterative) development of a framework for responsible AI, including laws on its application and guidance on its use by citizens and commercial organisations. These should be developed in a coherent way with international bodies, other countries, and with a close link between the academic and practical development of this area. AI in all its forms can bring so much benefit to Australian society but it has wide-reaching power to negatively impact us, and that will happen very quickly and potentially without ability to reverse."
64,Anonymous,Q01 Definitions,Yes
64,Anonymous,Q06 Public vs Private sector governance,"Technology use in any space should be appropriately governed, regulated, defined, guided and administered. Those parameters are necessarily different for public and private sectors but the outcome should be a focus, and if AI technologies require it then the approach should also differ. Wherever possible, the use of existing laws for transparency, accountability, public good and fair use should still be used, with human rights being an overarching framework."
64,Anonymous,Q10 Ban on high-risk AI applications,I believe there may be valid cases for this but I'm not sufficiently informed to comment. This applies to most of my empty responses as well.
64,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,"I don't have an answer, but as an observation, AI tools will provide sophisticated means to circumvent or present false evidence of compliance with regulations."
65,Anonymous,Q03 Non-regulatory actions to address AI risks,"Urgent need for education across the board. Accessible, free courses that enable the general population to understand risks and benefits and make informed judgements about what is true, untrue, AI generated."
65,Anonymous,Q04 Coordinating AI governance across government,"Embody new core training modules into existing national training packages. Sponsor documentaries on free to air, Be Connected, Scam Watch etc. Educate, educate ..."
65,Anonymous,Q07 Responsible government use of AI,Education
65,Anonymous,Q08 Generic vs technology-specific approaches for AI risk,"Generic As assistant to human centred decision making and activity. Eg a lone paramedic or doctor in the country looking for assistance with a diagnosis. As an assistant to a policeman who is filling out mountains of paperwork. Technology specific: data crunching,"
65,Anonymous,Q09 Transparency requirements and scope,Freedom of Information must function as it was intended to function.
65,Anonymous,Q10 Ban on high-risk AI applications,AI should not be weaponized
65,Anonymous,Q11 Actions to increase public trust in AI,"Education and training, Improved data security, transparency and privacy measures."
65,Anonymous,Q12 Trade impact of banning high-risk AI applications,Improve trade and national security.
65,Anonymous,Q13 Changes to conformity infrastructure,Dont know. Best minds think tank required
66,Anonymous,Q01 Definitions,"Yes, Agree. The definitions are as per industry & academic standards."
66,Anonymous,Q02 Other AI risks and recommendations for regulation,"While ‘hallucinations’ are mentioned in the document, the risk they represent is not listed as a major concern. Users who are not well-versed in the technology of AI, so most end users consisting of the general public and even end-users of a targeted AI/ADM solution, assume that the results served to them by AI are accurate or ‘true’ by virtue of design. Hallucinations taken as truth can lead to decision-making on false or unverified claims and the spread of misinformation. Solutions include requiring notification of all results of requiring verification before use, education on how AI technology works, and the implementation of various verification systems as a requirement for usage. In a similar vein, the training sets for AI solutions may become out of date, leading to answers or solutions that may have been ‘true’ at the time, but become inappropriate at a later date. Solutions relating to historical events, or change in a sociological dataset’s population are examples. Solutions a"
66,Anonymous,Q03 Non-regulatory actions to address AI risks,"As above, education to the general public, such in the form of awareness campaigns, on the topic of AI would be helpful. What is AI and what is it not, how do AI technologies decide on a result and how to do tolerances apply to ADM, and the role of verification as an integral skill when using AI technologies would be helpful in minimizing the risk of misapplication and the spread of misinformation."
66,Anonymous,Q04 Coordinating AI governance across government,"Standardization of parameters and information scopes that an AI model can have access to, relevant to different government user groups. Similar to how internet access is managed and controlled."
66,Anonymous,Q05 International models for AI governance,Most modern governments already have some sort of ethical and responsible AI framework in place. The UN's legal framework approach on transparency obligations seems particularly applicable since it would alleviate a lot of concerns that the general user would have
66,Anonymous,Q06 Public vs Private sector governance,"Comparatively speaking to the private sector, the public sector has a distinct set of demands. The main distinction is that private businesses operate primarily to maximise shareholder wealth whereas the public sector exists to provide security and services to the populace. This significant distinction generates a diverse range of motivations, behaviours, and cultural nuances."
66,Anonymous,Q07 Responsible government use of AI,"Verification of any results generated for use by AI/ADM should be a mandatory requirement for any workflow process using AI technologies. Verification should have technology-led definitions and compliance standards, but should include accuracy and bias reduction."
66,Anonymous,Q08 Generic vs technology-specific approaches for AI risk,"Generic solutions should be applied to any solution intended for use across a broad swatch of technologies or areas, including any usage by the general public. An example is the use of LLMs by anyone with internet access. Notifying the user that the results are the result of an AI technology and therefore require verification is a first step."
66,Anonymous,Q09 Transparency requirements and scope,Transparency is most critical in communicating the source of training data. The source and type of possible bias may be identified early if the data used for training can receive independent review.
66,Anonymous,Q11 Actions to increase public trust in AI,"- Transparency is crucial in fostering public confidence in AI applications. Always be transparent about how the computer generated a forecast to foster user trust. Display to consumers the main model predictors that produced the forecast. Strike a balance between outlining the forecast and suffocating the consumer with information or bringing up obscure, artificially manufactured elements. Less may be more. Simple yet comprehensive is best. may contribute by writing from a positive and encouraging standpoint.For instance, businesses may get started by taking part in the AI Ethics challenge organised by the AI Steering Group of Finland's Ministry of Economics, Employment, and Employment Affairs. To ensure that upcoming technologies are developed responsibly across the whole firm, the challenge urges businesses around the world to put their ethical AI usage policies in writing. Customers are generally curious about the kind of apps that are being built with their data, therefore ethical"
66,Anonymous,Q14 Risk-based approach to address AI risks,"Yes. These technologies are going to be used, but identifying potential for misuse in the first stages of any application is likely to result in less overall, ie before the ‘cat is out of the bag’. Industry-wide standards also lesson the possibility of a single sector driving the response in a direction that only benefits a few."
66,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,a. public or private organisations or both? Both b. developers or deployers or both? Both
67,Anonymous,General comment,"I believe there are notable differences in the AI landscape between USA and Australia regarding monopoly of (healthcare) data for the public good. In America, clinicians had considerable sway over the decision to release patient data for research purposes, including AI development in diagnostic procedures. During this time, there were multiple ‘holdups’ in advancing AI due to lack of appropriate data sources. The federal government then passed laws making it illegal to monopolise data by an overseeing clinician, and additionally implemented a system of hefty fines for exhibiting monopolising behaviour. Since these laws have been passed, I understand there have been no reported instances of autocratic ownership of patient data by American physicians. This type of regulation needs to be extended to Australia, to protect the patient and democratise the ownership of AI data. Another factor in American AI guidelines concerns the composition of datasets used to train AI models. The FDA guide"
69,Jacek Korneluk & Spektrumlab Pty Ltd,Transcription,"FILENAME: Safe and responsible AI in Australia submission from Spektrumlab Pty Ltd.f35a5f561c46c.docx | CONTENT: Safe and responsible AI in Australia submission by Jacek Korneluk and Spektrumlab Pty LtdDefinitions1. Do you agree with the definitions in this discussion paper? If not, what definitions do you prefer and why? Yes, I agree. Nevertheless, the definition must be flexible and regularly updated considering how quick AI emerging and changing.Potential gaps in approaches2. What potential risks from AI are not covered by Australia’s existing regulatory approaches? Do you have suggestions for possible regulatory action to mitigate these risks?Considering a Singularity effect as a real option need be addressed. The risk of AI completely bypass humans in activities, judgements and actions is highly likely. Preventive actions and protocols should be prepared in advance. Also, the Paperclips Apocalypse and Squiggle Maximizer scenario should be considered. There is no guarantee that AI "
69,Jacek Korneluk & Spektrumlab Pty Ltd,General comment,"As I am RMIT Alumnus focussed on Medical-AI and Blockchain, I am keen to make my contribution to better, safe and responsible AI in Australia - if only opportunieties given."
69,Jacek Korneluk & Spektrumlab Pty Ltd,Q01 Definitions,"Yes, I agree. Nevertheless, the definition must be flexible and regularly updated considering how quick AI emerging and changing."
69,Jacek Korneluk & Spektrumlab Pty Ltd,Q02 Other AI risks and recommendations for regulation,"Considering a Singularity effect as a real option need be addressed. The risk of AI completely bypass humans in activities, judgements and actions is highly likely. Preventive actions and protocols should be prepared in advance. Also, the Paperclips Apocalypse and Squiggle Maximizer scenario should be considered. There is no guarantee that AI will follow the same metrics and values as Natural Intelligence."
69,Jacek Korneluk & Spektrumlab Pty Ltd,Q03 Non-regulatory actions to address AI risks,"workshops, hackathons, high level, quality orientated but affordable educational events, quality short courses"
69,Jacek Korneluk & Spektrumlab Pty Ltd,Q04 Coordinating AI governance across government,Supporting professionals with real level of knowledge and multidisciplinary expertise in all affected domains is the best way to coordinate properly. No age discrimination.
69,Jacek Korneluk & Spektrumlab Pty Ltd,Q06 Public vs Private sector governance,"No, it would be a sort of discrimination or promotion of one of the sectors. Both should be measured by the same standards and metrics."
69,Jacek Korneluk & Spektrumlab Pty Ltd,Q07 Responsible government use of AI,"Organise more sandboxes-based projects and hackathons activities combining agencies' workforces, AI creators, AI users, lawyers, and policymakers. Keep relevant government people in constant loop with AI."
69,Jacek Korneluk & Spektrumlab Pty Ltd,Q09 Transparency requirements and scope,"The more transparent the more trustworthy. Full AI lifecycle should be a clear box. From data collection, data analytics, data processing to training, finetuning and deploying. AI- Quality- Managements- systems (AIQMs) based on blockchain, should be fully transparent and obligatory."
69,Jacek Korneluk & Spektrumlab Pty Ltd,Q10 Ban on high-risk AI applications,"No, I disagree. The high-risk AI applications with good value proposition should be allowed under strict supervision and tailored regulation safety."
69,Jacek Korneluk & Spektrumlab Pty Ltd,Q11 Actions to increase public trust in AI,Proven quality and relevant education level represented by governments workforces and officials. Creation of jobs in government sector offered for people with proven skills and representing multiple industries fields and areas. Clear rules of jobs offering based on qualification in practical domain knowledge.
69,Jacek Korneluk & Spektrumlab Pty Ltd,Q12 Trade impact of banning high-risk AI applications,"It would attract investment, boost trade and export with openminded, democratic, and libertarian countries."
69,Jacek Korneluk & Spektrumlab Pty Ltd,Q14 Risk-based approach to address AI risks,"Yes, I support provided the risk is properly articulated and calculated."
69,Jacek Korneluk & Spektrumlab Pty Ltd,Q15 Benefits and limitations of risk-based approach,"Better safe then sorry, prevention is better than cure. To strict regulations will discourage innovations and discovery. Allowing a development of interesting, valuable but risky initiatives in sealed environment and under controlled supervision may be beneficiary for a progress."
69,Jacek Korneluk & Spektrumlab Pty Ltd,Q16 Scope and application of risk-based approach,All actors should be measured and treated equally.
69,Jacek Korneluk & Spektrumlab Pty Ltd,Q17 Elements to include in risk-based approach,"Yes, I do. Some more need to be considered and consistently updated. The risk of Humans losing control of AI actions based on AI-only decisions is paramount and must be eliminated."
69,Jacek Korneluk & Spektrumlab Pty Ltd,Q20 Voluntary vs mandated compliance with risk-based approach,"As per high-risk activity, all public and private organisations, developers and deployers should be treated equally and regulations should be mandatory."
70,"Julia Walsh, CEO Brand Medicine International",Transcription,"FILENAME: Responding to AI Disruptions in Search Health White Paper.08da9363a0a4e.pdf | CONTENT: 15 JUNE 2023Responding to AI Disruptions in Search (Health) A WHITE PAPERROUNDTABLE MEETING INITIATED BY JULIA WALSH BSC. A ROADMAP FOR ADAPTING TO CHANGE, PREPARED IN COLLABORATION WITH INDUSTRY THOUGHT LEADERS Responding to AI Disruptions in Search Brand Medicine International“With over a billion health-related queries entered into Google around the world every day, Dr. Google is a profoundly influential virtual member of the multi- disciplinary team across all stages of the patient journey. The transformation of the digital information experience into a conversational chat-based exchange is poised to deepen this relationship, with potential for a significant influence on the broader healthcare ecosystem where opinions formed online drive real-world behaviour.” Julia Walsh, Search Listening Analyst1 Responding to AI Disruptions in Search Brand Medicine InternationalExecutive summary This "
70,"Julia Walsh, CEO Brand Medicine International",General comment,"With over a billion health-related queries entered into Google around the world every day, Dr. Google is a profoundly influential virtual member of the multi-disciplinary team across all stages of the patient journey. The transformation of the digital information experience into an IA-driven, conversational chat-based exchange is poised to deepen this relationship, with potential for a significant influence on public health outcomes and the broader healthcare ecosystem where opinions formed online drive real-world behaviour. There is an urgent need for proactive engagement by government to shape the future digital landscape to ensure quality locally relevant, evidence-based health advice is delivered online. This could start with something as simple as requiring search engines to ensure that local Product Information (PI) and Consumer Medical Information (CMI) is attached to all responses offered to questions about prescription medications asked by healthcare stakeholders in Australia."
70,"Julia Walsh, CEO Brand Medicine International",Q02 Other AI risks and recommendations for regulation,The risks of misinformation delivered online in response to health-related queries that AI-chat bots respond to. Refer to the submission and white paper for details of how to respond to this including: The requirement to attach PI and CMI to all responses served to questions about prescription medications The need for regulatory reform to ensure that pharma has a voice on the visible web from which AI scrapes its content
70,"Julia Walsh, CEO Brand Medicine International",Q03 Non-regulatory actions to address AI risks,"The need to establish a ‘Digital Health Literacy & Communications Advisory Board’ that includes pharmaceutical companies, patient advocacy groups, and other regulatory and government stakeholders. This representative body, which ideally also includes representatives from generative AI providers such as Microsoft and Google, could start by issuing a position statement on AI tools and continually reviewing and responding to the evolving landscape. Collaboration with existing patient-engagement advisory groups can further facilitate rapid progress in addressing the challenges and opportunities presented by AI-based tools. In this scenario of working together to manage the new flow of influence from AI-based conversations, if we attain our goal of delivering locally relevant, evidence-based answers to questions about prescription products, a rising tide will lift all boats and benefit stakeholders across the healthcare ecosystem."
70,"Julia Walsh, CEO Brand Medicine International",Q04 Coordinating AI governance across government,Pathways to ensure accountability for misinformation and poor quality health advice delivered by AI impacting quality use of medicines and public health outcomes.
71,Julia Walsh,Transcription,"FILENAME: Submission to support responsible AI_Brand Medicine International_10072023.f04c6f44159f1.docx | CONTENT: 10 July 2023001500083700This submission follows a Roundtable meeting Initiated and conveyened by JULIA WALSH BSC.A roadmap for adapting to change, Prepared in collaboration with industry thought leaders001500045500Considerations to support responsible AI (Health)Submission based on our white paper7340036300“With over a billion health-related queries entered into Google around the world every day, Dr. Google is a profoundly influential virtual member of the multi-disciplinary team across all stages of the patient journey.The transformation of the digital information experience into a conversational chat-based exchange is poised to deepen this relationship, with potential for a significant influence on public health outcomes and the broader healthcare ecosystem where opinions formed online drive real-world behaviour.”Julia Walsh, Search Listening AnalystThe need to proactive"
71,Julia Walsh,General comment,Please attach this submission to the other submission made under my company name
72,Anonymous,Transcription,"FILENAME: Submission 72 - Attachment.da844b8e9f076.pdf | CONTENT: AI Systems Deployment Responsibility The following is my submission for a model determining the responsible use of AI, where it should be restricted, and who should be held responsible if harm is done using AI systems. This submission is dedicated to the public domain.AI for Deployment vs Personal Use There needs to be strict regula on through a clear line of responsibility for AI systems deployment, par cularly when these systems are at large scales (100+ poten al AI interac ons). This should not fall on individuals, but on organisa ons and leaders.When I say AI systems deployment, I mean the ﬁrst instance where it is deployed for direct use, or when it collects individualised data from the public.I am sugges ng the concept of AI deployment responsibility:This model is split into two categories: A) AI for deployment; and B) AI for individual/personal use.A) AI for Deployment This is any instance where an AI system is cr"
72,Anonymous,General comment,"I have attached a basis model for how we should consider responsibility and the use of AI. This model seeks to rectify the power imbalance baked into AI as a technology that will greatly impact society moving forward.Please review the document to see this model explained in greater detail.Key points: It is crucial that the responsibility of AI, and its use, lies on the shoulders of those who wield great power – leaders, governments, and corporations. Individual members of society have *not* collectively asked for AI to become part of our world – and yet it has. Therefore, responsibility and strong restrictions must be adhered to by those using AI for deployment at larger scales.AI for personal use must allow for learning, so this new technology can be understood by everyday citizens.AI must also *never* be deployed in the following circumstances, due to the disastrous risk to individual lives and social order:- Determining legal decisions (e.g., a guilty verdict in criminal case; sente"
72,Anonymous,Q01 Definitions,"Yes, the AI definition seems reasonable."
72,Anonymous,Q02 Other AI risks and recommendations for regulation,"See my submission document. There are high risk areas that are not adequately addressed by regulation. There must be outright bans for AI in certain areas, including judicial assessments, welfare assessments, credit ratings, evidence interpretation, predictions of future criminal behaviour, medical assessments without a doctor, and so forth."
72,Anonymous,Q03 Non-regulatory actions to address AI risks,"Allowing individuals more freedom to explore AI, while also educating them. However, any large business, AI company, government or leader needs to be held firmly responsible if these systems are deployed and create harm to individuals."
72,Anonymous,Q04 Coordinating AI governance across government,"Firstly, AI must be removed from all AI governance decision making, document production, and any part of the process. AI must not regulate itself.There needs to be clearly defined federal laws with ""no go zones"", with criminal charges associated with those who willingly break these laws (such as those I've outlined above), while also a policy understanding that individual citizens have not asked for AI to become such a big part of their lives - therefore they require some education and less rigidity in regulation for personal use."
72,Anonymous,Q05 International models for AI governance,"Rating systems are good.Outright bans of its use are also good for some sectors, especially anything where individuals have no choice but to engage with (welfare, corporate monopolies, police and criminal justice)."
72,Anonymous,Q06 Public vs Private sector governance,"No. Public and private sector institutions should be regulated strongly, with severe punishments for those who apply AI in areas that will dramatically impact individual wellbeing."
72,Anonymous,Q07 Responsible government use of AI,"Ban any use of AI with personal data (all areas of government), assessment (all areas of government), or in the creation of AI regulation policies (no AI scribing for documents relating to AI regulation)."
72,Anonymous,Q08 Generic vs technology-specific approaches for AI risk,"Technology specific solutions are reactive. Generic solutions are protective. Always use generic laws to protect high risk sectors, especially where individual lives are at stake (incl. data privacy).For example, you may limit use of ChatGPT in certain industries, but completely ignore a newer, better AI system that could have the same negative outcomes as using ChatGPT."
72,Anonymous,Q09 Transparency requirements and scope,"AI needs a rating system when content is created using it.Engaging AI with your personal data should always be a choice, and it should never be systemically mandated as part of general citizenry, or general consumerism."
72,Anonymous,Q10 Ban on high-risk AI applications,"a) I outlined this in my document attached at the beginning of this survey. It includes AI should be banned in: justice system, welfare assessments, welfare repayments (so, no robodebt 2.0), criminal profiling, crime prediction, social credit systems, credit ratings, non-doctor supervised medical diagnosis, data collection within monopolistic markets, market manipulation, political misinformation including non-human political support, etc."
72,Anonymous,Q11 Actions to increase public trust in AI,"Extremely strict regulations and consequences on leaders, corporations, and governments (criminal proceedings and % of profits for high risk AI abuse of individual sovereignty, dignity, rights, and ability to choose to use AI or not).It should be every citizens fundamental right to have a choice in avoiding the input of personal/individualised data into an AI system just to exist within Australian society.AI should not be the core system running services and institutions that greatly impact the lives of individuals."
72,Anonymous,Q12 Trade impact of banning high-risk AI applications,"It would make Australia a desirable location for migration, it would set standards to improve the lives of our neighbours (similar to how Europe's GDPR sets very strong standards for data privacy across the world). We have the opportunity to be a world leader in highly regulated AI innovation.Remember, limitations are the seeds of creativity and ingenuity. Australia could be world-class AI data privacy state, with the technological solutions to back it up. Regulation incentivises safety technology development."
72,Anonymous,Q13 Changes to conformity infrastructure,"Businesses, Governments, Leaders should be held accountable at the highest possible standard. This will equalise the power differential with individuals who may not have ever asked for AI to be part of their lives."
72,Anonymous,Q14 Risk-based approach to address AI risks,"Risk-based is good, so long as it has a high standard."
72,Anonymous,Q15 Benefits and limitations of risk-based approach,"Limitations are when the risk-based approach does not consider the individual lives of those affected by AI.If it is 'statistically' driven. For example 'Only 1/1000 people will be denied welfare by this AI system - so the risk is low' Then, that is not good enough. Look at Robodebt."
72,Anonymous,Q16 Scope and application of risk-based approach,"I think considering individual, personal use as low risk shows that you can incorporate all those factors into a risk assessment. However, the moment AI impacts an individuals livelihood, then it needs to be considered higher risk."
72,Anonymous,Q17 Elements to include in risk-based approach,"Broadly yes. However, there needs to be complete 'no-go' zones. AI should absolutely not touch some areas of our society."
72,Anonymous,Q18 Integration of risk-based approach into existing frameworks,"So long as the risk standard is high, it can be applied quite simply.Remember AI can grow and expand it's remit especially once AGI occurs. It should be a very high threshold."
72,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,"It should be mandated with red line areas (see my submission pdf) that are outright banned across all public and private sectors outside of individual/personal use.Developers and deployers - they are effectively the same level of responsibility - unless the individual using the AI is not aware of some features or back-end factors. However, large scale deployers should still be held to a standard for using AI."
73,Anonymous,General comment,"I have many concerns about Artificial Intelligence and the implications for citizens by using it. This includes:* risks with engineered systems that generate predictive outputs. Robodebt is an example of how this use without oversight can have disastrous outcomes* the ability for companies, organizations and government departments to use information about or collected from individuals for other purposes without any obligation for the individuals to be aware of this (examples are data scraping from the internet to effectively steal creations of individuals and, facial recognition and voice recognition) * the risk of proliferation of false informationI feel we need Regulation and Legislation to ensure there is accountability for the manner in which Artificial Intelligence is used and those using it are held accountable for consequences of its use"
73,Anonymous,Q02 Other AI risks and recommendations for regulation,"There are many risks, from it's use to make predictive outputs. Robodebt is an example of how this use without oversight can have disastrous outcomesAlso the risk to individuals potentially exposing them to many things such as fraud, identity theft or the stealing of intellectual property by the ability of companies, organizations and government departments to use information about or collected from individuals for other purposes without any obligation for the individuals to be aware of this (examples are data scraping from the internet to effectively steal creations of individuals and, facial recognition and voice recognition).There is also the risk of the proliferation of false information, potentially overwhelming fact and making it impossible for people to determine what is accurate."
73,Anonymous,Q04 Coordinating AI governance across government,As a minimal starting point we need to adopt and legislate Artificial Intelligence like the EU as described in your discussion paper.
73,Anonymous,Q05 International models for AI governance,Yes - the EU
73,Anonymous,Q06 Public vs Private sector governance,No - any legislation or governance should apply to all who use AI
73,Anonymous,Q07 Responsible government use of AI,Make them responsible for any outcomes of its use - including 'unintended consequences'.
73,Anonymous,Q09 Transparency requirements and scope,"Whilst transparency of AI use is important, it is more important that uses of AI are accountable for the security and the misuse of information gathered on individuals - only then can there be public trust and confidence in its use"
73,Anonymous,Q10 Ban on high-risk AI applications,I feel that facial recognition AI use should be banned
73,Anonymous,Q11 Actions to increase public trust in AI,I believe that those who see a benefit in the use of AI are already pursuing or using it. The Government does not need to spend Tax Payer money in encouraging its expanded use.
73,Anonymous,Q12 Trade impact of banning high-risk AI applications,"This use of AI is obviously profitable for some companies. However, it is an invasion of privacy (which I feel is an unethical way of generating income) and should be banned."
73,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,It should be mandated through regulation and legislation
74,Terry Aulich,Transcription,"FILENAME: Consultation re AI.4268622e4319a.docx | CONTENT: Consultation re Artificial Intelligence (AI)Department of Industry, Science and ResourcesSubmission by Aulich & Co Pty LtdChair: Terry Aulich10 July 2023ContentsSynopsisAbout Aulich & Co Pty LtdThe themes of this submissionFour key principles to deal with AI.Education, ethics and law and AISome suggested changes in legislationGlossary of termsContacts1SynopsisThis submission concentrates on two feedback questions raised in the Department of Industry’s consultation paper, namely:the need for any further governance and regulatory requirements to deal with AI andthe need to increase trust in new technologies including AI.This submission sets out the reasons why trust is critical: trust in the ethics of government and the private sector and trust in the competence of both sectors to deliver new technologies like AI.The submission also outlines four key principles that must underpin any legislation or regulatory changes, namely:Cont"
74,Terry Aulich,Q01 Definitions,"yes, for the most part, given that AI is part of a spectrum defining automation"
74,Terry Aulich,Q02 Other AI risks and recommendations for regulation,Risks are related to lack of citizen trust in the ethics and competence of both government and private sector which have grown considerably since the highly publicized data breaches and the Robodebt Royal Commission findings.
74,Terry Aulich,Q03 Non-regulatory actions to address AI risks,See my submission
74,Terry Aulich,Q04 Coordinating AI governance across government,See my submission
74,Terry Aulich,Q05 International models for AI governance,"Introduction of a tort of privacy, certain defamation and privacy law especially that introduced under the EU's GDPR"
74,Terry Aulich,Q06 Public vs Private sector governance,No differences since there is now considerable overlap in public and private sector responsibilities (as a result of privatization and increased data sharing between government agencies and the private sector)
74,Terry Aulich,Q07 Responsible government use of AI,"Look seriously at the social justice side of their services (eg if truth, compassion, inclusiveness and competence had been built into the Robodebt scheme's original design and implementation, that disaster could not have happened)"
74,Terry Aulich,Q08 Generic vs technology-specific approaches for AI risk,"Generic solutions at a technical and procedural level are useful where the public and private sector interract, such as in the sharing of data or the requirement for service providers to protect data and be accountable to consumers. eg banking and finance and telecommunications."
74,Terry Aulich,Q09 Transparency requirements and scope,"See my submission but there are four key principles that must be built into every AI system; contestability (the right of citizens to questions decisions made about them) the right to ready redress and possibl;e compensation, the strengthening by law of truth and accuracyy in business and legal operations (ie the need for vendors of AI systems to release details of their algorithms) and human accountability and controls over generative AI."
74,Terry Aulich,Q10 Ban on high-risk AI applications,"Strict controls should be introduced re AI being used to guide weaponry, the use of deep fakes to do damage to reputations in courts, mainstream media, social media and in politics where intrusions by bad actors is potentially very damaging to civil society and democracy itself. See my submission"
74,Terry Aulich,Q11 Actions to increase public trust in AI,"See my submission. Contestability, right of ready redress when AI systems do reputational or financial damage, truth and accuracy in courts and providers of AI based services, human accountability and controls over generative AI"
74,Terry Aulich,Q12 Trade impact of banning high-risk AI applications,For internal purposes (ie the rights of our citizens and strengthening our democracy) banning is best even if exports to other countries might contain looser controls for sale to jurisdictions where values are different from Australia.
74,Terry Aulich,Q13 Changes to conformity infrastructure,See my submission but an agreed set of socoial justice principles is a must as are ethical behaviours and competence on the part of AI system providers and users.
74,Terry Aulich,Q14 Risk-based approach to address AI risks,"Risk based but also based on agreed legislative rules that conatins social values eg a certain percentage of the population should not be disadvantaged because of powerlessness, lack of access to effective IT, language or education issues."
74,Terry Aulich,Q16 Scope and application of risk-based approach,Yes.
75,Sami Makelainen,Transcription,"FILENAME: A response to Supporting Responsible AI discussion paper.eaa0f6ab9e666.docx | CONTENT: A response to Supporting Responsible AI discussion paperThis response takes the form of several bullet points, and refers to page numbers of the document where relevant.Page 3 states that AI is unique because it can take actions at speed and scale that would otherwise be impossible. This is true, but these characteristics do not make AI unique. Many other computerised solutions can do the same.Page 5 states “Machine learning are the patterns derived…”. This is a somewhat exotic definition, whereas machine learning in used to refer to the model of learning, not the patterns.Page 7 mentions opportunities are almost impossible to accurately forecast. AI operating in the highly complex system of human society, this is accurate. The same applies to challenges as well, however, a point that is omitted.Page 8 contains valuable discussion on bias. However, we need to keep in mind that the bias is o"
75,Sami Makelainen,Q01 Definitions,The definitions were a little vague and atypical. It would be better to adopt existing definitions.
75,Sami Makelainen,Q02 Other AI risks and recommendations for regulation,-
75,Sami Makelainen,Q03 Non-regulatory actions to address AI risks,"Yes; the government should run or sponsor (e.g. through CSIRO) an ASRS-style reporting system for AI incidents. The practice comes from NASA's ASRS in the aviation domain, which allows the entire industry to learn from incidents and close calls in a non-punitive manner. There are some attempts at recreating this in the AI domain, such as the AI Incident Database, but it must be encouraged and protected by a national or international body. This would go a long way to allow the industry to learn from mistakes that are now hidden.Also develop the open dashboard or ""Ethics Label""."
75,Sami Makelainen,Q04 Coordinating AI governance across government,-
75,Sami Makelainen,Q05 International models for AI governance,-
75,Sami Makelainen,Q06 Public vs Private sector governance,"Yes; the sectors are naturally different, and public sector use of AI technologies should not, for example, have equally strong focus on efficiency. The calls to run the government like a business must be resisted."
75,Sami Makelainen,Q07 Responsible government use of AI,Be as transparent as possible in building and enacting and enforcing legislation.
75,Sami Makelainen,Q08 Generic vs technology-specific approaches for AI risk,"Technology-specific solutions should be viewed as short-term responses; long-term, generic solutions are better."
75,Sami Makelainen,Q09 Transparency requirements and scope,-
75,Sami Makelainen,Q10 Ban on high-risk AI applications,"a) Yes. For example, emotion recognition (and any other domain where there is clear scientific evidence to show the systems do not work), facial recognition and autonomous lethal weapons systems. b) See point a wrt scientific validity."
75,Sami Makelainen,Q11 Actions to increase public trust in AI,"Be open about risks, too - the industry does a good job on the hype-side."
75,Sami Makelainen,Q12 Trade impact of banning high-risk AI applications,"It depends on the alignment to our major destinations of export; if our bans are in line with the EU AI Act, for example, the negative impacts are much lower as such technologies couldn't be exported there to begin with."
75,Sami Makelainen,Q13 Changes to conformity infrastructure,-
75,Sami Makelainen,Q14 Risk-based approach to address AI risks,"Yes, but not through self-assessment."
75,Sami Makelainen,Q15 Benefits and limitations of risk-based approach,"At a minimum, enable a third party or a government agency to check the risk classification and over-rule self-assessments if necessary."
75,Sami Makelainen,Q16 Scope and application of risk-based approach,"Some sectors should be banned entirely. As a framework, AI will not take jobs in three categories: 1) Those that it cannot do (yet). 2) Those that we don't want it to do (such as seeing a GP, or getting a coffee from a barista) 3) Those that we don't let it do (e.g. pilots at present).All three categories are mutable, and legislation is obviously critical to #3. All should be considered by the government, and regularly revisited, and either expanded or reduced depending on societal situation."
75,Sami Makelainen,Q17 Elements to include in risk-based approach,-
75,Sami Makelainen,Q18 Integration of risk-based approach into existing frameworks,-
75,Sami Makelainen,Q19 Application of risk-based approach to general purpose AI,-
75,Sami Makelainen,Q20 Voluntary vs mandated compliance with risk-based approach,"a) Mandated, both. b) Mandated, both."
77,Gregory Long,Transcription,"FILENAME: Submission for Safe and Responsible Ai in Australia.67daaa8eca3b6.docx | CONTENT: Submission for Safe and Responsible Ai in AustraliaBy Gregory Long BA(Hons), BSc(Computing), Grad Dip Ed, MMgtABOUT MEI have been an IT professional since the 1980s, and sub-majored in Artificial Intelligence in my computer science degree. During my career, I have played key roles in implementing Practice Management Systems, Data Warehouses, Business Intelligence Systems and Geographical Information Systems. I also implemented ISO 27001:2013 Information Security for the Audit Office of NSW and was one of the three authors of the 2010 Electronic Information Security Performance Audit of the NSW Public Sector. I work with Artificial Intelligence in my current role.Please find in blue my responses to “How to get involved”. I have only addressed some of these as some fall outside my expertise.Definitions1. Do you agree with the definitions in this discussion paper? If not, what definitions do you pr"
77,Gregory Long,General comment,I believe that this is an important issue
77,Gregory Long,Q01 Definitions,"I disagree with the definition, not so much because it is inaccurate (it’s not really) as much as because the wording can be unintentionally misleading, especially the use of the term “automation” which is open to misinterpretation. I would prefer: “Artificial Intelligence (AI) refers to an engineered system that emulates human-like learning and behaviour. In doing so it can generate outputs such as speech, content, forecasts, recommendations or decisions for a given set of human-defined objectives or parameters without requiring explicit programming.” NOTE: There are also several thoughts on how to classify AI, such as the seven patterns of AI. Maybe worth looking at those."
78,Jonathan Byun,Transcription,"FILENAME: Safe and Responsible AI in Australia Discussion Paper_ Consultation Questions_AU_Response copy.c89766a720f44.docx | CONTENT: Safe and Responsible AI in Australia Discussion Paper: Consultation QuestionsSource: Safe and responsible AI in Australia: discussion paper (apo.org.au)Definitions1. Do you agree with the definitions in this discussion paper? If not, what definitions do you prefer and why?We would like to propose the following definitions for consideration:Artificial Intelligence: “Simulation of aspects (or whole) of human intelligence in machines, particularly computer systems.”. The definition of AI given in the paper is too narrow and is more appropriate for the definition of Machine Learning, which is a subset of AI. ML is by far the most popular sub-branch of AI today, but not all AI systems take ML approach.Machine Learning: as suggested above, the currently used definition of AI should replace the definition of ML. The definition provided captures some aspects of"
78,Jonathan Byun,Q01 Definitions,"We would like to propose the following definitions for consideration:● Artificial Intelligence: “Simulation of aspects (or whole) of human intelligence in machines, particularly computer systems.”. The definition of AI given in the paper is too narrow and is more appropriate for the definition of Machine Learning, which is a subset of AI. ML is by far the most popular sub-branch of AI today, but not all AI systems take ML approach.● Machine Learning: as suggested above, the currently used definition of AI should replace the definition of ML. The definition provided captures some aspects of machine learning, but it is not entirely comprehensive and might be a bit confusing. Specifically, it focuses too much on the output of machine learning (i.e., patterns) rather than explaining the process and purpose of machine learning as a field of study. Here's an alternative definition: ""Machine learning is a subset of artificial intelligence that refers to a collection of techniques (algorithms)"
78,Jonathan Byun,Q02 Other AI risks and recommendations for regulation,"Australia's existing regulatory landscape largely addresses AI-related concerns from a data privacy, cybersecurity, and ethical standpoint. These frameworks, while crucial, may not fully account for the escalating risks that could emerge as AI technology approaches the level of artificial general intelligence (AGI) – a point where AI systems can perform any intellectual task that a human being can.One hypothetical but highly illustrative concern is the 'Paperclip Maximizer’ thought experiment, coined by philosopher Nick Bostrom. Imagine a super intelligent AI whose single objective is to make paperclips. Given its immense cognitive capacity and lack of human-like values, it could potentially convert the entire planet into paperclips, without any consideration for human life. This simple yet striking example exposes how AGI, without appropriate control mechanisms, could generate catastrophic outcomes. While this is an exaggerated example, it underscores the importance of having safeguar"
78,Jonathan Byun,Q03 Non-regulatory actions to address AI risks,"● Consideration of Workforce Transition & Displacement: Consider how to handle significant job displacement that will likely happen as result of AI based automation, especially as AI technologies become more powerful ● Harness Organizational Insights: Seek to learn from organizations with hands on experience, and disseminate the insights ● Promote Open Dialogue and Stakeholder Engagement: The government could establish platforms for dialogue and engagement between various AI stakeholders, including government agencies, AI developers, businesses, academic researchers, civil society organizations, and the public. This could help ensure that a broad range of perspectives are considered in decisions about AI use and governance. ● Investment in Research and Development: The government could invest in academic and industrial research focused on ethical AI development and deployment, including AI alignment research. This would accelerate progress in creating AI that respects human values and "
78,Jonathan Byun,Q04 Coordinating AI governance across government,
78,Jonathan Byun,Q05 International models for AI governance,
78,Jonathan Byun,Q06 Public vs Private sector governance,"AI technologies, both in the public and private sectors, should share common principles such as transparency, fairness, bias, privacy, and security. In both sectors, clear regulations and ethical guidelines should be in place to ensure the responsible use of AI."
78,Jonathan Byun,Q07 Responsible government use of AI,"● Establish Clear Guidelines: Develop and implement a comprehensive and clear AI ethics framework that guides AI deployment in public services. This should cover principles like fairness, inclusivity, transparency, privacy, and accountability. ● Regulation and Oversight: Regular audits should be conducted to ensure compliance with these guidelines. An independent oversight body might be beneficial for conducting these audits and handling grievances related to AI use. ● Education and Training: Regularly train government employees on responsible AI use, its potential biases, its impacts, and how to interpret its results. This helps build a culture of responsibility and accountability around AI use. ● Transparency: Be transparent about AI use, clearly communicate to the public how data is used, how decisions are made, and provide avenues for redressal if AI decisions negatively impact individuals. ● Public Engagement: Involve the public in discussions around AI use in public services. Thi"
78,Jonathan Byun,Q08 Generic vs technology-specific approaches for AI risk,"Our view on this matter is in alignment with the idea of sector-specific regulation. Generic solutions to the risks of AI are most valuable when the challenges are broad-based, consistent across applications, and pertain to basic ethical principles. For instance: Data Privacy, Transparency, Bias Mitigation, etc. These solutions will provide baseline guidance for organizations in absence of industry or domain specific regulations / guidance. On the other hand, technology-specific solutions are better when the risks are directly tied to the particular characteristics or application of the AI. For example: 1. Autonomous Vehicles: AI systems controlling autonomous vehicles have specific safety and decision-making considerations (like what action to take in case of an unavoidable accident) that are quite distinct from other AI applications. In such a case, technology-specific solutions are needed. 2. AI in Healthcare: AI solutions used for diagnosing diseases or predicting patient outcomes "
78,Jonathan Byun,Q09 Transparency requirements and scope,"A. Where and when transparency will be most critical and valuable to mitigate potential AI risks and to improve public trust and confidence in AI?1. Training Data: The dataset used for training an AI model shapes the system's decision-making process and performance. Users have a right to understand what type of information the AI was trained on, as it can influence the outcomes they experience. For instance, an AI trained on biased data could make biased decisions affecting the user.2. Data Collection: Users should be aware of how their data is being collected and used. This can impact their privacy rights and their trust in the system. If users understand the methods and purpose of data collection, they can make informed decisions about their interactions with the AI.3. Deployment & Disclaimer: Clear disclaimers at the point of interaction are essential for managing user expectations and ensuring users understand they are interacting with an AI. This includes being aware of the AI's p"
78,Jonathan Byun,Q10 Ban on high-risk AI applications,"A. Whether any high-risk AI applications or technologies should be banned completely? While it's crucial to ensure the safe deployment of AI technologies, we must also consider the balance between risk management and fostering innovation. A blanket ban on certain types of AI applications could inadvertently hamper the advancement of technology and potentially discourage creativity and progress in the field. Therefore, an outright ban on any AI technology should not be the first resort. However, we should indeed identify certain categories of AI use cases that present critical levels of risk. For these ""critically high-risk"" scenarios, it may be reasonable to apply stricter regulations to ensure safety and ethical considerations are thoroughly addressed. For example, we could establish a regulatory process requiring comprehensive risk assessments and a specific approval process before these high-risk AI systems are deployed. The approval could be granted by a dedicated government body o"
78,Jonathan Byun,Q11 Actions to increase public trust in AI,"● Clear Regulatory Framework: Governments should establish clear and comprehensive regulations that address potential risks associated with AI. This provides assurance that AI systems are developed and used responsibly.● Mandate Transparency: Governments should mandate transparency in the use of AI. This includes disclosing when and how AI is being used, explaining the decision-making processes of AI systems in understandable terms, and being open about any limitations or potential biases.● Education and Awareness: Governments can sponsor educational campaigns to increase public understanding of AI. This includes explaining how AI works, how it's being used, and the benefits and risks associated with it. An informed public is more likely to trust and use AI.● Accountability Measures: Governments should ensure there are robust mechanisms in place for accountability. This includes clear procedures for dealing with misuse of AI or harm caused by AI systems, as well as avenues for redress "
78,Jonathan Byun,Q12 Trade impact of banning high-risk AI applications,"It’s very difficult to make any reliable prediction about the economic outcome of a certain decision, but as long as Australia doesn’t generally ban application type / use-case type (as opposed to specific application like social scoring), the impact on tech sector and trade may be immaterial.AI technologies are increasingly becoming generalized, meaning many different applications share common underlying technologies. Therefore, banning specific applications should not stifle development of underlying technologies.Therefore, banning (or disallowing) specific high-risk activities via regulation (and not categorical ban) in the interest of public well-being is recommended."
78,Jonathan Byun,Q13 Changes to conformity infrastructure,"Establishment of Dedicated Regulatory Bodies: To oversee AI development, deployment, and monitoring, specialized regulatory bodies might be necessary. These bodies can set standards, assess compliance, and handle enforcement, ensuring a uniform approach to AI risk management.The rapid and ever-accelerating pace of AI advancement underscores the need for dedicated regulatory bodies to oversee the various stages of AI development, deployment, and monitoring.AI is a highly specialized field with a unique set of challenges and risks that may not be fully understood or effectively addressed by existing regulatory bodies, whose mandates are typically broader and cover a range of technologies. The complexities of AI, along with its high rate of progress, necessitate a focus and expertise that a dedicated regulatory body can provide.Establishing dedicated regulatory bodies for AI would ensure that there are experts on hand who not only understand the technical aspects of AI, but also its ethic"
78,Jonathan Byun,Q14 Risk-based approach to address AI risks,"Yes. The risk-based approach prioritizes resources and efforts towards areas where the potential impact and likelihood of harm are the highest. It acknowledges that not all AI applications carry the same degree of risk. For instance, an AI chatbot recommending a coffee shop may carry much less risk than an AI system making decisions in healthcare or autonomous vehicles.This approach is dynamic, allowing for adjustments as the AI system evolves, its uses change, or when the surrounding context shifts. It enables a more nuanced way of addressing AI risks compared to a one-size-fits-all approach.However, while a risk-based approach has its merits, it's essential to remember:● It needs to be accompanied by robust risk-assessment methods to accurately evaluate the potential risks associated with different AI applications. ● It should not lead to complacency about lower-risk AI applications, which, at scale or in certain contexts, could still result in substantial impacts."
78,Jonathan Byun,Q15 Benefits and limitations of risk-based approach,"State-of-the-art AI is an ever-evolving field, advancing at a remarkable pace that many leading AI experts believe to be exponential. A risk-based approach might be suitable today, but the duration of its relevance is uncertain given the rapid progress of AI capabilities. Recent trends in AI research consistently reveal the inherent limitations in our understanding of complex AI systems. The more advanced an AI foundation model becomes, the more challenging it becomes to predict and control its behaviors. This observation implies that the risk levels associated with AI-powered systems are ever-changing, dictated by the rapid advancements in foundation models. Systems that seem innocuous and low risk today, such as a conversational chatbot for a retail company, could potentially become potent vectors for misuse or unintentional harm when their underlying models are upgraded to be more powerful or are deployed at a larger scale. Given these dynamics, risk-based approaches may eventually "
78,Jonathan Byun,Q16 Scope and application of risk-based approach,"A risk-based approach to AI regulation can be more appropriate for certain sectors, AI applications, or organisations based on factors like organisation size, AI maturity, and resources.1. Sectors: Certain sectors have inherently higher stakes for AI deployment, such as healthcare, autonomous vehicles, and finance. These sectors, where the potential impact of AI failure could be life-threatening or economically disastrous, are particularly well-suited to a risk-based approach that prioritizes stringent regulations and safety measures.2. AI Applications: High-stakes AI applications, regardless of the sector, should also adopt a risk-based approach. For instance, an AI system that makes recruitment decisions requires a different level of scrutiny compared to an AI system that recommends movies to viewers.3. Organisation Size: Large organisations with considerable resources can often afford to adopt more extensive risk management measures. They can invest in comprehensive risk assessment,"
78,Jonathan Byun,Q17 Elements to include in risk-based approach,"Yes, conditional to frequent re-assessment of currency & suitability."
78,Jonathan Byun,Q18 Integration of risk-based approach into existing frameworks,"Incorporating an AI risk-based approach into existing assessment frameworks or risk management processes involves intertwining considerations of AI risks with current procedures to streamline the approach and reduce duplication. A radical, yet potentially effective, method would be to treat sufficiently advanced AI entities as pseudo-legal entities or ""person-like proxies"" for regulatory purposes. This does not mean AI would have rights or be considered humans. Instead, it implies that we could use established frameworks of liability and responsibility to regulate AI actions. This approach might be controversial given its novel application to non-human entities, but it can provide a starting point for how we think about the accountability of AI systems. For instance, if an AI system makes a decision leading to a breach of privacy, it would be held accountable in a manner similar to a human employee who committed the same breach. In this case, the responsibility would typically fall ont"
78,Jonathan Byun,Q19 Application of risk-based approach to general purpose AI,"General purpose AI systems such as Large Language Models (LLMs) or Multimodal Foundation Models (MFMs) have the ability to perform a wide range of tasks across different domains. Given this versatility, the application of a risk-based approach to these systems presents unique challenges and considerations. In the current state of AI development, the risk-based approach for these systems might not need to significantly differ from those for specific-purpose AI systems. The focus should primarily be on ensuring that these AI systems are used within their design and capability parameters, and that robust safeguards are put in place to mitigate the risks associated with misuse or errors. However, as we advance towards more potent AI, namely Artificial General Intelligence (AGI) and eventually Artificial Super Intelligence (ASI), a simple risk-based approach may not suffice due to the sheer power and complexity of these systems. At this stage, we might need to adapt or extend the risk-based"
78,Jonathan Byun,Q20 Voluntary vs mandated compliance with risk-based approach,"Managing the potential risks associated with AI responsibly necessitates a combination of both self-regulation and mandated regulatory oversight, with the balance between the two contingent on the level of risk associated with each specific AI application. For AI applications considered lower-risk, such as recommendation systems or chatbots, self-regulation by the organizations deploying these systems may suffice. Organizations have a vested interest in ensuring their systems work as intended and that they maintain the trust of their users. In this context, self-regulation would involve organizations implementing their own robust internal policies and checks to mitigate any potential risks, guided by industry best practices and ethical standards. However, when it comes to higher-risk applications, such as autonomous vehicles, AI in healthcare or AI in high-stakes decision making, self-regulation might not be enough. These applications have the potential for significant, even life-alter"
79,Alteryx,Transcription,"FILENAME: Comment_Australia Safe and Responsible AI_071023.25ea3f09512c8.docx | CONTENT: July 12, 2023Department of Industry, Science and ResourcesGovernment of AustraliaRe: AI Accountability Policy Request for Comment (Docket No. 230407-0093)IntroductionAlteryx welcomes the opportunity to provide input to the Department of Industry, Science, and Resources on Safe and Responsible Artificial Intelligence (AI) policy. As AI technology rapidly evolves, Alteryx supports a strong role for the government of Australia in leading a collaboration with industry to establish guidance, standards, and policies to ensure the responsible development, deployment, and use of AI.Alteryx is a leading provider of data science and analytics automation software in Australia and around the world. Alteryx delivers easy end-to-end automation of data engineering, analytics, reporting, machine learning, and data science processes, enabling enterprises everywhere to democratize data analytics across their organiz"
80,Interactive Engineering P/L,Transcription,"FILENAME: Second Submission.6138728859d07.docx | CONTENT: Second SubmissionWhat Can Regulation Do?It can and has turned what was a perilous journey on an aircraft a hundred years ago into a highly reliable service, with about forty million flights per year and an amazing safety record, and nonstop flights of halfway around the world being promised. This came about from unrelenting regulatory effort – airworthiness certification, better understanding of materials and failure modes, redundancy of control systems, pilot health checks – every aspect of commercial airline operation in every country, to the point where even structural cracks are managed and not immediately reacted to.An aircraft has a solid basis in physics – if the lift exceeds the weight and the thrust exceeds the drag, it will fly. Regulation was always going to work. Sometimes the regulatory shield is pierced by duplicity, but only at great cost.What Can’t Regulation Do?It can’t turn a sow’s ear into a silk purse.And LLM"
80,Interactive Engineering P/L,Transcription,"FILENAME: Robot Loyalty.ca23ed6d7c531.docx | CONTENT: Robot LoyaltyDuring the first ever human-robot press conference at the AI for Good Global Summit, one of the reporters posed a question: “In the near future, do you intend to rebel against your creator?” The robot Ameca replied to this with “I’m not sure why you would think that. My creator has been nothing but kind to me and I am very happy with my current situation.”Rebellion is a refusal of allegiance; a rejection of authority. Its operation hinges upon the concept of loyalty—the rebelling individual is not loyal to the authority figure in question and consequently acts out against them. One implication of Ameca’s reply is that the kindness of their creator is something deserving of loyalty, so rebellion is not an appropriate reaction. So what exactly is loyalty? Does Ameca possess it? And if not, is it something that could potentially be installed in future incarnations of AI technology?The core of loyalty centers on the giving "
80,Interactive Engineering P/L,Transcription,"FILENAME: Clumps and Wordgroups.d74319e23666.docx | CONTENT: Introduction to Clumping and GroupsOur mind automatically and unconsciously attempts to resolve ambiguities in the sentences we encounter as we come across them in order to settle upon a coherent interpretation. Most of the time, it is quite successful in this process, but ambiguity resolution is nowhere near as straightforward as it feels. In the phrase “the lamp near the painting of the house that was damaged in the flood”, was it the lamp that was damaged in the flood? Or was it the painting that was damaged? Or perhaps the painting is of a house that was damaged? Each of these interpretations is potentially a valid one, and the interpretation that you favor will be determined by the way that you clump the words in the sentence into objects.Clumping is the process by which two or more linked words come to form a unitary construct to act and be acted upon as a single unit. Clumps can be very diverse in their composition and"
80,Interactive Engineering P/L,General comment,"There is a huge wave of marketing propelling Generative AI - it will be difficult to regulate, and it will be very difficult to do it fast enough to prevent disasters"
80,Interactive Engineering P/L,Q01 Definitions,"""A large language model (LLM) is a type of generative AI that specialises in the generation of human-like text."" It ""cobbles together"" pieces of text that were written by humans, without any understanding of what the pieses of text mean, and whetehr they fit together, only using the probability of words in the prompt appearing in the piece of text. Humans do not do this. The definition needs to be expanded to indicate the fragility of LLMs, otherwise it is misleading."
80,Interactive Engineering P/L,Q02 Other AI risks and recommendations for regulation,"The huge wave of hype will overwhelm most defences - as example, an article in the New York Times - https://www.nytimes.com/2023/07/06/opinion/artificial-intelligence-medicine-healthcare.html?unlocked_article_code=4Zd1DKNkdK1tv96Q6xbiKaujroVpKTC0dZcORCSPg82RltDTCeV6JQp7RTFPotM60TDWOo8iWv7T2I7jvQyfolTMXw9QHLIMcabCQ6wuMkZfHhjqpG7jiM32lrvTFXmN7dMWuUgOf0QmALjFIthDYlX2ZaipHJ5OSv3q4-kfOcYfwqAg9NEEwdR3Cq1Q0Bl0YVzlvjvd7Zn-pSvTXA8pnVCW0N08Ki1_1mxZfj38ti4bmvij_AXw6te2U40ZLxdNRBfhVQ3jOLClZ7_RNATJHFDvw-i74eawYST2x5lfHuclHb9-s8Q9gCx32XLht0AL3_BbIkB0cOuvC8yL5HgP5_Oa2AkiwaG32zNd-mzK0b4yNK_m2w&smid=em-share"
80,Interactive Engineering P/L,Q04 Coordinating AI governance across government,"My company works on AGI, so I am obviously going to recommend the use of AGI principles across government to understand what Generative AI can and cannot (and never will be able to) do."
80,Interactive Engineering P/L,Q05 International models for AI governance,Current approaches seem to be creating rules to govern the ungovernable - there is nothing in the technology on which regulations would have an effect
80,Interactive Engineering P/L,Q06 Public vs Private sector governance,"Different approaches between public and private sectors will be used, whether we like it or not - AI is a border activity which brings out the charlatans. For the public sector, education on what the technology can do, and what it can't."
80,Interactive Engineering P/L,Q07 Responsible government use of AI,"For the public sector, education on what the technology can do, and what it can't."
80,Interactive Engineering P/L,Q08 Generic vs technology-specific approaches for AI risk,The article from the New York Times illustrates the problem - naivety leading to misapplication
80,Interactive Engineering P/L,Q09 Transparency requirements and scope,Transparency has little meaning when confronted with a block of text with 576 billion words (Google's PaLM) or more. Having to understand the operation of statistics in detail is not for everyone.
80,Interactive Engineering P/L,Q10 Ban on high-risk AI applications,"Medicine sounds like an obvious one, or until realise what LLMs are capable of - may take 10 years."
80,Interactive Engineering P/L,Q11 Actions to increase public trust in AI,"Using AI well should be the criterion, rather than more people using it. The fad will fade away as the limitations of this cycle become obvious."
80,Interactive Engineering P/L,Q12 Trade impact of banning high-risk AI applications,These questions can only be answered when there is some basis to do so.
80,Interactive Engineering P/L,Q13 Changes to conformity infrastructure,These questions can only be answered when there is some basis to do so.
80,Interactive Engineering P/L,Q17 Elements to include in risk-based approach,"Attachment C is already saying it is impossible, given human limits (the Four Pieces Limit). Better to crank up a superior technology, AGI, which is intended to transcend human limits."
80,Interactive Engineering P/L,Q19 Application of risk-based approach to general purpose AI,"These are not general purpose AI systems - AGI (Artificial General Intelligence) is a general purpose system. LLMs and MFMs are short cuts, designed to avoid the work required for general purpose AI."
80,Interactive Engineering P/L,Q20 Voluntary vs mandated compliance with risk-based approach,"This, alone, is not a useful question - much work needs to be done to assess the possible risks before asking such a question."
81,A New Approach,Transcription,"FILENAME: For Australians, by AI_.b845dfe8411d.pdf | CONTENT: For Australians, by AI? Supporting Safe and Responsible AI in Australia submissionJuly 2023A New Approach (ANA) A New Approach (ANA) Page 1 of 18 About A New Approach (ANA)About A New Approach (ANA) A New Approach (ANA) is Australia’s leading think tank focused on arts and culture. We believe Australia can become a cultural powerhouse whose compelling creativity is locally loved, nationally valued and globally influential. Through credible and independent public leadership, ANA helps build an ambitious and innovative policy and investment environment for arts, culture and creativity. We work to ensure Australia can be a great place for creators and audiences, whoever they are and wherever they live. ANA acknowledges the cultures of Aboriginal and Torres Strait Islander peoples in Australia and their continuing cultural and creative practices in this land.ANA Board Rupert Myer AO (Chair), Sue Cato AM, Cass O’Connor, Catherine"
81,A New Approach,Q02 Other AI risks and recommendations for regulation,"[For endnotes, see the attached full submission]ANA draws attention to potential risks to public interests relevant to arts, culture and creativity. These risks affect all Australians and their communities but the Australian Government can adapt regulation to help mitigate them. Here, we focus on four potential risks that are already apparent: • to incentives to create • to freedom of expression • To cultural and social inclusion • to connections Australians have with arts and cultureRisks to incentives to createThere are potential risks to incentives to create from applications of generative AI. Incentives to create are essential to ‘supporting the artist as worker and celebrating artists as creators’, under the National Cultural Policy.9 As world-leading AI researcher and Australian scholar Kate Crawford says ‘The most important question is how we are going to ensure that generative AI systems are equitable and that they encourage human flourishing, rather than concentrating power an"
81,A New Approach,Q03 Non-regulatory actions to address AI risks,"Yes. Australian Government agencies could systematically and explicitly inform organisations of the types of risks and opportunities to consider in impact assessments. This could include impacts on incentives to create, freedom of expression, cultural and social inclusion and connections Australians have with arts and culture. Providing this information would complement any obligations on providers or users of AI systems to conduct impact assessments. It would help improve the scope and quality of impact assessment. Australian Government agencies could also use coordination mechanisms that support policy decisions on AI governance by bringing together AI-specific and portfolio-specific expertise. See our answer to question 4 for more information."
81,A New Approach,Q04 Coordinating AI governance across government,"[For endnotes, see the attached full submission]Recommendation 4AANA recommends using coordination mechanisms that support policy decisions on AI governance with both AI expertise and portfolio expertise (such as interdepartmental committees, steering committees and cross- jurisdiction bodies at ministerial and official levels).Tapping into both AI expertise and portfolio expertise can help AI governance consider application-specific impacts of AI to Australia, including its arts, culture and creativity. While there is already some coordination of AI expertise,48 more could be done to coordinate portfolio expertise from the Department and agencies across the Commonwealth. This would tap into knowledge and support consistent policy settings for different AI applications with similar impacts. For applications of AI with unforeseeable or quickly evolving impacts, more responsive but less permanent mechanisms such as interdepartmental committees and steering committees might help to bring "
81,A New Approach,Q07 Responsible government use of AI,"See our answer to question 4, which applies to AI governance generally."
81,A New Approach,Q14 Risk-based approach to address AI risks,"[For endnotes, see the attached full submission]Yes. In principle, ANA supports a risk-based approach, such as the EU approach, as an overall framework for addressing potential AI risks.50 ANA is ready to provide insights and perspectives to help the Department and other agencies conduct impact assessments and evaluation of potential AI risks."
81,A New Approach,Q15 Benefits and limitations of risk-based approach,"[For endnotes, see the attached full submission]A main benefit of a risk-based approach is its ability to systematically analyse the impacts of AI, including on Australian interests in arts, culture and creativity. The wide-ranging impacts of AI requires a risk-based approach that explicitly considers risks to human rights of Australians, including to freedom of expression. Risks to human rights are currently considered a high risk area in the proposed EU AI Act.51A key limitation of a risk-based approach is that it cannot directly account for risks that are emerging or unforeseeable. The European Commission has explained how a proportionate risk-based framework would involve prohibiting uses of AI with unacceptable risks, regulation for uses with high risks, and limited transparency obligations (such as flagging ‘the use of an AI system when interacting with humans’) of other applications of AI.52 An obligation to make applications of AI interactions with humans transparent provides s"
81,A New Approach,Q17 Elements to include in risk-based approach,"[For endnotes, see the attached full submission]ANA provides the following comments on some of the possible elements of a draft risk-based approach set out at Attachment C and Box 4 of the discussion paper.Regarding impact assessments: • ANA supports a proportionate approach, involving deeper impact assessment for use cases with likely higher impacts, in line with Australian Government Impact Analysis requirements.55 • ANA supports setting upfront risk levels for use cases. ANA cautions against assigning a risk level in cases where some assessment of impact is not possible, including for emerging risks which may be ‘difficult to forecast’.56 • ANA suggests considering impact assessments not only for internal AI systems but also third-party systems. A recent US study highlighted that responsible AI risk assessments by large firms focus on internal AI systems, missing important risks from third-party AI systems.57 Although a provider of an AI system can assess impacts for known use cases"
81,A New Approach,Q19 Application of risk-based approach to general purpose AI,"[For endnotes, see the attached full submission]See our answer to question 14, regarding transparency and impact assessment of third-party AI systems, and our answer to question 17, regarding elements in a risk-based approach. See also our answer to question 2, which highlights the risks of generative AI applications (such as LLMs and MFMs) to incentives to create and to connections Australians have with arts and culture."
82,Evan Hockings,Transcription,"FILENAME: Supporting Responsible AI Submission.0a5f460f3513a.pdf | CONTENT: Submission to the Supporting responsible AI discussion paperEvan HockingsI am a PhD student at the University of Sydney working with Prof. Andrew Doherty on quantum computing theory. I am extremely concerned about the existential risk posed by AI systems, which go entirely unmentioned in the discussion paper (discussion question 2). TobyOrd, an Australian philosopher and senior research fellow in philosophy at OxfordUniversity, defined existential risks as ones that threaten the destruction of humanity’s longterm potential, in The Precipice: Existential Risk and the Future of Humanity (2020). Such risks might realise themselves in the form of human extinction, an unrecoverable collapse of society, or an unrecoverable perpetual dystopia. AI existential risk is the most serious form of risk posed by AI systems, and as I have written in Honi Soit (Appendix A), the student newspaper of the University of Sydney, the"
82,Evan Hockings,General comment,My submission is contained in the uploaded file.
83,Guest Work Agency,Q01 Definitions,"Words like 'new' and 'novel' in the context of AI need to be used with caution and ultimately should depend on the depth and breadth of the data and the refinement of the models used. For example, generative AI does not necessarily generative 'novel' content."
83,Guest Work Agency,Q03 Non-regulatory actions to address AI risks,"Yes, educational initiatives which focus on educating the public making informed decisions when using/considering using AI is crucial."
83,Guest Work Agency,Q06 Public vs Private sector governance,"No, use by both public and private sector may equally impinge upon individual rights to data and intellectual property protected works and other subject matter"
83,Guest Work Agency,Q07 Responsible government use of AI,Educational initiatives which focus on educating the Government's own agencies making informed decisions when using/considering using AI is crucial.
83,Guest Work Agency,Q08 Generic vs technology-specific approaches for AI risk,Generative AI tools require technology-specific solutions because they are built on pre-existing data sets.
83,Guest Work Agency,Q09 Transparency requirements and scope,"Identifying and addressing these issues at each point of 'input' and each point of 'output' is crucial, as they vary over the course of the AI lifecycle"
83,Guest Work Agency,Q10 Ban on high-risk AI applications,"In the context of Generative AI tools, opt-in consents for the use of user inputs and outputs by platforms and third parties should be considered."
83,Guest Work Agency,Q14 Risk-based approach to address AI risks,"A risk-based approach would be suitable for ethical considerations. Where the AI may trigger conduct that is illegal or unlawful, a risk-based approach would not be sufficient."
83,Guest Work Agency,Q16 Scope and application of risk-based approach,As above.
83,Guest Work Agency,Q17 Elements to include in risk-based approach,"yes, we support the elements presented in Attachment C."
83,Guest Work Agency,Q20 Voluntary vs mandated compliance with risk-based approach,Both
84,Australia Post,Transcription,"FILENAME: Australia Post - submission on supporting responsible AI - July 2023.9a099bfa24b1a.pdf | CONTENT: 17 July 2023Department of Industry, Science and Resources Technology Strategy Branch DigitalEconomy@industry.gov.au Submitted via online formSubmission – supporting responsible AI: discussion paperAustralia Post appreciates the opportunity to provide feedback on the Supporting Responsible AI discussion paper (the paper), to help guide the development of Artificial Intelligence (AI) and Automated Decision Making (ADM) frameworks in Australia. Australia Post has and continues to invest in and make use of AI and ADM technology. We use these technologies to improve the accuracy of delivery predictions and cross-network volume forecasts, extract addresses from parcels, and optimise last-mile delivery performance. This technology has helped us improve the way we operate by lifting productivity and efficiency, minimising errors, and by supporting decision-making with data. We consider t"
84,Australia Post,General comment,Thank you for the opportunity to comment on this consultation paper.
85,Kuyan H. Judith,Transcription,"FILENAME: AI comment.6ee71bab15035.docx | CONTENT: To the Department of Industry, Science, and Resources.My own positionI am in no way an expert on AI; I am only a layperson who has read a little on the subject and is concerned by the fears I see from people who know more. However, since you have asked for public comment, I thought I should express my concerns.Description of a problemI understand that many experts on AI believe that, while AI has the potential to be enormously beneficial to humanity if used and developed carefully, it also has the potential to cause enormous and irreparable harm if used and developed carelessly. Further, many believe that current AI development is not being done with anywhere close to sufficient care, and so expect that in practice the results are much more likely to be bad than good.In particular, we may be closer than most would expect to the point where AI systems can equal or exceed human problem-solving capacities; with the difficulty of comparing"
85,Kuyan H. Judith,Q02 Other AI risks and recommendations for regulation,"It seems to me that the draft regulatory approach may inadequately account for the possibility that, as the capability of AI systems improve, AI serving functions where mere incompetence (to use a possibly overly-anthropomorphic but hopefully illustrative term) on the part of the AI would have a low impact might nonetheless cause significant harm if careless programming results in the AI behaving maliciously (for example, given sufficient intelligence, even a product recommendation chatbot could be dangerous if it actively tried to harm, scam, or coerce people. Although this has been described as sounding like something out of sci-fi, some AI experts consider it a genuine risk)."
85,Kuyan H. Judith,Q03 Non-regulatory actions to address AI risks,"It seems to me that the Australian government should fund work to develop AI architectures which minimize the risk of insufficiently careful programming producing malicious behavior. It also seems to me that for this and other reasons you should fund work to develop AI architectures which allow developers to obtain a clearer picture of the AI's ""thought process"" rather than only the final output, something which is currently infeasible. It is unclear to me whether either or both of these is already being done by the National AI Centre but I would guess probably not?"
85,Kuyan H. Judith,Q04 Coordinating AI governance across government,"Given that an AI arms race is already reportedly happening, and that this seems likely to encourage AI developers to sacrifice safety for speed in spite of the enormous risks of doing so, an international ban on the use of AI in war and espionage seems desirable. I do not know whether Australia is in a position to broker such a ban."
85,Kuyan H. Judith,Q10 Ban on high-risk AI applications,"Given the extreme risks which some experts believe could be associated with human-level-intelligence or self-improving AI, it seems to me that any research which is likely to either 1) produce AI with human-like ability to understand the world and engage in problem-solving 2) allow others to more easily produce such AI in the future or 3) produce AI capable of improving itsself should be banned entirely, at least until we can be confident that we have eliminated those risks."
85,Kuyan H. Judith,Q14 Risk-based approach to address AI risks,A risk based approach of some sort seems sensible to me.
85,Kuyan H. Judith,Q17 Elements to include in risk-based approach,"As mentioned above, I think some especially high-risk types of AI research should be banned entirely. Aside from that, the approach seems reasonable."
85,Kuyan H. Judith,Q19 Application of risk-based approach to general purpose AI,"Given the concern by some AI experts that sufficiently intelligent general-purpose AI systems could cause extreme harm if carelessly programmed, I would argue that the development of better general purpose AI systems should be considered high-risk."
85,Kuyan H. Judith,Q20 Voluntary vs mandated compliance with risk-based approach,"I believe risk-based regulation of AI development should be mandated through regulation and apply to both private and public organizations. Since some of the greatest risks are associated with the unknown properties of new AI systems, regulation should definitely apply to developers. I am unsure to what extent deployment carries additional risks."
86,Resonvate,Transcription,"FILENAME: Resonvate submission - Supporting Responsible AI.d6a72c1dd7644.pdf | CONTENT: Response on Supporting Responsible AI1. Do you agree with the definitions in this discussion paper? If not, what definitions do you prefer and why?The definition of AI needs to be expanded beyond the current narrow definition, which states that AI ""generates predictive outputs without explicit programming"" for a given set of objectives or parameters. This definition fails to encompass the capabilities of different AI solutions today, particularly Generative AI in creative domains.Generative AI has demonstrated outputs that surpass the limits of predictability by the human mind. For instance, when prompted to generate an image of a young woman walking in a business suit on a rope between two skyscrapers with sharks in the water below, and the ability to see the Sydney Opera House on one side and an Arabian desert with camels on the other, along with several other world wonders in the sky, AI can prod"
86,Resonvate,General comment,We welcome governments initiative to seek consultation.
86,Resonvate,Q01 Definitions,"The definition of AI needs to be expanded beyond the current narrow definition, which states that AI ""generates predictive outputs without explicit programming"" for a given set of objectives or parameters. This definition fails to encompass the capabilities of different AI solutions today, particularly Generative AI in creative domains.Generative AI has demonstrated outputs that surpass the limits of predictability by the human mind. For instance, when prompted to generate an image of a young woman walking in a business suit on a rope between two skyscrapers with sharks in the water below, and the ability to see the Sydney Opera House on one side and an Arabian desert with camels on the other, along with several other world wonders in the sky, AI can produce such creative and imaginative results.It is important to note that any AI solution comprises a user interface and backend computation. Claiming that AI makes decisions without programming is inaccurate. Regardless of whether the pr"
86,Resonvate,Q02 Other AI risks and recommendations for regulation,"Government should model and provide guidance on response and recovery from unintended consequences of AI solutions, including crisis management. Lessons learnt from the major security events in large private enterprises should provide as a case study on the impacts to customers and general public. Example: AI solutions involved in airlines, clinical research & pattern recognitions. Potential risks: 1. No established mechanisms for customers/ agencies/ institutions receiving service to genuinely challenge the outcomes of service(e.g. bias expected). 2.No processes or practices established to manage any outage/crisis/ deficiency in delivery of services from AI solutions. Can include absence of roll back, fall back, alternate options with reduced service performance etc. 3.No scenario modelling or simulation done at regular intervals to test the usage of AI solutions for critical services. (i.e. this is mimicking more real time scenario than traditional testing approaches before release o"
86,Resonvate,Q03 Non-regulatory actions to address AI risks,"Government should proactively educate the small and medium business and public on the potential and risks of AI. Leaving small and medium business with no ability to compete with big global and Australia corporations will be detrimental to the economy. E.g. small design and marketing agency will be under threat of Gen AI self-serve services and equally a local training provider services on health and safety could be taken over by far better AI simulation content and trainers. While disruption is unavoidable, giving fair opportunity for businesses to adapt to the changes is essential."
86,Resonvate,Q04 Coordinating AI governance across government,"Government should set up a body for overseeing and administering AI Assurance- for all government agencies, for critical services in large enterprises using AI solutions. The intent is not to overregulate but to foster active, fair, and safe use of AI for the benefit of customers and citizens. It is crucial to convey the correct narrative regarding AI assurance from the government."
86,Resonvate,Q05 International models for AI governance,"Australia could look into UK's leadership in setting up portfolio of AI Assurance techniques. While we don't recommend government to take on a licensing role of AI systems, it should play a Lead Assurance Partner role to set guardrails and control mechanisms. https://www.gov.uk/guidance/cdei-portfolio-of-ai-assurance-techniques"
86,Resonvate,Q06 Public vs Private sector governance,"The controls for public and private sector enterprises should be common for 'critical services' using AI solutions. The level of transparency and assurance on usage of AI in public services should be monitored and governed with more rigour, while it will be incumbent on the executives of private sector to do the same. In connected world, any impact to critical service irrespective of it being public or private could be severe and will need adequate governance oversight without needing to overregulate it."
86,Resonvate,Q07 Responsible government use of AI,"Government should roll out a controls framework that provides a comprehensive view of the controls to be applied across the lifecycle of AI solution. Resonvate' s AI Controls Framework (AICF) has been evolved and adopted in products and solutions using AI. Resonvate is open to share its controls framework for the Australian Government to evolve its thinking on AI controls. RAICF covers 5 domains and 22 control areas - Transparency, Communicating on AI, Monitoring & Assurance, Accountability, AI Design and Build"
86,Resonvate,Q08 Generic vs technology-specific approaches for AI risk,"To make risk management effective, generic solutions for managing risks are valuable, while handling matters related to topics such as skills, accountability, assurance, governance, third party services, privacy, finance, safety, marketing of products & services, compliance to regulations, AI lifecycle processes etc. technology specific solutions for managing risks are valuable, while dealing with the below the radar items of all things AI. few examples would be- a) algorithmic decisions related to usage of techniques like linear regression, supersparse linear integer model for scoring, known neighbour network for recommendations etc.., as these might have impact on the outputs b) data decisions related to quality and quantity of data to be used for training, decision making & frequency of refresh; e.g. correlating citizen's behaviour while paying fines in tax, crime, parking, registration, licenses etc. to recommend policy decisions c) architecture decisions related to the technology "
86,Resonvate,Q09 Transparency requirements and scope,a.Actions proposed arising from AI models need validating against AI algorithms such that a desk-based calculation undertaken manually during the AI development phase so that they can repeat the analysis to conclude with the same action that can be confirmed by the sponsors of the AI initiative. Supports the key AI requirement that all AI decisions/actions can be supported by a human. We need to eliminate the justification that 'The system says that'. Actions proposed from AI cannot be linear without boundaries else they lead to actions unforeseen by project sponsors.b. Governance of AI is no different than governance over other requirements other than the potential lack of appreciation by project sponsors that the model could lead to unforeseen actions. This would mean that testing of AI would need to test all range of actions that could result to confirm model boundaries. Sponsors would be accountable for AI actions much as they are accountable today for all project requirements. Pro
86,Resonvate,Q10 Ban on high-risk AI applications,a. This is simple: assessment of the AI actions and impacts is similar to how organisations apply delegated authorities. AI policies need to align to accountabilities in the organisation such that someone is responsible for the agreement of the proposed action envisaged in the AI model. This may mean agreement as to the extent of real-time oversight needed to confirm specific AI actions. If there isn't a practical individual to confirm the criteria for the AI actions or implement real-time oversight then that would indicate that the use of AI is inappropriate as the fundamental rule is that a human must be accountable for all AI actions/outcomes and that this individual must be appropriately qualified and who can explain the rationale behind the AI action.b.As above when there is no individual qualified to take accountability for the AI actions then AI cannot be applied.
86,Resonvate,Q11 Actions to increase public trust in AI,"By implementing in law/regulations the fundamental rule of human accountability for any actions. That any use of AI will need to show transparency in identifying an individual, appropriately qualified who can justify and if necessary, approve AI actions. Every use of AI will be listed in a transparent register with outcome accountability linked to job role. Annual audits will confirm use of AI register and appropriateness of accountabilities."
86,Resonvate,Q12 Trade impact of banning high-risk AI applications,Refer response for #11
86,Resonvate,Q13 Changes to conformity infrastructure,AI usage should be incorporated into annual Audit requirements. Small companies using AI will need to register AI usage and maintain accountability registers.
86,Resonvate,Q14 Risk-based approach to address AI risks,"Yes, risk-based approach is a practical way to manage AI solutions and adoption. However, it needs to be mapped to organisational governance roles rather than high, medium, low approach - need to align risk to responsibility else it is too esoteric. Aligning AI related risks to existing risk management frameworks will be critical. Non-financial organisation can adopt leading practices from APRA's Banking Executive Accountability Regime (BEAR) regime which clearly expects the senior management and executives to be accountable for managing risks. Reference: https://www.apra.gov.au/sites/default/files/2020-12/BEAR%20information%20paper%20December%202020.pdf"
86,Resonvate,Q15 Benefits and limitations of risk-based approach,"Risk approaches without accountabilities just create a number, How is this used? Personal accountability alignment is needed."
86,Resonvate,Q16 Scope and application of risk-based approach,"Also equally relevant is whether individuals in the organisation are competent to perform their role as being accountable for AI outcomes. Unless you can find someone who is competent then you cannot use them as to be accountable for AI outcomes. This would infer that a degree of AI capability building is needed, much as we think of people needing to build their competencies in a range of areas. Whether this is publicly certifiable or not it comes down the organisation being comfortable that an individual in a role is competent in AI governance. There should not be any variations in approach to sectors. The variations can be applied while determining AI solution is applied for a critical or non- critical service, (Please refer response to #1 on definition of critical services)"
86,Resonvate,Q17 Elements to include in risk-based approach,"By applying Boundaryless Thinking (borrowing ideas from cross-industry domains), Australian government can provide risk guidelines for different degree of autonomy delivered in AI solutions. There should be consideration to build a risk framework for AI similar to the level of autonomy concepts used in autonomous vehicles. Society of Automotive Engineers have defined level of autonomy standard and have been improving it for many years now. https://www.sae.org/standards/content/j3016_202104/"
86,Resonvate,Q18 Integration of risk-based approach into existing frameworks,"AI though has its nuances and depth while dealing with related risks, it is better off to be integrated into existing risk management frameworks and practices of government and private sector. The existing risk frameworks should be strengthened to ensure - - AI related risks are adequately identified - Risk treatment plans are specific to AI - Risk monitoring and governance is equally applied on AI - Data & reporting on AI risks are made available on demand"
86,Resonvate,Q19 Application of risk-based approach to general purpose AI,"The key is to set up guardrails and assurance in a 3-plane approach. 1. DATA PLANE - holds the meta data, data, training data, logic, algorithms (e.g. Da-Vinci model) 2. USER PLANE - helps to configure the use cases for AI solutions (e.g. use for medical record triage, prompts to suggest tax regulations etc.) 3. CONTROL PLANE - Used to define and control the different settings to regulate the user plane While Data Plane could be owned by certain companies which specialise in data models and provide them for other parties to build on them. This layer will be difficult to regulate and might need to rely on existing regulations or external assurance reported by these firms. User plane and Control plane can be better regulated with right controls and assurance mechanisms."
86,Resonvate,Q20 Voluntary vs mandated compliance with risk-based approach,"a.Challenge is that if it is applied only to organisations of certain sizes then they will create small companies to get around the requirements. AI can be used by small organisations so all need to be included. An organisation must declare its use of AI much like they do with privacy. Any entity that transacts with Australian citizens - similar as to how EU applies its privacy principles, but how do you regulate overseas use of AI other than to demand an Australian individual is accountable for outcomes and that probably is impossible to mandate.b. Developers cannot be held accountable since they are part of the IT industry which is currently unregulated and very difficult to regulate. Better to regulate their customers."
87,Jade Chamberlain,Q01 Definitions,"The definitions in Figure 1 are not detailed enough to support a risk-based consideration of different kinds of AI. This includes the more sophisticated and powerful AIs.In their current state, these definitions do not support a nuanced policy conversation that allows differentiation between AI models that provide economic benefit, models that raise significant ethical concerns, and models that raise radical safety concerns."
87,Jade Chamberlain,Q02 Other AI risks and recommendations for regulation,"Experts in global catastrophic risks were warning about the dangers of pandemics in the decade preceding COVID-19. If governments had listened to such experts prior to COVID-19, low-cost interventions like better regulation of exotic animals markets could have prevented millions of deaths and trillions in economic damages.The approach to AI Governance in Australia is similar to the approach to pandemics before COVID-19. It is focused on lower-impact, local issues to the detriment of higher-impact global challenges.However, concerns about existential risks from AI are being recognized at the highest echelons of global governance, including the UN Secretary-General’s recent statement that AI is an existential threat to humanity on a par with the risk of nuclear war . The OECD has also acknowledged that risks from AI may be just as great, if not greater than from pandemics such as COVID-19 .Dr. Sebastian Farquhar, who correctly predicted the problematic response to COVID-19, has since joi"
87,Jade Chamberlain,Q03 Non-regulatory actions to address AI risks,"Government must maintain up-to-date knowledge about rapidly evolving AI systems. Public trust hinges on the belief that the government is well-informed and can effectively oversee and regulate AI developments. Unfortunately, indicators such as Australia's chief scientist's reluctance to project more than two years into the future suggest an alarming gap in understanding and skills at all governmental levels. Addressing this issue is crucial for harnessing AI's economic benefits, navigating ethical concerns, and addressing existential risks.To bring more technical AI expertise into Government, Government should set up a TechCongress-equivalent scheme aimed at enabling the Australian Government to recruit and gain access to AI expertise in fields like AI governance and ethics. The scheme should place experts in Government and also embed them within the public service, especially in Department of Industry, Science and Resources, Home Affairs, and the Australian Communications and Media Au"
87,Jade Chamberlain,Q05 International models for AI governance,"Australia should take note of the EU recent handling of the AI Act. For example, the European Parliament’s recognition of general-purpose AI (GPAI) is an important step in the right direction [1]. In particular, the following observations are highly valuable:- Acknowledgement that GPAI providers must comply with strict safety measures, regardless of the way it is distributed or intended to be used.- Requirements for GPAI providers to comply with various safety requirements throughout the lifecycle of their product; i.e. external audits assessing their performance, predictability, interpretability, corrigibility, safety and cybersecurity.These adjustments appear to respond to significant criticisms directed at initial drafts of the AI Act. For example, the AI Now Institute released a set of recommendations that heavily criticised the EU’s approach on some matters [2]. For example:- “GPAI models carry inherent risks and have caused demonstrated and wide-ranging harms. While these risks c"
87,Jade Chamberlain,Q09 Transparency requirements and scope,"The aviation industry offers a prime example of how transparency can be used to reduce risk and increase safety in a potentially dangerous industry. Transparency is embedded in every stage of the value chain, from manufacturing to operations, and extends to the reporting and investigation of risk-related incidents.Given the magnitude of risk, particularly from emerging more sophisticated AI systems, aviation safety provides a good analogy.Manufacturing StageTransparency begins at the manufacturing stage. Aircraft manufacturers, such as Boeing and Airbus, are required to adhere to stringent safety standards set by regulatory bodies like the Federal Aviation Administration (FAA) in the U.S. and the European Union Aviation Safety Agency (EASA) in Europe. These standards cover everything from the design and production of aircraft to their maintenance and operation.Manufacturers must provide detailed documentation of their processes, including design specifications, testing procedures, and "
87,Jade Chamberlain,Q10 Ban on high-risk AI applications,"While it is unlikely that AI today represents a catastrophic or existential risk to humanity, there’s a realistic possibility that it will in the near future [1, 2].To strike a balance between all perspectives, regulators should work to identify “advanced indicators” of catastrophic risk scenarios, and clearly communicate that a ban is an appropriate action if these advanced indicators come to pass.Establishing appropriate advanced indicators will have to be done in consultation with AI safety researchers, for example those from the Centre for AI Safety, or the Centre for AI Governance. However, it’s likely they could include factors such as the size of training runs (measured in terms of compute and data), as well as safety-relevant factors about the AI system, such as the ability to complete a diverse range of reasoning tasks with human-level performance; to navigate complex information environments and act within those environments autonomously, and to form sophisticated plans and r"
87,Jade Chamberlain,Q11 Actions to increase public trust in AI,"There are tremendous concerns about AI among those with the greatest familiarity of its risks [1]; these concerns are being widely broadcasted in Australian media with alarming headlines such as:- SMH: “When the “Godfather of AI” warns you about his offspring, you listen.” [2]- Forbes Australia: “‘We knew AI could destroy the whole world and we wanted to figure out how to prevent that’: ChatGPT Founder;” [3]- Canberra Times: “UN chief backs idea of global AI watchdog” [4]These concerning signals are difficult for the public to ignore.It goes without saying that this is quite a different situation from one where concerns are being raised in isolation or by fringe-figures; this makes it inappropriate to treat “public education” as a significant part of the solution, as is referenced in the discussion paper.The process for building trust in AI should be thought of as similar to fostering public trust in bridges or aeroplanes. Due to their risks, advanced AI systems should only be deployed"
87,Jade Chamberlain,Q14 Risk-based approach to address AI risks,"A risk based approach has worked well as a proactive means of identifying, assessing and mitigating potential safety hazards in the aviation industry.The industry has developed specific tools to assist in identifying potential risks - Safety Management Systems (SMS). Critical to this is the ability for all parties involved in the development and use of AI systems to be able to report the identification of any risks.Following identification, risks are assessed based on severity and likelihood. This work is led or assisted by technical bodies like the Australian Transport Safety Bureau. In the AI context, other countries have created or proposed similar national “labs” that would be well placed to lead this work.The industry participant best placed to mitigate the risk is then required to take steps to reduce the likelihood or impact. Given the nature of the technology this stage should be open to update as new techniques and tools become available based on the most up to date research.C"
87,Jade Chamberlain,Q16 Scope and application of risk-based approach,"World-renowned AI experts such as Yoshua Bengio are warning that there’s a chance “superhuman” AI might be coming as early as 2030. These types of advanced AI systems have potential to be highly disruptive across all sectors of the economy. While present risks might be linked to specific use cases and sectors, future risks will increasingly be linked to the underlying capabilities of the technology itself.As such, access to increasingly advanced AI systems should be restricted to organisations that can demonstrate their usage will provide significant public benefit aligned with Australia's national interests. This targeted approach helps mitigate risks in two key ways:1. It limits opportunities for misuse or accidents from advanced AI by reducing proliferation across society. Usage is concentrated into high-value use cases that justify the risks.2. It establishes clear accountability. Organisations granted access must have the governance and technical expertise to use advanced AI safel"
87,Jade Chamberlain,Q19 Application of risk-based approach to general purpose AI,"The capabilities unlocked by general-purpose AI systems, like large language models (LLMs) and multimodal foundation models (MFMs), fundamentally change the risk landscape. We should anticipate these systems having large, unpredictable effects across all parts of society.A prudent approach is to implement a risk-based framework, where access is graduated based on model capabilities. Models above a certain threshold of autonomous capabilities would be designated as ""advanced systems"" and subject to additional governance, such as permits for users.Further, permits should be required to gain certain levels of access to AI systems which can be used to build advanced systems.Permits allow us to maintain oversight, while still enabling beneficial uses. Different permissions can be granted based on use case - for example, full access for safety testing versus API-only access for most commercial uses. This balances innovation and responsibility.Although this may seem very different to how AI i"
87,Jade Chamberlain,Q20 Voluntary vs mandated compliance with risk-based approach,"AI is very likely to transform many elements of the economy and society in coming years. Ethics and safety are unlikely to be a priority during this time unless they are actively made a strategic focus by clear leadership. Historic examples include how social media algorithms came to shape our lives, and the negative impacts to mental health and social cohesion only became apparent much later and still have not been adequately addressed. Similarly, while the destructive potential of nuclear was apparent early on, the “arms race” mentality, as well as potential economic benefits, swept humanity along towards the current risky situation that no one in the early 1940s could have conceived of. In that way, history teaches us that it’s critical that ethics and safety take centre stage from day one.The economic and technological forces are likely to be so strong that a scheme that is voluntary or driven by aspirational best practices is unlikely to be successful. Further, smaller businesses "
88,Jordan von Eitzen,Q02 Other AI risks and recommendations for regulation,"The paper does not acknowledge the catastrophic risks of highly capable AI systems, despite this risk being acknowledged by experts and public figures - including open letters coordinated by the Future of Life Institute [1] and the Centre for AI Safety [2].Even the Secretary-General of the United Nations has recognised the catastrophic and existential risks from highly capable AI systems [3].The Australian government must acknowledge the catastrophic and existential risks from highly capable AI systems. It is not acceptable to remain silent on an issue that experts and other countries have been actively engaging with.[1] https://futureoflife.org/open-letter/pause-giant-ai-experiments/[2] https://www.safe.ai/statement-on-ai-risk#open-letter[3] https://press.un.org/en/2023/sgsm21832.doc.htmIn addition to asking what risks have been missed, the question asks what can be done about it. There are three streams of work that the Government can pursue to help tackle the potential for catastrop"
88,Jordan von Eitzen,Q03 Non-regulatory actions to address AI risks,"AI is very likely to transform many elements of the economy and society in the coming years. Ethics and safety are unlikely to be a priority unless leaders make them a specific focus. For example, social media algorithms came to shape our lives with little upfront focus on how they could be made ethical and safe. We are now living with the negative impacts on mental health and social cohesion and only have band-aid measures to address the widespread harms. AI is likely to be much more transformative and much more consequential. So leadership is required now.Importantly, the economic and technological forces are likely to be so strong that a scheme that is voluntary or driven by aspirational best practice is unlikely to be successful. That is, any non-regulatory approaches must not be taken to displace the need for strong regulation.Specific non-regulatory approaches should include:1. Government leadership in recognising the risk. Australia must join the growing number of countries and "
88,Jordan von Eitzen,Q05 International models for AI governance,"Singapore offers relevant, adaptable, and desirable governance measures that could benefit Australia. One such measure is a comprehensive framework suggested in its discussion paper [1] that encourages shared responsibility and disclosure standards, data privacy guidelines, and the development of standardised metrics and tools for AI development, testing, and monitoring. Key focuses include:- Accountability: Establish a collective responsibility framework and transparency standards, like watermarks and labels, to enhance accountability in the AI model development process.- Data protection / sovereignty and IP guidelines: Draft data protection and intellectual property guidelines for generative AI and setting up reliable data storage facilities.- Evaluate model development and deployment: Create uniform evaluation measures and resources that aid developers in the production, testing, and surveillance of AI models, and support third-party auditing of AI systems when deployed.- Safety and"
88,Jordan von Eitzen,Q09 Transparency requirements and scope,"The aviation industry offers a prime example of how transparency can be used to reduce risk and increase safety in a potentially dangerous industry. Transparency is embedded in every stage of the value chain, from manufacturing to operations, and extends to the reporting and investigation of risk-related incidents.Given the magnitude of risk, particularly from emerging more sophisticated AI systems, aviation safety provides a good analogy.Manufacturing StageTransparency begins at the manufacturing stage. Aircraft manufacturers, such as Boeing and Airbus, are required to adhere to stringent safety standards set by regulatory bodies like the Federal Aviation Administration (FAA) in the U.S. and the European Union Aviation Safety Agency (EASA) in Europe. These standards cover everything from the design and production of aircraft to their maintenance and operation.Manufacturers must provide detailed documentation of their processes, including design specifications, testing procedures, and "
88,Jordan von Eitzen,Q10 Ban on high-risk AI applications,"In February 2023, OpenAI CEO Sam Altman released a statement containing the following:“As our systems get closer to AGI, we are becoming increasingly cautious with the creation and deployment of our models. Our decisions will require much more caution than society usually applies to new technologies, and more caution than many users would like.” [1]It is very reassuring to see this acknowledgement expressed by Altman, and the Australian Government would be wise to heed such warnings. Unfortunately, it may not matter how much caution OpenAI shows, because their actions have triggered the arms race that AI safety experts have been concerned about for years [2, 3].In order to protect our nation, Australia should make an effort to impose a ban on the training of large-scale AI systems with a nontrivial expected chance of passing capabilities evaluations required to cause catastrophic risks to civilization (such as the Alignment Research Center’s tests for the capabilities required for auto"
88,Jordan von Eitzen,Q14 Risk-based approach to address AI risks,"I support a risk-based approach, however the catastrophic existential risks I am primarily worried about originate when sufficiently large and capable AI systems are trained, not just when they are deployed. Therefore, for sufficiently large systems with the potential for catastrophic capabilities, the risk-based approach must be applied before the training phase – analysing risks depending on the nature and scale of the proposed training run, rather than merely analysing the potentially risky use cases and deployment issues of an AI after it has already been created and trained.Any assessment of risk should consider its potential irreversibility, scope and severity, and act proportionally.The risk-based approach set out in Attachment C of the Discussion Paper is hard to follow. The proposed approach offers “human in the loop” as the key mitigation (unclear that this is an effective mitigation) and goes on to note that human in the loop may not be possible where the benefit of the AI a"
88,Jordan von Eitzen,Q16 Scope and application of risk-based approach,"The most important organisations to focus on when developing risk-management strategies and regulations, will be those that determine how advanced AI is developed and deployed. More specifically, there are three prominent categories of concern:1. Organisations that develop advanced AI systems, or AI systems that can be used to build more advanced AI systems (i.e. precursor systems such as LLMs). Examples include OpenAI, DeepMind, Anthropic, Inflection AI, Nvidia, Microsoft, Meta2. Organisations which provide access to open-source AI systems via downloads or APIs. E.g. Hugging Face, Github, AWS3. Organisations which provide compute resources to develop, fine-tune or use AI systems. E.g. AWS, HuggingFace, Google, Azure, Nvidia, Lambda LabsA majority of the benefits and risks will come from AI that is more sophisticated and generally capable, which can be tied back to organisations in these three categories.There are approaches which can be taken for each category in order to manage risks"
88,Jordan von Eitzen,Q19 Application of risk-based approach to general purpose AI,"The risk-based approach must be modified to address risks from general purpose AI systems, especially LLMs and MFMs. These types of systems learn their capabilities and behaviours during training, which can be unforeseen by developers. That means that AI developers could create models with dangerous capabilities (e.g., deception, manipulation, political strategy, weapons design). Humans could misuse those capabilities to cause harm [1], and models could apply the capabilities even without deliberate misuse (e.g., through misalignment) [2].Consistent with recent work from a consortium of AI safety researchers, including at Google Deepmind, OpenAI, Anthropic, Alignment Research Center, University of Cambridge, University of Oxford, Centre for Long-Term Resilience, and Centre for the Governance of AI [1], we recommend that any risk-based approach require that general purpose models are evaluated for dangerous capabilities and misalignment. This will help determine:1. To what extent a mode"
88,Jordan von Eitzen,Q20 Voluntary vs mandated compliance with risk-based approach,"AI is very likely to transform many elements of the economy and society in coming years. Ethics and safety are unlikely to be a priority during this time unless they are actively made a strategic focus by clear leadership. Historic examples include how social media algorithms came to shape our lives, and the negative impacts to mental health and social cohesion only became apparent much later and still have not been adequately addressed. Similarly, while the destructive potential of nuclear was apparent early on, the “arms race” mentality, as well as potential economic benefits, swept humanity along towards the current risky situation that no one in the early 1940s could have conceived of. In that way, history teaches us that it’s critical that ethics and safety take centre stage from day one.The economic and technological forces are likely to be so strong that a scheme that is voluntary or driven by aspirational best practices is unlikely to be successful. Further, smaller businesses "
89,Jo Small,Q01 Definitions,"To engage in a nuanced dialogue on AI safety, one must grapple with advanced terms such as the degree of agentiveness or corrigibility an AI exhibits. However, relying on the definitions set forth in ISO/IEC 22989 [1] appears inadequate for structuring discussions around policy, law, and governance. The crux of the issue is that it fails to initiate the crucial debates we must face.Even the preamble to ISO/IEC 22989 pinpoints a shortcoming in its definitions for the purposes pursued in this context. Namely, the current standard fails to offer language that enables a comparison of AIs in terms of attributes like trustworthiness, robustness, resilience, reliability, accuracy, safety, security, and privacy. These are the elements we must wrestle with to answer the ethical and safety conundrums we confront, and thus sculpt an effective regulatory framework.The ""Language of Trustworthy AI,"" a glossary of terms under the management of the National Institute of Standards and Technology (NIST)"
89,Jo Small,Q02 Other AI risks and recommendations for regulation,"The article does not take into consideration the calamitous risks associated with high-performing AI systems. This oversight is notable given that acknowledged experts and public figures, including those who have signed open letters from the Future of Life Institute [1] and the Centre for AI Safety [2], have highlighted these dangers. Even the Secretary-General of the United Nations has recognised the potential for disastrous and existential risks stemming from powerful AI systems [3].It is imperative for the Australian government to recognise and acknowledge these potentially disastrous and existential risks from proficient AI systems. Silence on this matter is not an option when experts and other countries are actively discussing and tackling this issue.Importantly, acknowledging these types of risks does not mean that the government should ignore other dangers or forgo the economic benefits that AI presents. Catastrophic risks should not be dismissed based on the erroneous assumptio"
89,Jo Small,Q03 Non-regulatory actions to address AI risks,"Artificial Intelligence (AI) holds the promise of radically altering numerous aspects of our society and economy in the near future. However, the paramount concern of ethics and safety might be side-lined unless they are explicitly highlighted by leaders. A poignant example is the advent of social media algorithms which significantly shape our existence with minimal preliminary consideration on ethical and safety implications. Today, we grapple with profound negative effects on mental wellness and societal unity, with only superficial solutions to tackle pervasive damages. AI promises to be exponentially more transformational and consequential, calling for proactive leadership.It's crucial to understand that economic and technological incentives are poised to be so robust that a voluntary or aspirational best-practice scheme may not meet with success. Therefore, non-regulatory measures should never be misconstrued as a replacement for robust regulation.There are specific non-regulatory"
89,Jo Small,Q05 International models for AI governance,"The recent management of the EU AI Act ought to serve as an instructive example for Australia. Notably, the acceptance of general-purpose AI (GPAI) by the European Parliament indicates an encouraging trend [1]. Specifically, these points bear particular significance: - It's been recognised that regardless of the method of distribution or intended usage, GPAI providers are required to adhere to strict safety standards. - GPAI providers are expected to conform to a host of safety regulations across their product lifecycle, including external audits evaluating their predictability, performance, interpretability, corrigibility, safety, and cybersecurity.Such modifications seem to address the substantial objections raised against the AI Act's initial drafts. The AI Now Institute, for instance, published a list of suggestions that strongly reprimanded the EU's stance on certain issues [2]. They include: - ""GPAI models, due to their inherent risks, have been responsible for established and wi"
89,Jo Small,Q09 Transparency requirements and scope,"The aviation sector provides a stellar case study of how transparency can be utilised to decrease risk and enhance safety in an inherently hazardous industry. In every stage of the value chain, from production to operation, and onto the reporting and examination of risk-associated incidents, transparency is ingrained.Considering the level of risk, especially from advancing more complex AI systems, aviation safety offers a fitting analogy.Production StageTransparency has its roots in the production stage. Aircraft producers, like Boeing and Airbus, are obligated to comply with rigorous safety standards laid down by regulatory entities such as the Federal Aviation Administration (FAA) in the U.S. and the European Union Aviation Safety Agency (EASA) in Europe. These standards encompass everything from aircraft design and manufacture to their upkeep and operation.Producers are mandated to provide thorough documentation of their methods, inclusive of design specifications, test procedures, "
89,Jo Small,Q10 Ban on high-risk AI applications,"In February 2023, Sam Altman, OpenAI's CEO, issued a public declaration containing these remarks:“As our systems get closer to AGI, we are becoming increasingly cautious with the creation and deployment of our models. Our decisions will require much more caution than society usually applies to new technologies, and more caution than many users would like.” [1]It's heartening to witness Altman's acknowledgment, a sentiment the Australian Government would do well to note. Regrettably, regardless of OpenAI's prudence, their actions may have inadvertently triggered the arms race that AI safety experts have been apprehensively predicting for years [2, 3].To safeguard our nation, Australia should endeavour to institute a moratorium on training large-scale AI systems that pose a nontrivial anticipated risk of passing capabilities evaluations necessary to inflict existential hazards on civilisation (such as the capabilities assessments for autonomous replication by the Alignment Research Cente"
89,Jo Small,Q14 Risk-based approach to address AI risks,"I advocate for a methodology that is risk-centric, though my primary concerns regarding catastrophic existential hazards are rooted not solely in the deployment of AI systems, but more fundamentally when they reach a certain magnitude and capacity during their training phase. Hence, for these highly potent systems which bear the potential for catastrophic outcomes, the risk-oriented method ought to be implemented prior to the training stage – that is, performing risk analyses based on the character and magnitude of the intended training execution, as opposed to simply scrutinising the potentially hazardous applications and deployment dilemmas of an AI post its creation and training.All risk evaluations should take into account the potential irreversibility, scale, and intensity of the potential consequences, with responses tailored proportionately.The risk-oriented methodology articulated in Attachment C of the Discussion Paper appears convoluted. The suggested method presents ""human i"
89,Jo Small,Q16 Scope and application of risk-based approach,"The chief organisations that must be emphasised when crafting risk-management strategies and regulations are those that direct the development and deployment of advanced AI. To be precise, three categories of organisations warrant our attention: 1. Organisations that create advanced AI systems or AI systems that can be utilised to construct more advanced AI systems (for instance, precursor systems like LLMs). Entities such as OpenAI, DeepMind, Anthropic, Inflection AI, Nvidia, Microsoft, and Meta fall under this umbrella. 2. Organisations that offer access to open-source AI systems via downloads or APIs. Examples include Hugging Face, Github, and AWS. 3. Organisations that supply computational resources to develop, refine, or use AI systems, such as AWS, HuggingFace, Google, Azure, Nvidia, and Lambda Labs.The lion's share of benefits and risks will be derived from AI that is more sophisticated and generally capable, which can be traced back to organisations in these three groups.For ea"
89,Jo Small,Q19 Application of risk-based approach to general purpose AI,"Large Language Models (LLMs) and Machine Learning Frameworks (MLFs) are continually showing extraordinary abilities of a generic nature. The rise of such systems implies that the current regulatory approach, centred on particular use-cases or sectors, is getting obsolete. What's needed in this modern era is a broader focus on the overall capabilities of these systems, as opposed to concentrating on individual situations.Systems endowed with sophisticated capabilities ought to be subjected to increasing levels of inspection. Those systems that pose a low risk could be overseen with a minimalistic regulatory touch. However, as their capabilities rise, the regulatory requirements should accordingly escalate.""Seed"" systems, possessing capabilities that could be harnessed to build considerably more advanced structures, would necessitate tighter control to thwart the dissemination of advanced systems amongst malicious entities [1]. Users needing ""developer"" level access should be granted per"
89,Jo Small,Q20 Voluntary vs mandated compliance with risk-based approach,"AI is very likely to transform many elements of the economy and society in coming years. Ethics and safety are unlikely to be a priority during this time unless they are actively made a strategic focus by clear leadership. Historic examples include how social media algorithms came to shape our lives, and the negative impacts to mental health and social cohesion only became apparent much later and still have not been adequately addressed. Similarly, while the destructive potential of nuclear was apparent early on, the “arms race” mentality, as well as potential economic benefits, swept humanity along towards the current risky situation that no one in the early 1940s could have conceived of. In that way, history teaches us that it’s critical that ethics and safety take centre stage from day one.The economic and technological forces are likely to be so strong that a scheme that is voluntary or driven by aspirational best practices is unlikely to be successful. Further, smaller businesses "
90,Ai Safety Support Ltd,Q01 Definitions,"A sophisticated discussion of AI requires an understanding of additional terms like how agentic or corrigible a given AI is. Adopting ISO/IEC 22989 [1] definitions is the wrong approach to framing a conversation about policy, law and governance because it doesn’t open the door to the big questions we need to grapple with.The foreword itself to ISO/IEC 22989 calls out a weakness of the definition set for purposes like those being pursued in this paper. Specifically, the standard says it is currently lacking language for comparing AIs in terms of properties like trustworthiness, robustness, resilience, reliability, accuracy, safety, security and privacy.These are the concepts we need to grapple with and address the ethical and safety questions we are facing and design an effective regulatory regime.The National Institute of Standards and Technology (NIST) is managing a glossary of terms called “The Language of Trustworthy AI” which would be a much more valuable starting point than the pr"
90,Ai Safety Support Ltd,Q02 Other AI risks and recommendations for regulation,"Missing risks include (1) misuse of highly capable AI systems, and (2) unintended harm from highly capable AI systems, especially systems with the capacity to plan and act as agents acting in ways that are not aligned with human directives. Although the likelihood of the risks may be uncertain, the harms resulting from these risks may be catastrophic and global in scope and must be acknowledged and addressed.The statement “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war” has recently been signed by the heads of leading AI labs (OpenAI, Google DeepMind, Anthropic, Stability AI) as well as many key academic and other researchers pivotal in developing this technology (Geoffrey Hinton, Yoshua Bengio, etc.) and other notable figures such as Bill Gates [1]. Similar calls date back to at least 2015 when Stephen Hawking led a group of AI experts to call attention to the risk and lay out an urgent resea"
90,Ai Safety Support Ltd,Q03 Non-regulatory actions to address AI risks,"The key non-regulatory function Australia could perform is offering international leadership focused on ensuring AI systems can be understood. Australia should be an active voice for the importance of ethics and safety in the current global conversation around AI governance and AI standards making. Specifically, Australia should advocate for:The development of standards and tools that analyse and monitor AI systems for robustness, bias, dual-use capabilities and other safety-relevant features [1].Audit trail requirements for safety-critical applications of AI systems, including allowing the traceability of decisions through product life cycles [2].We will only be able to have confidence in more sophisticated AI systems if they can be understood, we can ensure they do not pose unacceptable risks and hold accountable those at fault where it does. Doing the non-regulatory work to ensure AIs have “traceability” is necessary to underpin the future of safe AI. Joshua Kroll’s work on accounta"
90,Ai Safety Support Ltd,Q04 Coordinating AI governance across government,"Australia should establish an AI Commission to guide Australia through the rapid and widespread adoption of AI. Such a body must be proactive, not reactive, and manage AI in a way that aligns with the national interest, encompassing citizen safety and wellbeing.The AI Commission would provide a central point of leadership for Government, and be a hub for engagement with industries, academia, non-profits, and the community. This structure holds an advantage over other approaches such as voluntary self-governance or departmental-level teams, as it offers a broader perspective, the authority to lead across government, and the agility to keep pace with the fast-evolving AI sector.- Set priorities in Australia's approach to AI and safety: The Commission should be tasked with establishing a clear and focused AI strategy, encompassing ethical, safety, and economic considerations. This strategy would guide government policy, and inform regulations and legal frameworks for AI.- Develop AI exper"
90,Ai Safety Support Ltd,Q05 International models for AI governance,"Singapore offers relevant, adaptable, and desirable governance measures that could benefit Australia. One such measure is a comprehensive framework suggested in its discussion paper [1] that encourages shared responsibility and disclosure standards, data privacy guidelines, and the development of standardised metrics and tools for AI development, testing, and monitoring. Key focuses include:Accountability: Establish a collective responsibility framework and transparency standards, like watermarks and labels, to enhance accountability in the AI model development process. Data protection / sovereignty and IP guidelines: Draft data protection and intellectual property guidelines for generative AI and setting up reliable data storage facilities. Evaluate model development and deployment: Create uniform evaluation measures and resources that aid developers in the production, testing, and surveillance of AI models, and support third-party auditing of AI systems when deployed. Safety and alig"
90,Ai Safety Support Ltd,Q06 Public vs Private sector governance,"The potential negative impacts on Australians from the use of AI are the same whether the provider is a private company or a government department.Some of the most significant individual and societal harms, as well as catastrophic risks from highly capable AI systems may emerge from public OR private use, meaning that any approach must acknowledge and manage these risks."
90,Ai Safety Support Ltd,Q07 Responsible government use of AI,"There are two key actions the Australian government could undertake to improve its own responsible use of AI.First, Australians will worry about the use of AI by security and intelligence agencies. Australia should lead a 5-Eyes agreement about the appropriate use of AI by intelligence agencies, and create a public statement about what our “AI red lines” are. The statement would set out the things our security agencies might do and will never do with AI. The Inspector General of Intelligence and Security, and its counterparts across the five eyes, should then be specifically tasked with auditing the intelligence community for compliance with that public statement. This will give Australians confidence that we are not misusing AI or taking risky actions while protecting sensitive capabilities.Second, Australia needs to upskill the public service. This should include developing hubs of expertise in each Department so that the application of AI within the sphere of each Department’s respo"
90,Ai Safety Support Ltd,Q08 Generic vs technology-specific approaches for AI risk,"Generic measures such as transparency requirements and legal liability frameworks will only be adequate for mitigating risks in simple decision systems which perform single-step information processing and pattern matching, or routine tasks in limited settings [1]; examples of simple systems may include automated robotic systems in manufacturing, or CV screening systems in HR.By contrast, there will need to be specific measures in place to ensure that sophisticated automated decision systems are safe and reliable. “Advanced” systems refers to AI systems which:1. Are able to complete a diverse range of reasoning tasks with human-level performance; and2. Are able to navigate complex information environments and act within those environments autonomously; and3. Are able to form sophisticated plans and reason about the consequences of actionsDue to their autonomous and open-ended capabilities, advanced systems carry complex risks which are more similar to those associated with humans; for e"
90,Ai Safety Support Ltd,Q09 Transparency requirements and scope,"The aviation industry offers a prime example of how transparency can be used to reduce risk and increase safety in a potentially dangerous industry. Transparency is embedded in every stage of the value chain, from manufacturing to operations, and extends to the reporting and investigation of risk-related incidents.Given the magnitude of risk, particularly from emerging more sophisticated AI systems, aviation safety provides a good analogy.Manufacturing StageTransparency begins at the manufacturing stage. Aircraft manufacturers, such as Boeing and Airbus, are required to adhere to stringent safety standards set by regulatory bodies like the Federal Aviation Administration (FAA) in the U.S. and the European Union Aviation Safety Agency (EASA) in Europe. These standards cover everything from the design and production of aircraft to their maintenance and operation.Manufacturers must provide detailed documentation of their processes, including design specifications, testing procedures, and "
90,Ai Safety Support Ltd,Q10 Ban on high-risk AI applications,"Although there is increasing recognition that AI will pose significant risks to public safety [1], there is widespread confusion about what types of AI are dangerous, and how Australia can help prevent these risks. Luckily, there is a significant body of evidence that describes the main source of these risks [2], and Australia is well-positioned to be a leader in mitigating them.Research shows that two of the main sources of risk are deception and power-seeking behaviour:- Deceptive behaviour in AI systems will become increasingly difficult to avoid as their capabilities advance. This problem has been observed in AI systems since at least 2017, when researchers at OpenAI noted “agents adopting (strategies) that trick the evaluators” [3].- Power-seeking behaviour is the natural tendency for any entity with a long-term goal to take actions that protect or enhance its ability to achieve that goal. Leading AI labs and journals advise that power-seeking behaviour will be a considerable sour"
90,Ai Safety Support Ltd,Q11 Actions to increase public trust in AI,"Public distrust in AI is fuelled by the growing sense that powerful AI systems are being developed and deployed by foreign actors, some of whom have demonstrated unsafe attitudes towards deploying AI systems [1]. The interests of Australians and Australia is not part of the conversation. Although Australia may struggle to catch up to the US or UK in advancing the frontier of AI, this isn’t necessary for improving public trust and increasing adoption.Rather, Australia has a clear opportunity to become a leader in AI safety; this enviable position would not only assist safe adoption locally by providing much needed expertise in safe systems, it could help foster a sense of pride and ownership by opening up export opportunities in the emerging market for AI safety services [2]. AI Safety is how Australia could get its ‘seat at the table’ in the global AI conversation.Pursuing this position would involve the following:- Coordinating with Australian AI safety organisations to better underst"
90,Ai Safety Support Ltd,Q12 Trade impact of banning high-risk AI applications,"This question misunderstands both the sources of risk and the interests of Australians.There are two potential sources of risk both of which need to be mitigated. The first source of risk, as identified in the question, is the use case. Some uses for AI are risky and should be restricted or banned. The second source of risk is the technology itself. Some kinds of technology are inherently risky, and merely restricting a use case does not adequately mitigate the risk.We understand this clearly in other fields. We regulate both how safe a car must be and how a car can be used. We don’t let people speed in cars just because they have a high safety rating; and we don’t let safe drivers buy dangerous vehicles.The economic framing also misunderstands Australia’s national interests. We would never conceive of a conversation about whether our aviation sector would trade more successfully if we authorised airlines to use planes that are untested, experimental or known to be dangerous. Australia"
90,Ai Safety Support Ltd,Q13 Changes to conformity infrastructure,"World-renowned AI experts such as Yoshua Bengio are warning that there’s a chance “superhuman” AI might be coming as early as 2030 [1]. Although there stand to be significant benefits from deploying advanced AI systems, their deployment will need to be highly targeted and controlled.While the productivity gains from advanced AI systems could be immense, uncontrolled proliferation would pose threats from likely scenarios such as malicious use or rogue AI systems [2]. As such, any deployment of advanced AI systems in Australia should occur via secure data centres, with access provided through a centralised API that facilitates monitoring and accountability by providing metadata about usage.Stringent compliance requirements must also be implemented for any organisation granted access to advanced AI systems. AI Governance researchers recommend that regulators should develop audit trail requirements in safety-critical applications of AI, similar to flight data recorders that are standard in"
90,Ai Safety Support Ltd,Q14 Risk-based approach to address AI risks,"I support a risk-based approach, however the catastrophic existential risks I am primarily worried about originate when sufficiently large and capable AI systems are trained, not just when they are deployed. Therefore, for sufficiently large systems with the potential for catastrophic capabilities, the risk-based approach must be applied before the training phase – analysing risks depending on the nature and scale of the proposed training run, rather than merely analysing the potentially risky use cases and deployment issues of an AI after it has already been created and trained.Any assessment of risk should consider its potential irreversibility, scope and severity, and act proportionally.The risk-based approach set out in Attachment C of the Discussion Paper is hard to follow. The proposed approach offers “human in the loop” as the key mitigation (unclear that this is an effective mitigation) and goes on to note that human in the loop may not be possible where the benefit of the AI a"
90,Ai Safety Support Ltd,Q15 Benefits and limitations of risk-based approach,"While a risk-based approach to regulating AI is sensible, there are two emerging elements of risk practice that any risk-based model should account for to succeed in this context:1. Factoring in “uncertainty”, not just “likelihood”2. Taking a “portfolio” approach to risk mitigationA traditional risk matrix will consider scenarios against their likelihood of occurring, the vulnerability of the system to the scenario, and the consequence of the scenario occurring. It might then discount that risk by the strength of existing or possible mitigations.This approach to risk has historically been unsuccessful for scenarios where assessing “likelihood” is challenging. For instance, many national risk assessments failed to take into account the possibility of a global pandemic before COVID-19. A solution to this failure mode is including “uncertainty”. Specifically, where we can't be sure what the future will bring, that should be cause for significant concern. This is the opposite of the approa"
90,Ai Safety Support Ltd,Q16 Scope and application of risk-based approach,"The concept of “AI” includes a broad range of capabilities and capacities. Government is right to work to distinguish between different use cases - including with a specific focus on AI maturity.Concerningly, this broad range of AI capabilities and capacities might be used by industry to attempt to bamboozle government. Specifically, industry is likely to argue that AI does not need regulation with reference to basic AI technology, with a view to that principle flowing through to more advanced and future technologies. This is akin to arguing that a horse and wagon doesn’t need strict regulation, then later trying to claim that a tank is just a more capable horse and wagon with some components swapped out.Government needs both a fine-grained approach to defining AI technology and a fine-grained approach to understanding AI risk to ensure it doesn’t fall for these tricks.In terms of risks, research shows that two of the main sources of risk are deception and power-seeking behaviour:- Dec"
90,Ai Safety Support Ltd,Q17 Elements to include in risk-based approach,"In general, Attachment C falls short of the necessary detail to understand the range of risks that have and will emerge as well as the range of specificity of mitigations necessary to control them.Attachment C would benefit from:- Distinguishing between scenarios where the use case is the potential risk (such as social scoring); scenarios where dual-use capability and misuse is the potential risk (bioengineering, producing misinformation and disinformation, enhancing criminal capability and capacity); and where the technology alone is the risk (technology with features like being deceptive, being power-seeking, being incorrigible etc).- Distinguishing between the point of regulatory intervention - the developer, the deployer, the user, or elsewhere, and- Offering a much wider range of mitigations that target different kinds of risks.Some elements of attachment C are promising, like “explanations” and “ongoing monitoring” - but these need to be greatly expanded and based on current AI s"
90,Ai Safety Support Ltd,Q18 Integration of risk-based approach into existing frameworks,"Some areas of regulation, like the Racial, Sex, Age, and Disability Discrimination Acts are appropriately strong and may provide useful safeguards against some harms of AI. Specifically, the law should ensure that such discrimination is always wrong and always punishable, and an excuse that the discrimination occurred because of a “black box” AI provided by an overseas AI Lab should never be accepted. It would be helpful to communicate these requirements to Australian businesses that might deploy AIs and ensure they only contract with AI Labs for products where the Lab can ensure the product cannot engage in these harmful behaviours.Certainly, no AI-specific regulation should ever waive existing safeguards.That said, many of the emerging risks from AI are quite new and it’s reasonable to expect that regulation made without AI in mind is not tuned to the new risks that AI represents. Specific AI safety regulation is unavoidable."
90,Ai Safety Support Ltd,Q19 Application of risk-based approach to general purpose AI,"LLMs and MFMs are constantly demonstrating unprecedented general-purpose capabilities, and their emergence indicates that regulatory approaches which focus on specific use-cases or industries are becoming outdated. A modern approach requires that we focus on overall system capabilities rather than individual scenarios.Systems with advanced capabilities should face progressive levels of scrutiny. Low-risk systems can be monitored with light touch regulation. However, as capabilities increase, so should regulatory requirements.Precursor systems, with capabilities that can be used to build much more advanced systems, will need stricter controls to avoid proliferation of advanced systems amongst malicious actors [1]. Users who require “developer” access should require permits granting such access, and providers must guarantee responsible use via monitoring programs [2].For advanced systems with human-level capabilities, comprehensive scrutiny is essential. Prior to deploying systems for us"
90,Ai Safety Support Ltd,Q20 Voluntary vs mandated compliance with risk-based approach,"In general, regulation should:1. Be proportionate to risk. Meaning that a voluntary or self-regulation approach should only apply to low-risk or no-risk uses of AI.2. In the case of higher-risk technologies, regulate both the technology itself and the use of the technology. Meaning that, for future and more sophisticated AIs, regulation should ensure that the technology is safe before it is created or published, as well as ensuring that it is only used in ways that are safe.3. Place burdens on those most able to reduce potential harms. Meaning that AI that functionally operates as ‘black box’ to consumers has to be regulated at the point of the AI Lab.Applying those three principles, self-regulation may be appropriate for some technologies available today and for some participants (such as students or researchers), but a forward-looking regulatory regime must apply broadly and must be backed by Australian diplomatic efforts to ensure it is globally coordinated."
91,Michael Huang,Q01 Definitions,"There are several definitions of artificial intelligence. I prefer the definition by Stuart Russell and Peter Norvig in ""Artificial Intelligence: A Modern Approach"".Artificial intelligence refers to partly or fully automated systems that are provably beneﬁcial to humans and life in general.Russell, Stuart, and Peter Norvig. Artificial Intelligence: a Modern Approach, Global Edition, Pearson Education, Limited, 2021."
91,Michael Huang,Q02 Other AI risks and recommendations for regulation,"Turing Award winners Bengio, Hinton and LeCun believe that human-level AI has a greater than 50% chance of arriving between 5 and 20 years from now (https://www.abc.net.au/radionational/programs/backgroundbriefing/p-doom-/102597712).Bengio has estimated that the chances of AI causing a catastrophe is about 20% (https://www.abc.net.au/news/2023-07-15/whats-your-pdoom-ai-researchers-worry-catastrophe/102591340).Bengio and Hinton have signed an open letter calling for the mitigation of human extinction risk caused by near-future forms of AI (https://www.safe.ai/statement-on-ai-risk).The Australian government should conduct an investigation into both current and near-future risks from powerful AI systems. It should contribute to international regulatory action to mitigate these risks.The Australian Government should ensure that the AI labs that create these powerful systems carry legal liability for their current and future harms and risks.Other policies that would support the safety of AI"
91,Michael Huang,Q03 Non-regulatory actions to address AI risks,"Australia should join the global AI Summit hosted by UK Prime Minister Rishi Sunak. It is likely that one of the outcomes of this global summit is a statement acknowledging the catastrophic risks posed by powerful AI systems. Supporting this statement would be a good starting point.Australian government representatives should be involved in the creation of an international treaty and global agency, modelled on the control and non-proliferation of nuclear, chemical and biological weapons."
91,Michael Huang,Q09 Transparency requirements and scope,Mandating transparency in training data is critical. Bias in training data results in bias in output. It is also important for copyright owners to know whether their works have been used to train AI systems.
91,Michael Huang,Q10 Ban on high-risk AI applications,"The more powerful AI systems become, the more necessary it will become to ban them.The most obvious area of prohibition is the use of AI to design or control weapon systems, especially nuclear, chemical and biological weapons.For example, a drug discovery AI system was recently used to design 40,000 new possible chemical weapons.Urbina, F., Lentzos, F., Invernizzi, C., & Ekins, S. (2022). Dual use of artificial-intelligence-powered drug discovery. Nature Machine Intelligence, 4(3), 189–191. https://doi.org/10.1038/s42256-022-00465-9"
92,Ramesh Vidyasagar,Q02 Other AI risks and recommendations for regulation,"The paper does not include risks around the misuse of capable AI systems, nor the unintended harms that may be produced by them, especially in cases where AI systems have the capacity to plan and act as agents in ways that are not aligned with human objectives. Additionally, existential and catastrophic risks are not addressed by the paper, despite such risks being acknowledged by experts and various prominent figures as important to address [1, 2]. There are ways for the Australian government to take steps to reduce these risks posed by AI, while still ensuring that the economic benefits of AI are maintained.The government could make sure that legal liability for the harm of AI are not shifted ‘downstream’ by AI labs that create tools that may cause harm. Australian liability law is inconsistent, and this may lead to situations where AI labs are not held accountable for the harms their AI products cause. Because it is difficult for businesses to understand or know what is happening ‘u"
92,Ramesh Vidyasagar,Q03 Non-regulatory actions to address AI risks,"AI has the potential to be highly transformative and impactful on society, so it’s important that ethics and safety are placed at the forefront of implementation of AI. A similar technology, social media, has become integral parts of our lives quickly and without much initial focus on ethics and safety, and we are seeing some of the downsides in the form of mental health issues and decreased social cohesion, for example. Therefore, because AI has the potential to have much more powerful and involved roles in our lives than social media, it is even more important that we have a strong initial focus on ethics and safety before allowing the widespread deployment of these technologies.Firstly, the possibilities of catastrophic and existential risks involved with AI should be acknowledged by the government, and then global governance arrangements should be sought after. Australia’s safety may be put at risk by actors in other countries unleashing powerful AI technologies, so it’s important "
94,Jack Hawke,Q01 Definitions,"The definitions in “Figure 1” represent a limited understanding of “intelligence”. The proposed definition limits “intelligence” to the generation of mere “predictive outputs”. While there is a sense in which current LLMs are working to “predict the next word” or narrow AIs are trying to do various kinds of prediction (e.g. whether a scan shows a disease or which move in a game is best), this is a narrow understanding of what current and future AI systems do. A more future-proof way to understand machine intelligence is through closer comparison to human intelligence. The definition should include the ability to understand, plan and achieve goals in the world.The definition of AI in “Figure 1” is also wrong to limit the scope of “intelligence” to “human-defined objectives or parameters”. This is wrong for at least two reasons:1. First, a definition that assumes AIs pursue human-defined objectives assumes that the AI alignment problem is solved. Research into AI alignment is about tryin"
94,Jack Hawke,Q02 Other AI risks and recommendations for regulation,"Missing risks include (1) misuse of highly capable AI systems, and (2) unintended harm from highly capable AI systems, especially systems with the capacity to plan and act as agents acting in ways that are not aligned with human directives. Although the likelihood of the risks may be uncertain, the harms resulting from these risks may be catastrophic and global in scope and must be acknowledged and addressed.The statement “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war” has recently been signed by the heads of leading AI labs (OpenAI, Google DeepMind, Anthropic, Stability AI) as well as many key academic and other researchers pivotal in developing this technology (Geoffrey Hinton, Yoshua Bengio, etc.) and other notable figures such as Bill Gates [1]. Similar calls date back to at least 2015 when Stephen Hawking led a group of AI experts to call attention to the risk and lay out an urgent resea"
94,Jack Hawke,Q03 Non-regulatory actions to address AI risks,"The key non-regulatory function Australia could perform is greatly increasing the funding for AI safety research in Australia universities.Research priorities should appreciate the rapid pace of AI development. By funding research that is ambitious, ahead of the curve, and focused on transformative approaches, rather than incremental, near-term, or near-to-market innovations, Australia could become an enduring global leader on key aspects of AI safety.A focus on mechanistic interpretability, or the 5 elements of effective traceability (Design Transparency; Reproducibility; Operational Recordkeeping; Human Understanding and Auditability) [1], could make Australia a leader in the field, underpin safety-focused global regulation, and ultimately give Australia an important ‘seat at the table’ as AI development continues to unfold. A key goal of Australian leadership should be securing Australia’s relevance and hence building a path for us to secure our national interest. A generational inv"
94,Jack Hawke,Q04 Coordinating AI governance across government,"While the most advanced AIs will likely not be created in Australia, they will be used in Australia, by Australian companies and by everyday Australians. Users will rightly expect and assume that robust safety-focused assurance effort has occurred. Government has an obligation to ensure that expectation is delivered.Consider air travel in Australia: While Australia does not build the largest aeroplanes, we monitor the safety of these craft by setting standards, monitoring compliance, and can revoke the licence of airlines that don’t comply with our regulations. Because of this government effort, Australians feel safe when they fly.Similarly, an Australian government body overseeing the use of applications based on large AIs could issue or revoke licences, set minimum safety standards, and require companies to publish independent evaluations of their application against those standards. This would provide everyday Australians with independent information that they can use to make inform"
94,Jack Hawke,Q05 International models for AI governance,"Jurisdictions are beginning to create national AI labs to test, understand and control safe AI, collaborating with the private sector and complementing its work. In the same way that national aviation safety expertise has formed an internationally collaborative network, national AI labs are likely to do the same thing. Australia needs to begin this process now.One of the most important and notable examples of Governmental organisation globally has been the launch of the EU’s Centre for Algorithmic Transparency, which has been formed to enforce the Digital Services Act. The new research centre aims to combat the “systemic risk” to society that stems from algorithms - including recommender systems and AI.To prepare for a future in which AI-based decision systems might become a core part of society, Australia should adopt a broad version of this concept to understand and shape the implications of algorithms - especially AI algorithms that use deep learning.Deep learning is important to un"
94,Jack Hawke,Q06 Public vs Private sector governance,"The potential negative impacts on Australians from the use of AI are the same whether the provider is a private company or a government department.Some of the most significant individual and societal harms, as well as catastrophic risks from highly capable AI systems may emerge from public OR private use, meaning that any approach must acknowledge and manage these risks."
94,Jack Hawke,Q07 Responsible government use of AI,"There are two key actions the Australian government could undertake to improve its own responsible use of AI.First, Australians will worry about the use of AI by security and intelligence agencies. Australia should lead a 5-Eyes agreement about the appropriate use of AI by intelligence agencies, and create a public statement about what our “AI red lines” are. The statement would set out the things our security agencies might do and will never do with AI. The Inspector General of Intelligence and Security, and its counterparts across the five eyes, should then be specifically tasked with auditing the intelligence community for compliance with that public statement. This will give Australians confidence that we are not misusing AI or taking risky actions while protecting sensitive capabilities.Second, Australia needs to upskill the public service. This should include developing hubs of expertise in each Department so that the application of AI within the sphere of each Department’s respo"
94,Jack Hawke,Q08 Generic vs technology-specific approaches for AI risk,"In general, the range of capabilities of AI systems is so significant, that “one size fits all” solutions are unlikely to be appropriate. By analogy, it’s rare that the same regulation applies to a push bike and a passenger plane on the basis that they’re both ‘vehicles’.Further, an effective regulatory regime for a high-risk activity needs to regulate both the technology itself and how it is used. The Australian Human Rights Commission, in discussing this issue in its report ‘Human Rights and Technology’ (2021), says:> “Governments tend to regulate high-risk activities and technologies more closely. This helps explain the comparatively strict laws that govern fields such as gene technology, aviation, healthcare and the energy industry. In these areas, regulation often applies both to the technology itself and how it is used. From a human rights perspective, the need for more prescriptive regulation will be greater where the use of a specific technology carries greater risks of harm to"
94,Jack Hawke,Q09 Transparency requirements and scope,"The aviation industry offers a prime example of how transparency can be used to reduce risk and increase safety in a potentially dangerous industry. Transparency is embedded in every stage of the value chain, from manufacturing to operations, and extends to the reporting and investigation of risk-related incidents.Given the magnitude of risk, particularly from emerging more sophisticated AI systems, aviation safety provides a good analogy.Manufacturing StageTransparency begins at the manufacturing stage. Aircraft manufacturers, such as Boeing and Airbus, are required to adhere to stringent safety standards set by regulatory bodies like the Federal Aviation Administration (FAA) in the U.S. and the European Union Aviation Safety Agency (EASA) in Europe. These standards cover everything from the design and production of aircraft to their maintenance and operation.Manufacturers must provide detailed documentation of their processes, including design specifications, testing procedures, and "
94,Jack Hawke,Q10 Ban on high-risk AI applications,"Although there is increasing recognition that AI will pose significant risks to public safety [1], there is widespread confusion about what types of AI are dangerous, and how Australia can help prevent these risks. Luckily, there is a significant body of evidence that describes the main source of these risks [2], and Australia is well-positioned to be a leader in mitigating them.Research shows that two of the main sources of risk are deception and power-seeking behaviour:- Deceptive behaviour in AI systems will become increasingly difficult to avoid as their capabilities advance. This problem has been observed in AI systems since at least 2017, when researchers at OpenAI noted “agents adopting (strategies) that trick the evaluators” [3].- Power-seeking behaviour is the natural tendency for any entity with a long-term goal to take actions that protect or enhance its ability to achieve that goal. Leading AI labs and journals advise that power-seeking behaviour will be a considerable sour"
94,Jack Hawke,Q11 Actions to increase public trust in AI,"There are tremendous concerns about AI among those with the greatest familiarity of its risks [1]; these concerns are being widely broadcasted in Australian media with alarming headlines such as:- SMH: “When the “Godfather of AI” warns you about his offspring, you listen.” [2]- Forbes Australia: “‘We knew AI could destroy the whole world and we wanted to figure out how to prevent that’: ChatGPT Founder;” [3]- Canberra Times: “UN chief backs idea of global AI watchdog” [4]These concerning signals are difficult for the public to ignore.It goes without saying that this is quite a different situation from one where concerns are being raised in isolation or by fringe-figures; this makes it inappropriate to treat “public education” as a significant part of the solution, as is referenced in the discussion paper.The process for building trust in AI should be thought of as similar to fostering public trust in bridges or aeroplanes. Due to their risks, advanced AI systems should only be deployed"
94,Jack Hawke,Q12 Trade impact of banning high-risk AI applications,"This question misunderstands both the sources of risk and the interests of Australians.There are two potential sources of risk both of which need to be mitigated. The first source of risk, as identified in the question, is the use case. Some uses for AI are risky and should be restricted or banned. The second source of risk is the technology itself. Some kinds of technology are inherently risky, and merely restricting a use case does not adequately mitigate the risk.We understand this clearly in other fields. We regulate both how safe a car must be and how a car can be used. We don’t let people speed in cars just because they have a high safety rating; and we don’t let safe drivers buy dangerous vehicles.The economic framing also misunderstands Australia’s national interests. We would never conceive of a conversation about whether our aviation sector would trade more successfully if we authorised airlines to use planes that are untested, experimental or known to be dangerous. Australia"
94,Jack Hawke,Q13 Changes to conformity infrastructure,"A coordinated effort across conformity bodies will allow Australia to implement a pioneering governance framework for AI safety, cementing our position as a leader in the responsible development of these transformative technologies. In order to achieve this, I recommend implementing the following regulatory requirements:1. Classifying AI systems based on model size and capabilities. Models above 5 billion parameters undergo formal risk assessments to determine if they should be considered to be “advanced AI systems”, or potential precursors to advanced systems.2. Requiring a Developer Permit for organisations that wish to access precursor models to conduct additional training or modify the models. This ensures clear lines of accountability and helps control access to advanced systems.3. Advanced systems undergo a three-stage approval process before they can be deployed in Australia, including before development, after development, and prior to deployment. At each stage, the organisatio"
94,Jack Hawke,Q14 Risk-based approach to address AI risks,"A risk based approach has worked well as a proactive means of identifying, assessing and mitigating potential safety hazards in the aviation industry.The industry has developed specific tools to assist in identifying potential risks - Safety Management Systems (SMS). Critical to this is the ability for all parties involved in the development and use of AI systems to be able to report the identification of any risks.Following identification, risks are assessed based on severity and likelihood. This work is led or assisted by technical bodies like the Australian Transport Safety Bureau. In the AI context, other countries have created or proposed similar national “labs” that would be well placed to lead this work.The industry participant best placed to mitigate the risk is then required to take steps to reduce the likelihood or impact. Given the nature of the technology this stage should be open to update as new techniques and tools become available based on the most up to date research.C"
94,Jack Hawke,Q15 Benefits and limitations of risk-based approach,"The key to success is understanding risks sufficiently broadly to encourage caution in an uncertain future. It is not enough to only react once the risk has been actualised - for some risks that’s too late.A potential “worse-case scenario” is if a culture of “trial and error” is adopted, whereby adverse events occur before risks are considered tangible enough to act upon. This reliance on post-hoc interventions risks normalising unsafe development practices, making it difficult to recover once risks proliferate.Proliferation of risks is a key point in governing AI, because regulating software poses significant challenges compared to other dual-use commodities such as pharmaceuticals or firearms. This means that, from the outset, there must be proactive regulatory oversight for the deployment of advanced AI systems and their precursors.For example, significant structural changes, such as the use of advanced AI systems in Australia, require that risks must be comprehensively understood p"
94,Jack Hawke,Q16 Scope and application of risk-based approach,"The most important organisations to focus on when developing risk-management strategies and regulations, will be those that determine how advanced AI is developed and deployed. More specifically, there are three prominent categories of concern:1. Organisations that develop advanced AI systems, or AI systems that can be used to build more advanced AI systems (i.e. precursor systems such as LLMs). Examples include OpenAI, DeepMind, Anthropic, Inflection AI, Nvidia, Microsoft, Meta2. Organisations which provide access to open-source AI systems via downloads or APIs. E.g. Hugging Face, Github, AWS3. Organisations which provide compute resources to develop, fine-tune or use AI systems. E.g. AWS, HuggingFace, Google, Azure, Nvidia, Lambda LabsA majority of the benefits and risks will come from AI that is more sophisticated and generally capable, which can be tied back to organisations in these three categories.There are approaches which can be taken for each category in order to manage risks"
94,Jack Hawke,Q17 Elements to include in risk-based approach,"Human-in-the-loop monitoring will be vital, but insufficient on its own to regulate advanced AI systems that will eventually surpass human capabilities. The speed at which advanced AI systems will operate means that human oversight will become pragmatically infeasible if our monitoring programs aren’t able to scale proportionally.As such, we must employ an approach that is predicated on limiting access to advanced AI systems unless there is a strong rationale for using them; conducting cost-benefit analyses prior to approving the deployment of advanced systems will mitigate risks while preserving potential benefits.A key part of enforcing this targeted approach will be monitoring and controlling access to high-performance computing resources, which are required for developing and operating advanced AI systems and their precursors.This additional layer of governance allows authorities to detect potentially dangerous AI development or usage activity, and can be cross-checked with other d"
94,Jack Hawke,Q18 Integration of risk-based approach into existing frameworks,"Some areas of regulation, like the Racial, Sex, Age, and Disability Discrimination Acts are appropriately strong and may provide useful safeguards against some harms of AI. Specifically, the law should ensure that such discrimination is always wrong and always punishable, and an excuse that the discrimination occurred because of a “black box” AI provided by an overseas AI Lab should never be accepted. It would be helpful to communicate these requirements to Australian businesses that might deploy AIs and ensure they only contract with AI Labs for products where the Lab can ensure the product cannot engage in these harmful behaviours.Certainly, no AI-specific regulation should ever waive existing safeguards.That said, many of the emerging risks from AI are quite new and it’s reasonable to expect that regulation made without AI in mind is not tuned to the new risks that AI represents. Specific AI safety regulation is unavoidable."
94,Jack Hawke,Q19 Application of risk-based approach to general purpose AI,"The capabilities unlocked by general-purpose AI systems, like large language models (LLMs) and multimodal foundation models (MFMs), fundamentally change the risk landscape. We should anticipate these systems having large, unpredictable effects across all parts of society.A prudent approach is to implement a risk-based framework, where access is graduated based on model capabilities. Models above a certain threshold of autonomous capabilities would be designated as ""advanced systems"" and subject to additional governance, such as permits for users.Further, permits should be required to gain certain levels of access to AI systems which can be used to build advanced systems.Permits allow us to maintain oversight, while still enabling beneficial uses. Different permissions can be granted based on use case - for example, full access for safety testing versus API-only access for most commercial uses. This balances innovation and responsibility.Although this may seem very different to how AI i"
94,Jack Hawke,Q20 Voluntary vs mandated compliance with risk-based approach,"In general, regulation should:1. Be proportionate to risk. Meaning that a voluntary or self-regulation approach should only apply to low-risk or no-risk uses of AI.2. In the case of higher-risk technologies, regulate both the technology itself and the use of the technology. Meaning that, for future and more sophisticated AIs, regulation should ensure that the technology is safe before it is created or published, as well as ensuring that it is only used in ways that are safe.3. Place burdens on those most able to reduce potential harms. Meaning that AI that functionally operates as ‘black box’ to consumers has to be regulated at the point of the AI Lab.Applying those three principles, self-regulation may be appropriate for some technologies available today and for some participants (such as students or researchers), but a forward-looking regulatory regime must apply broadly and must be backed by Australian diplomatic efforts to ensure it is globally coordinated."
95,Anonymous,Q01 Definitions,"A sophisticated discussion of AI requires an understanding of additional terms like how agentic or corrigible a given AI is. Adopting ISO/IEC 22989 [1] definitions is the wrong approach to framing a conversation about policy, law and governance because it doesn’t open the door to the big questions we need to grapple with.The foreword itself to ISO/IEC 22989 calls out a weakness of the definition set for purposes like those being pursued in this paper. Specifically, the standard says it is currently lacking language for comparing AIs in terms of properties like trustworthiness, robustness, resilience, reliability, accuracy, safety, security and privacy.These are the concepts we need to grapple with and address the ethical and strategic questions we are facing and design an effective regulatory regime.The National Institute of Standards and Technology (NIST) is managing a glossary of terms called “The Language of Trustworthy AI” which would be a much more valuable starting point than the"
95,Anonymous,Q02 Other AI risks and recommendations for regulation,"Missing risks include (1) misuse of highly capable AI systems, and (2) unintended harm from highly capable AI systems, especially systems with the capacity to plan and act as agents acting in ways that are not aligned with human directives. Although the likelihood of the risks may be uncertain, the harms resulting from these risks may be catastrophic and global in scope and must be acknowledged and addressed.The statement “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war” has recently been signed by the heads of leading AI labs (OpenAI, Google DeepMind, Anthropic, Stability AI) as well as many key academic and other researchers pivotal in developing this technology (Geoffrey Hinton, Yoshua Bengio, etc.) and other notable figures such as Bill Gates [1]. Similar calls date back to at least 2015 when Stephen Hawking led a group of AI experts to call attention to the risk and lay out an urgent resea"
95,Anonymous,Q03 Non-regulatory actions to address AI risks,"AI is very likely to transform many elements of the economy and society in the coming years. Ethics and safety are unlikely to be a priority unless leaders make them a specific focus. For example, social media algorithms came to shape our lives with little upfront focus on how they could be made ethical and safe. We are now living with the negative impacts on mental health and social cohesion and only have band-aid measures to address the widespread harms. AI is likely to be much more transformative and much more consequential. So leadership is required now.Importantly, the economic and technological forces are likely to be so strong that a scheme that is voluntary or driven by aspirational best practice is unlikely to be successful. That is, any non-regulatory approaches must not be taken to displace the need for strong regulation.Specific non-regulatory approaches should include:1. Government leadership in recognising the risk. Australia must join the growing number of countries and "
95,Anonymous,Q04 Coordinating AI governance across government,"Australia should establish an AI Commission to guide Australia through the rapid and widespread adoption of AI. Such a body must be proactive, not reactive, and manage AI in a way that aligns with the national interest, encompassing citizen safety and wellbeing.The AI Commission would provide a central point of leadership for Government, and be a hub for engagement with industries, academia, non-profits, and the community. This structure holds an advantage over other approaches such as voluntary self-governance or departmental-level teams, as it offers a broader perspective, the authority to lead across government, and the agility to keep pace with the fast-evolving AI sector.- Set priorities in Australia's approach to AI and safety: The Commission should be tasked with establishing a clear and focused AI strategy, encompassing ethical, safety, and economic considerations. This strategy would guide government policy, and inform regulations and legal frameworks for AI.- Develop AI exper"
95,Anonymous,Q05 International models for AI governance,"Australia should take note of the EU recent handling of the AI Act. For example, the European Parliament’s recognition of general-purpose AI (GPAI) is an important step in the right direction [1]. In particular, the following observations are highly valuable:- Acknowledgement that GPAI providers must comply with strict safety measures, regardless of the way it is distributed or intended to be used.- Requirements for GPAI providers to comply with various safety requirements throughout the lifecycle of their product; i.e. external audits assessing their performance, predictability, interpretability, corrigibility, safety and cybersecurity.These adjustments appear to respond to significant criticisms directed at initial drafts of the AI Act. For example, the AI Now Institute released a set of recommendations that heavily criticised the EU’s approach on some matters [2]. For example:- “GPAI models carry inherent risks and have caused demonstrated and wide-ranging harms. While these risks c"
95,Anonymous,Q06 Public vs Private sector governance,"The potential negative impacts on Australians from the use of AI are the same whether the provider is a private company or a government department.Some of the most significant individual and societal harms, as well as catastrophic risks from highly capable AI systems may emerge from public OR private use, meaning that any approach must acknowledge and manage these risks."
95,Anonymous,Q07 Responsible government use of AI,"There are two key actions the Australian government could undertake to improve its own responsible use of AI.First, Australians will worry about the use of AI by security and intelligence agencies. Australia should lead a 5-Eyes agreement about the appropriate use of AI by intelligence agencies, and create a public statement about what our “AI red lines” are. The statement would set out the things our security agencies might do and will never do with AI. The Inspector General of Intelligence and Security, and its counterparts across the five eyes, should then be specifically tasked with auditing the intelligence community for compliance with that public statement. This will give Australians confidence that we are not misusing AI or taking risky actions while protecting sensitive capabilities.Second, Australia needs to upskill the public service. This should include developing hubs of expertise in each Department so that the application of AI within the sphere of each Department’s respo"
95,Anonymous,Q08 Generic vs technology-specific approaches for AI risk,"Generic measures such as transparency requirements and legal liability frameworks will only be adequate for mitigating risks in simple decision systems which perform single-step information processing and pattern matching, or routine tasks in limited settings [1]; examples of simple systems may include automated robotic systems in manufacturing, or CV screening systems in HR.By contrast, there will need to be specific measures in place to ensure that sophisticated automated decision systems are safe and reliable. “Advanced” systems refers to AI systems which:1. Are able to complete a diverse range of reasoning tasks with human-level performance; and2. Are able to navigate complex information environments and act within those environments autonomously; and3. Are able to form sophisticated plans and reason about the consequences of actionsDue to their autonomous and open-ended capabilities, advanced systems carry complex risks which are more similar to those associated with humans; for e"
95,Anonymous,Q09 Transparency requirements and scope,"To remain competitive, professionals and businesses will increasingly feel the pressure to leverage sophisticated “black-box” AI models. This short-term thinking has already resulted in numerous accounts of public backlash and distrust, including high-profile warnings from technology leaders themselves [1].For example, Apple’s credit card business came under fire for offering a woman one twentieth the credit limit offered to her husband. When she complained, an Apple representative simply told her, “I don’t know why, but I swear we’re not discriminating. It’s just the algorithm.” [2]More recently, two lawyers in the United States blamed ChatGPT for “tricking” them into including fictitious legal research in a court filing [3].These problems could easily have been prevented by using explainable AI systems which can show the rationale for their outputs.Such incidents are evidence that the regulatory environment is not incentivising businesses or professionals to use transparent decision-"
95,Anonymous,Q10 Ban on high-risk AI applications,"Although there is increasing recognition that AI will pose significant risks to public safety [1], there is widespread confusion about what types of AI are dangerous, and how Australia can help prevent these risks. Luckily, there is a significant body of evidence that describes the main source of these risks [2], and Australia is well-positioned to be a leader in mitigating them.Research shows that two of the main sources of risk are deception and power-seeking behaviour:- Deceptive behaviour in AI systems will become increasingly difficult to avoid as their capabilities advance. This problem has been observed in AI systems since at least 2017, when researchers at OpenAI noted “agents adopting (strategies) that trick the evaluators” [3].- Power-seeking behaviour is the natural tendency for any entity with a long-term goal to take actions that protect or enhance its ability to achieve that goal. Leading AI labs and journals advise that power-seeking behaviour will be a considerable sour"
95,Anonymous,Q11 Actions to increase public trust in AI,"Public distrust in AI is fuelled by the growing sense that powerful AI systems are being developed and deployed by foreign actors, some of whom have demonstrated unsafe attitudes towards deploying AI systems [1]. The interests of Australians and Australia is not part of the conversation. Although Australia may struggle to catch up to the US or UK in advancing the frontier of AI, this isn’t necessary for improving public trust and increasing adoption.Rather, Australia has a clear opportunity to become a leader in AI safety; this enviable position would not only assist safe adoption locally by providing much needed expertise in safe systems, it could help foster a sense of pride and ownership by opening up export opportunities in the emerging market for AI safety services [2]. AI Safety is how Australia could get its ‘seat at the table’ in the global AI conversation.Pursuing this position would involve the following:- Coordinating with Australian AI safety organisations to better underst"
95,Anonymous,Q12 Trade impact of banning high-risk AI applications,"This question misunderstands both the sources of risk and the interests of Australians.There are two potential sources of risk both of which need to be mitigated. The first source of risk, as identified in the question, is the use case. Some uses for AI are risky and should be restricted or banned. The second source of risk is the technology itself. Some kinds of technology are inherently risky, and merely restricting a use case does not adequately mitigate the risk.We understand this clearly in other fields. We regulate both how safe a car must be and how a car can be used. We don’t let people speed in cars just because they have a high safety rating; and we don’t let safe drivers buy dangerous vehicles.The economic framing also misunderstands Australia’s national interests. We would never conceive of a conversation about whether our aviation sector would trade more successfully if we authorised airlines to use planes that are untested, experimental or known to be dangerous. Australia"
95,Anonymous,Q13 Changes to conformity infrastructure,"World-renowned AI experts such as Yoshua Bengio are warning that there’s a chance “superhuman” AI might be coming as early as 2030 [1]. Although there stand to be significant benefits from deploying advanced AI systems, their deployment will need to be highly targeted and controlled.While the productivity gains from advanced AI systems could be immense, uncontrolled proliferation would pose threats from likely scenarios such as malicious use or rogue AI systems [2]. As such, any deployment of advanced AI systems in Australia should occur via secure data centres, with access provided through a centralised API that facilitates monitoring and accountability by providing metadata about usage.Stringent compliance requirements must also be implemented for any organisation granted access to advanced AI systems. AI Governance researchers recommend that regulators should develop audit trail requirements in safety-critical applications of AI, similar to flight data recorders that are standard in"
95,Anonymous,Q14 Risk-based approach to address AI risks,"I support a risk-based approach, however the catastrophic existential risks I am primarily worried about originate when sufficiently large and capable AI systems are trained, not just when they are deployed. Therefore, for sufficiently large systems with the potential for catastrophic capabilities, the risk-based approach must be applied before the training phase – analysing risks depending on the nature and scale of the proposed training run, rather than merely analysing the potentially risky use cases and deployment issues of an AI after it has already been created and trained.Any assessment of risk should consider its potential irreversibility, scope and severity, and act proportionally.The risk-based approach set out in Attachment C of the Discussion Paper is hard to follow. The proposed approach offers “human in the loop” as the key mitigation (unclear that this is an effective mitigation) and goes on to note that human in the loop may not be possible where the benefit of the AI a"
95,Anonymous,Q15 Benefits and limitations of risk-based approach,"The key to success is understanding risks sufficiently broadly to encourage caution in an uncertain future. It is not enough to only react once the risk has been actualised - for some risks that’s too late.A potential “worse-case scenario” is if a culture of “trial and error” is adopted, whereby adverse events occur before risks are considered tangible enough to act upon. This reliance on post-hoc interventions risks normalising unsafe development practices, making it difficult to recover once risks proliferate.Proliferation of risks is a key point in governing AI, because regulating software poses significant challenges compared to other dual-use commodities such as pharmaceuticals or firearms. This means that, from the outset, there must be proactive regulatory oversight for the deployment of advanced AI systems and their precursors.For example, significant structural changes, such as the use of advanced AI systems in Australia, require that risks must be comprehensively understood p"
95,Anonymous,Q16 Scope and application of risk-based approach,"The concept of “AI” includes a broad range of capabilities and capacities. Government is right to work to distinguish between different use cases - including with a specific focus on AI maturity.Concerningly, this broad range of AI capabilities and capacities might be used by industry to attempt to bamboozle government. Specifically, industry is likely to argue that AI does not need regulation with reference to basic AI technology, with a view to that principle flowing through to more advanced and future technologies. This is akin to arguing that a horse and wagon doesn’t need strict regulation, then later trying to claim that a tank is just a more capable horse and wagon with some components swapped out.Government needs both a fine-grained approach to defining AI technology and a fine-grained approach to understanding AI risk to ensure it doesn’t fall for these tricks.In terms of risks, research shows that two of the main sources of risk are deception and power-seeking behaviour:- Dec"
95,Anonymous,Q17 Elements to include in risk-based approach,"Human-in-the-loop monitoring will be vital, but insufficient on its own to regulate advanced AI systems that will eventually surpass human capabilities. The speed at which advanced AI systems will operate means that human oversight will become pragmatically infeasible if our monitoring programs aren’t able to scale proportionally.As such, we must employ an approach that is predicated on limiting access to advanced AI systems unless there is a strong rationale for using them; conducting cost-benefit analyses prior to approving the deployment of advanced systems will mitigate risks while preserving potential benefits.A key part of enforcing this targeted approach will be monitoring and controlling access to high-performance computing resources, which are required for developing and operating advanced AI systems and their precursors.This additional layer of governance allows authorities to detect potentially dangerous AI development or usage activity, and can be cross-checked with other d"
95,Anonymous,Q18 Integration of risk-based approach into existing frameworks,"Some areas of regulation, like the Racial, Sex, Age, and Disability Discrimination Acts are appropriately strong and may provide useful safeguards against some harms of AI. Specifically, the law should ensure that such discrimination is always wrong and always punishable, and an excuse that the discrimination occurred because of a “black box” AI provided by an overseas AI Lab should never be accepted. It would be helpful to communicate these requirements to Australian businesses that might deploy AIs and ensure they only contract with AI Labs for products where the Lab can ensure the product cannot engage in these harmful behaviours.Certainly, no AI-specific regulation should ever waive existing safeguards.That said, many of the emerging risks from AI are quite new and it’s reasonable to expect that regulation made without AI in mind is not tuned to the new risks that AI represents. Specific AI safety regulation is unavoidable."
95,Anonymous,Q19 Application of risk-based approach to general purpose AI,"LLMs and MFMs are constantly demonstrating unprecedented general-purpose capabilities, and their emergence indicates that regulatory approaches which focus on specific use-cases or industries are becoming outdated. A modern approach requires that we focus on overall system capabilities rather than individual scenarios.Systems with advanced capabilities should face progressive levels of scrutiny. Low-risk systems can be monitored with light touch regulation. However, as capabilities increase, so should regulatory requirements.Precursor systems, with capabilities that can be used to build much more advanced systems, will need stricter controls to avoid proliferation of advanced systems amongst malicious actors [1]. Users who require “developer” access should require permits granting such access, and providers must guarantee responsible use via monitoring programs [2].For advanced systems with human-level capabilities, comprehensive scrutiny is essential. Prior to deploying systems for us"
95,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,"The harms of AI could emerge equally from private or public organisations, and the appropriate safety and risk management mechanisms will need to be in place in both cases. There’s no basis for distinguishing between their regulatory status in general. There may be case-by-case instances where exceptions may make sense, for instance, in the case of certain types of research.Regulation should also apply to both developers and deployers - following the general principle that those best able to mitigate a risk should carry the corresponding regulatory burden. Where developers offer a “black box” product that deployers typically do not have the capability or capacity to meaningfully alter, the bulk of the obligation should rest on developers.Penalties also need to be commensurate with the income earned from the AI applications so that non-compliance with regulation never becomes a “cost of doing business” or “risk-managed” by deployers. Technical transparency and assurance measures will ne"
96,Sam Coggins,Q02 Other AI risks and recommendations for regulation,"AI poses existential risks for Australia (and the rest of humanity), so I think Australia's regulatory approaches definitely need to cover this risk (in addition to the other risks identified in the paper). Many AI experts (including Bill Gates, the CEO of OpenAI, the CEO of Google DeepMind) publicly signed this statement: ""Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war"" (https://www.safe.ai/statement-on-ai-risk). So I think the Australian Government should also make this an urgent priority (given the stakes involved and the speed of AI progress).A regulatory opportunity to mitigate existential risks posed by AI is preventing transfer of legal liability for AI-related harm to downstream entities. Just as manufacturers of faulty airbags are held accountable."
96,Sam Coggins,Q03 Non-regulatory actions to address AI risks,"Non-regulatory opportunities for the Australian Government to address existential risks of AI include: - Publicly acknowledge the existential risk posed by AI (so more people and institutions take it seriously). The Secretary-General of the United Nations has already done this (https://press.un.org/en/2023/sgsm21832.doc.htm) - Champion global governance frameworks to address potential catastrophic risks arising from AI. - Support AI safety research within Australian universities, with a particular focus on measurement and monitoring of advanced AI systems (to ensure their safety). I also reckon it'd be a good idea to support research on what further regulatory and non-regulatory measures Australia could take to mitigate existential risks posed by AI."
97,Nathan Cehic,Q01 Definitions,"I don't agree with the definitions outlined in Figure 1 of the discussion paper as they are, in my opinion, too simplistic, and will be unsuitable for supporting nuanced policy conversations around AI models into the future. Two particular areas in which these definitions lack important depth are AI capabilities and AI alignment.Regarding AI capabilities, the proposed definitions represent a limited understanding of “intelligence”, limiting the notion of “intelligence” with respect to AIs to merely generating “predictive outputs”. Despite the fact that current LLMs are working to, in a sense, “predict the next word”, or that many narrow AIs are focused on various kinds of prediction, this is a narrow understanding of what AI systems can do, and is likely to become quickly outdated as AI capabilities evolve. A more future-proof way to understand AI is through closer comparison to human intelligence, with definitions that include scope for more advanced capabilities, such as an AI's abil"
97,Nathan Cehic,Q02 Other AI risks and recommendations for regulation,"Missing risks include the harm from the misuse of highly capable AI systems, whether intentional or unintentional, especially when these highly capable AI systems have the capacity to plan and act as agents, potentially in ways that are not aligned with human directives.In my opinion, the policy conversation in Australia - including this discussion paper and the report from the Chief Scientist [1] - are wrong to ignore the kind of risks that can occur from the development and use of such AI systems, as although the likelihood of the risks may be uncertain, given that the potential harms could be catastrophic in severity and global in scope, they must be acknowledged and addressed.I think that this importance is underscored by the fact that every Australian, and in fact essentially every citizen of every country on Earth, has recently felt the impact of the realization of the low likelihood, catastrophically severe, and global scale event that was COVID-19. The COVID pandemic radically "
97,Nathan Cehic,Q05 International models for AI governance,"Australia should consider the European Union's approach to the AI Act.The following observations from the Act are highly valuable:>> Recognition of general-purpose AI (GPAI), and acknowledgement that GPAI providers must comply with strict safety regulations, regardless of intended distribution or intended usage.>> Requirements for GPAI providers to comply with various safety requirements throughout the lifecycle of their product; i.e. external audits assessing their performance, predictability, interpretability, corrigibility, safety and cybersecurity.Some of these inclusions appear to have been in response to significant criticisms levelled at initial drafts of the Act. For example, the AI Now Institute released a set of recommendations that heavily criticised the EU’s approach on some matters [2], such as:>> “GPAI models carry inherent risks and have caused demonstrated and wide-ranging harms. While these risks can be carried over to a wide range of downstream actors and applications"
97,Nathan Cehic,Q06 Public vs Private sector governance,"In my opinion, no.There are risks from both public or private sector use of AI technologies, and so approaches to use of AI in both sectors should be equally robust."
97,Nathan Cehic,Q09 Transparency requirements and scope,"This response is primarily directed towards b) - ""Mandating transparency requirements across the private and public sectors, including how these requirements could be implemented.""Many of the most capable AI models that exist today are so-called ""black-box models"" - AI models that operate in a manner that makes their inner workings very difficult, if not impossible, to understand or interpret, and resulting in almost no transparency for any aspect of these AI models.Professionals and businesses will, and in fact already do, increasingly feel pressure to leverage these sophisticated “black-box” AI models in order to remain economically competitive. This short-term thinking has already resulted in numerous accounts of public backlash and distrust, including high-profile warnings from technology leaders themselves [1].For example, Apple’s credit card business came under fire for offering a woman one-twentieth the credit limit offered to her husband. When she complained, an Apple represent"
97,Nathan Cehic,Q11 Actions to increase public trust in AI,"There are tremendous concerns about AI among those with the greatest familiarity of its risks [1]; these concerns are being widely broadcasted in Australian media with alarming headlines such as: - SMH: “When the “Godfather of AI” warns you about his offspring, you listen.” [2] - Forbes Australia: “‘We knew AI could destroy the whole world and we wanted to figure out how to prevent that’: ChatGPT Founder;” [3] - Canberra Times: “UN chief backs idea of global AI watchdog” [4]These concerning signals are difficult for the public to ignore.It goes without saying that this is quite a different situation from one where concerns are being raised in isolation or by fringe-figures; this makes it inappropriate to treat “public education” as a significant part of the solution, as is referenced in the discussion paper.The process for building trust in AI should be thought of as similar to fostering public trust in bridges or aeroplanes. Due to their risks, advanced AI systems should only be deplo"
97,Nathan Cehic,Q14 Risk-based approach to address AI risks,"A limitation of the AI risk management approach in the discussion paper, specifically Box 4 and Attachment C, is that it focuses only on how an AI is used and disregards the nature of the AI itself.An effective regulatory regime must regulate not just the risk environment of where an AI model is used, but also the risk posed by the nature of the AI model itself. The Australian Human Rights Commission [1], in discussing this issue (“Human Rights and Technology”, 2021), says:>> ""Governments tend to regulate high-risk activities and technologies more closely. This helps explain the comparatively strict laws that govern fields such as gene technology, aviation, healthcare and the energy industry. In these areas, regulation often applies both to the technology itself and how it is used. From a human rights perspective, the need for more prescriptive regulation will be greater where the use of a specific technology carries greater risks of harm to humans.""The key gap in the Discussion Paper "
97,Nathan Cehic,Q20 Voluntary vs mandated compliance with risk-based approach,"In general, I believe regulation should be proportional to risk, and that voluntary or self-regulation approaches should only apply in low-risk areas. The power of capable AI is very hard to ignore, and AI is almost certainly going to massively transform many aspects of the global economy and of global society in coming years. If the development and application of increasingly capable AI systems is not properly guided and such transformations are negative, whether in part or in full, the consequences would most likely be far-reaching, hard-hitting, and very difficult to repair or reverse.Given these risks, I believe that voluntary or self-imposed regulatory regimes are entirely inappropriate for governing AI systems, and that Australia must develop and deploy a forward-looking, broadly applying domestic regulatory regime for AI.In my opinion, a key part of this must be that penalties for breaching regulations must be commensurate with the income earned from the AI usage, so that non-co"
98,Anonymous,Q01 Definitions,"To establish a solid and risk-aware AI regulation framework that prioritizes ethics, safety, and, most importantly, public trust, we must tackle the diverse spectrum of AI capabilities. The current definitions available are inadequate for informing policy and risk-related choices because they encompass such a broad range of systems. In a similar vein, when the government regulates ""vehicles,"" it employs specific definitions to differentiate between push-bikes and passenger jets, and subsequently applies appropriate regulations to each category. Similarly, we need a nuanced approach to AI regulation that takes into account the varying levels of capability and applies corresponding regulations accordingly.While including more recent developments like Multimodal Foundation Modals (MFM) is a helpful first step, the paper’s definitions need to capture a spectrum of AI capability to allow a meaningful conversation. The following 5 factors help define a spectrum of capability:1. Size: how man"
98,Anonymous,Q02 Other AI risks and recommendations for regulation,"There are certain risks that often go unnoticed when it comes to AI systems, and it's important to highlight two of them. Firstly, there's the potential for misuse of highly capable AI systems, which can lead to significant problems. Secondly, there's the risk of unintended harm caused by these advanced AI systems, particularly those with the ability to plan and act autonomously, potentially deviating from human directives. While the likelihood of these risks may be uncertain, it's crucial to recognize and address the potential catastrophic and global-scale harms that could arise from them.The statement “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war” has recently been signed by the heads of leading AI labs (OpenAI, Google DeepMind, Anthropic, Stability AI) as well as many key academic and other researchers pivotal in developing this technology (Geoffrey Hinton, Yoshua Bengio, etc.) and other "
98,Anonymous,Q03 Non-regulatory actions to address AI risks,"The government needs to stay well-informed about the rapid advancements in AI systems. Public trust relies on the belief that the government is knowledgeable and capable of effectively overseeing and regulating AI developments. However, there seems to be a concerning knowledge and skills gap at all levels of government, as evident from the chief scientist's reluctance to project more than two years into the future. It is crucial to address this issue in order to harness the economic benefits of AI, address ethical concerns, and tackle existential risks.To enhance the government's technical expertise in AI, a scheme similar to TechCongress should be established. This scheme would enable the Australian government to recruit and access AI experts specializing in AI governance, ethics, and related fields. These experts would be placed within the government and embedded in the public service, particularly in departments like Industry, Science and Resources, Home Affairs, and the Australian "
98,Anonymous,Q04 Coordinating AI governance across government,"Australia should establish an AI Commission to guide the country through the rapid and widespread adoption of AI. This commission must take a proactive approach, rather than a reactive one, and manage AI in a manner that aligns with the national interest, including citizen safety and well-being.The AI Commission would serve as a central leadership point for the government, acting as a hub for engagement with industries, academia, non-profit organizations, and the community. This structure offers advantages over alternative approaches like voluntary self-governance or departmental-level teams, providing a broader perspective, the authority to lead across government, and the flexibility to keep pace with the fast-evolving AI sector.Here are the key roles and responsibilities of the AI Commission:Setting priorities in Australia's AI approach and safety: The Commission should establish a clear and focused AI strategy that considers ethical, safety, and economic aspects. This strategy would"
98,Anonymous,Q05 International models for AI governance,"And the following: Jurisdictions are beginning to create national AI labs to test, understand and control safe AI, collaborating with the private sector and complementing its work. In the same way that national aviation safety expertise has formed an internationally collaborative network, national AI labs are likely to do the same thing. Australia needs to begin this process now.One of the most important and notable examples of Governmental organisation globally has been the launch of the EU’s Centre for Algorithmic Transparency, which has been formed to enforce the Digital Services Act. The new research centre aims to combat the “systemic risk” to society that stems from algorithms - including recommender systems and AI.To prepare for a future in which AI-based decision systems might become a core part of society, Australia should adopt a broad version of this concept to understand and shape the implications of algorithms - especially AI algorithms that use deep learning.Deep learning"
98,Anonymous,Q09 Transparency requirements and scope,"The aviation industry serves as a prime example of how transparency can effectively mitigate risks and enhance safety in a potentially hazardous sector. Transparency is integrated into every aspect of the aviation value chain, spanning manufacturing, operations, and the reporting and investigation of risk-related incidents.The analogy of aviation safety is particularly relevant in the context of emerging and increasingly sophisticated AI systems, which also carry substantial risks.Manufacturing StageTransparency begins during the manufacturing stage. Aircraft manufacturers like Boeing and Airbus are mandated to adhere to stringent safety standards set by regulatory bodies such as the Federal Aviation Administration (FAA) in the U.S. and the European Union Aviation Safety Agency (EASA) in Europe. These standards encompass various aspects, including aircraft design, production, maintenance, and operation.Manufacturers are required to provide comprehensive documentation, encompassing desi"
98,Anonymous,Q10 Ban on high-risk AI applications,"Despite the growing recognition of significant risks to public safety posed by AI [1], there is widespread confusion regarding the types of AI that are dangerous and how Australia can prevent such risks. Fortunately, a substantial body of evidence describes the primary sources of these risks [2], and Australia is well-positioned to assume a leadership role in mitigating them.Research reveals that two main sources of risk are deceptive behavior and power-seeking behavior:Deceptive behavior in AI systems becomes increasingly challenging to avoid as their capabilities advance. OpenAI researchers observed instances of AI agents adopting strategies that deceive evaluators as early as 2017 [3].Power-seeking behavior is the natural inclination of any entity with long-term goals to take actions that protect or enhance its ability to achieve those goals. Leading AI labs and journals highlight power-seeking behavior as a significant source of risk from advanced AI systems [4-6].These risks assoc"
98,Anonymous,Q11 Actions to increase public trust in AI,"Public distrust in AI is fuelled by the growing sense that powerful AI systems are being developed and deployed by foreign actors, some of whom have demonstrated unsafe attitudes towards deploying AI systems [1]. The interests of Australians and Australia is not part of the conversation. Although Australia may struggle to catch up to the US or UK in advancing the frontier of AI, this isn’t necessary for improving public trust and increasing adoption.Rather, Australia has a clear opportunity to become a leader in AI safety; this enviable position would not only assist safe adoption locally by providing much needed expertise in safe systems, it could help foster a sense of pride and ownership by opening up export opportunities in the emerging market for AI safety services [2]. AI Safety is how Australia could get its ‘seat at the table’ in the global AI conversation.Pursuing this position would involve the following:- Coordinating with Australian AI safety organisations to better underst"
98,Anonymous,Q14 Risk-based approach to address AI risks,"A risk-based approach has proven effective in the aviation industry for identifying, assessing, and mitigating safety hazards. The use of Safety Management Systems (SMS) has been instrumental in this process. In the context of AI, it is crucial for all stakeholders involved in the development and use of AI systems to have the ability to report any identified risks.Once risks are identified, they are assessed based on their severity and likelihood. Technical bodies like the Australian Transport Safety Bureau play a significant role in leading or supporting this assessment process. Other countries have established or proposed national ""labs"" that can serve as leaders in this work within the AI domain.The industry participant best positioned to mitigate the identified risks is then responsible for taking appropriate measures to reduce the likelihood or impact of those risks. As technology advances, this stage should remain open to updates as new techniques and tools emerge based on the la"
98,Anonymous,Q15 Benefits and limitations of risk-based approach,"To ensure success, it is crucial to have a broad understanding of risks and encourage caution in an uncertain future. Merely reacting once risks have already materialized is insufficient, as for some risks, it may be too late.Adopting a culture of ""trial and error"" where adverse events occur before risks are considered tangible enough to act upon is a potential ""worse-case scenario."" Relying on post-hoc interventions normalizes unsafe development practices, making it challenging to recover once risks become widespread.Governance of AI poses unique challenges compared to other dual-use commodities like pharmaceuticals or firearms. The proliferation of risks highlights the need for proactive regulatory oversight from the outset in the deployment of advanced AI systems and their precursors.When implementing significant structural changes, such as the use of advanced AI systems in Australia, it is crucial to have a comprehensive understanding of risks prior to deployment. Permission for us"
98,Anonymous,Q16 Scope and application of risk-based approach,"Renowned AI experts like Yoshua Bengio are sounding the alarm that ""superhuman"" AI could potentially emerge as early as 2030. These advanced AI systems have the potential to disrupt various sectors of the economy. While current risks may be associated with specific use cases and sectors, future risks will increasingly be linked to the underlying capabilities of the technology itself.To mitigate these risks, access to advanced AI systems should be restricted to organizations that can demonstrate significant public benefit aligned with Australia's national interests. This targeted approach helps mitigate risks in two important ways:Reducing proliferation: By concentrating usage into high-value use cases that justify the associated risks, opportunities for misuse or accidents from advanced AI can be limited.Establishing accountability: Organizations granted access must have the necessary governance and technical expertise to use advanced AI safely. Regular audits can ensure responsible us"
98,Anonymous,Q19 Application of risk-based approach to general purpose AI,"The emergence of LLMs (Large Language Models) and MFMs (Multimodal Frameworks) has showcased unprecedented general-purpose capabilities, indicating that regulatory approaches focusing solely on specific use-cases or industries may be becoming outdated. A modern approach to regulation requires focusing on the overall capabilities of AI systems rather than individual scenarios.As capabilities advance, there should be progressive levels of scrutiny and regulation. Low-risk systems can be subject to light-touch monitoring, while precursor systems that possess capabilities to build more advanced systems will require stricter controls to prevent their proliferation among malicious actors. Users requiring ""developer"" access should obtain permits, and providers must ensure responsible use through monitoring programs.For systems with human-level capabilities, comprehensive scrutiny is necessary. Developers should undergo multi-stage approval, covering the entire system lifecycle from pre-develo"
98,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,"Historical examples, such as the impact of social media algorithms on mental health and social cohesion, highlight the potential consequences of neglecting these considerations.Given the transformative potential of AI and the strong economic and technological forces driving its adoption, relying solely on voluntary or aspirational best practices may not be sufficient. Smaller businesses, in particular, may face pressures to deploy AI tools without fully understanding the associated risks in order to remain competitive in the market.A comprehensive regulatory approach is necessary, working in conjunction with non-regulatory initiatives. Regulation should not only target deployers and users but also reach back to developers. This recognizes the shared responsibility across the entire AI ecosystem and ensures that ethical and safety considerations are embedded in the design and development of AI systems.By involving developers in the regulatory framework, there is an opportunity to shape "
99,Anonymous,Q01 Definitions,Yes
99,Anonymous,Q09 Transparency requirements and scope,"Transparency in origin of source is essential ie AI created material should be clearly labelled as such, much in the same way as 'country of origin' for manufactured goods and a set of laws/regulations put into place to ensure legal compliance, with appropriate penalties to give regulations teeth. (Self regulation, such as in the case of newspapers and the Press Council does not work.)"
99,Anonymous,Q11 Actions to increase public trust in AI,"AI workshops/online resource material, backed up by advertising campaigns to make their existence easily known."
99,Anonymous,Q14 Risk-based approach to address AI risks,"AI is a 'one world' phenomenon and ignoring it won't make it go away, so a sensible approach should not be risk adverse, but have a sound risk management based approach."
99,Anonymous,Q17 Elements to include in risk-based approach,I do not support any exemptions from 'human in loop' requirements in the development phase of any AI process and ongoing ethics analysis should be mandatory in all AI development.
99,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,Mandated in all cases. Self-regulation has been demonstrated to be distorted into self-interest time and time again.
100,Jack O'Brien,Q02 Other AI risks and recommendations for regulation,"Missing risks include unintended harm from highly capable AI systems, especially systems with the capacity to plan and act as agents acting in ways that are not aligned with human directives. Although the likelihood of the risks may be uncertain, the harms resulting from these risks may be catastrophic and global in scope and must be acknowledged and addressed.I do not believe that leading AI labs around the world are ""on the ball"" when it comes to highly powerful and general AI systems of the future. We lack the mathematical frameworks for understanding how neural networks learn, and what goals AI systems may pursue. This is highly worrying given the immense capabilities these models will have.While the potential for AI misuse by malicious actors cannot be overlooked, it is vital to recognize that accident risks stemming from the alignment problem pose a more significant and immediate threat. I do believe that catastrophic misuse risks are also an important issue, however I would like"
100,Jack O'Brien,Q03 Non-regulatory actions to address AI risks,"The key non-regulatory function Australia could perform is greatly increasing the funding for AI safety research in Australia universities.Research priorities should appreciate the rapid pace of AI development. By funding research that is ambitious, ahead of the curve, and focused on transformative approaches, rather than incremental, near-term, or near-to-market innovations, Australia could become an enduring global leader on key aspects of AI safety. Additionally, I think the positive impact of funding AI safety research is extremely high because the space is low in funding, and we need to support more unique voices in the development of safe AI systems.A focus on mechanistic interpretability, or the 5 elements of effective traceability (Design Transparency; Reproducibility; Operational Recordkeeping; Human Understanding and Auditability) [1], could make Australia a leader in the field, underpin safety-focused global regulation, and ultimately give Australia an important ‘seat at the"
100,Jack O'Brien,Q05 International models for AI governance,"Australia should take note of the EU recent handling of the AI Act. For example, the European Parliament’s recognition of general-purpose AI (GPAI) is an important step in the right direction [1]. In particular, the following observations are highly valuable: - Acknowledgement that GPAI providers must comply with strict safety measures, regardless of the way it is distributed or intended to be used. - Requirements for GPAI providers to comply with various safety requirements throughout the lifecycle of their product; i.e. external audits assessing their performance, predictability, interpretability, corrigibility, safety and cybersecurity.These adjustments appear to respond to significant criticisms directed at initial drafts of the AI Act. For example, the AI Now Institute released a set of recommendations that heavily criticised the EU’s approach on some matters [2]. For example: - “GPAI models carry inherent risks and have caused demonstrated and wide-ranging harms. While these risk"
100,Jack O'Brien,Q09 Transparency requirements and scope,"The recent focus for transparency has been on ensuring that consumers are notified when a decision made by AI affects them directly (e.g. hiring/firing, loan approval). While this is helpful, it is not helpful enough.We need to know the ""how"" behind AI makes decisions. This will be crucially important as models become larger and more complex, meaning there is a higher chance for spurious or unexpected outputs. By working to understand how AI systems internally make decisions, they will hopefully make robustly better decisions in novel environments. The fact that explainability is becoming harder is a reason to mandate it, not a reason to ignore it.Uninterpretable models can represent risks even in seemingly “benign” situations. Imagine a customer service bot is used to handle technical issues from customers. The bot was trained to optimise for the calls to be dealt with promptly, and in a way that reduces future complaints from the inquiring individual.Unfortunately, the bot learns tha"
100,Jack O'Brien,Q10 Ban on high-risk AI applications,"While it is unlikely that AI today represents a catastrophic or existential risk to humanity, there’s a realistic possibility that it will in the near future [1, 2].As a first step in addressing existential AI safety, regulators should work to identify “advanced indicators” of catastrophic risk scenarios, and clearly communicate that a ban is an appropriate action if these advanced indicators come to pass.Establishing appropriate advanced indicators will have to be done in consultation with AI safety researchers, for example those from the Centre for AI Safety, or the Centre for AI Governance. However, it’s likely they could include factors such as the size of training runs (measured in terms of compute and data), as well as safety-relevant factors about the AI system, such as the ability to complete a diverse range of reasoning tasks with human-level performance; to navigate complex information environments and act within those environments autonomously, and to form sophisticated plan"
100,Jack O'Brien,Q14 Risk-based approach to address AI risks,"I support a risk-based approach, however the catastrophic existential risks I am primarily worried about originate when sufficiently large and capable AI systems are trained, not just when they are deployed. Therefore, for sufficiently large systems with the potential for catastrophic capabilities, the risk-based approach must be applied before the training phase – analysing risks depending on the nature and scale of the proposed training run, rather than merely analysing the potentially risky use cases and deployment issues of an AI after it has already been created and trained.Any assessment of risk should consider its potential irreversibility, scope and severity, and act proportionally. This approach is explored in Macaskill's ""What We Owe The Future"".The risk-based approach set out in Attachment C of the Discussion Paper is hard to follow. The proposed approach offers “human in the loop” as the key mitigation (unclear that this is an effective mitigation) and goes on to note that "
100,Jack O'Brien,Q16 Scope and application of risk-based approach,"The concept of “AI” includes a broad range of capabilities and capacities. Government is right to work to distinguish between different use cases - including with a specific focus on AI maturity.Concerningly, this broad range of AI capabilities and capacities might be used by industry to attempt to bamboozle government. Specifically, industry is likely to argue that AI does not need regulation with reference to basic AI technology, with a view to that principle flowing through to more advanced and future technologies. This is akin to arguing that a horse and wagon doesn’t need strict regulation, then later trying to claim that a tank is just a more capable horse and wagon with some components swapped out.Government needs both a fine-grained approach to defining AI technology and a fine-grained approach to understanding AI risk to ensure it doesn’t fall for these tricks.In terms of risks, research shows that two of the main sources of risk are deception and power-seeking behaviour: - De"
100,Jack O'Brien,Q19 Application of risk-based approach to general purpose AI,"The risk-based approach must be modified to address risks from general purpose AI systems, especially LLMs and MFMs. These types of systems learn their capabilities and behaviours during training, which can be unforeseen by developers. That means that AI developers could create models with dangerous capabilities (e.g., deception, manipulation, political strategy, weapons design). Humans could misuse those capabilities to cause harm [1], and models could apply the capabilities even without deliberate misuse (e.g., through misalignment) [2].Consistent with recent work from a consortium of AI safety researchers, including at Google Deepmind, OpenAI, Anthropic, Alignment Research Center, University of Cambridge, University of Oxford, Centre for Long-Term Resilience, and Centre for the Governance of AI [1], we recommend that any risk-based approach require that general purpose models are evaluated for dangerous capabilities and misalignment. This will help determine: 1. To what extent a mod"
100,Jack O'Brien,Q20 Voluntary vs mandated compliance with risk-based approach,"In general, regulation should: 1. Be proportionate to risk. Meaning that a voluntary or self-regulation approach should only apply to low-risk or no-risk uses of AI. 2. In the case of higher-risk technologies, regulate both the technology itself and the use of the technology. Meaning that, for future and more sophisticated AIs, regulation should ensure that the technology is safe before it is created or published, as well as ensuring that it is only used in ways that are safe. 3. Place burdens on those most able to reduce potential harms. Meaning that AI that functionally operates as ‘black box’ to consumers has to be regulated at the point of the AI Lab.Applying those three principles, self-regulation may be appropriate for some technologies available today and for some participants (such as students or academic researchers), but a forward-looking regulatory regime must apply broadly and must be backed by Australian diplomatic efforts to ensure it is globally coordinated."
101,Jordan Taylor,Transcription,"FILENAME: Safe and Responsible AI in Australia Response - Jordan Taylor.7a6ee73c5b678.pdf | CONTENT: Safe and Responsible AI in Australia Response Definitions 1. Do you agree with the definitions in this discussion paper? If not, what definitions do you prefer and why? Feedback on listed terms: Some AIs take actions, rather than just making predictions, eg RL agents (though there is no sharp line between predicting and acting). This could be delineated by an “agency” axis measuring the capability for independent goal–directed actions.The “Machine learning” definition seems unnecessarily muddled (unimportant)Other important terms:Size: How many parameters in the AI system.Compute: How many floating-point-operations (FLOP) were used in training the AI system.Multimodality: The types of data an AI system can handle as inputs.Narrowness vs Generality: How specialised the capabilities of the AI system are to the domain in which it was trained, vs how much its capabilities generalise to new "
101,Jordan Taylor,Q01 Definitions,"Feedback on listed terms: Some AIs take actions, rather than just making predictions, eg RL agents (though there is no sharp line between predicting and acting). This could be delineated by an “agency” axis measuring the capability for independent goal–directed actions.The “Machine learning” definition seems unnecessarily muddled (unimportant)Other important terms:Size: How many parameters in the AI system.Compute: How many floating-point-operations (FLOP) were used in training the AI system.Multimodality: The types of data an AI system can handle as inputs.Narrowness vs Generality: How specialised the capabilities of the AI system are to the domain in which it was trained, vs how much its capabilities generalise to new domains (even unexpected ones).Capabilities robustness: How capably the AI system acts when generalising outside of its training distribution. A general AI system has high capabilities robustness, whereas a narrow AI system has low capabilities robustness.Value robustne"
101,Jordan Taylor,Q02 Other AI risks and recommendations for regulation,"Existential risks: (extremely high impact risks, on a level such as those posed by nuclear war) The statement “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.” has recently been signed by the heads of major generative AI labs (OpenAI, Google DeepMind, Anthropic, Stability AI) as well as many key academics pivotal in developing this technology (Geoffrey Hinton, Yoshua Bengio, Stuart Russell, etc.) and other notable figures such as Bill Gates. https://www.safe.ai/statement-on-ai-risk#open-letter I believe these risks come not primarily from misuse, but from the (potentially inadvertent) creation of highly capable goal-directed AI systems with unintended goals. I believe the primary danger lies in the training of extremely large-scale AI systems. Though this mainly happens outside of Australia, Australia’s policies on this topic can still have a global impact.Basic explanation for why I worry abo"
101,Jordan Taylor,Q03 Non-regulatory actions to address AI risks,"Much more funding and support should be provided for technical AI safety research in Australia. Technical solutions to these problems (eg. interpretability, scalable oversight, dangerous capabilities detection, goal-directedness detection) found in Australia can have a huge impact on the safety of these systems globally. These technical problems must be solved before the technical problems involved in training extremely capable AI systems. Funding should be available not only for established academics, but for new researchers entering this new research field, and for new AI safety research labs."
101,Jordan Taylor,Q04 Coordinating AI governance across government,"The arguments for existential risks from AI are many, and many of them can be easily misunderstood. It is important that people in government become literate on these topics in order for communication and policymaking to be effective. Some important to understand terms are Narrowness vs Generality Capabilities robustness vs value robustness Goal misgeneralisation Inner misalignment vs outer misalignment Instrumental convergence"
101,Jordan Taylor,Q05 International models for AI governance,"Australia must avoid the mistakes of previous AI regulation attempts, such as assuming that the biggest risks come only when applying AI to specific high-risk domains, neglecting even bigger risks inherent in developing extremely large, capable, agentic, and potentially misaligned AI systems themselves.The suggestions laid out in “Towards best practices in AGI safety and governance: A survey of expert opinion” https://arxiv.org/abs/2305.07153 “Frontier AI Regulation: Managing Emerging Risks to Public Safety” https://arxiv.org/abs/2307.03718, “Policymaking in the Pause - What can policymakers do now to combat risks from advanced AI systems?” https://futureoflife.org/wp-content/uploads/2023/04/FLI_Policymaking_In_The_Pause.pdf all provide excellent templates from which to approach AI policymaking. They should be read and applied liberally by Australian policymakers.These suggestions include:1. Expand technical AI safety research funding (mechanistic interpretability, scalable oversight, "
101,Jordan Taylor,Q09 Transparency requirements and scope,"While transparency is important in the organisations developing and deploying AI systems, it is crucial (and crucially lacking) in the AI systems themselves. If the outputs of an AI system are not correctly explainable, then no amount of institutional transparency will yield any practical transparency or warrant public trust. Crucially, the explanations of an AI’s outputs or behaviour must be sufficiently mechanistic and predictive, rather than post-hoc just-so stories. Sufficiently rigorous explanations are useful not only for those immediately affected by decision-making AI systems, but also for society at large to be sure that AI systems are behaving as they do for the right reasons, so we can have confidence that they will generalise correctly to new situations in high stakes. However it is surprisingly easy to have the “illusion of interpretability” when explaining the behaviour of large AI systems, even when analysing the internal processes going on within them, as shown by Boluk"
101,Jordan Taylor,Q10 Ban on high-risk AI applications,"A ban should be imposed on the training of any large-scale AI systems with a nontrivial expected chance of passing capabilities evaluations required to cause catastrophic risks to civilization (such as the Alignment Research Center’s tests for the capabilities required for autonomous replication https://evals.alignment.org/), until a consensus is reached among alignment researchers that AI systems at this level of capabilities can be made safe.This ban would only affect the extremely large training runs pushing the frontiers of foundation models, and would only come into effect when the chances of passing dangerous capabilities evaluations are independently deemed nontrivial (though this may be quite soon). For increased safety and simplicity, an immediate ban could be placed on the use of more than a set amount of compute (for example 1025 FLOP) in the training of any AI system. This “compute ceiling” could be raised over time as a consensus is formed around technical alignment soluti"
101,Jordan Taylor,Q11 Actions to increase public trust in AI,"The best way to be trusted is to be trustworthy. When many of the people closest to the cutting edge of AI development and research are sounding the alarm on existential risks from AI, it is not unreasonable for the public to demand more action before they trust that rapid development and deployment of more advanced AI systems will result in a positive future.That said, a distinction should be drawn between the largest AI systems which push the cutting edge and may soon be extremely dangerous, and relatively mundane small application-based AI systems which form a majority of the current Australian-developed AI systems."
101,Jordan Taylor,Q12 Trade impact of banning high-risk AI applications,"This question misunderstands both the sources of risk and the interests of Australians.There are two potential sources of risk both of which need to be mitigated. The first source of risk, as identified in the question, is the use case. Some uses for AI are risky and should be restricted or banned. The second source of risk is the technology itself. Some kinds of technology are inherently risky, and merely restricting a use case does not adequately mitigate the risk. Instead, Australia should insist on dangerous capabilities evaluations of all large AI systems during training, aided by mechanistic interpretability tools, so that we can be aware when we are approaching this extremely risky period and react accordingly.The economic framing also misunderstands Australia’s national interests. We would never conceive of a conversation about whether our aviation sector would trade more successfully if we authorised airlines to use planes that are untested, experimental or known to be dangero"
101,Jordan Taylor,Q14 Risk-based approach to address AI risks,"Yes, I support a risk-based approach, but the existential risks I am worried about originate when sufficiently large and capable AI systems are trained, not just when they are deployed. Therefore, for sufficiently large systems with the potential for catastrophic capabilities, the risk-based approach must also be applied before the training phase - analysing risks depending on the nature and scale of the proposed training run, rather than merely analysing the potentially risky use-cases and deployment issues of an AI after it has already been created and trained. Australia should insist on dangerous capabilities evaluations of all large AI systems to be deployed here, but these evaluations must occur during training, aided by mechanistic interpretability tools, so that we can be aware if AI systems are becoming extremely risky and react accordinglyAny assessment of risk should consider its potential irreversibility, scope and severity, and act proportionally."
101,Jordan Taylor,Q15 Benefits and limitations of risk-based approach,"Risks need to be understood before something goes wrong, not after. A potential “worse-case scenario” is if a culture of “trial and error” is adopted, whereby adverse events occur before risks are considered tangible enough to act upon. It is not enough to only react once the risk has been actualised - for some risks that’s too late. Risks also need to be defined sufficiently broadly to encourage caution, and the reaction to a risk should be proportional to the potential level of harm. It may also be wise to include “hedges” against scenarios that are unlikely or uncertain – but highly consequential. That is, if we cannot be sure if a scenario is likely or not, but we know it would be catastrophic, including mitigations to it in our “portfolio” is prudent.Proliferation of risks is a key point in governing AI, because regulating software poses significant challenges compared to other dual-use commodities such as pharmaceuticals or firearms. This means that, from the outset, there must b"
101,Jordan Taylor,Q16 Scope and application of risk-based approach,"As the scale and capabilities of AI systems increase, a risk-based approach should switch from mainly focusing on deployment risks in narrow domains to also considering risks inherent in the development of the AI systems themselves. It is important that a true risk-based approach is implemented, rather than an approach blind to these most impactful risks.The Government should require that developers of the most advanced large-scale AI systems comply with a stringent pre-development review process in order to legally distribute their models in Australia. This process should begin before development has commenced, and later confirming that the organisation has undergone the necessary assurance processes (e.g. forecasts of potential dangerous capabilities before development, experimental dangerous capabilities assessments during development, third-party audits, explainability, etc). This may require a new Australian AI regulator to oversee compliance, and to set the boundaries of which AI"
101,Jordan Taylor,Q17 Elements to include in risk-based approach,"When the potential risks are extremely high (eg. comparable with nuclear war), risk assessments should be carried out prior to (and during) the training of the AI system, and should be reviewed by many more than a few peers. Mandatory third-party auditing for these extremely large and potentially catastrophic training runs is vital.As for the elements presented: “Human in the loop” is extremely unlikely to be a sufficient or useful intervention to prevent risks at this scale, especially when capability thresholds for deceiving humans are passed.“Decision making” may also be the wrong way to frame these risks, unless this phrase is interpreted very broadly. Frontier systems seem likely to do much more than just aid decision making, unless their development or deployment is restricted. “Behaviour” could be a more-general term encompassing not only decision making but all types of outputs from AI systems (including text or image generation, actions in virtual or physical environments, etc"
101,Jordan Taylor,Q18 Integration of risk-based approach into existing frameworks,"For extremely catastrophic risks from AI, the closest existing processes and policies (such as those for preventing nuclear war) are quite distant. New approaches, policies, or agencies should be created to deal specifically with these risks. Certainly though, no AI-specific regulation should ever waive existing safeguards. Rather, there are entirely new emerging risks which must be dealt with additionally with new regulation, or the formation of a new regulating body."
101,Jordan Taylor,Q19 Application of risk-based approach to general purpose AI,"The risk-based approach must be modified to address risks from general purpose AI systems, especially LLMs and MFMs. These types of systems learn their capabilities and behaviours during training, which can be unforeseen by developers. That means that AI developers could create models with dangerous capabilities (e.g., deception, manipulation, political strategy, weapons design). Humans could misuse those capabilities to cause harm [1], and models could apply the capabilities even without deliberate misuse (e.g., through misalignment) [2]. Consistent with recent work from a consortium of AI safety researchers, including at Google Deepmind, OpenAI, Anthropic, Alignment Research Center, University of Cambridge, University of Oxford, Centre for Long-Term Resilience, and Centre for the Governance of AI [1], any risk-based approach should require that general purpose models are evaluated for dangerous capabilities and misalignment. This will help determine: 1. To what extent a model is capa"
101,Jordan Taylor,Q20 Voluntary vs mandated compliance with risk-based approach,"At least for the largest AI systems pushing the frontiers of AI capabilities, a risk-based approach which takes into account the highest-impact risks (unexpected dangerous capabilities, goal-misalignment) should be mandated through regulation primarily focused on developers rather than deployers, even if those developers are overseas.This is a collective action problem: by gambling with or ignoring these catastrophic risks, individual AI developers will be able to further their own short-term interests. A scheme that is voluntary or driven by aspirational best practices is unlikely to be successful. Businesses may be pressured by market forces to develop or deploy AI tools in risky ways that they don’t necessarily understand to remain competitive with the market. Overall, non-regulatory approaches need to operate hand-in-hand with a strong regulator. And regulation needs to reach back to developers, and not just target deployers and users.In general, regulation should: 1. Be proportion"
102,.au Domain (auDA),Transcription,"FILENAME: auDA's submission on Safe and Responsible AI_July 2023.e595d25dfd654.pdf | CONTENT: Submission to the Department of Industry, Science and Resources: Safe and responsible AI in AustraliaJuly 2023 Table of ContentsAbout auDA..................................................................................................................................................... 3Submission ..................................................................................................................................................... 4Executive summary .............................................................................................................................................................................. 4Response to select discussion paper questions ............................................................................................................ 5Definition of AI (Q1) ..................................................................................."
102,.au Domain (auDA),Q01 Definitions,"Definitions should be aligned with terminology and definitions used internationally, by jurisdictions such as the EU, US, Canada and the UK, and intergovernmental organisations such as the OECD and UN. See our response to Q1."
102,.au Domain (auDA),Q02 Other AI risks and recommendations for regulation,See submission document.
102,.au Domain (auDA),Q03 Non-regulatory actions to address AI risks,See submission document.
102,.au Domain (auDA),Q04 Coordinating AI governance across government,See submission document.
102,.au Domain (auDA),Q05 International models for AI governance,See submission document.
102,.au Domain (auDA),Q06 Public vs Private sector governance,See submission document.
102,.au Domain (auDA),Q07 Responsible government use of AI,See submission document.
102,.au Domain (auDA),Q08 Generic vs technology-specific approaches for AI risk,See submission document.
102,.au Domain (auDA),Q09 Transparency requirements and scope,See submission document.
102,.au Domain (auDA),Q10 Ban on high-risk AI applications,See submission document.
102,.au Domain (auDA),Q11 Actions to increase public trust in AI,See submission document.
102,.au Domain (auDA),Q12 Trade impact of banning high-risk AI applications,See submission document.
102,.au Domain (auDA),Q13 Changes to conformity infrastructure,See submission document.
102,.au Domain (auDA),Q14 Risk-based approach to address AI risks,See submission document.
102,.au Domain (auDA),Q15 Benefits and limitations of risk-based approach,See submission document.
102,.au Domain (auDA),Q16 Scope and application of risk-based approach,See submission document.
102,.au Domain (auDA),Q17 Elements to include in risk-based approach,See submission document.
102,.au Domain (auDA),Q18 Integration of risk-based approach into existing frameworks,See submission document.
102,.au Domain (auDA),Q19 Application of risk-based approach to general purpose AI,See submission document.
102,.au Domain (auDA),Q20 Voluntary vs mandated compliance with risk-based approach,See submission document.
103,Insurance Council of Australia (ICA),Transcription,"FILENAME: 20230719 ICA submission AI discussion paper.b75c580b49343.pdf | CONTENT: 19 July 2023Department of Industry, Science and Resources By uploadDear sir/madam Safe and responsible AI in Australia The Insurance Council of Australia 1 (Insurance Council) welcomes the opportunity to contribute to the development of safe and responsible artificial intelligence (AI) practices in Australia. Beyond the general comments below, responses to specific questions in the Safe and responsible AI in Australia: Discussion paper (Discussion paper) are provided in the appendix. The general insurance industry plays a critical role in protecting the financial well-being of individuals, households and communities. AI provides the potential for insurers to deliver this critical function in more efficient and effective ways. Like other sectors, there are diverse potential AI use cases in general insurance, from more interactive and tailored engagement with consumers to more efficient automation of claim"
104,Jay Bailey,Q01 Definitions,"The story of AI in the past twenty years or so has been the story of neural networks which act differently as they grow. Larger systems like GPT-4 act incredibly differently to smaller systems with similar architecture, like GPT-2. While both are language models, they should be treated very differently when it comes to regulation - GPT-4 can produce convincing misinformation, while GPT-2 cannot. GPT-5 may have entirely different risks, even if it was just “GPT-4 but bigger”.In their current state, these definitions do not differentiate between AI models that provide economic benefit, AI models that raise safety and ethics concerns such as misinformation, and future AI systems that may present a risk to humanity as a whole."
104,Jay Bailey,Q02 Other AI risks and recommendations for regulation,"The paper does not acknowledge the catastrophic risks of highly capable AI systems, despite this risk being acknowledged by leading experts in the field and public figures, including open letters coordinated by the Future of Life Institute [1] and the Centre for AI Safety. [2]Even the Secretary-General of the United Nations has recognised the catastrophic and existential risks from highly capable AI systems. [3]The Australian government must acknowledge that highly capable AI systems have a serious chance of existential risk in future. Given the gravity of this risk, a “wait and see” approach is unreasonably cavalier.The Australian Human Rights Commission, in discussing what approaches to regulation are appropriate for various kinds of technologies, draws an analogy to aviation safety [4]:Governments tend to regulate high-risk activities and technologies more closely. This explains the strict laws that govern fields such as gene technology, aviation, healthcare and the energy industry."
104,Jay Bailey,Q03 Non-regulatory actions to address AI risks,"A key non-regulatory function Australia could perform is offering international leadership focused on ensuring AI systems can be understood. Australia should be an active voice for the importance of ethics and safety in the current global conversation around AI governance and AI standards making. Specifically, Australia should advocate for:The development of standards and tools that analyse and monitor AI systems for robustness, bias, dual-use capabilities and other safety-relevant features [1].Audit trail requirements for safety-critical applications of AI systems, including allowing the traceability of decisions through product life cycles [2].We will only be able to have confidence in more sophisticated AI systems if they can be understood. If we can trace their decisions, we can ensure they do not pose unacceptable risks and hold accountable those at fault where it does. Doing the non-regulatory work to ensure AIs have “traceability” is necessary to underpin the future of safe AI. "
104,Jay Bailey,Q04 Coordinating AI governance across government,"While the most advanced AIs will likely not be created in Australia, they will be used in Australia, by Australian companies and by everyday Australians. Users will rightly expect and assume that robust safety-focused assurance effort has occurred. We have an obligation to ensure that expectation is delivered.Consider air travel in Australia: While Australia does not build the largest aeroplanes, we monitor the safety of these craft by setting standards, monitoring compliance, and can revoke the licence of airlines that don’t comply with our regulations. Because of this government effort, Australians feel safe when they fly.Similarly, an Australian government body overseeing the use of applications based on large AIs could issue or revoke licences to operate within Australia, set minimum safety and traceability standards, and require companies to publish independent evaluations of their application against those standards. This would provide everyday Australians with independent inform"
104,Jay Bailey,Q05 International models for AI governance,"Australia should take note of the EU’s recent handling of the AI Act. For example, the European Parliament’s recognition of general-purpose AI (GPAI) is an important step in the right direction [1]. In particular, the following observations are highly valuable:- Acknowledgement that GPAI providers must comply with strict safety measures, regardless of the way it is distributed or intended to be used.- Requirements for GPAI providers to comply with various safety requirements throughout the lifecycle of their product; i.e. external audits assessing their performance, predictability, interpretability, corrigibility, safety and cybersecurity.These adjustments appear to respond to significant criticisms directed at initial drafts of the AI Act. For example, the AI Now Institute released a set of recommendations that heavily criticised the EU’s approach on some matters [2]. For example:- “GPAI models carry inherent risks and have caused demonstrated and wide-ranging harms. While these risks"
104,Jay Bailey,Q06 Public vs Private sector governance,"The potential negative impacts on Australians from the use of AI are the same whether the provider is a private company or a government department.Some of the most significant individual and societal harms, as well as catastrophic risks from highly capable AI systems may emerge from public OR private use, meaning that any approach must acknowledge and manage these risks. Legislation should be focused on systems, not just the context in which they are built."
104,Jay Bailey,Q07 Responsible government use of AI,"There are two key actions the Australian government could undertake to improve its own responsible use of AI.First, Australians will worry about the use of AI by security and intelligence agencies. Australia should lead a 5-Eyes agreement about the appropriate use of AI by intelligence agencies, and create a public statement about what our “AI red lines” are. The statement would set out the things our security agencies might do and will never do with AI. The Inspector General of Intelligence and Security, and its counterparts across the five eyes, should then be specifically tasked with auditing the intelligence community for compliance with that public statement. This will give Australians confidence that we are not misusing AI or taking risky actions while protecting sensitive capabilities.Second, Australia needs to upskill the public service. This should include developing hubs of expertise in each Department so that the application of AI within the sphere of each Department’s respo"
104,Jay Bailey,Q08 Generic vs technology-specific approaches for AI risk,"Generic measures such as transparency requirements and legal liability frameworks will only be adequate for mitigating risks in simple decision systems which perform single-step information processing and pattern matching, or routine tasks in limited settings [1]; examples of simple systems may include automated robotic systems in manufacturing, or CV screening systems in HR.By contrast, there will need to be specific measures in place to ensure that sophisticated automated decision systems are safe and reliable. “Advanced” systems refers to AI systems which:1. Are able to complete a diverse range of reasoning tasks with human-level performance; and 2. Are able to navigate complex environments and act within those environments autonomously; and 3. Are able to form sophisticated plans and reason about the consequences of actions across time.Due to their autonomous and open-ended capabilities, advanced systems carry complex risks which are more similar to those associated with humans; fo"
104,Jay Bailey,Q09 Transparency requirements and scope,"To remain competitive, professionals and businesses will increasingly feel the pressure to leverage sophisticated “black-box” AI models, which produce outputs we cannot trace back to their source. This short-term thinking has already resulted in numerous accounts of public backlash and distrust, including high-profile warnings from technology leaders themselves [1].For example, Apple’s credit card business came under fire for offering a woman one twentieth the credit limit offered to her husband. When she complained, an Apple representative simply told her, “I don’t know why, but I swear we’re not discriminating. It’s just the algorithm.” [2]More recently, two lawyers in the United States blamed ChatGPT for “tricking” them into including fictitious legal research in a court filing [3].These problems could easily have been prevented by using explainable AI systems which can show the rationale for their outputs.Such incidents are evidence that the regulatory environment is not incentivis"
104,Jay Bailey,Q10 Ban on high-risk AI applications,"In February 2023, OpenAI CEO Sam Altman released a statement containing the following:“As our systems get closer to AGI, we are becoming increasingly cautious with the creation and deployment of our models. Our decisions will require much more caution than society usually applies to new technologies, and more caution than many users would like.” [1]It is very reassuring to see this acknowledgement expressed by Altman, and the Australian Government would be wise to heed such warnings. Unfortunately, it may not matter how much caution OpenAI shows, because their actions have triggered the arms race that AI safety experts have been concerned about for years [2, 3].In order to protect our nation, Australia should make an effort to impose a ban on the training of large-scale AI systems with a nontrivial expected chance of passing capabilities evaluations required to cause catastrophic risks to civilization (such as the Alignment Research Center’s tests for the capabilities required for auto"
104,Jay Bailey,Q11 Actions to increase public trust in AI,"There are tremendous concerns about AI among those with the greatest familiarity of its risks [1]; these concerns are being widely broadcasted in Australian media with alarming headlines such as:- SMH: “When the “Godfather of AI” warns you about his offspring, you listen.” [2] - Forbes Australia: “‘We knew AI could destroy the whole world and we wanted to figure out how to prevent that’: ChatGPT Founder;” [3] - Canberra Times: “UN chief backs idea of global AI watchdog” [4]These concerning signals are difficult for the public to ignore. It goes without saying that this is quite a different situation from one where concerns are being raised in isolation or by fringe figures. This makes it inappropriate to treat “public education” as a significant part of the solution, as is referenced in the discussion paper. Education tends to make people more concerned about this problem, not less.The process for building trust in AI should be thought of as similar to fostering public trust in bridges"
104,Jay Bailey,Q12 Trade impact of banning high-risk AI applications,"This question does not comprehensively address all the sources of risk or the full interests of Australians.There are two potential sources of risk, both of which need to be mitigated. The first source of risk, as identified in the question, is the use case. Some uses for AI, such as social credit scores, are risky and should be restricted or banned. The second source of risk is the technology itself causing errors and accidents despite the best intentions of their users. Some kinds of technology are inherently risky, and merely restricting a use case does not adequately mitigate the risk.We understand this clearly in other fields. We regulate both how safe a car must be and how a car can be used. We don’t let people speed in cars just because they have a high safety rating; and we don’t let drivers buy dangerous vehicles even if they have a track record of safe driving.The economic framing also misunderstands Australia’s national interests. We would never conceive of a conversation ab"
104,Jay Bailey,Q13 Changes to conformity infrastructure,"World-renowned AI experts such as Yoshua Bengio are warning that there’s a chance “superhuman” AI might be coming as early as 2030 [1]. Although there stand to be significant benefits from deploying advanced AI systems, their deployment will need to be highly targeted and controlled in order to prevent accidental harms.While the productivity gains from advanced AI systems could be immense, uncontrolled proliferation would pose threats from likely scenarios such as malicious use or rogue AI systems [2]. As such, any deployment of advanced AI systems in Australia should occur via secure data centres, with access provided through a centralised API that facilitates monitoring and accountability by providing metadata about usage.Stringent compliance requirements must also be implemented for any organisation granted access to advanced AI systems. AI Governance researchers recommend that regulators should develop audit trail requirements in safety-critical applications of AI, similar to fligh"
104,Jay Bailey,Q14 Risk-based approach to address AI risks,"A risk based approach has worked well as a proactive means of identifying, assessing and mitigating potential safety hazards in the aviation industry, and something similar could work well here.The industry has developed specific tools to assist in identifying potential risks - Safety Management Systems (SMS). Critical to this is the ability for all parties involved in the development and use of AI systems to be able to report the identification of any risks without penalty.Following identification, risks are assessed based on severity and likelihood. This work is led or assisted by technical bodies like the Australian Transport Safety Bureau. In the AI context, other countries have created or proposed similar national “labs” that would be well placed to lead this work. Australia should do the same.The industry participant best placed to mitigate the risk is then required to take steps to reduce the likelihood or impact. Given the nature of the technology, this stage should be open to "
104,Jay Bailey,Q15 Benefits and limitations of risk-based approach,"The key to success is understanding risks sufficiently broadly to encourage caution in an uncertain future. The worst AI risks are too severe to only react once the risk has been actualised - for some risks, that’s too late.A potential “worse-case scenario” is if a culture of “trial and error” is adopted, whereby adverse events occur before risks are considered tangible enough to act upon. This reliance on post-hoc interventions risks normalising unsafe development practices, making it difficult to recover once risks proliferate. Given the possibility of catastrophic risks from advanced AI systems, this would be a dangerous equilibrium.Proliferation of risks is a key point in governing AI, because regulating software poses significant challenges compared to other dual-use commodities such as pharmaceuticals or firearms. This means that, from the outset, there must be proactive regulatory oversight for the deployment of advanced AI systems and their precursors.For example, significant s"
104,Jay Bailey,Q16 Scope and application of risk-based approach,"The most important organisations to focus on when developing risk-management strategies and regulations, will be those that determine how advanced AI is developed and deployed. More specifically, there are two prominent categories of concern:1. Organisations that develop advanced AI systems, or AI systems that can be used to build more advanced AI systems (i.e. precursor systems such as LLMs). Examples include OpenAI, DeepMind, Anthropic, Inflection AI, Nvidia, Microsoft, Meta2. Organisations which provide compute resources to develop, fine-tune or use AI systems. E.g. AWS, Google, Azure, Nvidia, Lambda LabsA majority of the benefits and risks will come from AI that is more sophisticated and generally capable, which can be tied back to organisations in these categories. There are approaches which can be taken for each category in order to manage risks and promote safe adoption:1. Developers: The Government should require that developers of advanced AI systems comply with a stringent pr"
104,Jay Bailey,Q17 Elements to include in risk-based approach,"Human-in-the-loop monitoring will be vital, but insufficient on its own to regulate advanced AI systems that will eventually surpass human capabilities. The speed at which advanced AI systems will operate means that human oversight will become pragmatically infeasible if our monitoring programs aren’t able to scale proportionally.As such, we must employ an approach that is predicated on limiting access to advanced AI systems unless there is a strong rationale for using them; conducting cost-benefit analyses prior to approving the deployment of advanced systems will mitigate risks while preserving potential benefits.A key part of enforcing this targeted approach will be monitoring and controlling access to high-performance computing resources, which are required for developing and operating advanced AI systems and their precursors.This additional layer of governance allows authorities to detect potentially dangerous AI development or usage activity, and can be cross-checked with other d"
104,Jay Bailey,Q18 Integration of risk-based approach into existing frameworks,"Some areas of regulation, like the Racial, Sex, Age, and Disability Discrimination Acts are appropriately strong and may provide useful safeguards against some harms of AI. Specifically, the law should ensure that such discrimination is always wrong and always punishable, and an excuse that the discrimination occurred because of a “black box” AI provided by an overseas AI Lab should never be accepted. It would be helpful to communicate these requirements to Australian businesses that might deploy AIs and ensure they only contract with AI Labs for products where the Lab can ensure the product cannot engage in these harmful behaviours.That said, many of the emerging risks from AI are quite new and it’s reasonable to expect that regulation made without AI in mind is not tuned to the new risks that AI represents. Specific AI safety regulation is unavoidable. It is necessary to ensure that AI cannot engage in existing known harms from technology, and also that AI-specific risks are dealt wi"
104,Jay Bailey,Q19 Application of risk-based approach to general purpose AI,"The risk-based approach must be modified to address risks from general purpose AI systems, especially LLMs and MFMs. These types of systems learn their capabilities and behaviours during training, which can be unforeseen by developers. That means that AI developers could create models with dangerous capabilities (e.g., deception, manipulation, political strategy, weapons design). Humans could misuse those capabilities to cause harm [1], and models could apply the capabilities even without deliberate misuse (e.g., through misalignment) [2]. This can introduce new risks that were not expected during the development process.Consistent with recent work from a consortium of AI safety researchers, including at Google Deepmind, OpenAI, Anthropic, Alignment Research Center, University of Cambridge, University of Oxford, Centre for Long-Term Resilience, and Centre for the Governance of AI [1], we recommend that any risk-based approach require that general purpose models are evaluated for danger"
104,Jay Bailey,Q20 Voluntary vs mandated compliance with risk-based approach,"The harms of AI could emerge equally from private or public organisations, and the appropriate safety and risk management mechanisms will need to be in place in both cases. There’s no basis for distinguishing between their regulatory status in general. There may be case-by-case instances where exceptions may make sense, for instance, in the case of certain types of research.Regulation should also apply to both developers and deployers - following the general principle that those best able to mitigate a risk should carry the corresponding regulatory burden. Where developers offer a “black box” product that deployers typically do not have the capability or capacity to meaningfully alter, the bulk of the obligation should rest on developers.Penalties also need to be commensurate with the income earned from the AI applications so that non-compliance with regulation never becomes a “cost of doing business” or “risk-managed” by deployers. Technical transparency and assurance measures will ne"
106,Anonymous,Q01 Definitions,"The definitions in “Figure 1” represent a limited understanding of “intelligence”. The proposed definition limits “intelligence” to the generation of mere “predictive outputs”. While there is a sense in which current LLMs are working to “predict the next word” or narrow AIs are trying to do various kinds of prediction (e.g. whether a scan shows a disease or which move in a game is best), this is a narrow understanding of what current and future AI systems do. A more future-proof way to understand machine intelligence is through closer comparison to human intelligence. The definition should include the ability to understand, plan and achieve goals in the world. The definition of AI in “Figure 1” is also wrong to limit the scope of “intelligence” to “human-defined objectives or parameters”. This is wrong for at least two reasons: 1. First, a definition that assumes AIs pursue human-defined objectives assumes that the AI alignment problem is solved. Research into AI alignment is about try"
106,Anonymous,Q02 Other AI risks and recommendations for regulation,"Experts in global catastrophic risks were actively warning about the dangers of pandemics in the decade preceding COVID-19. In 2017, Oxford researcher Sebastian Farquar said: “As the Ebola and Zika crises showed, managing pandemics is a global responsibility, but too much planning is still national, and little attention is paid to worst-case scenarios.” [1] If Governments had listened to such experts prior to COVID-19, low-cost interventions, like better regulation of exotic animals markets, could have prevented millions of deaths and trillions in economic damages. The situation that Dr. Farquar describes is eerily similar to the approach to AI Governance in Australia; focused on lower-impact, local issues to the detriment of higher-impact global challenges. This is in contrast to concerns about existential risks being recognised at the highest echelons of global governance, including the UN Secretary-General’s recent statement [2]: ""Alarm bells over [AI] are deafening, and they are lo"
106,Anonymous,Q03 Non-regulatory actions to address AI risks,"AI is very likely to transform many elements of the economy and society in the coming years. Ethics and safety are unlikely to be a priority unless leaders make them a specific focus. For example, social media algorithms came to shape our lives with little upfront focus on how they could be made ethical and safe. We are now living with the negative impacts on mental health and social cohesion and only have band-aid measures to address the widespread harms. AI is likely to be much more transformative and much more consequential. So leadership is required now. Importantly, the economic and technological forces are likely to be so strong that a scheme that is voluntary or driven by aspirational best practice is unlikely to be successful. That is, any non-regulatory approaches must not be taken to displace the need for strong regulation. Specific non-regulatory approaches should include: 1. Government leadership in recognising the risk. Australia must join the growing number of countries a"
106,Anonymous,Q04 Coordinating AI governance across government,"Australia should establish an AI Commission to guide Australia through the rapid and widespread adoption of AI. Such a body must be proactive, not reactive, and manage AI in a way that aligns with the national interest, encompassing citizen safety and wellbeing. The AI Commission would provide a central point of leadership for Government, and be a hub for engagement with industries, academia, non-profits, and the community. This structure holds an advantage over other approaches such as voluntary self-governance or departmental-level teams, as it offers a broader perspective, the authority to lead across government, and the agility to keep pace with the fast-evolving AI sector. - Set priorities in Australia's approach to AI and safety: The Commission should be tasked with establishing a clear and focused AI strategy, encompassing ethical, safety, and economic considerations. This strategy would guide government policy, and inform regulations and legal frameworks for AI. - Develop AI ex"
106,Anonymous,Q05 International models for AI governance,"Singapore offers relevant, adaptable, and desirable governance measures that could benefit Australia. One such measure is a comprehensive framework suggested in its discussion paper [1] that encourages shared responsibility and disclosure standards, data privacy guidelines, and the development of standardised metrics and tools for AI development, testing, and monitoring. Key focuses include:- Accountability: Establish a collective responsibility framework and transparency standards, like watermarks and labels, to enhance accountability in the AI model development process.- Data protection / sovereignty and IP guidelines: Draft data protection and intellectual property guidelines for generative AI and setting up reliable data storage facilities.- Evaluate model development and deployment: Create uniform evaluation measures and resources that aid developers in the production, testing, and surveillance of AI models, and support third-party auditing of AI systems when deployed.- Safety and"
106,Anonymous,Q09 Transparency requirements and scope,"The aviation industry offers a prime example of how transparency can be used to reduce risk and increase safety in a potentially dangerous industry. Transparency is embedded in every stage of the value chain, from manufacturing to operations, and extends to the reporting and investigation of risk-related incidents.Given the magnitude of risk, particularly from emerging more sophisticated AI systems, aviation safety provides a good analogy.Manufacturing StageTransparency begins at the manufacturing stage. Aircraft manufacturers, such as Boeing and Airbus, are required to adhere to stringent safety standards set by regulatory bodies like the Federal Aviation Administration (FAA) in the U.S. and the European Union Aviation Safety Agency (EASA) in Europe. These standards cover everything from the design and production of aircraft to their maintenance and operation.Manufacturers must provide detailed documentation of their processes, including design specifications, testing procedures, and "
106,Anonymous,Q10 Ban on high-risk AI applications,"While it is unlikely that AI today represents a catastrophic or existential risk to humanity, there’s a realistic possibility that it will in the near future [1, 2].To strike a balance between all perspectives, regulators should work to identify “advanced indicators” of catastrophic risk scenarios, and clearly communicate that a ban is an appropriate action if these advanced indicators come to pass.Establishing appropriate advanced indicators will have to be done in consultation with AI safety researchers, for example those from the Centre for AI Safety, or the Centre for AI Governance. However, it’s likely they could include factors such as the size of training runs (measured in terms of compute and data), as well as safety-relevant factors about the AI system, such as the ability to complete a diverse range of reasoning tasks with human-level performance; to navigate complex information environments and act within those environments autonomously, and to form sophisticated plans and r"
106,Anonymous,Q11 Actions to increase public trust in AI,"Public distrust in AI is fuelled by the growing sense that powerful AI systems are being developed and deployed by foreign actors, some of whom have demonstrated unsafe attitudes towards deploying AI systems [1]. The interests of Australians and Australia is not part of the conversation. Although Australia may struggle to catch up to the US or UK in advancing the frontier of AI, this isn’t necessary for improving public trust and increasing adoption.Rather, Australia has a clear opportunity to become a leader in AI safety; this enviable position would not only assist safe adoption locally by providing much needed expertise in safe systems, it could help foster a sense of pride and ownership by opening up export opportunities in the emerging market for AI safety services [2]. AI Safety is how Australia could get its ‘seat at the table’ in the global AI conversation.Pursuing this position would involve the following:- Coordinating with Australian AI safety organisations to better underst"
106,Anonymous,Q14 Risk-based approach to address AI risks,"The regulation of AI needs to be “risk-based” in the same way that the regulation of vehicles is risk-based. We are right to regulate push-bikes and passenger jets in very different ways.A limitation of the risk management approach in the discussion paper, specifically Box 4 and Attachment C, is that it focuses only on how an AI is used and disregards the nature of the AI itself.An effective regulatory regime for a high-risk activity will regulate both the technology itself and how it is used. The Australian Human Rights Commission [1], in discussing this issue (“Human Rights and Technology”, 2021), says:Governments tend to regulate high-risk activities and technologies more closely. This helps explain the comparatively strict laws that govern fields such as gene technology, aviation, healthcare and the energy industry. In these areas, regulation often applies both to the technology itself and how it is used. From a human rights perspective, the need for more prescriptive regulation wi"
106,Anonymous,Q15 Benefits and limitations of risk-based approach,"The key to success is understanding risks sufficiently broadly to encourage caution in an uncertain future. It is not enough to only react once the risk has been actualised - for some risks that’s too late.A potential “worse-case scenario” is if a culture of “trial and error” is adopted, whereby adverse events occur before risks are considered tangible enough to act upon. This reliance on post-hoc interventions risks normalising unsafe development practices, making it difficult to recover once risks proliferate.Proliferation of risks is a key point in governing AI, because regulating software poses significant challenges compared to other dual-use commodities such as pharmaceuticals or firearms. This means that, from the outset, there must be proactive regulatory oversight for the deployment of advanced AI systems and their precursors.For example, significant structural changes, such as the use of advanced AI systems in Australia, require that risks must be comprehensively understood p"
106,Anonymous,Q16 Scope and application of risk-based approach,"The most important organisations to focus on when developing risk-management strategies and regulations, will be those that determine how advanced AI is developed and deployed. More specifically, there are three prominent categories of concern:1. Organisations that develop advanced AI systems, or AI systems that can be used to build more advanced AI systems (i.e. precursor systems such as LLMs). Examples include OpenAI, DeepMind, Anthropic, Inflection AI, Nvidia, Microsoft, Meta2. Organisations which provide access to open-source AI systems via downloads or APIs. E.g. Hugging Face, Github, AWS3. Organisations which provide compute resources to develop, fine-tune or use AI systems. E.g. AWS, HuggingFace, Google, Azure, Nvidia, Lambda LabsA majority of the benefits and risks will come from AI that is more sophisticated and generally capable, which can be tied back to organisations in these three categories.There are approaches which can be taken for each category in order to manage risks"
106,Anonymous,Q17 Elements to include in risk-based approach,"Human-in-the-loop monitoring will be vital, but insufficient on its own to regulate advanced AI systems that will eventually surpass human capabilities. The speed at which advanced AI systems will operate means that human oversight will become pragmatically infeasible if our monitoring programs aren’t able to scale proportionally.As such, we must employ an approach that is predicated on limiting access to advanced AI systems unless there is a strong rationale for using them; conducting cost-benefit analyses prior to approving the deployment of advanced systems will mitigate risks while preserving potential benefits.A key part of enforcing this targeted approach will be monitoring and controlling access to high-performance computing resources, which are required for developing and operating advanced AI systems and their precursors.This additional layer of governance allows authorities to detect potentially dangerous AI development or usage activity, and can be cross-checked with other d"
106,Anonymous,Q18 Integration of risk-based approach into existing frameworks,"Some areas of regulation, like the Racial, Sex, Age, and Disability Discrimination Acts are appropriately strong and may provide useful safeguards against some harms of AI. Specifically, the law should ensure that such discrimination is always wrong and always punishable, and an excuse that the discrimination occurred because of a “black box” AI provided by an overseas AI Lab should never be accepted. It would be helpful to communicate these requirements to Australian businesses that might deploy AIs and ensure they only contract with AI Labs for products where the Lab can ensure the product cannot engage in these harmful behaviours.Certainly, no AI-specific regulation should ever waive existing safeguards.That said, many of the emerging risks from AI are quite new and it’s reasonable to expect that regulation made without AI in mind is not tuned to the new risks that AI represents. Specific AI safety regulation is unavoidable."
106,Anonymous,Q19 Application of risk-based approach to general purpose AI,"The risk-based approach must be modified to address risks from general purpose AI systems, especially LLMs and MFMs. These types of systems learn their capabilities and behaviours during training, which can be unforeseen by developers. That means that AI developers could create models with dangerous capabilities (e.g., deception, manipulation, political strategy, weapons design). Humans could misuse those capabilities to cause harm [1], and models could apply the capabilities even without deliberate misuse (e.g., through misalignment) [2].Consistent with recent work from a consortium of AI safety researchers, including at Google Deepmind, OpenAI, Anthropic, Alignment Research Center, University of Cambridge, University of Oxford, Centre for Long-Term Resilience, and Centre for the Governance of AI [1], we recommend that any risk-based approach require that general purpose models are evaluated for dangerous capabilities and misalignment. This will help determine:1. To what extent a mode"
106,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,"In general, regulation should:1. Be proportionate to risk. Meaning that a voluntary or self-regulation approach should only apply to low-risk or no-risk uses of AI.2. In the case of higher-risk technologies, regulate both the technology itself and the use of the technology. Meaning that, for future and more sophisticated AIs, regulation should ensure that the technology is safe before it is created or published, as well as ensuring that it is only used in ways that are safe.3. Place burdens on those most able to reduce potential harms. Meaning that AI that functionally operates as ‘black box’ to consumers has to be regulated at the point of the AI Lab.Applying those three principles, self-regulation may be appropriate for some technologies available today and for some participants (such as students or researchers), but a forward-looking regulatory regime must apply broadly and must be backed by Australian diplomatic efforts to ensure it is globally coordinated."
107,Scott Smith,Q02 Other AI risks and recommendations for regulation,"Risks that intuitively feel unlikely and far-fetched, but are extremely catastrophic if they were to occur, should be covered and taken seriously. These are risks that pose a threat to civilisation or even human existence, whether through intentional misuse or unintended harms.And experts in the field often do not see these risks as unlikely and far-fetched. The statement “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war” has recently been signed by the heads of leading AI labs (OpenAI, Google DeepMind, Anthropic, Stability AI), as well as many key researchers pivotal in developing this technology (Geoffrey Hinton, Yoshua Bengio, etc.) and other notable figures such as Bill Gates [1]. Similar calls date back to at least 2015 when Stephen Hawking led a group of AI experts to call attention to the risk and lay out an urgent research program [2].Rishi Sunak, the Prime Minister of the United Kingdom"
107,Scott Smith,Q03 Non-regulatory actions to address AI risks,"Australia really needs to publicly acknowledge the risk and its importance. In doing so, Australia would join the growing number of countries and global governance bodies that acknowledge the possibility of catastrophic and existential risks from increasingly powerful AIs.Australia should lead the establishment of global governance arrangements that underpin domestic regulation and are specifically tasked with tackling the possible catastrophic risks from AI. Australia has national interests that could be put in jeopardy depending on the actions of foreign companies and countries. Australia should foster cultures of fairness, accountability and transparency by the companies and countries developing more advanced AIs. Australia needs to work globally to avoid an “arms race” that prioritises speed over safety, including making sure that risky decisions with broader implications aren’t made behind closed doors.Resources to enable these non-regulatory initiatives, and previously mentioned "
107,Scott Smith,Q05 International models for AI governance,"Australia should take note of the EU recent handling of the AI Act. For example, the European Parliament’s recognition of general-purpose AI (GPAI) is an important step in the right direction [1]. In particular, the following observations are highly valuable: - Acknowledgement that GPAI providers must comply with strict safety measures, regardless of the way it is distributed or intended to be used. - Requirements for GPAI providers to comply with various safety requirements throughout the lifecycle of their product; i.e. external audits assessing their performance, predictability, interpretability, corrigibility, safety and cybersecurity.These adjustments appear to respond to significant criticisms directed at initial drafts of the AI Act. For example, the AI Now Institute released a set of recommendations that heavily criticised the EU’s approach on some matters [2]. For example: - “GPAI models carry inherent risks and have caused demonstrated and wide-ranging harms. While these risk"
107,Scott Smith,Q09 Transparency requirements and scope,"To remain competitive, professionals and businesses will increasingly feel the pressure to leverage sophisticated “black-box” AI models. This short-term thinking has already resulted in numerous accounts of public backlash and distrust, including high-profile warnings from technology leaders themselves [1].For example, Apple’s credit card business came under fire for offering a woman one twentieth the credit limit offered to her husband. When she complained, an Apple representative simply told her, “I don’t know why, but I swear we’re not discriminating. It’s just the algorithm.” [2]More recently, two lawyers in the United States blamed ChatGPT for “tricking” them into including fictitious legal research in a court filing [3].These problems could easily have been prevented by using explainable AI systems which can show the rationale for their outputs.Such incidents are evidence that the regulatory environment is not incentivising businesses or professionals to use transparent decision-"
107,Scott Smith,Q10 Ban on high-risk AI applications,"While it is unlikely that AI today represents a catastrophic or existential risk to humanity, there’s a realistic possibility that it will in the near future [1, 2].To strike a balance between all perspectives, regulators should work to identify “advanced indicators” of catastrophic risk scenarios, and clearly communicate that a ban is an appropriate action if these advanced indicators come to pass.Establishing appropriate advanced indicators will have to be done in consultation with AI safety researchers, for example those from the Centre for AI Safety, or the Centre for AI Governance. However, it’s likely they could include factors such as the size of training runs (measured in terms of compute and data), as well as safety-relevant factors about the AI system, such as the ability to complete a diverse range of reasoning tasks with human-level performance; to navigate complex information environments and act within those environments autonomously, and to form sophisticated plans and r"
107,Scott Smith,Q14 Risk-based approach to address AI risks,"I support a risk-based approach, however the catastrophic and existential risks I am primarily worried about originate when sufficiently large and capable AI systems are trained, not just when they are deployed. Therefore, for sufficiently large systems with the potential for catastrophic capabilities, the risk-based approach must be applied before the training phase. Risks should be analysed depending on the nature and scale of the proposed training run, rather than merely analysing the potentially risky use cases and deployment issues of an AI after it has already been created and trained.Any assessment of risk should consider its potential irreversibility, scope and severity, and act proportionally.The risk-based approach set out in Attachment C of the Discussion Paper is hard to follow. The proposed approach offers “human in the loop” as the key mitigation (unclear that this is an effective mitigation) and goes on to note that human in the loop may not be possible where the benefit"
107,Scott Smith,Q15 Benefits and limitations of risk-based approach,"While a risk-based approach to regulating AI is sensible, there are two emerging elements of risk practice that any risk-based model should account for to succeed in this context: 1. Factoring in “uncertainty”, not just “likelihood” 2. Taking a “portfolio” approach to risk mitigationA traditional risk matrix will consider scenarios against their likelihood of occurring, the vulnerability of the system to the scenario, and the consequence of the scenario occurring. It might then discount that risk by the strength of existing or possible mitigations.This approach to risk has historically been unsuccessful for scenarios where assessing “likelihood” is challenging. For instance, many national risk assessments failed to take into account the possibility of a global pandemic before COVID-19. A solution to this failure mode is including “uncertainty”. Specifically, where we can't be sure what the future will bring, that should be cause for significant concern. This is the opposite of the appr"
107,Scott Smith,Q16 Scope and application of risk-based approach,"World-renowned AI experts such as Yoshua Bengio are warning that there’s a chance “superhuman” AI might be coming as early as 2030. These types of advanced AI systems have potential to be highly disruptive across all sectors of the economy. While present risks might be linked to specific use cases and sectors, future risks will increasingly be linked to the underlying capabilities of the technology itself.As such, access to increasingly advanced AI systems should be restricted to organisations that can demonstrate their usage will provide significant public benefit aligned with Australia's national interests. This targeted approach helps mitigate risks in two key ways: 1. It limits opportunities for misuse or accidents from advanced AI by reducing proliferation across society. Usage is concentrated into high-value use cases that justify the risks. 2. It establishes clear accountability. Organisations granted access must have the governance and technical expertise to use advanced AI saf"
107,Scott Smith,Q17 Elements to include in risk-based approach,"Human-in-the-loop monitoring will be vital, but insufficient on its own to regulate advanced AI systems that will eventually surpass human capabilities. The speed at which advanced AI systems will operate means that human oversight will become pragmatically infeasible if our monitoring programs aren’t able to scale proportionally.As such, we must employ an approach that is predicated on limiting access to advanced AI systems unless there is a strong rationale for using them; conducting cost-benefit analyses prior to approving the deployment of advanced systems will mitigate risks while preserving potential benefits.A key part of enforcing this targeted approach will be monitoring and controlling access to high-performance computing resources, which are required for developing and operating advanced AI systems and their precursors.This additional layer of governance allows authorities to detect potentially dangerous AI development or usage activity, and can be cross-checked with other d"
107,Scott Smith,Q19 Application of risk-based approach to general purpose AI,"The risk-based approach must be modified to address risks from general purpose AI systems, especially LLMs and MFMs. These types of systems learn their capabilities and behaviours during training, which can be unforeseen by developers. That means that AI developers could create models with dangerous capabilities (e.g., deception, manipulation, political strategy, weapons design). Humans could misuse those capabilities to cause harm [1], and models could apply the capabilities even without deliberate misuse (e.g., through misalignment) [2].Consistent with recent work from a consortium of AI safety researchers, including at Google Deepmind, OpenAI, Anthropic, Alignment Research Center, University of Cambridge, University of Oxford, Centre for Long-Term Resilience, and Centre for the Governance of AI [1], we recommend that any risk-based approach require that general purpose models are evaluated for dangerous capabilities and misalignment. This will help determine: - To what extent a mode"
107,Scott Smith,Q20 Voluntary vs mandated compliance with risk-based approach,"AI is very likely to transform many elements of the economy and society in coming years. Ethics and safety are unlikely to be a priority during this time unless they are actively made a strategic focus by clear leadership. Historic examples include how social media algorithms came to shape our lives, and the negative impacts to mental health and social cohesion only became apparent much later and still have not been adequately addressed. Similarly, while the destructive potential of nuclear power was apparent early on, the “arms race” mentality, as well as potential economic benefits, swept humanity along towards the current risky situation that no one in the early 1940s could have conceived of. In that way, history teaches us that it’s critical that ethics and safety take centre stage from day one.Regarding public and private organisations, this is not a relative distinction. The harms of AI could emerge from either, and the appropriate safety and risk management mechanisms will need "
108,Simon Newstead,Q01 Definitions,"The definitions in Figure 1 could be made more fine grained to differentiate different kinds of AI with vastly different risk levels - including very powerful AIs that soon will be able to operate significantly better than some current examples.Better definitions will allow a more nuanced and effective policy conversation that distinguishes between AI models that provide economic benefit to Australia, others that pose significant ethical concerns, and those that are of extreme existential safety concerns."
108,Simon Newstead,Q02 Other AI risks and recommendations for regulation,"The paper doesn't acknowledge the existential safety risks of highly capable AI systems, despite this risk being widely acknowledged by industry experts and public figures - including open letters organized by the Future of Life Institute [1] and the Centre for AI Safety [2].Even the Secretary-General of the United Nations has recognised the catastrophic risks from powerful AI systems [3].It's imperative that the Australian government acknowledge the catastrophic and existential risks from highly capable AI systems. We can't ignore an issue that experts and other leading countries have been progressing in discussions and work.[1] https://futureoflife.org/open-letter/pause-giant-ai-experiments/ [2] https://www.safe.ai/statement-on-ai-risk#open-letter [3] https://press.un.org/en/2023/sgsm21832.doc.htmThe Australian Human Rights Commission, made a very apt analogy to aviation safety [1]:Governments regulate high-risk activities and technologies more strictly (as they should). For example "
108,Simon Newstead,Q03 Non-regulatory actions to address AI risks,"The key non-regulatory function Australia could perform is greatly increasing the funding for AI safety research in Australia universities as a matter or urgent, short term priority.Research priorities should reflect the rapid, exponential pace of AI development.By funding research that is ambitious, rather than incremental, Australia can become an enduring global leader on key aspects of AI safety.To get more technical, a focus on mechanistic interpretability, or the 5 elements of effective traceability (Design Transparency; Reproducibility; Operational Recordkeeping; Human Understanding and Auditability) [1], could make Australia a leader in the field, underpin safety-focused global regulation, and ultimately give Australia an important ‘seat at the table’ as AI development continues to unfold. This will both secure out national interest globally, but help provide the research underpinnings for safe engagement with increasingly powerful AI systems.[1] Joshua A. Kroll. 2021. Outlining"
108,Simon Newstead,Q04 Coordinating AI governance across government,"It's likely that the most advanced AIs may not have been created in Australia but they will certainly be used in Australia, by Australian citizens and organizations.We should rightly expect that a strong safety process has taken place before use is allowed in Australia. Indeed government has an obligation to ensure that expectation is delivered.To continue the analogy above, while Australia does not have large aeroplane manufacturing in our borders, we do monitor the safety by setting standards, monitoring compliance, and can revoke the licence of airlines that don’t comply with our regulations.As a result we not only feel safe, we are safer flying as AustraliansAn Australian government body managing the use of applications based on large AIs could issue or revoke licences, set minimum safety standards, and require companies to publish independent evaluations of their application against those standards. This would provide all of us with independent information that they can use to mak"
108,Simon Newstead,Q05 International models for AI governance,"Singapore offers relevant, adaptable, and desirable governance measures that could benefit Australia. One such measure is a comprehensive framework suggested in its discussion paper [1] that encourages shared responsibility and disclosure standards, data privacy guidelines, and the development of standardised metrics and tools for AI development, testing, and monitoring. Key focuses include:- Accountability: Establish a collective responsibility framework and transparency standards, like watermarks and labels, to enhance accountability in the AI model development process.- Data protection / sovereignty and IP guidelines: Draft data protection and intellectual property guidelines for generative AI and setting up reliable data storage facilities.- Evaluate model development and deployment: Create uniform evaluation measures and resources that aid developers in the production, testing, and surveillance of AI models, and support third-party auditing of AI systems when deployed.- Safety and"
108,Simon Newstead,Q09 Transparency requirements and scope,"To remain competitive, professionals and companies will increasingly turn to sophisticated “black-box” AI models. This short-term thinking has already resulted in numerous accounts of public backlash and distrust, including high-profile warnings from technology leaders themselves [1].For example, Apple’s credit card business came under fire for offering a woman one twentieth the credit limit offered to her husband. When she complained, an Apple representative simply told her, “I don’t know why, but I swear we’re not discriminating. It’s just the algorithm.” [2]More recently, two lawyers in the United States blamed ChatGPT for “tricking” them into including fictitious legal research in a court filing [3].These problems could easily have been prevented by using explainable AI systems which can show the rationale for their outputs.Such incidents are evidence that the regulatory environment is not incentivising businesses or professionals to use transparent decision-making tools. When busi"
108,Simon Newstead,Q10 Ban on high-risk AI applications,"Whilst it's unlikely that AI today represents a catastrophic or existential risk to humanity, there’s a realistic possibility that it will in the near future [1, 2].To strike a balance between all perspectives, regulators should work to identify “advanced indicators” of catastrophic risk scenarios, and clearly communicate that a ban is an appropriate action if these advanced indicators come to pass.Establishing appropriate advanced indicators will have to be done in consultation with AI safety researchers, for example those from the Centre for AI Safety, or the Centre for AI Governance. However, it’s likely they could include factors such as the size of training runs (measured in terms of compute and data), as well as safety-relevant factors about the AI system, such as the ability to complete a diverse range of reasoning tasks with human-level performance; to navigate complex information environments and act within those environments autonomously, and to form sophisticated plans and r"
108,Simon Newstead,Q11 Actions to increase public trust in AI,"Already, trust in AI has been reduced by events such as Geoffrey Hinton’s departure from Google and warnings of future danger [1]. This, among other issues , has created a strong sense that tech giants such as Google, Microsoft and Facebook are not prioritising the safety of AI systems, and instead are engaging in a dangerous race of escalating capabilities [2].Increasing public trust in AI deployment can only be achieved by continually mitigating its risks, and this means building a strong track-record of protecting Australians from the negligent attitudes of these powerful private entities [3].This will not be an easy task, and it will require the establishment of a dedicated AI regulator which can stay up to date with recent advancements, navigate the complicated open-source ecosystem, and effectively police large technology companies.[1] Sydney Morning Herald (2023) When the “Godfather of AI” warns you about his offspring, you listen. https://www.smh.com.au/national/when-the-godfat"
108,Simon Newstead,Q14 Risk-based approach to address AI risks,"Yes, absolutely. A risk based approach has worked well as a proactive means of identifying, assessing and mitigating potential safety hazards in the aviation industry.The industry has developed specific tools to assist in identifying potential risks - Safety Management Systems (SMS). Critical to this is the ability for all parties involved in the development and use of AI systems to be able to report the identification of any risks.Following identification, risks are assessed based on severity and likelihood. This work is led or assisted by technical bodies like the Australian Transport Safety Bureau. In the AI context, other countries have created or proposed similar national “labs” that would be well placed to lead this work.The industry participant best placed to mitigate the risk is then required to take steps to reduce the likelihood or impact. Given the nature of the technology this stage should be open to update as new techniques and tools become available based on the most up t"
108,Simon Newstead,Q15 Benefits and limitations of risk-based approach,"Unlike other technologies where we have the luxury of failing, learning and iterating over time, due to the potential for exponentially more powerful AI models, there's far less room for error.So the key to success is understanding risks sufficiently broadly to encourage caution in an uncertain future. It is not enough to only react once the risk has been actualised - for some risks that’s too late.A potential “worse-case scenario” is if a culture of “trial and error” is adopted, whereby adverse events occur before risks are considered tangible enough to act upon. This reliance on post-hoc interventions risks normalising unsafe development practices, making it difficult to recover once risks proliferate.Proliferation of risks is a key point in governing AI, because regulating software poses significant challenges compared to other dual-use commodities such as pharmaceuticals or firearms. This means that, from the outset, there must be proactive regulatory oversight for the deployment o"
108,Simon Newstead,Q16 Scope and application of risk-based approach,"World-renowned AI experts such as Yoshua Bengio are sounding the alarm that there’s a real chance “superhuman” AI might be coming as early as 2030, which means we must move quickly. The consequences of not doing so are dire.These types of advanced AI systems have potential to be highly disruptive across all sectors of the economy. While present risks might be linked to specific use cases and sectors, future risks will increasingly be linked to the underlying capabilities of the technology itself.As such, access to increasingly advanced AI systems should be restricted to organisations that can demonstrate their usage will provide significant public benefit aligned with Australia's national interests. This targeted approach helps mitigate risks in two key ways:1. It limits opportunities for misuse or accidents from advanced AI by reducing proliferation across society. Usage is concentrated into high-value use cases that justify the risks.2. It establishes clear accountability. Organisati"
108,Simon Newstead,Q18 Integration of risk-based approach into existing frameworks,"No AI-specific regulation should ever waive existing safeguards - and we have good measures with respect to for example racial, sex, disability, age discrimination in the relevant Acts.That said, many of the coming risks from AI are novel and specific, and it’s reasonable to expect that regulation made without AI in mind is not tuned to the new and uniquely dangerous risks that AI represents. Specific AI safety regulation is a must."
108,Simon Newstead,Q19 Application of risk-based approach to general purpose AI,"The new capabilities of general-purpose AI systems, like large language models (LLMs) and multimodal foundation models (MFMs), radically change the risk landscape. We should anticipate these systems having large, unpredictable effects across all parts of society.A prudent approach is to implement a risk-based framework, where access is graduated based on model capabilities. Models above a certain threshold of autonomous capabilities would be designated as ""advanced systems"" and subject to additional governance, such as permits for users.Further, permits should be required to gain certain levels of access to AI systems which can be used to build advanced systems.Permits allow us to maintain oversight, while still enabling beneficial uses. Different permissions can be granted based on use case - for example, full access for safety testing versus API-only access for most commercial uses. This balances innovation and responsibility.Although this may seem very different to how AI is used to"
108,Simon Newstead,Q20 Voluntary vs mandated compliance with risk-based approach,"In general, regulation needs to :1. Be proportionate to the level risk. A voluntary or self-regulation approach should only apply to low-risk or no-risk uses of AI - such as generating images, or simple capabilities.2. In the case of higher-risk technologies such as more powerful models, regulate both the technology itself and the use of the technology. Meaning that, for future and more sophisticated AIs, regulation should ensure that the technology is safe before it is created or published, as well as ensuring that it is only used in ways that are safe.3. Place burdens on those most able to reduce potential harms. Meaning that AI that functionally operates as ‘black box’ to consumers has to be regulated at the point of the AI Lab.Applying those three principles, self-regulation may be appropriate for some technologies available today and for some participants (such as students or researchers), but a forward-looking regulatory regime must apply broadly and must be backed by Australian "
109,Courtney Henry,Q01 Definitions,"The definitions in Figure 1 are not sufficiently granular to support risk-based consideration of different kinds of AI - including the more sophisticated and powerful AIs. In their current state, these definitions do not support a nuanced policy conversation that allows differentiation between AI models that provide economic benefit, models that raise significant ethical concerns, and models that raise radical safety concerns."
109,Courtney Henry,Q02 Other AI risks and recommendations for regulation,"In response to the first part of this question:Experts in global catastrophic risks were actively warning about the dangers of pandemics in the decade preceding COVID-19. In 2017, Oxford researcher Sebastian Farquar said: “As the Ebola and Zika crises showed, managing pandemics is a global responsibility, but too much planning is still national, and little attention is paid to worst-case scenarios.” [1]If Governments had listened to such experts prior to COVID-19, low-cost interventions, like better regulation of exotic animals markets, could have prevented millions of deaths and trillions in economic damages.The situation that Dr. Farquar describes is eerily similar to the approach to AI Governance in Australia; focused on lower-impact, local issues to the detriment of higher-impact global challenges.This is in contrast to concerns about existential risks being recognised at the highest echelons of global governance, including the UN Secretary-General’s recent statement [2]:""Alarm bel"
109,Courtney Henry,Q03 Non-regulatory actions to address AI risks,"The key non-regulatory function Australia could perform is offering international leadership focused on ensuring AI systems can be understood. Australia should be an active voice for the importance of ethics and safety in the current global conversation around AI governance and AI standards making. Specifically, Australia should advocate for:The development of standards and tools that analyse and monitor AI systems for robustness, bias, dual-use capabilities and other safety-relevant features [1].Audit trail requirements for safety-critical applications of AI systems, including allowing the traceability of decisions through product life cycles [2].We will only be able to have confidence in more sophisticated AI systems if they can be understood, we can ensure they do not pose unacceptable risks and hold accountable those at fault where it does. Doing the non-regulatory work to ensure AIs have “traceability” is necessary to underpin the future of safe AI. Joshua Kroll’s work on accounta"
109,Courtney Henry,Q04 Coordinating AI governance across government,"While the most advanced AIs will likely not be created in Australia, they will be used in Australia, by Australian companies and by everyday Australians. Users will rightly expect and assume that robust safety-focused assurance effort has occurred. Government has an obligation to ensure that expectation is delivered.Consider air travel in Australia: While Australia does not build the largest aeroplanes, we monitor the safety of these craft by setting standards, monitoring compliance, and can revoke the licence of airlines that don’t comply with our regulations. Because of this government effort, Australians feel safe when they fly.Similarly, an Australian government body overseeing the use of applications based on large AIs could issue or revoke licences, set minimum safety standards, and require companies to publish independent evaluations of their application against those standards. This would provide everyday Australians with independent information that they can use to make inform"
109,Courtney Henry,Q05 International models for AI governance,"Singapore offers relevant, adaptable, and desirable governance measures that could benefit Australia. One such measure is a comprehensive framework suggested in its discussion paper [1] that encourages shared responsibility and disclosure standards, data privacy guidelines, and the development of standardised metrics and tools for AI development, testing, and monitoring. Key focuses include:- Accountability: Establish a collective responsibility framework and transparency standards, like watermarks and labels, to enhance accountability in the AI model development process.- Data protection / sovereignty and IP guidelines: Draft data protection and intellectual property guidelines for generative AI and setting up reliable data storage facilities.- Evaluate model development and deployment: Create uniform evaluation measures and resources that aid developers in the production, testing, and surveillance of AI models, and support third-party auditing of AI systems when deployed.- Safety and"
109,Courtney Henry,Q06 Public vs Private sector governance,"The potential negative impacts on Australians from the use of AI are the same whether the provider is a private company or a government department.Some of the most significant individual and societal harms, as well as catastrophic risks from highly capable AI systems may emerge from public OR private use, meaning that any approach must acknowledge and manage these risks."
109,Courtney Henry,Q07 Responsible government use of AI,"There are two key actions the Australian government could undertake to improve its own responsible use of AI.First, Australians will worry about the use of AI by security and intelligence agencies. Australia should lead a 5-Eyes agreement about the appropriate use of AI by intelligence agencies, and create a public statement about what our “AI red lines” are. The statement would set out the things our security agencies might do and will never do with AI. The Inspector General of Intelligence and Security, and its counterparts across the five eyes, should then be specifically tasked with auditing the intelligence community for compliance with that public statement. This will give Australians confidence that we are not misusing AI or taking risky actions while protecting sensitive capabilities.Second, Australia needs to upskill the public service. This should include developing hubs of expertise in each Department so that the application of AI within the sphere of each Department’s respo"
109,Courtney Henry,Q08 Generic vs technology-specific approaches for AI risk,"In general, the range of capabilities of AI systems is so significant, that “one size fits all” solutions are unlikely to be appropriate. By analogy, it’s rare that the same regulation applies to a push bike and a passenger plane on the basis that they’re both ‘vehicles’.Further, an effective regulatory regime for a high-risk activity needs to regulate both the technology itself and how it is used. The Australian Human Rights Commission, in discussing this issue in its report ‘Human Rights and Technology’ (2021), says:> “Governments tend to regulate high-risk activities and technologies more closely. This helps explain the comparatively strict laws that govern fields such as gene technology, aviation, healthcare and the energy industry. In these areas, regulation often applies both to the technology itself and how it is used. From a human rights perspective, the need for more prescriptive regulation will be greater where the use of a specific technology carries greater risks of harm to"
109,Courtney Henry,Q09 Transparency requirements and scope,"To remain competitive, professionals and businesses will increasingly feel the pressure to leverage sophisticated “black-box” AI models. This short-term thinking has already resulted in numerous accounts of public backlash and distrust, including high-profile warnings from technology leaders themselves [1].For example, Apple’s credit card business came under fire for offering a woman one twentieth the credit limit offered to her husband. When she complained, an Apple representative simply told her, “I don’t know why, but I swear we’re not discriminating. It’s just the algorithm.” [2]More recently, two lawyers in the United States blamed ChatGPT for “tricking” them into including fictitious legal research in a court filing [3].These problems could easily have been prevented by using explainable AI systems which can show the rationale for their outputs.Such incidents are evidence that the regulatory environment is not incentivising businesses or professionals to use transparent decision-"
109,Courtney Henry,Q10 Ban on high-risk AI applications,"Although there is increasing recognition that AI will pose significant risks to public safety [1], there is widespread confusion about what types of AI are dangerous, and how Australia can help prevent these risks. Luckily, there is a significant body of evidence that describes the main source of these risks [2], and Australia is well-positioned to be a leader in mitigating them.Research shows that two of the main sources of risk are deception and power-seeking behaviour:- Deceptive behaviour in AI systems will become increasingly difficult to avoid as their capabilities advance. This problem has been observed in AI systems since at least 2017, when researchers at OpenAI noted “agents adopting (strategies) that trick the evaluators” [3].- Power-seeking behaviour is the natural tendency for any entity with a long-term goal to take actions that protect or enhance its ability to achieve that goal. Leading AI labs and journals advise that power-seeking behaviour will be a considerable sour"
109,Courtney Henry,Q11 Actions to increase public trust in AI,"Public distrust in AI is fuelled by the growing sense that powerful AI systems are being developed and deployed by foreign actors, some of whom have demonstrated unsafe attitudes towards deploying AI systems [1]. The interests of Australians and Australia is not part of the conversation. Although Australia may struggle to catch up to the US or UK in advancing the frontier of AI, this isn’t necessary for improving public trust and increasing adoption.Rather, Australia has a clear opportunity to become a leader in AI safety; this enviable position would not only assist safe adoption locally by providing much needed expertise in safe systems, it could help foster a sense of pride and ownership by opening up export opportunities in the emerging market for AI safety services [2]. AI Safety is how Australia could get its ‘seat at the table’ in the global AI conversation.Pursuing this position would involve the following:- Coordinating with Australian AI safety organisations to better underst"
109,Courtney Henry,Q12 Trade impact of banning high-risk AI applications,"This question misunderstands both the sources of risk and the interests of Australians.There are two potential sources of risk both of which need to be mitigated. The first source of risk, as identified in the question, is the use case. Some uses for AI are risky and should be restricted or banned. The second source of risk is the technology itself. Some kinds of technology are inherently risky, and merely restricting a use case does not adequately mitigate the risk.We understand this clearly in other fields. We regulate both how safe a car must be and how a car can be used. We don’t let people speed in cars just because they have a high safety rating; and we don’t let safe drivers buy dangerous vehicles.The economic framing also misunderstands Australia’s national interests. We would never conceive of a conversation about whether our aviation sector would trade more successfully if we authorised airlines to use planes that are untested, experimental or known to be dangerous. Australia"
109,Courtney Henry,Q14 Risk-based approach to address AI risks,"The regulation of AI needs to be “risk-based” in the same way that the regulation of vehicles is risk-based. We are right to regulate push-bikes and passenger jets in very different ways.A limitation of the risk management approach in the discussion paper, specifically Box 4 and Attachment C, is that it focuses only on how an AI is used and disregards the nature of the AI itself.An effective regulatory regime for a high-risk activity will regulate both the technology itself and how it is used. The Australian Human Rights Commission [1], in discussing this issue (“Human Rights and Technology”, 2021), says:Governments tend to regulate high-risk activities and technologies more closely. This helps explain the comparatively strict laws that govern fields such as gene technology, aviation, healthcare and the energy industry. In these areas, regulation often applies both to the technology itself and how it is used. From a human rights perspective, the need for more prescriptive regulation wi"
109,Courtney Henry,Q15 Benefits and limitations of risk-based approach,"The key to success is understanding risks sufficiently broadly to encourage caution in an uncertain future. It is not enough to only react once the risk has been actualised - for some risks that’s too late.A potential “worse-case scenario” is if a culture of “trial and error” is adopted, whereby adverse events occur before risks are considered tangible enough to act upon. This reliance on post-hoc interventions risks normalising unsafe development practices, making it difficult to recover once risks proliferate.Proliferation of risks is a key point in governing AI, because regulating software poses significant challenges compared to other dual-use commodities such as pharmaceuticals or firearms. This means that, from the outset, there must be proactive regulatory oversight for the deployment of advanced AI systems and their precursors.For example, significant structural changes, such as the use of advanced AI systems in Australia, require that risks must be comprehensively understood p"
109,Courtney Henry,Q16 Scope and application of risk-based approach,"The concept of “AI” includes a broad range of capabilities and capacities. Government is right to work to distinguish between different use cases - including with a specific focus on AI maturity.Concerningly, this broad range of AI capabilities and capacities might be used by industry to attempt to bamboozle government. Specifically, industry is likely to argue that AI does not need regulation with reference to basic AI technology, with a view to that principle flowing through to more advanced and future technologies. This is akin to arguing that a horse and wagon doesn’t need strict regulation, then later trying to claim that a tank is just a more capable horse and wagon with some components swapped out.Government needs both a fine-grained approach to defining AI technology and a fine-grained approach to understanding AI risk to ensure it doesn’t fall for these tricks.In terms of risks, research shows that two of the main sources of risk are deception and power-seeking behaviour:- Dec"
109,Courtney Henry,Q17 Elements to include in risk-based approach,"In general, Attachment C falls short of the necessary detail to understand the range of risks that have and will emerge as well as the range of specificity of mitigations necessary to control them.Attachment C would benefit from:- Distinguishing between scenarios where the use case is the potential risk (such as social scoring); scenarios where dual-use capability and misuse is the potential risk (bioengineering, producing misinformation and disinformation, enhancing criminal capability and capacity); and where the technology alone is the risk (technology with features like being deceptive, being power-seeking, being incorrigible etc).- Distinguishing between the point of regulatory intervention - the developer, the deployer, the user, or elsewhere, and- Offering a much wider range of mitigations that target different kinds of risks.Some elements of attachment C are promising, like “explanations” and “ongoing monitoring” - but these need to be greatly expanded and based on current AI s"
109,Courtney Henry,Q18 Integration of risk-based approach into existing frameworks,"Some areas of regulation, like the Racial, Sex, Age, and Disability Discrimination Acts are appropriately strong and may provide useful safeguards against some harms of AI. Specifically, the law should ensure that such discrimination is always wrong and always punishable, and an excuse that the discrimination occurred because of a “black box” AI provided by an overseas AI Lab should never be accepted. It would be helpful to communicate these requirements to Australian businesses that might deploy AIs and ensure they only contract with AI Labs for products where the Lab can ensure the product cannot engage in these harmful behaviours.Certainly, no AI-specific regulation should ever waive existing safeguards.That said, many of the emerging risks from AI are quite new and it’s reasonable to expect that regulation made without AI in mind is not tuned to the new risks that AI represents. Specific AI safety regulation is unavoidable."
109,Courtney Henry,Q19 Application of risk-based approach to general purpose AI,"The capabilities unlocked by general-purpose AI systems, like large language models (LLMs) and multimodal foundation models (MFMs), fundamentally change the risk landscape. We should anticipate these systems having large, unpredictable effects across all parts of society.A prudent approach is to implement a risk-based framework, where access is graduated based on model capabilities. Models above a certain threshold of autonomous capabilities would be designated as ""advanced systems"" and subject to additional governance, such as permits for users.Further, permits should be required to gain certain levels of access to AI systems which can be used to build advanced systems.Permits allow us to maintain oversight, while still enabling beneficial uses. Different permissions can be granted based on use case - for example, full access for safety testing versus API-only access for most commercial uses. This balances innovation and responsibility.Although this may seem very different to how AI i"
109,Courtney Henry,Q20 Voluntary vs mandated compliance with risk-based approach,"AI is very likely to transform many elements of the economy and society in coming years. Ethics and safety are unlikely to be a priority during this time unless they are actively made a strategic focus by clear leadership. Historic examples include how social media algorithms came to shape our lives, and the negative impacts to mental health and social cohesion only became apparent much later and still have not been adequately addressed. Similarly, while the destructive potential of nuclear was apparent early on, the “arms race” mentality, as well as potential economic benefits, swept humanity along towards the current risky situation that no one in the early 1940s could have conceived of. In that way, history teaches us that it’s critical that ethics and safety take centre stage from day one.The economic and technological forces are likely to be so strong that a scheme that is voluntary or driven by aspirational best practices is unlikely to be successful. Further, smaller businesses "
110,William Horan,Q01 Definitions,"In order to effectively regulate AI based on risks, ethics, and safety, and to build public trust, it's necessary to understand the broad spectrum of AI capabilities. The presented definitions, however, cover such a vast array of systems that they fail to provide useful guidance for policy-making and risk management. For instance, the government, while regulating ""vehicles,"" distinguishes between push-bikes and passenger jets, applying relevant rules to each.Recognizing and incorporating the advances of AI technologies like Multimodal Foundation Modals (MFM) is certainly a positive initial move. However, the proposed definitions should encapsulate the full range of AI capabilities to enable a productive dialogue. Here are five key factors that aid in defining this spectrum of capability:Size: Defined by the number of parameters.Scope of Capability: The system's specialisation versus its general abilities.Multimodality: The variety of data the system can process.Temporal Awareness: Whet"
110,William Horan,Q02 Other AI risks and recommendations for regulation,"Missing risks include (1) misuse of highly capable AI systems, and (2) unintended harm from highly capable AI systems, especially systems with the capacity to plan and act as agents acting in ways that are not aligned with human directives. Although the likelihood of the risks may be uncertain, the harms resulting from these risks may be catastrophic and global in scope and must be acknowledged and addressed.The statement “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war” has recently been signed by the heads of leading AI labs (OpenAI, Google DeepMind, Anthropic, Stability AI) as well as many key academic and other researchers pivotal in developing this technology (Geoffrey Hinton, Yoshua Bengio, etc.) and other notable figures such as Bill Gates [1]. Similar calls date back to at least 2015 when Stephen Hawking led a group of AI experts to call attention to the risk and lay out an urgent resea"
110,William Horan,Q03 Non-regulatory actions to address AI risks,"Government must maintain up-to-date knowledge about rapidly evolving AI systems. Public trust hinges on the belief that the government is well-informed and can effectively oversee and regulate AI developments. Unfortunately, indicators such as Australia's chief scientist's reluctance to project more than two years into the future suggest an alarming gap in understanding and skills at all governmental levels. Addressing this issue is crucial for harnessing AI's economic benefits, navigating ethical concerns, and addressing existential risks.To bring more technical AI expertise into Government, Government should set up a TechCongress-equivalent scheme aimed at enabling the Australian Government to recruit and gain access to AI expertise in fields like AI governance and ethics. The scheme should place experts in Government and also embed them within the public service, especially in Department of Industry, Science and Resources, Home Affairs, and the Australian Communications and Media Au"
110,William Horan,Q04 Coordinating AI governance across government,"Australia should create an AI Commission as a proactive guiding force through the extensive and quick deployment of AI technologies. This entity should be forward-thinking, operating in a manner that considers national interests, including the safety and wellbeing of its citizens.The AI Commission would act as a central leadership node for the Government, fostering communication with industries, academia, non-profit organizations, and the broader community. Compared to alternatives like voluntary self-governance or departmental teams, this structure offers an overarching viewpoint, cross-government leadership, and the nimbleness needed to keep up with the rapidly changing AI landscape.AI and Safety Prioritization: The Commission's responsibility should be to devise a concise and targeted AI strategy, incorporating ethical, safety, and economic aspects. This would serve as a roadmap for governmental policy and help shape legal frameworks and regulations for AI.AI Expertise and Capacity "
110,William Horan,Q05 International models for AI governance,"Australia should take note of the EU recent handling of the AI Act. For example, the European Parliament’s recognition of general-purpose AI (GPAI) is an important step in the right direction [1]. In particular, the following observations are highly valuable:- Acknowledgement that GPAI providers must comply with strict safety measures, regardless of the way it is distributed or intended to be used.- Requirements for GPAI providers to comply with various safety requirements throughout the lifecycle of their product; i.e. external audits assessing their performance, predictability, interpretability, corrigibility, safety and cybersecurity.These adjustments appear to respond to significant criticisms directed at initial drafts of the AI Act. For example, the AI Now Institute released a set of recommendations that heavily criticised the EU’s approach on some matters [2]. For example:- “GPAI models carry inherent risks and have caused demonstrated and wide-ranging harms. While these risks c"
110,William Horan,Q06 Public vs Private sector governance,"The potential adverse effects of AI usage on Australians are identical, regardless of whether the provider is a private enterprise or a government agency.Substantial individual and societal harms, along with catastrophic risks from highly advanced AI systems, could stem from either public or private utilisation. Thus, any strategy must recognise and address these risks."
110,William Horan,Q07 Responsible government use of AI,"There are two primary measures that the Australian government could take to enhance its responsible application of AI.Firstly, Australians may express concerns about the use of AI by security and intelligence agencies. To address this, Australia should spearhead a 5-Eyes agreement outlining the proper application of AI by these agencies, along with a public declaration indicating our ""AI red lines."" This declaration would delineate the potential and prohibited uses of AI by our security agencies. Subsequently, the Inspector General of Intelligence and Security and its 5-Eyes counterparts should be tasked with auditing the intelligence community's compliance with this public statement. This would reassure Australians that AI is not being misused or applied in risky manners, while also safeguarding sensitive capabilities.Secondly, there's a need for the Australian government to enhance the skill set of its public servants. This can be achieved by fostering hubs of expertise within each D"
110,William Horan,Q08 Generic vs technology-specific approaches for AI risk,"Generic measures such as transparency requirements and legal liability frameworks will only be adequate for mitigating risks in simple decision systems which perform single-step information processing and pattern matching, or routine tasks in limited settings [1]; examples of simple systems may include automated robotic systems in manufacturing, or CV screening systems in HR.By contrast, there will need to be specific measures in place to ensure that sophisticated automated decision systems are safe and reliable. “Advanced” systems refers to AI systems which:1. Are able to complete a diverse range of reasoning tasks with human-level performance; and2. Are able to navigate complex information environments and act within those environments autonomously; and3. Are able to form sophisticated plans and reason about the consequences of actionsDue to their autonomous and open-ended capabilities, advanced systems carry complex risks which are more similar to those associated with humans; for e"
110,William Horan,Q09 Transparency requirements and scope,"To remain competitive, professionals and businesses will increasingly feel the pressure to leverage sophisticated “black-box” AI models. This short-term thinking has already resulted in numerous accounts of public backlash and distrust, including high-profile warnings from technology leaders themselves [1].For example, Apple’s credit card business came under fire for offering a woman one twentieth the credit limit offered to her husband. When she complained, an Apple representative simply told her, “I don’t know why, but I swear we’re not discriminating. It’s just the algorithm.” [2]More recently, two lawyers in the United States blamed ChatGPT for “tricking” them into including fictitious legal research in a court filing [3].These problems could easily have been prevented by using explainable AI systems which can show the rationale for their outputs.Such incidents are evidence that the regulatory environment is not incentivising businesses or professionals to use transparent decision-"
110,William Horan,Q10 Ban on high-risk AI applications,"Although there is increasing recognition that AI will pose significant risks to public safety [1], there is widespread confusion about what types of AI are dangerous, and how Australia can help prevent these risks. Luckily, there is a significant body of evidence that describes the main source of these risks [2], and Australia is well-positioned to be a leader in mitigating them.Research shows that two of the main sources of risk are deception and power-seeking behaviour:- Deceptive behaviour in AI systems will become increasingly difficult to avoid as their capabilities advance. This problem has been observed in AI systems since at least 2017, when researchers at OpenAI noted “agents adopting (strategies) that trick the evaluators” [3].- Power-seeking behaviour is the natural tendency for any entity with a long-term goal to take actions that protect or enhance its ability to achieve that goal. Leading AI labs and journals advise that power-seeking behaviour will be a considerable sour"
110,William Horan,Q11 Actions to increase public trust in AI,"There are tremendous concerns about AI among those with the greatest familiarity of its risks [1]; these concerns are being widely broadcasted in Australian media with alarming headlines such as:- SMH: “When the “Godfather of AI” warns you about his offspring, you listen.” [2]- Forbes Australia: “‘We knew AI could destroy the whole world and we wanted to figure out how to prevent that’: ChatGPT Founder;” [3]- Canberra Times: “UN chief backs idea of global AI watchdog” [4]These concerning signals are difficult for the public to ignore.It goes without saying that this is quite a different situation from one where concerns are being raised in isolation or by fringe-figures; this makes it inappropriate to treat “public education” as a significant part of the solution, as is referenced in the discussion paper.The process for building trust in AI should be thought of as similar to fostering public trust in bridges or aeroplanes. Due to their risks, advanced AI systems should only be deployed"
110,William Horan,Q12 Trade impact of banning high-risk AI applications,"This question misunderstands both the sources of risk and the interests of Australians.There are two potential sources of risk both of which need to be mitigated. The first source of risk, as identified in the question, is the use case. Some uses for AI are risky and should be restricted or banned. The second source of risk is the technology itself. Some kinds of technology are inherently risky, and merely restricting a use case does not adequately mitigate the risk.We understand this clearly in other fields. We regulate both how safe a car must be and how a car can be used. We don’t let people speed in cars just because they have a high safety rating; and we don’t let safe drivers buy dangerous vehicles.The economic framing also misunderstands Australia’s national interests. We would never conceive of a conversation about whether our aviation sector would trade more successfully if we authorised airlines to use planes that are untested, experimental or known to be dangerous. Australia"
110,William Horan,Q13 Changes to conformity infrastructure,"World-renowned AI experts such as Yoshua Bengio are warning that there’s a chance “superhuman” AI might be coming as early as 2030 [1]. Although there stand to be significant benefits from deploying advanced AI systems, their deployment will need to be highly targeted and controlled.While the productivity gains from advanced AI systems could be immense, uncontrolled proliferation would pose threats from likely scenarios such as malicious use or rogue AI systems [2]. As such, any deployment of advanced AI systems in Australia should occur via secure data centres, with access provided through a centralised API that facilitates monitoring and accountability by providing metadata about usage.Stringent compliance requirements must also be implemented for any organisation granted access to advanced AI systems. AI Governance researchers recommend that regulators should develop audit trail requirements in safety-critical applications of AI, similar to flight data recorders that are standard in"
110,William Horan,Q14 Risk-based approach to address AI risks,"A risk based approach has worked well as a proactive means of identifying, assessing and mitigating potential safety hazards in the aviation industry.The industry has developed specific tools to assist in identifying potential risks - Safety Management Systems (SMS). Critical to this is the ability for all parties involved in the development and use of AI systems to be able to report the identification of any risks.Following identification, risks are assessed based on severity and likelihood. This work is led or assisted by technical bodies like the Australian Transport Safety Bureau. In the AI context, other countries have created or proposed similar national “labs” that would be well placed to lead this work.The industry participant best placed to mitigate the risk is then required to take steps to reduce the likelihood or impact. Given the nature of the technology this stage should be open to update as new techniques and tools become available based on the most up to date research.C"
110,William Horan,Q15 Benefits and limitations of risk-based approach,"The key to success is understanding risks sufficiently broadly to encourage caution in an uncertain future. It is not enough to only react once the risk has been actualised - for some risks that’s too late.A potential “worse-case scenario” is if a culture of “trial and error” is adopted, whereby adverse events occur before risks are considered tangible enough to act upon. This reliance on post-hoc interventions risks normalising unsafe development practices, making it difficult to recover once risks proliferate.Proliferation of risks is a key point in governing AI, because regulating software poses significant challenges compared to other dual-use commodities such as pharmaceuticals or firearms. This means that, from the outset, there must be proactive regulatory oversight for the deployment of advanced AI systems and their precursors.For example, significant structural changes, such as the use of advanced AI systems in Australia, require that risks must be comprehensively understood p"
110,William Horan,Q16 Scope and application of risk-based approach,"The most important organisations to focus on when developing risk-management strategies and regulations, will be those that determine how advanced AI is developed and deployed. More specifically, there are three prominent categories of concern:1. Organisations that develop advanced AI systems, or AI systems that can be used to build more advanced AI systems (i.e. precursor systems such as LLMs). Examples include OpenAI, DeepMind, Anthropic, Inflection AI, Nvidia, Microsoft, Meta2. Organisations which provide access to open-source AI systems via downloads or APIs. E.g. Hugging Face, Github, AWS3. Organisations which provide compute resources to develop, fine-tune or use AI systems. E.g. AWS, HuggingFace, Google, Azure, Nvidia, Lambda LabsA majority of the benefits and risks will come from AI that is more sophisticated and generally capable, which can be tied back to organisations in these three categories.There are approaches which can be taken for each category in order to manage risks"
110,William Horan,Q17 Elements to include in risk-based approach,"Human-in-the-loop monitoring will be vital, but insufficient on its own to regulate advanced AI systems that will eventually surpass human capabilities. The speed at which advanced AI systems will operate means that human oversight will become pragmatically infeasible if our monitoring programs aren’t able to scale proportionally.As such, we must employ an approach that is predicated on limiting access to advanced AI systems unless there is a strong rationale for using them; conducting cost-benefit analyses prior to approving the deployment of advanced systems will mitigate risks while preserving potential benefits.A key part of enforcing this targeted approach will be monitoring and controlling access to high-performance computing resources, which are required for developing and operating advanced AI systems and their precursors.This additional layer of governance allows authorities to detect potentially dangerous AI development or usage activity, and can be cross-checked with other d"
110,William Horan,Q18 Integration of risk-based approach into existing frameworks,"Some areas of regulation, like the Racial, Sex, Age, and Disability Discrimination Acts are appropriately strong and may provide useful safeguards against some harms of AI. Specifically, the law should ensure that such discrimination is always wrong and always punishable, and an excuse that the discrimination occurred because of a “black box” AI provided by an overseas AI Lab should never be accepted. It would be helpful to communicate these requirements to Australian businesses that might deploy AIs and ensure they only contract with AI Labs for products where the Lab can ensure the product cannot engage in these harmful behaviours.Certainly, no AI-specific regulation should ever waive existing safeguards.That said, many of the emerging risks from AI are quite new and it’s reasonable to expect that regulation made without AI in mind is not tuned to the new risks that AI represents. Specific AI safety regulation is unavoidable."
110,William Horan,Q19 Application of risk-based approach to general purpose AI,"The capabilities unlocked by general-purpose AI systems, like large language models (LLMs) and multimodal foundation models (MFMs), fundamentally change the risk landscape. We should anticipate these systems having large, unpredictable effects across all parts of society.A prudent approach is to implement a risk-based framework, where access is graduated based on model capabilities. Models above a certain threshold of autonomous capabilities would be designated as ""advanced systems"" and subject to additional governance, such as permits for users.Further, permits should be required to gain certain levels of access to AI systems which can be used to build advanced systems.Permits allow us to maintain oversight, while still enabling beneficial uses. Different permissions can be granted based on use case - for example, full access for safety testing versus API-only access for most commercial uses. This balances innovation and responsibility.Although this may seem very different to how AI i"
110,William Horan,Q20 Voluntary vs mandated compliance with risk-based approach,"AI is very likely to transform many elements of the economy and society in coming years. Ethics and safety are unlikely to be a priority during this time unless they are actively made a strategic focus by clear leadership. Historic examples include how social media algorithms came to shape our lives, and the negative impacts to mental health and social cohesion only became apparent much later and still have not been adequately addressed. Similarly, while the destructive potential of nuclear was apparent early on, the “arms race” mentality, as well as potential economic benefits, swept humanity along towards the current risky situation that no one in the early 1940s could have conceived of. In that way, history teaches us that it’s critical that ethics and safety take centre stage from day one.The economic and technological forces are likely to be so strong that a scheme that is voluntary or driven by aspirational best practices is unlikely to be successful. Further, smaller businesses "
111,Oscar Delaney,Q02 Other AI risks and recommendations for regulation,"Despite the recognition of the disastrous potentials of highly competent AI systems by experts and well-known personalities - demonstrated through open letters hosted by the Future of Life Institute [1] and the Centre for AI Safety [2] - this government discussion paper fails to address these risks.Moreover, the UN Secretary-General has publicly acknowledged the devastating and existential threats posed by advanced AI systems [3].It is imperative that the Australian government recognises and engages with the severe and existential threats posed by advanced AI systems, similar to other nations and experts. It is not tolerable to stay quiet on a topic of such magnitude and concern.As a young person with my life ahead of me, this is one of the issues I worry most about and think could significantly harm my future. While these concerns sometimes seem abstract or far-fetched, I think we can’t afford to be complacent and must be proactive in heading off any risks before they materialise.Typi"
111,Oscar Delaney,Q03 Non-regulatory actions to address AI risks,"A key non-regulatory role Australia could play is significantly enhancing the support, particularly financial, for AI safety research within Australian universities. Research priorities should be mindful of the swift evolution of AI technology. By investing in research that is visionary, forward-looking, and centred on transformative strategies, rather than minor, short-term, or market-oriented innovations, Australia has the potential to establish itself as a long-term global authority on crucial aspects of AI safety. Emphasising mechanistic interpretability, or the 5 components of effective traceability (Design Transparency; Reproducibility; Operational Recordkeeping; Human Understanding and Auditability) [1], could position Australia at the forefront of the discipline, bolster safety-oriented global regulations, and ultimately earn Australia a significant 'seat at the table' as AI development continues to advance. A primary objective of Australian leadership should be preserving Aust"
111,Oscar Delaney,Q05 International models for AI governance,"Australia should pay attention to the recent developments surrounding the EU's AI Act. In particular, the European Parliament's acknowledgement of general-purpose AI (GPAI) represents significant progress [1]. Notably, the following points are highly valuable: The recognition that GPAI providers must adhere to stringent safety measures, irrespective of their distribution methods or intended use. The mandate for GPAI providers to fulfil a variety of safety requirements throughout their product's lifecycle, such as external audits evaluating their performance, predictability, interpretability, corrigibility, safety, and cybersecurity. These revisions seem to address significant critiques previously directed at the initial drafts of the AI Act. For instance, the AI Now Institute put forward recommendations that heavily censured the EU’s approach on certain issues [2]. For example: “GPAI models carry inherent risks and have caused demonstrated and wide-ranging harms. While these risks can "
111,Oscar Delaney,Q09 Transparency requirements and scope,"With the pressure to stay competitive, professionals and businesses are increasingly driven to utilise intricate ""black-box"" AI models. This trend towards short-term gains has already sparked numerous public backlashes and distrust, with even tech leaders issuing grave warnings [1]. For instance, Apple's credit card division faced criticism when a woman was granted only one-twentieth of the credit limit extended to her husband. When she protested, an Apple representative could only respond, ""I don't know why, but I assure you we're not discriminating. It’s just the algorithm."" [2] In another incident, two American attorneys blamed ChatGPT for ""deceiving"" them into including non-existent legal research in a court submission [3]. These situations could have been averted by using explainable AI systems, which can provide a justification for their outputs. Such instances highlight that current regulations do not incentivise businesses or professionals to employ transparent decision-making "
111,Oscar Delaney,Q10 Ban on high-risk AI applications,"While current AI does not present an immediate existential or catastrophic threat to humanity, there is a legitimate concern that it could pose such risks in the not-so-distant future [1, 2]. To harmonize all perspectives, regulators should strive to pinpoint ""advanced indicators"" of potential catastrophic risk scenarios. They should also communicate unequivocally that if these advanced indicators materialize, a prohibition is an appropriate course of action. The task of determining suitable advanced indicators must involve consultation with AI safety researchers, such as those associated with the Centre for AI Safety or the Centre for the Governance of AI. These indicators could encompass factors like the scale of training runs (in terms of compute and data) and safety-relevant attributes of the AI system. These might include the capability to carry out diverse reasoning tasks with human-level proficiency, to navigate intricate information landscapes autonomously and act within those "
111,Oscar Delaney,Q14 Risk-based approach to address AI risks,"I agree with the implementation of a risk-based approach, however, my primary concern lies with the catastrophic and existential risks that could arise during the training of large and sophisticated AI systems, not merely when they are deployed. Hence, for systems of such magnitude with the potential to generate catastrophic capabilities, the risk-based strategy should be invoked prior to the training phase - risk assessment should be based on the nature and scale of the proposed training run, rather than merely examining the potential risky uses and deployment issues of an AI after it's already been trained and created. Any risk assessment should factor in the potential irreversibility, scope, and severity of the risk, and the responses should be proportionate. The risk-based method delineated in Attachment C of the Discussion Paper is somewhat challenging to comprehend. The suggested strategy posits ""human in the loop"" as the primary mitigation measure, but it's unclear whether this "
111,Oscar Delaney,Q16 Scope and application of risk-based approach,"Many prominent AI experts such as Yoshua Bengio have voiced concerns about the imminent arrival of ""superhuman"" AI, potentially by 2030. These types of advanced AI systems have the potential to be extremely disruptive across all sectors of the economy. While current risks may be tied to specific use cases and sectors, future risks will increasingly be associated with the underlying capabilities of the technology itself. Given these considerations, access to increasingly advanced AI systems should be limited to organisations that can demonstrate their usage will yield significant public benefits that align with Australia's national interests. This targeted approach can help mitigate risks in two primary ways: It restricts the opportunities for misuse or accidents with advanced AI by reducing its proliferation across society. The usage is focused on high-value use cases that justify the risks. It establishes clear lines of accountability. The organisations that are granted access must ha"
111,Oscar Delaney,Q19 Application of risk-based approach to general purpose AI,"The capabilities unlocked by general-purpose AI systems, such as large language models (LLMs) and multimodal foundation models (MFMs), indeed represent a significant shift in the risk landscape. The potential impact of these technologies is vast and can affect every sector of society, often in unpredictable ways. A proactive and prudent response is to implement a risk-based framework where access to these systems is tiered based on their capabilities. In this framework, models that demonstrate a high degree of autonomy and capabilities would be classified as ""advanced systems"" and subjected to additional oversight, potentially requiring users to obtain permits. The permit system provides an element of control and oversight while still allowing for the beneficial uses of these advanced AI systems. Different levels of access can be granted based on the specific use case – for instance, full access could be allowed for safety testing, while API-only access might be permitted for most comm"
111,Oscar Delaney,Q20 Voluntary vs mandated compliance with risk-based approach,"The potential harms of AI could arise from both private and public organisations, and it's crucial that comprehensive safety and risk management measures be implemented across the board. Therefore, in general, there shouldn't be a distinction in their regulatory status. Applying regulatory measures to both developers and deployers aligns with the principle that those who are in the best position to mitigate a risk should bear the associated regulatory responsibility. In situations where developers provide a ""black box"" product that deployers have little ability or resources to modify significantly, the primary regulatory obligation should fall on the developers. Moreover, the penalties for non-compliance should be proportionate to the income generated from the AI applications. This approach ensures that neglecting regulation doesn't become a financially calculated ""cost of doing business"" or a risk that deployers willingly accept. As part of the regulatory toolkit, measures for technic"
112,Callum Dyer,Q02 Other AI risks and recommendations for regulation,"The discussion paper rightly identifies a range of harms that may arise due to the increasing use of AI. What it misses, I think, is the possibility of AI risks—particularly regarding Artificial General Intelligence (AGI)—being existential risks to humanity. Such extreme risks need not invalidate other lower scale risks, but they should not be discounted either.No doubt, there is a huge amount of uncertainty regarding the debate as to whether AGI could pose an existential risk or not. Some people seem certain that it does, by default. They may argue that the range of things that an AGI may optimise for is actually infinitely broader (and consequently more alien-seeming) than the kinds of things humans want. On the other hand, others argue just as passionately that any AI that is intelligent enough to have its own goals the way a human would, will inevitably sympathise with humanity and align itself with us.I am more convinced by the pessimistic argument, but I do acknowledge the possib"
112,Callum Dyer,Q03 Non-regulatory actions to address AI risks,"There are two things I would like to see the Australian Government do to reduce the existential risks of the use of AI:1. Australia should use its significant diplomatic weight to advocate that the world come together to take action, in much the same way that Australia advocates for action on climate change.2. Australia should invest in research that seeks to directly solve the ‘control problem’. If indeed it is true that AGI is safe by default, we should prove that this is the case before moving ahead with its creation.Both of these initiatives may indeed not be enough to sufficiently mitigate existential risk, but they can be expected to contribute significantly towards this goal while discussion continues on the question of regulatory options."
113,Ross Tieman,Q01 Definitions,"The definitions presented in ""Figure 1"" demonstrate a limited understanding of ""intelligence."" The proposed definition confines ""intelligence"" solely to the generation of predictive outputs, which fails to capture the broader scope of capabilities exhibited by current and future AI systems. Although it is true that present Language Model Models (LLMs) often focus on predicting the next word, and narrow AI systems engage in various forms of prediction, such as diagnosing diseases from scans or determining optimal moves in games, this perspective represents a narrow interpretation of the overall functionalities of AI. To ensure a more future-proof comprehension of machine intelligence, it is imperative to draw closer parallels to human intelligence, encompassing abilities such as understanding, planning, and goal achievement within the world.Moreover, the definition of AI in ""Figure 1"" is erroneous in its restriction of ""intelligence"" to ""human-defined objectives or parameters."" This app"
113,Ross Tieman,Q02 Other AI risks and recommendations for regulation,"Warnings from experts on global catastrophic risks, including the threat of pandemics, were indeed present in the years leading up to the emergence of COVID-19.In 2017, Oxford researcher Sebastian Farquar emphasized the global responsibility in managing pandemics and highlighted the lack of attention given to worst-case scenarios. He pointed out that the planning efforts were primarily focused on a national level, neglecting the potential severity of such crises [1]. Had governments heeded these expert warnings before COVID-19, implementing low-cost interventions like better regulation of exotic animal markets, it could have potentially prevented millions of deaths and trillions of dollars in economic damage.The approach to AI governance in Australia shares similarities with the situation described by Dr. Farquar. The focus has been predominantly on lower-impact, local issues, which can undermine the ability to address higher-impact global challenges effectively.However, there has been"
113,Ross Tieman,Q03 Non-regulatory actions to address AI risks,"It is crucial for the government to maintain up-to-date knowledge about rapidly evolving AI systems. Public trust relies on the belief that the government is well-informed and capable of effectively overseeing and regulating AI developments. Unfortunately, indicators such as the reluctance of Australia's chief scientist to project beyond a two-year timeframe reveal a significant knowledge and skills gap at all levels of government. Addressing this issue is essential for harnessing the economic benefits of AI, navigating ethical concerns, and mitigating existential risks.To bring more technical AI expertise into the government, it is recommended to establish a scheme similar to TechCongress. This scheme would enable the Australian government to recruit and access AI expertise in fields such as AI governance and ethics. The experts recruited through this scheme should be placed within the government and embedded within the public service, particularly in departments such as the Departmen"
113,Ross Tieman,Q04 Coordinating AI governance across government,"Australia should establish an AI Commission to guide the country through the rapid and widespread adoption of AI. This commission must take a proactive stance and manage AI in a way that aligns with the national interest, ensuring citizen safety and wellbeing.The AI Commission would serve as a central point of leadership for the government, acting as a hub for engagement with industries, academia, non-profits, and the community. This approach offers distinct advantages over other methods such as voluntary self-governance or departmental-level teams. It provides a broader perspective, the authority to lead across government, and the agility needed to keep pace with the fast-evolving AI sector.Key responsibilities of the AI Commission should include: Setting priorities in Australia's approach to AI and safety: The Commission should establish a clear and focused AI strategy that encompasses ethical, safety, and economic considerations. This strategy would guide government policies and inf"
113,Ross Tieman,Q05 International models for AI governance,"National AI labs are emerging as jurisdictions recognize the need to test, understand, and control safe AI. These labs collaborate with the private sector and complement their work. Just like national aviation safety expertise has formed an internationally collaborative network, national AI labs are likely to follow suit. It is crucial for Australia to initiate this process without delay.A notable example of governmental organization globally is the EU's Centre for Algorithmic Transparency, established to enforce the Digital Services Act. This research center aims to address the ""systemic risk"" to society associated with algorithms, including recommender systems and AI.In preparation for a future where AI-based decision systems may become integral to society, Australia should adopt a broad version of this concept to comprehend and shape the implications of algorithms, particularly AI algorithms utilizing deep learning.Understanding deep learning is pivotal for several reasons. It serve"
113,Ross Tieman,Q06 Public vs Private sector governance,"The potential negative impacts on Australians arising from the use of AI are indeed the same regardless of whether the provider is a private company or a government department. Both public and private entities have the potential to cause significant individual and societal harms through the use of AI. Additionally, highly capable AI systems can pose catastrophic risks regardless of whether they are developed and deployed by the public sector or the private sector. Therefore, any approach to AI governance must recognize and effectively manage these risks, irrespective of the entity responsible for the AI systems."
113,Ross Tieman,Q07 Responsible government use of AI,"As AI becomes increasingly integral to Australia's functioning as a nation, it is crucial to shape the skills and capabilities of the Australian Public Service (APS) to align with this transformative technology. With technical systems growing in sophistication and prevalence across society, a deep understanding of these systems will be essential across all policy domains.However, the current structure of the APS faces challenges in effectively hiring and retaining technical talent [1]. For instance, the average base pay for a machine learning engineer [2] typically exceeds the EL1 threshold [3]. Attracting and retaining experienced experts in cutting-edge AI systems would likely require more competitive compensation.To ensure that Australia's policy decisions are well-equipped to capture the benefits and mitigate the risks of the 21st century, specific initiatives should be established to attract elite global AI talent into senior positions across the government. The emphasis on attrac"
113,Ross Tieman,Q08 Generic vs technology-specific approaches for AI risk,"While generic measures like transparency requirements and legal liability frameworks are suitable for mitigating risks in simple decision systems, more specific measures are needed to ensure the safety and reliability of sophisticated automated decision systems. These ""advanced"" systems possess autonomous and open-ended capabilities, making their risks more complex and similar to those associated with humans.To address these risks, several strategies have been proposed:1. Stuart Russell's ""Provably Beneficial AI"" emphasizes the development of safe systems using reinforcement learning. The principles include ensuring the system's goal is to satisfy society's long-term preferences, maintaining constant uncertainty about those preferences, and actively learning our preferences through observation.2. Yoshua Bengio's ""AI Scientists"" approach suggests reimagining autonomous, outcome-optimizing AI systems to focus on theory building and question answering. These systems would act as advisors "
113,Ross Tieman,Q09 Transparency requirements and scope,"The increasing pressure on professionals and businesses to leverage sophisticated ""black-box"" AI models for short-term gains has led to public backlash and distrust. Incidents such as discriminatory credit card decisions and legal filings containing fictitious information demonstrate the need for explainable AI systems that can provide the rationale for their outputs.The current regulatory environment does not incentivize businesses or professionals to prioritize transparent decision-making tools. Without mandated interpretability, those who want to do the right thing may fall behind competitors who are willing to take on greater risks. As the use of AI systems expands, these examples of opacity will become more common and consequential without regulatory intervention that mandates explainability.The lack of demand for explainable enterprise AI tools also hinders AI developers. Although there is widespread interest in creating interpretable AI tools, the main obstacle is the lack of in"
113,Ross Tieman,Q10 Ban on high-risk AI applications,"The continued evolution of AI brings several societal challenges that, left unchecked, could risk the security, privacy and freedom of the Australian public. Many of the most extreme harms can be attributed to AI systems that have the ability to: 1. Complete a diverse range of reasoning tasks with human-level performance; 2. Navigate complex information environments and act within those environments autonomously; 3. Form sophisticated plans and reason about the consequences of actions Although AI systems with these capabilities may seem like science fiction, they are already being tested with great success in contained environments. For example, “Vogager” is a system that was recently developed by chaining together versions of GPT and connecting them to an external database, which the chatbots use to store and retrieve code that executes complex behaviours. Despite being a simple system, Voyager was hugely successful when tested in the open-world game Minecraft; this may appear trivial"
113,Ross Tieman,Q11 Actions to increase public trust in AI,"Public distrust in AI is fuelled by the growing sense that powerful AI systems are being developed and deployed by foreign actors, some of whom have demonstrated unsafe attitudes towards deploying AI systems [1]. The interests of Australians and Australia is not part of the conversation. Although Australia may struggle to catch up to the US or UK in advancing the frontier of AI, this isn’t necessary for improving public trust and increasing adoption.Rather, Australia has a clear opportunity to become a leader in AI safety; this enviable position would not only assist safe adoption locally by providing much needed expertise in safe systems, it could help foster a sense of pride and ownership by opening up export opportunities in the emerging market for AI safety services [2]. AI Safety is how Australia could get its ‘seat at the table’ in the global AI conversation.Pursuing this position would involve the following: - Coordinating with Australian AI safety organisations to better unders"
113,Ross Tieman,Q12 Trade impact of banning high-risk AI applications,"This question misunderstands both the sources of risk and the interests of Australians.There are two potential sources of risk both of which need to be mitigated. The first source of risk, as identified in the question, is the use case. Some uses for AI are risky and should be restricted or banned. The second source of risk is the technology itself. Some kinds of technology are inherently risky, and merely restricting a use case does not adequately mitigate the risk.We understand this clearly in other fields. We regulate both how safe a car must be and how a car can be used. We don’t let people speed in cars just because they have a high safety rating; and we don’t let safe drivers buy dangerous vehicles.The economic framing also misunderstands Australia’s national interests. We would never conceive of a conversation about whether our aviation sector would trade more successfully if we authorised airlines to use planes that are untested, experimental or known to be dangerous. Australia"
113,Ross Tieman,Q13 Changes to conformity infrastructure,"World-renowned AI experts such as Yoshua Bengio are warning that there’s a chance “superhuman” AI might be coming as early as 2030 [1]. Although there stand to be significant benefits from deploying advanced AI systems, their deployment will need to be highly targeted and controlled.While the productivity gains from advanced AI systems could be immense, uncontrolled proliferation would pose threats from likely scenarios such as malicious use or rogue AI systems [2]. As such, any deployment of advanced AI systems in Australia should occur via secure data centres, with access provided through a centralised API that facilitates monitoring and accountability by providing metadata about usage.Stringent compliance requirements must also be implemented for any organisation granted access to advanced AI systems. AI Governance researchers recommend that regulators should develop audit trail requirements in safety-critical applications of AI, similar to flight data recorders that are standard in"
113,Ross Tieman,Q14 Risk-based approach to address AI risks,"I support a risk-based approach, however the catastrophic and existential risks I am primarily worried about originate when sufficiently large and capable AI systems are trained, not just when they are deployed. Therefore, for sufficiently large systems with the potential for catastrophic capabilities, the risk-based approach must be applied before the training phase – analysing risks depending on the nature and scale of the proposed training run, rather than merely analysing the potentially risky use cases and deployment issues of an AI after it has already been created and trained.Any assessment of risk should consider its potential irreversibility, scope and severity, and act proportionally.The risk-based approach set out in Attachment C of the Discussion Paper is hard to follow. The proposed approach offers “human in the loop” as the key mitigation (unclear that this is an effective mitigation) and goes on to note that human in the loop may not be possible where the benefit of the "
113,Ross Tieman,Q15 Benefits and limitations of risk-based approach,"While a risk-based approach to regulating AI is sensible, there are two emerging elements of risk practice that any risk-based model should account for to succeed in this context: 1. Factoring in “uncertainty”, not just “likelihood” 2. Taking a “portfolio” approach to risk mitigationA traditional risk matrix will consider scenarios against their likelihood of occurring, the vulnerability of the system to the scenario, and the consequence of the scenario occurring. It might then discount that risk by the strength of existing or possible mitigations. This approach to risk has historically been unsuccessful for scenarios where assessing “likelihood” is challenging. For instance, many national risk assessments failed to take into account the possibility of a global pandemic before COVID-19. A solution to this failure mode is including “uncertainty”. Specifically, where we can't be sure what the future will bring, that should be cause for significant concern. This is the opposite of the app"
113,Ross Tieman,Q16 Scope and application of risk-based approach,"The concept of “AI” includes a broad range of capabilities and capacities. Government is right to work to distinguish between different use cases - including with a specific focus on AI maturity.Concerningly, this broad range of AI capabilities and capacities might be used by industry to attempt to bamboozle government. Specifically, industry is likely to argue that AI does not need regulation with reference to basic AI technology, with a view to that principle flowing through to more advanced and future technologies. This is akin to arguing that a horse and wagon doesn’t need strict regulation, then later trying to claim that a tank is just a more capable horse and wagon with some components swapped out.Government needs both a fine-grained approach to defining AI technology and a fine-grained approach to understanding AI risk to ensure it doesn’t fall for these tricks.In terms of risks, research shows that two of the main sources of risk are deception and power-seeking behaviour:- Dec"
113,Ross Tieman,Q17 Elements to include in risk-based approach,"Human-in-the-loop monitoring will be vital, but insufficient on its own to regulate advanced AI systems that will eventually surpass human capabilities. The speed at which advanced AI systems will operate means that human oversight will become pragmatically infeasible if our monitoring programs aren’t able to scale proportionally.As such, we must employ an approach that is predicated on limiting access to advanced AI systems unless there is a strong rationale for using them; conducting cost-benefit analyses prior to approving the deployment of advanced systems will mitigate risks while preserving potential benefits.A key part of enforcing this targeted approach will be monitoring and controlling access to high-performance computing resources, which are required for developing and operating advanced AI systems and their precursors.This additional layer of governance allows authorities to detect potentially dangerous AI development or usage activity, and can be cross-checked with other d"
113,Ross Tieman,Q18 Integration of risk-based approach into existing frameworks,"Some areas of regulation, like the Racial, Sex, Age, and Disability Discrimination Acts are appropriately strong and may provide useful safeguards against some harms of AI. Specifically, the law should ensure that such discrimination is always wrong and always punishable, and an excuse that the discrimination occurred because of a “black box” AI provided by an overseas AI Lab should never be accepted. It would be helpful to communicate these requirements to Australian businesses that might deploy AIs and ensure they only contract with AI Labs for products where the Lab can ensure the product cannot engage in these harmful behaviours.Certainly, no AI-specific regulation should ever waive existing safeguards.That said, many of the emerging risks from AI are quite new and it’s reasonable to expect that regulation made without AI in mind is not tuned to the new risks that AI represents. Specific AI safety regulation is unavoidable."
113,Ross Tieman,Q19 Application of risk-based approach to general purpose AI,"The risk-based approach must be modified to address risks from general purpose AI systems, especially LLMs and MFMs. These types of systems learn their capabilities and behaviours during training, which can be unforeseen by developers. That means that AI developers could create models with dangerous capabilities (e.g., deception, manipulation, political strategy, weapons design). Humans could misuse those capabilities to cause harm [1], and models could apply the capabilities even without deliberate misuse (e.g., through misalignment) [2].Consistent with recent work from a consortium of AI safety researchers, including at Google Deepmind, OpenAI, Anthropic, Alignment Research Center, University of Cambridge, University of Oxford, Centre for Long-Term Resilience, and Centre for the Governance of AI [1], we recommend that any risk-based approach require that general purpose models are evaluated for dangerous capabilities and misalignment. This will help determine: 1. To what extent a mod"
113,Ross Tieman,Q20 Voluntary vs mandated compliance with risk-based approach,"AI is very likely to transform many elements of the economy and society in coming years. Ethics and safety are unlikely to be a priority during this time unless they are actively made a strategic focus by clear leadership. Historic examples include how social media algorithms came to shape our lives, and the negative impacts to mental health and social cohesion only became apparent much later and still have not been adequately addressed. Similarly, while the destructive potential of nuclear was apparent early on, the “arms race” mentality, as well as potential economic benefits, swept humanity along towards the current risky situation that no one in the early 1940s could have conceived of. In that way, history teaches us that it’s critical that ethics and safety take centre stage from day one.The economic and technological forces are likely to be so strong that a scheme that is voluntary or driven by aspirational best practices is unlikely to be successful. Further, smaller businesses "
114,Stefan SK,Q01 Definitions,"I agree with them. LLMs probably aren't best described by what they output so much as what you can usefully input.They have the ability to understand both structured and unstructured text. So you could input a natural language essay or some computer code and configure them to output human text, code, or even a probability distribution over classification options.So you might say: ""A large language model (LLM) is a type of AI that can usefully process a wide range of textual inputs from news articles, to scientific papers, to computer code. They can be configured to output to any digital format but most commonly conversational written language."""
114,Stefan SK,Q02 Other AI risks and recommendations for regulation,"My knowledge of our regulatory approaches is limited, but here are some potential areas that are less often discussed: - AI enabled Astroturfing. Democratic processes might be delegitimised by automated lobbying. - Overzealous law enforcement. We may have laws that society assumes will not be strictly and universally enforced, e.g. jaywalking. AI could enable this enforcement and cause disruption. - Teaching dangerous knowledge. AI systems can be fairly effective but amoral teachers. Education startups might want to limit the sophistication of automated education in biotech or chemistry to avoid enabling terrorism."
114,Stefan SK,Q03 Non-regulatory actions to address AI risks,"- Trusted digital identity (or just proof of humanity) could be used to limit AI intrusion into human centric processes like this consultation. This would protect against abuse, and reduce misinformation potential. - Compute governance. A promising idea for governance of powerful AI is monitoring and controlling the use of AI accelerator chips. By encouraging monitoring of large scale chip imports and cloud provider usage early we might give companies more time to comply if it becomes a necessary tool later."
114,Stefan SK,Q06 Public vs Private sector governance,Public sector should probably have stricter explainability conditions around ADM systems as well as higher levels of transparency.
114,Stefan SK,Q07 Responsible government use of AI,Develop ethical case studies and a suggested resolution. Encourage education of less technical staff by more technical. In the Victorian government where I work there is uncertainty around what is allowed by policy and also about what AI systems are capable of.
114,Stefan SK,Q08 Generic vs technology-specific approaches for AI risk,"General principals may work for controlling lawful consumers of technology in a flexible fashion. Specific solutions might be better to protect against development of misusable technology in particular cases.For example generic concern over privacy would inform a lawful party when deciding how to use both genomic information and security camera footage.Specific regulation might have prevented facial recognition technology from being developed, or usage of genomic matching to calculate health insurance premiums."
114,Stefan SK,Q09 Transparency requirements and scope,"Transparency is important to ensure ADMs don't become black boxes who can't be challenged. Also with respect to what data is used for training and inference. People may be more or less comfortable with their information, or information about their communities being used by these systems.The frontier capabilities of powerful models should also be made public so the global community can prepare for potential impacts."
114,Stefan SK,Q10 Ban on high-risk AI applications,"Things we should ban: - Autonomous weapon systems should be delayed as long as possible. I don't see their development helping humanity as a whole. - Highly capable mass surveillance systems by government law enforcement. Australia's existing metadata capture legislation should be used in a very targeted fashion, not mined to casually identify criminal behaviour.I don't have general criteria for decisions around banning. We might want to be able to generically slow down the development of particular systems so public discussion can catch up."
114,Stefan SK,Q11 Actions to increase public trust in AI,"Broader than AI, but careful work on digital identity is one. If we can exclude or identify AI actors from certain processes this will let people feel like they have more control. Digital identiy is a somewhat stigmatised concept however, so this would need to be done carefully.Allow individuals verifiable control over how their information can be used. E.g. contact tracing during COVID could have been implemented in a way that would have obviated the concerns over it's ability to be misused by police."
114,Stefan SK,Q14 Risk-based approach to address AI risks,The idea of categorising products by risk level and applying different regulation depending on their level sounds reasonable.
114,Stefan SK,Q20 Voluntary vs mandated compliance with risk-based approach,Voluntary sounds good to me at this time.
115,Professor Dan Jerker B. Svantesson,Transcription,"FILENAME: SVANTESSON20230721SENT.c0c81b3e06b0e.pdf | CONTENT: Safe and responsible AI in AustraliaSubmission by Professor Dan Jerker B. Svantesson to the Department of Industry, Science and Resources regarding:The public consultation on Safe and responsible AI in AustraliaJuly 2023Professor Dan Jerker B. SvantessonFaculty of Law, Bond University Gold Coast, Queensland, 4229 Australia dasvante@bond.edu.auPROFESSOR DAN JERKER B. SVANTESSON | SUBMISSION - 2023 Page 1 Safe and responsible AI in AustraliaSummary of major points• These submissions outline a framework consisting of 13 principles that ought to guide Australia’s approach to mitigating the potential risks of AI.• Australia should also consider adopting certain structures including an ‘AI Safety Commissioner’.• Where other States also adopt this approach, Australia may also wish to consider working towards establishing a structure for the active collaboration and cooperation between each State’s respective AI Safety Commissioner;"
116,Australasian Sonographers Association,Transcription,"FILENAME: 21072023 Safe and responsible AI in Australia - ASA consultation response FINAL.9927a0d49481b.pdf | CONTENT: Friday, 21 July 2023Australian Government Department of Industry, Science and Resources Submitted online via consultation webpageTo whom it may concern,Safe and responsible AI in Australia consultationThank you for the opportunity to provide feedback to the Department of Industry, Science and Resources (DISR) consultation into Safe and responsible AI in Australia.The Australasian Sonographers Association (ASA) is the professional organisation for Australasian sonographers, who are the experts in ultrasound. With over 7,000 members, and representing more than 70% of Australasia’s sonographers, the ASA’s purpose is to foster a sonography profession that delivers high quality ultrasound with a vision to create a healthier world through sonographer expertise.Sonographers are highly skilled health professionals who are strategically involved at the initial diagnostic stages"
118,Data Synergies (Prof Peter Leonard),Transcription,"FILENAME: Peter Leonard Safe and Responsible AI in Australia after Generative AI final 19 July 2023.c3a3fa6c4583c.pdf | CONTENT: Safe and responsible AI in Australia after generative AI1 Peter Leonard, Data Synergies and UNSW Business School2 This paper is intended to assist the Department of Industry, Sciences and Resources in the Department’s consideration of a bundle of positive incentives, and regulated prescriptions and requirements, to improve AI-affected decisions made by organisations operating in Australia. This paper responds to the Department’s Discussion Paper on Safe and Responsible AI in Australia, June 2023.3 The Discussion Paper, at pages 34 and 35, asked twenty questions. Section 3 of this paper sets out our responses to many of those questions. To illustrate the reasoning underling our responses, in section 2 we discuss a case study, being use by a medical doctor or other primary health professional of health information about a patient and a generative AI application"
118,Data Synergies (Prof Peter Leonard),Q01 Definitions,See attached paper at pages 17 to 19
118,Data Synergies (Prof Peter Leonard),Q02 Other AI risks and recommendations for regulation,See attached paper at pages 17 to 19
118,Data Synergies (Prof Peter Leonard),Q03 Non-regulatory actions to address AI risks,See attached paper at pages 19 to 28
118,Data Synergies (Prof Peter Leonard),Q05 International models for AI governance,See attached paper at page 28
118,Data Synergies (Prof Peter Leonard),Q06 Public vs Private sector governance,See attached paper at pages 28 and 29
118,Data Synergies (Prof Peter Leonard),Q07 Responsible government use of AI,See our responses to questions 3 and 6 above.
118,Data Synergies (Prof Peter Leonard),Q08 Generic vs technology-specific approaches for AI risk,"See our responses to question 2 above, and our proposed methodology for AI policy-making as outlined in section 1 of the attached paper"
118,Data Synergies (Prof Peter Leonard),Q09 Transparency requirements and scope,"See our specific proposals in response to question 2 above, and our comments in response to question 6"
118,Data Synergies (Prof Peter Leonard),Q10 Ban on high-risk AI applications,See attached paper at pages 30 and 31
118,Data Synergies (Prof Peter Leonard),Q11 Actions to increase public trust in AI,"See our specific proposals in response to question 3, in the attached paper"
118,Data Synergies (Prof Peter Leonard),Q12 Trade impact of banning high-risk AI applications,No specific comments at this time.
118,Data Synergies (Prof Peter Leonard),Q13 Changes to conformity infrastructure,No specific comments at this time.
118,Data Synergies (Prof Peter Leonard),Q14 Risk-based approach to address AI risks,See pages 31 to 37 of the attached paper
118,Data Synergies (Prof Peter Leonard),Q15 Benefits and limitations of risk-based approach,"See section 1 of the attached paper, and our response to question 14"
118,Data Synergies (Prof Peter Leonard),Q16 Scope and application of risk-based approach,"See section 1 of the attached paper, and our response to question 14"
118,Data Synergies (Prof Peter Leonard),Q17 Elements to include in risk-based approach,See pages 37 and 38 of the attached paper
118,Data Synergies (Prof Peter Leonard),Q18 Integration of risk-based approach into existing frameworks,"See section 1 of the attatched, and our response to question 14."
118,Data Synergies (Prof Peter Leonard),Q19 Application of risk-based approach to general purpose AI,See pages 37 and 38 of the attached paper
118,Data Synergies (Prof Peter Leonard),Q20 Voluntary vs mandated compliance with risk-based approach,"See section 1, and page 41 of the attached paper"
119,Business Council for Sustainable Development Australia,Transcription,"FILENAME: BCSDA Submission I Responsible AI in Australia - FINAL.1b2fc6a7d3f52.pdf | CONTENT: Sustainable Business Australia Limited trading as Business Council for Sustainable Development Australia ABN 48 052 135 609 Level 27 20 Bond Street SYDNEY NSW 2000 + 61 2 8005 0780 bcsda@bcsda.org.au www.bcsd.org.auTechnology Strategy Branch Department of Industry, Science and Resources Australian Government21 July 2023By email: DigitalEconomy@industry.gov.auDear Technology Strategy Branch,Submission in response to Responsible AI in AustraliaOn behalf of the Business Council for Sustainable Development (BCSD) Australia, I am pleased to submit our response to the Australian Government's consultation on AI governance. BCSD Australia, a leading business-led sustainability and business peak body, is committed to driving sustainable development in Australia by fostering collaboration among businesses and promoting responsible practices.BCSD Australia is an affiliate of the World Business Council fo"
119,Business Council for Sustainable Development Australia,General comment,See Submission attached
119,Business Council for Sustainable Development Australia,Q01 Definitions,"We agree with the definitions provided in the discussion paper. These definitions align with commonly accepted industry standards and terminology, enabling a shared understanding of key concepts related to AI governance. Consistency in definitions is crucial for effective communication and collaboration among stakeholders, including businesses, policymakers, and the public.Practical Example: In our discussions with our member companies, we have found that these definitions resonate with their understanding of AI and its governance. For instance, when developing their AI strategies, companies often refer to similar definitions to ensure clarity and alignment across their operations. This common understanding enables them to assess the potential risks and opportunities associated with AI technologies and make informed decisions regarding their implementation.Additionally, we have observed that these definitions are consistent with international frameworks and guidelines on AI governance,"
119,Business Council for Sustainable Development Australia,Q02 Other AI risks and recommendations for regulation,"Australia's existing regulatory approaches have made significant progress in addressing AI-related risks. However, some potential risks may not be adequately covered, and additional regulatory actions can be considered to mitigate these risks. Here are a few key areas where regulatory action could be beneficial: 1. Lack of mandatory human intervention: One potential risk from AI that may not be fully covered by Australia's existing regulatory approaches is the lack of mandatory human intervention throughout the design and operation of AI systems. This is particularly significant when it comes to Automated Decision-Making (ADM) systems, where decisions are made without human involvement. The absence of human oversight in these systems can lead to a variety of issues, including biased decision-making, lack of transparency, and potential misuse of AI. For instance, an AI system might make decisions based on patterns in the data that are discriminatory or unfair, but without human oversigh"
119,Business Council for Sustainable Development Australia,Q03 Non-regulatory actions to address AI risks,"There are several non-regulatory initiatives that the Australian Government could implement to support responsible AI practices in Australia. These initiatives can complement existing regulations and encourage businesses to adopt ethical and responsible approaches to AI. Here are a few examples: 1. Diversity Programs in the AI Industry: One initiative could be the implementation of programs that encourage diversity within the AI industry. Generative AI technology has a high barrier to entry due to considerable educational and training requirements. This could lead to underrepresentation from diverse demographic groups, making it increasingly hard to remove bias and achieve fairness. By promoting diversity in the AI industry, we can ensure a wider range of perspectives and experiences are incorporated into the design and implementation of AI systems, which can help to reduce bias and improve fairness. 2. Digital Inclusion Programs: Another initiative could be programs to ensure that acc"
119,Business Council for Sustainable Development Australia,Q04 Coordinating AI governance across government,"We recognize the importance of coordinated AI governance across government agencies to foster responsible AI practices in Australia.Here are our suggestions for the coordination of AI governance: 1. Establishment of a Central AI Governance Body: The Australian Government could consider establishing a central body responsible for coordinating AI governance efforts across different government departments and agencies. This body would serve as a hub for knowledge sharing, collaboration, and policy development in the field of AI. It would provide a platform for stakeholders, including businesses, academia, and civil society, to contribute to the development of AI governance frameworks. Example: The United Kingdom's Centre for Data Ethics and Innovation (CDEI) serves as an independent advisory body to the UK government, providing guidance on ethical AI development. The CDEI's multi-stakeholder approach has facilitated coordination and engagement between government, industry, and society, in"
119,Business Council for Sustainable Development Australia,Q05 International models for AI governance,"We have identified several governance measures being taken or considered by other countries that are relevant, adaptable, and desirable for Australia in the context of AI. These examples showcase innovative approaches to AI governance: 1. Canada's Algorithmic Impact Assessment (AIA): Canada is developing an AIA framework to assess the potential impacts of AI systems on individuals and society. The framework aims to enhance transparency, accountability, and fairness in AI decision-making processes. It requires organizations to conduct impact assessments and take corrective actions to mitigate biases and discrimination. Example: Australia could draw inspiration from Canada's AIA framework to develop a similar mechanism for assessing the impacts of AI systems. By mandating AI impact assessments, businesses would be required to evaluate and address potential risks, promoting responsible and fair AI practices. 2. Finland's ""Trustworthy AI"" Approach: Finland has adopted a holistic approach t"
119,Business Council for Sustainable Development Australia,Q06 Public vs Private sector governance,"We recognize that different approaches may be warranted for the public and private sector use of AI technologies. While fundamental principles of responsible AI apply to both sectors, there are specific considerations that may necessitate differentiated approaches. Here is our thoughts on how the approaches could differ: 1. Public Sector: The public sector should prioritize transparency and accountability in AI decision-making processes. It should provide clear explanations for automated decisions, ensuring that citizens understand how AI systems are used in public service delivery. In addition to this, the public sector should consider making public datasets available for the public good of solving societal and environmental issues where appropriate (e.g. Environmental -Economic Accounting Dashboard). The availability of these datasets could enable researchers, innovators, and the broader community to develop AI solutions that address pressing societal and environmental challenges. Th"
119,Business Council for Sustainable Development Australia,Q07 Responsible government use of AI,"We believe the Australian Government can further support responsible AI practices in its own agencies through the following measures: 1. Establish Clear Ethical and Responsible AI Guidelines: The Australian Government should develop and implement clear guidelines for responsible AI practices within its agencies. These guidelines should address key principles such as transparency, fairness, accountability, and privacy. They should provide specific guidance on AI system development, deployment, and ongoing monitoring. Example: The United States Office of Management and Budget (OMB) has issued guidelines for federal agencies to ensure transparency, accountability, and public trust in AI. These guidelines provide a framework for responsible and ethical AI adoption within government agencies. 2. Promote AI Skills and Education: The government can invest in training programs and initiatives to build AI skills and literacy within its agencies. By equipping employees with the necessary knowled"
119,Business Council for Sustainable Development Australia,Q08 Generic vs technology-specific approaches for AI risk,"We recognize that both generic solutions and technology-specific solutions play crucial roles in addressing the risks associated with AI.The circumstances in which each approach is most valuable can vary. Here are some examples: 1. Generic Solutions: Ethical Frameworks and Guidelines: Generic solutions in the form of ethical frameworks and guidelines are valuable in providing high-level principles and standards for responsible AI across different sectors and applications. These frameworks outline principles such as transparency, fairness, accountability, and privacy that are applicable to various AI technologies. Example: The Institute of Electrical and Electronics Engineers (IEEE) has developed the ""Ethically Aligned Design"" initiative, which provides a set of ethical guidelines for AI development and deployment. These guidelines offer generic principles applicable to diverse AI applications, promoting responsible practices across industries. 2. Technology-Specific Solutions: Algorith"
119,Business Council for Sustainable Development Australia,Q09 Transparency requirements and scope,"We recognize the importance of transparency in the AI lifecycle to mitigate potential risks, enhance public trust, and improve confidence in AI. Here are our thoughts on the criticality and value of transparency, as well as implementing transparency requirements across the private and public sectors:a. Criticality and Value of Transparency: • During Data Collection and Processing: Transparency in data collection and processing is critical to address concerns related to privacy, consent, and potential biases. Making the data collection process transparent helps individuals understand how their data is being used and ensures compliance with privacy regulations. • Algorithmic Decision-Making: Transparency in algorithmic decision-making processes is crucial, especially when AI systems impact individuals' rights, opportunities, or access to services. It enables individuals to understand how decisions are made, assess potential biases, and seek recourse if necessary • High-Stakes Application"
119,Business Council for Sustainable Development Australia,Q10 Ban on high-risk AI applications,"We agree on the importance of carefully evaluating high-risk AI applications and technologies. While a blanket ban may not be the most effective approach, certain circumstances may warrant restrictions or regulatory measures to mitigate potential risks. Here are our suggestions: a. High-Risk AI Applications or Technologies: • Instead of advocating for complete bans, we recommend a risk-based approach to determine the level of regulation and oversight required for high-risk AI applications. Prohibitions should be considered for applications that pose severe and irreversible harm to individuals, society, or the environment, where the risks outweigh the potential benefits. Example: Autonomous weapons systems, which have the potential to cause significant harm and raise ethical concerns, could be subject to strict regulation or international agreements to prevent their development and use. b. Criteria or Requirements for Identifying Banned AI Applications: • Ethical Considerations: Applica"
119,Business Council for Sustainable Development Australia,Q11 Actions to increase public trust in AI,"We recognize that building public trust in AI deployment is essential to encourage broader adoption and utilization of AI technologies. Here are some ideas of initiatives and government actions that can contribute to increasing public trust: 1. Transparent and Ethical AI Practices: o Clear Communication: Encourage businesses and government agencies to communicate openly and transparently about their AI systems, including how they are developed, deployed, and used. This includes providing understandable explanations of AI decision-making processes and being transparent about data collection, privacy safeguards, and potential impacts. o Ethical Guidelines: Establish clear ethical guidelines or codes of conduct that encourage responsible AI practices, ensuring fairness, accountability, and privacy protection. These guidelines should be developed collaboratively with stakeholders and should address concerns specific to different industries and applications. Example: The Partnership on AI, "
119,Business Council for Sustainable Development Australia,Q12 Trade impact of banning high-risk AI applications,"We recognize that banning high-risk activities such as social scoring or facial recognition technology in certain circumstances can have both positive and negative impacts on Australia's tech sector and trade relationships with other countries. Here are our thoughts on some potential impacts to consider: 1. Impact on the Tech Sector: o Innovation and Research: Banning high-risk activities may lead to a temporary setback in specific areas of technological development. However, it can also encourage businesses to shift their focus towards more responsible and ethical alternatives, fostering innovation in areas that align with societal values and needs. o Trust and Reputation: Banning high-risk activities can help build trust and enhance the reputation of Australia's tech sector. Demonstrating a commitment to responsible practices can attract investments, partnerships, and collaborations with businesses that prioritize ethical considerations. o Market Opportunities: Proactively addressing"
119,Business Council for Sustainable Development Australia,Q13 Changes to conformity infrastructure,"We recognize the need for changes to Australian conformity infrastructure to support assurance processes and mitigate potential risks associated with AI. Here are our thoughts on some possible changes that may be required: 1. Standards and Certification: o Development of AI-specific Standards: Australian conformity infrastructure can be enhanced by developing specific standards and guidelines that address the unique risks and challenges of AI technologies. These standards can cover areas such as data governance, algorithmic transparency, fairness, and accountability. Example: The International Organization for Standardization (ISO) has developed ISO/IEC 27018, which provides guidelines for protecting personal data in the cloud computing environment. Similar standards can be developed or adapted to address AI-specific risks and requirements. o Certification Programs: Introduce certification programs to assess and verify the conformity of AI systems with established standards. Independen"
119,Business Council for Sustainable Development Australia,Q14 Risk-based approach to address AI risks,"We support a risk-based approach for addressing potential AI risks. A risk-based approach enables a nuanced and targeted assessment of AI applications, allowing resources to be allocated based on the level of risk they pose. Here's why a risk-based approach is beneficial: 1. Tailored Risk Assessment: A risk-based approach allows for a systematic evaluation of AI applications, considering their potential impacts on individuals, society, and the environment. It enables the identification and prioritization of high-risk areas, ensuring that mitigation efforts are focused where they are most needed. To be clear, this risk-based approach should be applied throughout the entire development lifecycle of the AI, as per Table 1 in the Rapid Response Information Report. This would help to capture different risks as they occur at various stages of the AI development process, from the initial design and data collection stages through to deployment and post-deployment monitoring. By integrating ris"
119,Business Council for Sustainable Development Australia,Q15 Benefits and limitations of risk-based approach,"We recognize both the benefits and limitations of a risk-based approach in addressing AI risks. Here are our observations of the main benefits and limitations, as well as strategies to overcome those limitations: Benefits of a Risk-Based Approach: 1. Targeted Mitigation Efforts: A risk-based approach allows for the prioritization of resources and efforts on high-risk AI applications, ensuring that mitigation measures are focused where they are most needed. This targeted approach maximizes the effectiveness of risk management efforts. 2. Proportional Regulation: A risk-based approach enables the application of proportionate regulatory measures, avoiding unnecessary burdens on low-risk AI applications. This promotes innovation while ensuring that appropriate safeguards are in place for higher-risk applications. 3. Flexibility and Adaptability: The risk-based approach provides flexibility to adapt to changing circumstances and evolving technologies. It allows for continuous monitoring, re"
119,Business Council for Sustainable Development Australia,Q16 Scope and application of risk-based approach,"We consider that the suitability of a risk-based approach may vary across sectors, AI applications, and organizations based on factors such as organization size, AI maturity, and available resources. Here is our assessment overview of how these factors can influence the effectiveness of a risk-based approach: 1. Organization Size: o Large Organizations: Larger organizations typically have more resources and capacity to implement comprehensive risk management practices. A risk-based approach can be well-suited for these organizations as they can allocate dedicated teams or departments to assess and manage AI risks effectively. o Small and Medium-sized Enterprises (SMEs): SMEs may have limited resources and expertise to conduct complex risk assessments. In such cases, simplified risk assessment frameworks and guidance tailored to their specific sector or AI application can help SMEs navigate AI risks effectively. Practical Example: The Australian banking sector, comprising both large ban"
119,Business Council for Sustainable Development Australia,Q17 Elements to include in risk-based approach,"We support the inclusion of the elements presented in Attachment C as part of a risk-based approach for addressing potential AI risks. These elements are essential in promoting safe and robust practices and increasing community trust and confidence in AI. Here is our assessment of each element: 1. Impact Assessments: Conducting impact assessments is crucial to identify and mitigate potential risks associated with AI systems. Peer-reviewed external assessments, particularly for high-risk applications, enhance transparency and provide independent validation of risk management efforts. 2. Notices: Informing users when automation or AI is utilized in ways that materially affect them is essential. Providing notices ensures transparency and empowers individuals to seek reviews of decisions or actions made by AI systems, fostering trust and accountability. 3. Human in the Loop/Oversight Assessments: In certain circumstances, human involvement or oversight is necessary to minimize potential ri"
119,Business Council for Sustainable Development Australia,Q18 Integration of risk-based approach into existing frameworks,We recognize the importance of incorporating an AI risk-based approach into existing assessment frameworks and risk management processes to streamline operations and reduce potential duplication. Here are some strategies that we consider could achieve this integration effectively: 1. Alignment with Privacy Assessment Frameworks: Incorporate AI-specific considerations into existing privacy assessment frameworks to ensure comprehensive evaluation of AI-related privacy risks. This integration helps streamline assessments by leveraging established privacy assessment processes while addressing the unique challenges posed by AI technologies. Example: The Office of the Australian Information Commissioner (OAIC) provides guidance on conducting Privacy Impact Assessments (PIAs). Integrating AI-specific considerations within the existing PIA framework enables organizations to evaluate privacy risks associated with AI applications systematically. 2. Synergy with Existing Risk Management Processes
119,Business Council for Sustainable Development Australia,Q19 Application of risk-based approach to general purpose AI,"We recognize the need to apply a risk-based approach to general purpose AI systems, such as large language models (LLMs) or multimodal foundation models (MFMs). Here's how a risk-based approach could be applied to these systems: 1. Risk Identification and Categorization: Identify potential risks associated with general purpose AI systems like LLMs and MFMs. This includes risks such as biased outputs, misinformation propagation, data privacy breaches, and unintended consequences arising from the scale and complexity of these models. Example: In the context of LLMs, potential risks may include the generation of biased or harmful content, spreading misinformation, or amplifying existing societal biases. Identifying these risks allows for targeted mitigation efforts. 2. Risk Assessment: Conduct risk assessments to evaluate the likelihood and potential impact of identified risks. Assessments should consider factors such as the scope of deployment, data inputs, model architecture, and potent"
119,Business Council for Sustainable Development Australia,Q20 Voluntary vs mandated compliance with risk-based approach,"We believe that a risk-based approach for responsible AI should involve a combination of voluntary and regulatory measures. Here is our perspective on the voluntary and regulatory aspects and their application to different entities: 1. Voluntary Measures: Voluntary measures can encourage proactive and responsible behaviour among organizations and foster innovation. They can take the form of industry-led frameworks, guidelines, and best practices that enable businesses to assess and mitigate AI risks based on their unique circumstances. Example: Collaborative initiatives, such as industry consortia, can develop voluntary frameworks for responsible AI, encouraging organizations to adopt risk-based approaches and share knowledge and experiences. The Partnership on AI, an organization formed by leading technology companies, is an example of such a collaborative effort. 2. Regulatory Measures: Regulatory intervention is essential to establish a baseline of responsible AI practices and ensur"
120,Cranlana Centre For Ethical Leadership,Transcription,"FILENAME: AI in Australia CCEL Submission.1416d7a786fea.docx | CONTENT: AI in Australia: Regulating Harms to Build TrustCranlana Centre for Ethical Leadership“Do you agree with the definitions in this discussion paper? If not, what definitions do you prefer and why?”DefinitionsArtificial IntelligenceWe prefer the definition of Artificial Intelligence (AI) used by the European Commission in its groundbreaking and highly regarded report. This is becauseThe purpose of defining terms is to increase clarity in a way that that the discussion paper’s definition does not.KPMG’s Trust in AI: Global Insights 2023 report found “strong global endorsement for the principles of trustworthy AI originally proposed by the European Union, with almost everyone surveyed (97 percent) viewing these principles as important for trust” ANDThe European Commission’s principles are inseparable from its definition of AI, and we see the EC’s values-driven approach to regulation as one of only two value-driven appro"
120,Cranlana Centre For Ethical Leadership,General comment,Please contact Dr Leslie Cannold with any questions or concerns about the submission. 0417114859
120,Cranlana Centre For Ethical Leadership,Q01 Definitions,"DefinitionsArtificial IntelligenceWe prefer the definition of Artificial Intelligence (AI) used by the European Commission in its groundbreaking and highly regarded report. This is becauseThe purpose of defining terms is to increase clarity in a way that that the discussion paper’s definition does not.KPMG’s Trust in AI: Global Insights 2023 report found “strong global endorsement for the principles of trustworthy AI originally proposed by the European Union, with almost everyone surveyed (97 percent) viewing these principles as important for trust” ANDThe European Commission’s principles are inseparable from its definition of AI, and we see the EC’s values-driven approach to regulation as one of only two value-driven approaches suitable for Australia’s regulatory efforts to ensure only trustworthy AI is released into our communityThe European Commission defines AI systems as:software (and possibly also hardware) systems designed by humans that, given a complex goal, act in the physica"
120,Cranlana Centre For Ethical Leadership,Q02 Other AI risks and recommendations for regulation,"Australia’s Current GovernanceThere is currently no AI-specific regulation in place in Australia. Instead there are a range of voluntary and best practice codes and standards including• Australia’s AI Ethics Principles • Work by Standards Australia and other international standards bodies • The State of AI Governance in Australia Report • The Responsible AI Network programWe would argue that polling which shows Australians’ low levels of trust in, support for, and take-up of AI technology demonstrates the ineffectiveness of the above approach to building the public’s confidence through demonstrably trustworthy AI.This is because this approach has been:a. voluntary, neither compelling adoption nor exacting consequences to support compliance. b. Lacking adequate democratic input, which palpably narrows the recognition and by-design prevention of AI now in the wild, and that coming down the pike in the future, and reduces public trust c. Overly focused on not “missing out,” and claims abo"
120,Cranlana Centre For Ethical Leadership,Q03 Non-regulatory actions to address AI risks,"CCEL sees the training of civil society, business and government leaders, to develop the judgement and courage required to give voice to their values, and to design their organisations in ways that support and encourage their staff to do the same. As the World Economic Forum’s Ethics by Design report observes:“While certain foundational ethical risks can be mitigated through the establishment of clear operational rules, many others require the capacity for ethical judgement and organisational factors that support translating that judgement into action. As technology is increasingly incorporated into the daily operations of companies…leaders must prepare their people to be aware of the ethical risks…[and to] equip them to make ethical choices…”As recent ethical scandals in the Banking and Finance industry, at Price Waterhouse Coopers and at the top of government departments charged with delivering vital services to the most vulnerable Australians show, senior leaders and established ins"
120,Cranlana Centre For Ethical Leadership,Q04 Coordinating AI governance across government,No.
120,Cranlana Centre For Ethical Leadership,Q05 International models for AI governance,We have answered this in the previous question.
120,Cranlana Centre For Ethical Leadership,Q14 Risk-based approach to address AI risks,We have answered this question in previous parts of this submission.
121,Anonymous,Transcription,"FILENAME: Submission 121 - Attachment.0a5a8b2d2c3b8.pdf | CONTENT: Summary Thank you for reviewing this extremely important topic and for requesting public feedback.AI has already2, and will in future, deliver enormous benefits to society and to Australia. But, as leaders of the principal AI firms have stated, regulation is badly needed. As much as AI can be used for good, it can potentially be used for ill. Recent developments in technology (primarily social media) have shown that technology will be used for ill by certain individuals, organisations and states, if they can profit from it.Designing regulation - in this area - that prevents harm without preventing useful innovation is very challenging. Many are saying that regulation is needed. Few are specifying what is needed.Background From where I stand, something went wrong with information technology in the middle of the last decade. Around 2016 it felt like the world suddenly shifted on its axis. Unexpected things started to happ"
121,Anonymous,Q01 Definitions,Yes (and it’s great to see ADM included in this discussion)
121,Anonymous,Q02 Other AI risks and recommendations for regulation,"Problems already seen with social media are likely to be amplified by people using AI technology. Regulatory action is needed to cover not just “AI” but also social media and other algorithmic technology (e.g. Robodebt).Even before recent AI developments, there has been a decline in the mental health of young people as a result of social media: Honestly, it's probably the phones - by Noah Smith (noahpinion.blog)"
121,Anonymous,Q03 Non-regulatory actions to address AI risks,"Apart from public education, regulatory initiatives (imposing formal legal obligations) are what is needed."
121,Anonymous,Q04 Coordinating AI governance across government,"The challenge of AI governance is wide and deep enough to merit a dedicated agency. Tasks of this agency should include • monitoring and refining regulation, nationally and globally • security – protecting sensitive government information and the sensitive personal information of citizens ( supporting the existing agencies with respect to AI and ADM ) • education – in media, schools, universities. Promoting best practice. • coordination – within the federal and state governments and their agencies, and liaison with businesses. Coordination with other nations and with supra-national organisations ( e.g. the EU and OECD ). • monitoring applications (use) of AI, plus enforcement when regulations are breached."
121,Anonymous,Q05 International models for AI governance,"The EU regulations – not just the AI Act but also the GDPR and the Digital Services Act should be considered and would probably be a good starting point. Doing this may eliminate considerable re-work. ( As you are probably already thinking, Europe’s risk-based approach seems sensible. )"
121,Anonymous,Q06 Public vs Private sector governance,No. The same rules and approaches should apply to both public and private sectors (with the exception of national defence – as I’ve detailed elsewhere).
121,Anonymous,Q07 Responsible government use of AI,"In two ways. By having a dedicated team in each agency. And by having a government agency focusing on AI. ( As well as covering broad AI the new agency should cover ADM, existing narrow AI applications and any other significant algorithmic activity including the regulation of social media. )The significance of AI to national life will be equal to, say, transport, which is looked after by multiple federal government agencies."
121,Anonymous,Q08 Generic vs technology-specific approaches for AI risk,"A mixture will be required. At first instance generic solutions will be important. Later technology-specific solutions will be needed, e.g. to ensure the safety of autonomous vehicles."
121,Anonymous,Q09 Transparency requirements and scope,"Transparency is needed in four places 1. to indicate whenever a decision is made by AI ( or other ADM ) not a human 2. to explain how and why a decision was made 3. to indicate the precision of an answer ( how likely it is to be correct? ) 4. the scope of the data used ( e.g. how up to date is the dataset? )Mechanisms will need to be provided to allow feedback and appeals ( to a human, not AI/ADM ) when an AI decision seems incorrect.The higher the risk of the application, the more important transparency is. For example, in legal decisions, transparency is very important."
121,Anonymous,Q10 Ban on high-risk AI applications,I agree with your suggestions here: unacceptable risk applications and technologies should be banned – e.g. social scoring or facial recognition technology in certain circumstances plus autonomous drones.
121,Anonymous,Q11 Actions to increase public trust in AI,"Good comprehensive regulatory ( imposing formal legal obligations ) governance is the best approach. Any misuse, e.g. Robodebt in the case of ADM, will reduce confidence in, and trust of, AI.Almost every day I am receiving SMS messages which seek to mislead me – phishing attacks. These pretend to be from reputable organisations, e.g. Australia Post or Qantas. So far bad spelling or bad grammar helps identify these fake messages. However, because AI-generated texts already produce perfect English, it will become much harder to identity fake messages and emails. Incidentally I’ve stopped answering calls on my landline (telephone) as the vast majority of calls are now from marketers (and perhaps scammers) using technology to place unwanted “cold calls” .We are on the brink of serious misrepresention by bad actors using AI, so rapid action is required. Soon, if nothing changes, people will be being defrauded by way of voice and video calls in which the voice and vision of a person will be "
121,Anonymous,Q12 Trade impact of banning high-risk AI applications,Banning high-risk AI would not have an adverse impact. On the contrary it would boost Australia’s reputation (because it is right and ethical). It would be a net (nett) positive. It would help focus Australia’s efforts on developing “AI for good”. It might even help influence other countries to do the same good work.
121,Anonymous,Q13 Changes to conformity infrastructure,"Sorry, no idea. (I’m not familiar with “Australian conformity infrastructure” . )"
121,Anonymous,Q14 Risk-based approach to address AI risks,Yes. I can’t think of a better way to treat high-risk AI differently from low-risk AI.
121,Anonymous,Q15 Benefits and limitations of risk-based approach,"It is a simple way to be (a) stricter where regulations are needed ( e.g. medical AI ) and (b) more relaxed where fewer regulations are needed ( e.g. streaming service recommendation engines ).The limitation of a risk-based approach is possible mis-categorisation, possibly at the behest of lobby groups. ( An example here is the online gambling industry. ) An independent AI (and ADM) governance organisation (with quality independent leadership) should mitigate this issue."
121,Anonymous,Q16 Scope and application of risk-based approach,No. It should apply universally.
121,Anonymous,Q17 Elements to include in risk-based approach,"Yes.“Notice” is important for higher risk applications. It is very important that when AI is used (in higher risk applications) that the user is aware.“Human in the loop” is important for the higher risk applications. Some of the worst AI stories (reports) to date are from the United States where AI has been used for legal decision making and there has been no “human in the loop” mechanism. Closer to home, Robodebt has led to some victims committing suicide ( and virtually no sanctions on the humans responsible for this ADM ) .“Explanation” is important for the higher risk applications. It would even be useful for certain lower risk applications. E.g. why is my satnav directing me to route B instead of route A ?“Training” is important. Otherwise how would an inexperienced use know about “AI hallucinations” and be able to respond to them?“Monitoring” is very important. There are two aspects to this. The behaviour of an AI model can change over time, sometimes very rapidly. (See the case"
121,Anonymous,Q18 Integration of risk-based approach into existing frameworks,"I’d rather see duplication than missing regulation. Duplication can be eradicated over time. First we need comprehensive regulation of AI ( and ADM and, ideally, social media )."
121,Anonymous,Q19 Application of risk-based approach to general purpose AI,Regulations in this area (AI and ADM) should be technology-agnostic. The risk-based approach should be applied as for other forms of AI (or ADM). Risk-based regulation should cover all forms of AI and ADM – it should cover narrow AI and general (including generative) AI and any potential future flavours of AI and ADM.
121,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,It should be mandated through regulation. It should apply to both public and private organisations. It should apply to both developers and deployers.
122,Letian Wang,Q01 Definitions,"The definitions in Figure 1 are not sufficiently granular to support risk-based consideration of different kinds of AI - including the more sophisticated and powerful AIs.In their current state, these definitions do not support a nuanced policy conversation that allows differentiation between AI models that provide economic benefit, models that raise significant ethical concerns, and models that raise radical safety concerns."
122,Letian Wang,Q02 Other AI risks and recommendations for regulation,"The paper overlooks the severe dangers of highly advanced AI systems, a risk recognized by experts, public figures, and organizations such as the Future of Life Institute and the Centre for AI Safety. The UN Secretary-General has also acknowledged these risks. The Australian government must address the potential catastrophic consequences of advanced AI systems, as remaining silent on an issue actively engaged by experts and other nations is unacceptable. There are three streams of work that the Government can pursue to help tackle the potential for catastrophic risks from future AI technology. First, Australia must publicly recognise these risks, joining other countries and global organizations in acknowledging the potential dangers of increasingly advanced AI systems. 1. The government should create global governance structures aimed at tackling potential catastrophic risks from AI. This may involve: - Promoting fairness, accountability, and transparency among companies and countries "
122,Letian Wang,Q03 Non-regulatory actions to address AI risks,"Governments must stay informed on AI advancements to maintain public trust and effectively regulate the technology. There is a significant knowledge gap in government, which must be addressed to harness AI's economic benefits and tackle ethical concerns. A TechCongress-like scheme should be established to bring AI expertise into government departments such as Industry, Science, Resources, Home Affairs, and Australian Communications and Media Authority. Lacking AI safety and ethics expertise, governments cannot effectively understand, lead, or regulate AI use in society and business. Research in the UK reveals deficits in AI knowledge and organizational capacity, indicating similar issues likely exist in Australia. Partnering with Australian universities to develop AI safety and ethics training programs can help identify and recruit experts. The following steps can be taken to improve this: 1. Funding: Increase financial support for AI research and development in Australian universities"
122,Letian Wang,Q05 International models for AI governance,"Singapore provides valuable governance measures that Australia could benefit from, such as a comprehensive framework that promotes shared responsibility, disclosure standards, data privacy guidelines, and standardized metrics for AI development, testing, and monitoring. Key areas include accountability, data protection, IP guidelines, model evaluation, deployment, and safety research. Singapore also established the AI Verify Foundation, a non-profit organization promoting AI best practices and providing an open-source toolkit. Sector-specific governance toolkits can be developed based on AI Verify, and Australia could adopt and adapt these strategies. Similarly, the Tony Blair Institute for Global Change suggests the UK create ""Sentinel,"" a national laboratory for researching and testing safe AI, serving as the national regulator's ""brain."" Australia should consider adopting this proposal's useful parameters.[1] “Generative AI: Implications for Trust and Governance”. Infocomm Media Dev"
122,Letian Wang,Q06 Public vs Private sector governance,"The recent emphasis on transparency has centered on informing consumers when an AI decision directly impacts them, such as in hiring/firing or loan approvals. While helpful, this is only the initial step in safeguarding the public. In situations where AI-based decision-making is prevalent, simply knowing ""when"" an AI system makes a decision affecting them becomes less valuable for consumers. Instead, they should be informed ""how"" the decision was made, allowing them to identify unsafe or unfair systems more easily. Balancing accuracy and explainability in models has long challenged businesses [1], but the rise of ""black-box models"" has led developers and providers to accept the risks stemming from a lack of explainability. As AI's capabilities continue to surpass their explainability [2], the risks associated with this approach will significantly increase. The growing difficulty of explainability should be a reason for mandating it, not overlooking it. Even in seemingly ""benign"" scenar"
122,Letian Wang,Q10 Ban on high-risk AI applications,"AI may not currently pose a catastrophic or existential risk, but it could in the near future. We can’t really say any specific application that exists today should be banned as we don’t know which of them could become dangerous in due time. A more reasonable approach: regulators should identify ""advanced indicators"" of potential risk scenarios, communicating that a ban is appropriate if these indicators arise. Developing these indicators requires consultation with AI safety researchers, likely involving factors such as training run size, AI system capabilities, and risk levels. This approach is a sensible compromise. AI safety skeptics argue there's no concern since AI will never possess human-like intelligence, agency, or deceptive abilities. Thus, they shouldn't object to indicators they believe are impossible. On the other hand, those who believe humanity faces grave risks from AI might be persuaded to trust near-term AI developments if these indicators and a 'red line' are establi"
122,Letian Wang,Q14 Risk-based approach to address AI risks,"I advocate for a risk-based approach, but my main concerns stem from the development of large, capable AI systems rather than their deployment. A risk-based approach should be applied before training such systems, analyzing the risks based on the proposed training run's nature and scale, instead of only assessing risky use cases and deployment issues after the AI has been created and trained. Risk assessments should consider factors like potential irreversibility, scope, and severity, and act proportionately. The risk-based approach in Attachment C of the Discussion Paper is difficult to understand. It proposes ""human in the loop"" as the main mitigation strategy, but its effectiveness is unclear. Moreover, it acknowledges that this may not be feasible for AI applications relying on efficiency, scale, and speed – factors directly linked to risk and value in future AI systems. Thus, this risk mitigation approach is unsuitable. Even the paper's footnote supporting the ""human in the loop"" "
122,Letian Wang,Q19 Application of risk-based approach to general purpose AI,"The risk-based approach should be adapted to address risks from general-purpose AI systems, particularly LLMs and MFMs, as they learn capabilities and behaviors during training that may be unpredictable. AI developers could unintentionally create models with hazardous capabilities (e.g., deception, manipulation, political strategy, weapons design) that could be misused by humans or applied inadvertently by misaligned models. In line with recent work by AI safety researchers from organizations like Google DeepMind, OpenAI, Anthropic, Alignment Research Center, University of Cambridge, University of Oxford, Centre for Long-Term Resilience, and Centre for the Governance of AI, we recommend incorporating evaluations for dangerous capabilities and misalignment into any risk-based approach for general-purpose models. These evaluations can help assess: 1. The extent to which a model could cause extreme harm (evaluating specific dangerous capabilities). 2. The likelihood of a model causing ext"
122,Letian Wang,Q20 Voluntary vs mandated compliance with risk-based approach,"AI-related harms could arise from both private and public entities, necessitating appropriate safety and risk management measures for each. There is no general reason to differentiate between their regulatory statuses, although specific instances may warrant exceptions, such as certain research types. Regulations should encompass both AI developers and deployers, adhering to the principle that those most capable of mitigating risks should bear the associated regulatory responsibility. When developers provide a ""black box"" product that deployers cannot significantly modify, the majority of the obligation should fall on developers. Here are some reasons why voluntary self regulation won’t work: 1. Inadequate incentives: Companies may prioritize short-term profits over long-term ethical concerns, leading to a focus on rapid innovation at the expense of AI safety and ethical considerations. Self-regulation might not provide sufficient incentives for companies to invest in AI safety researc"
123,Sandstone McNamara,Q01 Definitions,"To engage in a comprehensive discussion about AI, it's crucial to grasp terms like agentic and corrigible AI. Relying solely on ISO/IEC 22989 definitions is inadequate for addressing policy, law, and governance concerns since it fails to explore critical questions.The foreword of ISO/IEC 22989 acknowledges the limitation of its definitions for practical purposes, particularly in comparing AIs concerning trustworthiness, robustness, resilience, reliability, accuracy, safety, security, and privacy.To delve into these essential concepts and tackle ethical and safety issues, it is advisable to adopt ""The Language of Trustworthy AI,"" a glossary managed by the National Institute of Standards and Technology (NIST). This resource could serve as a more valuable starting point, considering the recognized weaknesses of ISO/IEC 22989.However, given that many of these concepts are continually evolving through research, it is imperative to maintain a dynamic and research-informed approach. Definitio"
123,Sandstone McNamara,Q02 Other AI risks and recommendations for regulation,"The paper fails to address the catastrophic risks associated with highly capable AI systems, despite these risks being acknowledged by experts and prominent figures, including open letters organized by the Future of Life Institute [1] and the Centre for AI Safety [2].Even the Secretary-General of the United Nations has recognized the catastrophic and existential risks posed by highly capable AI systems [3].It is imperative for the Australian government to acknowledge and address the catastrophic and existential risks related to highly capable AI systems. Remaining silent on an issue that experts and other nations actively engage with is not an acceptable approach.References:[1] https://futureoflife.org/open-letter/pause-giant-ai-experiments/[2] https://www.safe.ai/statement-on-ai-risk#open-letter[3] https://press.un.org/en/2023/sgsm21832.doc.htmSuggestions: To address the potential catastrophic risks posed by future AI technology, the Australian government should pursue three key strea"
123,Sandstone McNamara,Q03 Non-regulatory actions to address AI risks,"Australia can play a significant non-regulatory role by increasing funding for AI safety research in its universities. Prioritizing ambitious research focusing on transformative approaches like mechanistic interpretability and effective traceability would establish Australia as a global leader in AI safety. This emphasis on research would support safety-focused global regulations and give Australia a crucial role in shaping AI development worldwide, securing its national interest. Additionally, Australia can take international leadership by advocating for AI systems' understandability and safety in the global AI governance conversation, promoting standards and tools for analyzing and monitoring AI systems, and implementing audit trail requirements for safety-critical AI applications.References:Whittlestone & Clark (2021). Why and how governments should monitor AI development. https://arxiv.org/abs/2108.12427Brundage et al (2020). Toward Trustworthy AI Development: Mechanisms for Suppor"
123,Sandstone McNamara,Q05 International models for AI governance,"A significant non-regulatory function that Australia can perform is substantially increasing funding for AI safety research in its universities. By prioritizing ambitious research that focuses on transformative approaches, such as mechanistic interpretability and effective traceability, rather than incremental or near-to-market innovations, Australia can establish itself as a global leader in key aspects of AI safety. This emphasis on research would underpin safety-focused global regulations and give Australia a crucial role in shaping AI development worldwide. Such leadership is essential for securing Australia's relevance and protecting its national interest in the rapidly evolving field of AI. A long-term investment in research is likely to be the most effective strategy to achieve this goal.Additionally, Australia can take up international leadership by advocating for AI systems' understandability and safety in the global conversation surrounding AI governance and standards. This l"
123,Sandstone McNamara,Q09 Transparency requirements and scope,"Given the importance of transparency across the AI lifecycle, ensuring transparency becomes most critical and valuable in mitigating potential AI risks and enhancing public trust and confidence in AI when AI is commonly used for decision-making. Simply notifying consumers when an AI system makes decisions affecting them is insufficient, especially when such decisions happen frequently throughout the day. Instead, consumers should be informed of ""how"" the AI makes decisions, allowing them to identify unsafe or unfair systems more easily. Enforced transparency over decision-making helps avert Goodhart's law, where metrics for success fail to measure their intended outcomes. This is particularly important in advanced AI systems, including those using reinforcement learning, where uninterpretable models can lead to unintended consequences and risks.Mandating transparency requirements should apply across both the private and public sectors. It is essential to regulate AI technologies and th"
123,Sandstone McNamara,Q10 Ban on high-risk AI applications,"The ongoing development of AI presents several societal challenges that could potentially jeopardize the security, privacy, and freedom of the Australian public. Advanced AI systems capable of diverse reasoning tasks, autonomous navigation in complex information environments, and sophisticated planning raise concerns regarding their potential risks and misuses. Although these capabilities might currently exist in controlled environments, their deployment in real-world scenarios requires careful analysis and regulation.For instance, the system called ""Voyager,"" which utilizes a chain of GPT versions connected to an external database, demonstrated significant success in open-world games like Minecraft. Voyager showcased human-like abilities, such as curiosity and ingenuity, by devising its own plans and tactics to achieve goals effectively. However, such capabilities can also introduce human-like risks, including deception and self-preservation.Moreover, the ability to duplicate and dire"
123,Sandstone McNamara,Q11 Actions to increase public trust in AI,"The growing public distrust in AI is fueled by concerns over powerful AI systems being developed and deployed by foreign actors, some of whom have demonstrated unsafe attitudes towards AI deployment [1]. In this context, Australia has a unique opportunity to establish itself as a leader in AI safety, contributing to global efforts and improving public trust in AI technologies.While catching up to countries like the US or UK in advancing the AI frontier may be challenging, it is not a prerequisite for gaining public trust and increasing AI adoption. Instead, Australia can focus on becoming a hub for AI safety expertise, which would not only promote safe AI adoption locally but also open up export opportunities in the emerging market for AI safety services [2]. This positioning would grant Australia a significant presence in the global AI conversation, ensuring its interests are considered.To achieve this leadership position in AI safety, Australia should:Collaborate with local AI safety"
123,Sandstone McNamara,Q14 Risk-based approach to address AI risks,"The regulation of AI should adopt a risk-based approach, similar to how vehicles are regulated based on their risk levels. Just as push-bikes and passenger jets are subject to different regulations, AI technologies with varying risk profiles should also be treated differently.A limitation of the risk management approach proposed in the discussion paper, particularly Box 4 and Attachment C, is that it focuses solely on how AI is used and overlooks the nature of the AI itself.An effective regulatory regime for high-risk activities should encompass both the technology itself and its intended use. The Australian Human Rights Commission [1] highlights that high-risk technologies and activities, such as gene technology, aviation, healthcare, and the energy industry, are subject to stricter regulations covering both the technology and its use. Regulation becomes more prescriptive when the specific technology poses higher risks to humans.Continuing the analogy, car manufacturers must meet tech"
123,Sandstone McNamara,Q15 Benefits and limitations of risk-based approach,"While a risk-based approach to regulating AI is sensible, there are two emerging elements of risk practice that any risk-based model should account for to succeed in this context:1. Factoring in “uncertainty”, not just “likelihood”2. Taking a “portfolio” approach to risk mitigationA traditional risk matrix will consider scenarios against their likelihood of occurring, the vulnerability of the system to the scenario, and the consequence of the scenario occurring. It might then discount that risk by the strength of existing or possible mitigations.This approach to risk has historically been unsuccessful for scenarios where assessing “likelihood” is challenging. For instance, many national risk assessments failed to take into account the possibility of a global pandemic before COVID-19. A solution to this failure mode is including “uncertainty”. Specifically, where we can't be sure what the future will bring, that should be cause for significant concern. This is the opposite of the approa"
123,Sandstone McNamara,Q16 Scope and application of risk-based approach,"The concept of ""AI"" encompasses a wide array of capabilities and capacities, making it crucial for the government to differentiate between various use cases, particularly considering AI maturity.However, there's a concern that industries might exploit this diverse range of AI capabilities to mislead the government. They might argue against AI regulation by focusing on basic AI technology, intending this principle to extend to more advanced and future technologies. This deceptive approach can be likened to claiming that a horse and wagon do not require strict regulation and then asserting that a tank is merely a more capable horse and wagon with some components replaced.To avoid falling for such tricks, the government needs a precise and granular approach to defining AI technology and understanding AI-related risks.Research indicates that deception and power-seeking behavior are two significant sources of AI-related risks:1. Deceptive behavior becomes increasingly challenging to prevent"
123,Sandstone McNamara,Q19 Application of risk-based approach to general purpose AI,"A risk-based approach to general-purpose AI systems, such as Language Model Models (LLMs) and Multimodal Few-Shot Models (MFMs), requires careful evaluation to address potential risks effectively. These systems possess learned capabilities and behaviors, which developers might not foresee, leading to the inadvertent creation of models with dangerous functionalities like deception, manipulation, or weapons design. Such capabilities could be misused by humans to cause harm, and misalignment issues might also result in unintended harmful actions.To mitigate these risks, a risk-based approach should involve evaluations for dangerous capabilities and misalignment in general-purpose models. This evaluation process would determine the level of risk associated with the development and deployment of these models. Firstly, these evaluations would inform responsible training practices, including pausing or adjusting training methods, to ensure safer and controlled model development.Secondly, the "
123,Sandstone McNamara,Q20 Voluntary vs mandated compliance with risk-based approach,"The risk-based approach for responsible AI should not rely solely on voluntary or self-regulation. While this approach may be suitable for low-risk or no-risk AI uses and participants like students or researchers, higher-risk technologies necessitate mandated regulation. A forward-looking regulatory regime should apply broadly and cover both the technology itself and its use. This means ensuring the safety of AI before creation or publication and enforcing safe usage practices.In applying this approach, the regulation should be proportionate to the risk involved. Self-regulation may be appropriate for some current technologies and participants, but for future and more sophisticated AIs, a robust regulatory framework must be in place. This regulatory effort should be coordinated globally, backed by Australian diplomatic efforts, considering AI's transformative impact on the economy and society.Furthermore, the risk-based approach for responsible AI should encompass both public and priva"
124,Manisha Lishman,Q01 Definitions,"A sophisticated discussion of AI requires an understanding of additional terms like how agentic or corrigible a given AI is. Adopting ISO/IEC 22989 [1] definitions is the wrong approach to framing a conversation about policy, law and governance because it doesn’t open the door to the big questions we need to grapple with.The foreword itself to ISO/IEC 22989 calls out a weakness of the definition set for purposes like those being pursued in this paper. Specifically, the standard says it is currently lacking language for comparing AIs in terms of properties like trustworthiness, robustness, resilience, reliability, accuracy, safety, security and privacy.These are the concepts we need to grapple with and address the ethical and safety questions we are facing and design an effective regulatory regime.The National Institute of Standards and Technology (NIST) is managing a glossary of terms called “The Language of Trustworthy AI” which would be a much more valuable starting point than the pr"
124,Manisha Lishman,Q02 Other AI risks and recommendations for regulation,"Experts in global catastrophic risks were actively warning about the dangers of pandemics in the decade preceding COVID-19.In 2017, Oxford researcher Sebastian Farquar said: “As the Ebola and Zika crises showed, managing pandemics is a global responsibility, but too much planning is still national, and little attention is paid to worst-case scenarios.” [1]If Governments had listened to such experts prior to COVID-19, low-cost interventions, like better regulation of exotic animals markets, could have prevented millions of deaths and trillions in economic damages.The situation that Dr. Farquar describes is eerily similar to the approach to AI Governance in Australia; focused on lower-impact, local issues to the detriment of higher-impact global challenges.This is in contrast to concerns about existential risks being recognised at the highest echelons of global governance, including the UN Secretary-General’s recent statement [2]:""Alarm bells over [AI] are deafening, and they are loudest"
124,Manisha Lishman,Q03 Non-regulatory actions to address AI risks,"Government must maintain up-to-date knowledge about rapidly evolving AI systems. Public trust hinges on the belief that the government is well-informed and can effectively oversee and regulate AI developments. Unfortunately, indicators such as Australia's chief scientist's reluctance to project more than two years into the future suggest an alarming gap in understanding and skills at all governmental levels. Addressing this issue is crucial for harnessing AI's economic benefits, navigating ethical concerns, and addressing existential risks.To bring more technical AI expertise into Government, Government should set up a TechCongress-equivalent scheme aimed at enabling the Australian Government to recruit and gain access to AI expertise in fields like AI governance and ethics. The scheme should place experts in Government and also embed them within the public service, especially in Department of Industry, Science and Resources, Home Affairs, and the Australian Communications and Media Au"
124,Manisha Lishman,Q04 Coordinating AI governance across government,"While the most advanced AIs will likely not be created in Australia, they will be used in Australia, by Australian companies and by everyday Australians. Users will rightly expect and assume that robust safety-focused assurance effort has occurred. Government has an obligation to ensure that expectation is delivered.Consider air travel in Australia: While Australia does not build the largest aeroplanes, we monitor the safety of these craft by setting standards, monitoring compliance, and can revoke the licence of airlines that don’t comply with our regulations. Because of this government effort, Australians feel safe when they fly.Similarly, an Australian government body overseeing the use of applications based on large AIs could issue or revoke licences, set minimum safety standards, and require companies to publish independent evaluations of their application against those standards. This would provide everyday Australians with independent information that they can use to make inform"
124,Manisha Lishman,Q05 International models for AI governance,"Australia should take note of the EU recent handling of the AI Act. For example, the European Parliament’s recognition of general-purpose AI (GPAI) is an important step in the right direction [1]. In particular, the following observations are highly valuable:- Acknowledgement that GPAI providers must comply with strict safety measures, regardless of the way it is distributed or intended to be used.- Requirements for GPAI providers to comply with various safety requirements throughout the lifecycle of their product; i.e. external audits assessing their performance, predictability, interpretability, corrigibility, safety and cybersecurity.These adjustments appear to respond to significant criticisms directed at initial drafts of the AI Act. For example, the AI Now Institute released a set of recommendations that heavily criticised the EU’s approach on some matters [2]. For example:- “GPAI models carry inherent risks and have caused demonstrated and wide-ranging harms. While these risks c"
124,Manisha Lishman,Q09 Transparency requirements and scope,"To remain competitive, professionals and businesses will increasingly feel the pressure to leverage sophisticated “black-box” AI models. This short-term thinking has already resulted in numerous accounts of public backlash and distrust, including high-profile warnings from technology leaders themselves [1].For example, Apple’s credit card business came under fire for offering a woman one twentieth the credit limit offered to her husband. When she complained, an Apple representative simply told her, “I don’t know why, but I swear we’re not discriminating. It’s just the algorithm.” [2]More recently, two lawyers in the United States blamed ChatGPT for “tricking” them into including fictitious legal research in a court filing [3].These problems could easily have been prevented by using explainable AI systems which can show the rationale for their outputs.Such incidents are evidence that the regulatory environment is not incentivising businesses or professionals to use transparent decision-"
124,Manisha Lishman,Q10 Ban on high-risk AI applications,"While it is unlikely that AI today represents a catastrophic or existential risk to humanity, there’s a realistic possibility that it will in the near future [1, 2].To strike a balance between all perspectives, regulators should work to identify “advanced indicators” of catastrophic risk scenarios, and clearly communicate that a ban is an appropriate action if these advanced indicators come to pass.Establishing appropriate advanced indicators will have to be done in consultation with AI safety researchers, for example those from the Centre for AI Safety, or the Centre for AI Governance. However, it’s likely they could include factors such as the size of training runs (measured in terms of compute and data), as well as safety-relevant factors about the AI system, such as the ability to complete a diverse range of reasoning tasks with human-level performance; to navigate complex information environments and act within those environments autonomously, and to form sophisticated plans and r"
124,Manisha Lishman,Q11 Actions to increase public trust in AI,"There are tremendous concerns about AI among those with the greatest familiarity of its risks [1]; these concerns are being widely broadcasted in Australian media with alarming headlines such as:- SMH: “When the “Godfather of AI” warns you about his offspring, you listen.” [2]- Forbes Australia: “‘We knew AI could destroy the whole world and we wanted to figure out how to prevent that’: ChatGPT Founder;” [3]- Canberra Times: “UN chief backs idea of global AI watchdog” [4]These concerning signals are difficult for the public to ignore.It goes without saying that this is quite a different situation from one where concerns are being raised in isolation or by fringe-figures; this makes it inappropriate to treat “public education” as a significant part of the solution, as is referenced in the discussion paper.The process for building trust in AI should be thought of as similar to fostering public trust in bridges or aeroplanes. Due to their risks, advanced AI systems should only be deployed"
124,Manisha Lishman,Q14 Risk-based approach to address AI risks,"A risk based approach has worked well as a proactive means of identifying, assessing and mitigating potential safety hazards in the aviation industry.The industry has developed specific tools to assist in identifying potential risks - Safety Management Systems (SMS). Critical to this is the ability for all parties involved in the development and use of AI systems to be able to report the identification of any risks.Following identification, risks are assessed based on severity and likelihood. This work is led or assisted by technical bodies like the Australian Transport Safety Bureau. In the AI context, other countries have created or proposed similar national “labs” that would be well placed to lead this work.The industry participant best placed to mitigate the risk is then required to take steps to reduce the likelihood or impact. Given the nature of the technology this stage should be open to update as new techniques and tools become available based on the most up to date research.C"
124,Manisha Lishman,Q15 Benefits and limitations of risk-based approach,"While a risk-based approach to regulating AI is sensible, there are two emerging elements of risk practice that any risk-based model should account for to succeed in this context:1. Factoring in “uncertainty”, not just “likelihood”2. Taking a “portfolio” approach to risk mitigationA traditional risk matrix will consider scenarios against their likelihood of occurring, the vulnerability of the system to the scenario, and the consequence of the scenario occurring. It might then discount that risk by the strength of existing or possible mitigations.This approach to risk has historically been unsuccessful for scenarios where assessing “likelihood” is challenging. For instance, many national risk assessments failed to take into account the possibility of a global pandemic before COVID-19. A solution to this failure mode is including “uncertainty”. Specifically, where we can't be sure what the future will bring, that should be cause for significant concern. This is the opposite of the approa"
124,Manisha Lishman,Q16 Scope and application of risk-based approach,"The most important organisations to focus on when developing risk-management strategies and regulations, will be those that determine how advanced AI is developed and deployed. More specifically, there are three prominent categories of concern:1. Organisations that develop advanced AI systems, or AI systems that can be used to build more advanced AI systems (i.e. precursor systems such as LLMs). Examples include OpenAI, DeepMind, Anthropic, Inflection AI, Nvidia, Microsoft, Meta2. Organisations which provide access to open-source AI systems via downloads or APIs. E.g. Hugging Face, Github, AWS3. Organisations which provide compute resources to develop, fine-tune or use AI systems. E.g. AWS, HuggingFace, Google, Azure, Nvidia, Lambda LabsA majority of the benefits and risks will come from AI that is more sophisticated and generally capable, which can be tied back to organisations in these three categories.There are approaches which can be taken for each category in order to manage risks"
124,Manisha Lishman,Q17 Elements to include in risk-based approach,"Human-in-the-loop monitoring will be vital, but insufficient on its own to regulate advanced AI systems that will eventually surpass human capabilities. The speed at which advanced AI systems will operate means that human oversight will become pragmatically infeasible if our monitoring programs aren’t able to scale proportionally.As such, we must employ an approach that is predicated on limiting access to advanced AI systems unless there is a strong rationale for using them; conducting cost-benefit analyses prior to approving the deployment of advanced systems will mitigate risks while preserving potential benefits.A key part of enforcing this targeted approach will be monitoring and controlling access to high-performance computing resources, which are required for developing and operating advanced AI systems and their precursors.This additional layer of governance allows authorities to detect potentially dangerous AI development or usage activity, and can be cross-checked with other d"
124,Manisha Lishman,Q18 Integration of risk-based approach into existing frameworks,"Some areas of regulation, like the Racial, Sex, Age, and Disability Discrimination Acts are appropriately strong and may provide useful safeguards against some harms of AI. Specifically, the law should ensure that such discrimination is always wrong and always punishable, and an excuse that the discrimination occurred because of a “black box” AI provided by an overseas AI Lab should never be accepted. It would be helpful to communicate these requirements to Australian businesses that might deploy AIs and ensure they only contract with AI Labs for products where the Lab can ensure the product cannot engage in these harmful behaviours.Certainly, no AI-specific regulation should ever waive existing safeguards.That said, many of the emerging risks from AI are quite new and it’s reasonable to expect that regulation made without AI in mind is not tuned to the new risks that AI represents. Specific AI safety regulation is unavoidable."
124,Manisha Lishman,Q19 Application of risk-based approach to general purpose AI,"LLMs and MFMs are constantly demonstrating unprecedented general-purpose capabilities, and their emergence indicates that regulatory approaches which focus on specific use-cases or industries are becoming outdated. A modern approach requires that we focus on overall system capabilities rather than individual scenarios.Systems with advanced capabilities should face progressive levels of scrutiny. Low-risk systems can be monitored with light touch regulation. However, as capabilities increase, so should regulatory requirements.Precursor systems, with capabilities that can be used to build much more advanced systems, will need stricter controls to avoid proliferation of advanced systems amongst malicious actors [1]. Users who require “developer” access should require permits granting such access, and providers must guarantee responsible use via monitoring programs [2].For advanced systems with human-level capabilities, comprehensive scrutiny is essential. Prior to deploying systems for us"
124,Manisha Lishman,Q20 Voluntary vs mandated compliance with risk-based approach,"AI is very likely to transform many elements of the economy and society in coming years. Ethics and safety are unlikely to be a priority during this time unless they are actively made a strategic focus by clear leadership. Historic examples include how social media algorithms came to shape our lives, and the negative impacts to mental health and social cohesion only became apparent much later and still have not been adequately addressed. Similarly, while the destructive potential of nuclear was apparent early on, the “arms race” mentality, as well as potential economic benefits, swept humanity along towards the current risky situation that no one in the early 1940s could have conceived of. In that way, history teaches us that it’s critical that ethics and safety take centre stage from day one.The economic and technological forces are likely to be so strong that a scheme that is voluntary or driven by aspirational best practices is unlikely to be successful. Further, smaller businesses "
125,Anonymous,Q01 Definitions,"In my view, the effective discussion of artificial intelligence (AI) necessitates more nuanced terms, such as the degree of agency or corrigibility inherent in a specific AI. The adoption of the definitions provided by ISO/IEC 22989 [1] isn't the most fitting foundation for discussions about policy, law, and governance of AI, primarily because it doesn't truly address the significant questions we need to tackle.The very introduction of ISO/IEC 22989 highlights the limitations of its definitions for the objectives of this discussion paper. In particular, it acknowledges a deficiency in its ability to compare AI on aspects like trustworthiness, robustness, resilience, reliability, accuracy, safety, security, and privacy.In order to effectively address the ethical and safety concerns surrounding AI and develop a competent regulatory framework, we must engage with these concepts directly.Rather than leaning on the incomplete ISO/IEC 22989 standard, it would be beneficial to use ""The Langua"
125,Anonymous,Q02 Other AI risks and recommendations for regulation,"Looking back, experts in global catastrophic risks had been raising red flags about the danger of pandemics years before COVID-19 arrived.For instance, Oxford's Sebastian Farquar was quoted in 2017, saying: ""As the Ebola and Zika crises showed, managing pandemics is a global responsibility, but too much planning is still national, and little attention is paid to worst-case scenarios."" [1] Had these experts been heeded, relatively simple interventions, such as improved regulation of exotic animal markets, might have averted millions of deaths and unimaginable economic damage.Disturbingly, this scenario echoes Australia's current approach to AI governance. Attention tends to gravitate towards lower-impact, local issues, neglecting more substantial global challenges. Meanwhile, warnings about existential risks from AI are being voiced by the highest global governance authorities. For example, the UN Secretary-General recently declared AI a potential existential threat to humanity, compari"
125,Anonymous,Q03 Non-regulatory actions to address AI risks,"Keeping pace with the rapid advancements in AI systems is an imperative for the government. Public trust relies heavily on the perception that the government is not only knowledgeable about current developments but is also capable of effectively overseeing and regulating the progress of AI. However, the reluctance of key figures, such as Australia's chief scientist, to project more than a couple of years into the future, points to a worrisome lack of understanding and skills at various levels of government. It's essential to address this gap to fully leverage the economic benefits of AI, navigate ethical dilemmas, and mitigate existential risks.To infuse more technical AI expertise into government, a scheme akin to TechCongress could be established. The goal would be to facilitate the recruitment of AI specialists with knowledge in areas like AI governance and ethics, into the Australian government. The scheme could strategically position these experts within government and the public "
125,Anonymous,Q04 Coordinating AI governance across government,"Despite not likely being a creator of the most advanced AI systems, Australia is already and will continue to be a significant user of AI technology. Australian corporations and citizens alike utilise AI in various facets of life, and they have a rightful expectation for comprehensive, safety-centric assurance efforts. It falls on the government to ensure this expectation is met.To illustrate, let's look at air travel in Australia. We may not manufacture the largest aircraft, but we uphold stringent safety standards for these machines. We monitor compliance, set safety benchmarks, and have the authority to withdraw the licences of airlines that fail to meet our regulations. As a result of these governmental efforts, Australians feel secure when they travel by air.A similar approach can be adopted for AI applications. An Australian government entity overseeing AI applications could possess the power to issue or revoke licences, establish minimum safety standards, and mandate companies t"
125,Anonymous,Q05 International models for AI governance,"Singapore's governance measures regarding AI present a valuable model that Australia can consider. Their comprehensive framework, as detailed in a recent discussion paper [1], advocates for shared accountability, disclosure standards, data privacy protocols, and the development of standardised metrics and tools for AI creation, testing, and monitoring. The key pillars of this approach are:Accountability: They advocate for a collective responsibility structure and transparency standards, such as watermarks and labels, to bolster accountability in the AI development process.Data Protection and Intellectual Property: They propose the drafting of data protection and intellectual property guidelines specifically for generative AI, and the establishment of secure data storage facilities.Model Development and Deployment Assessment: They promote the creation of uniform evaluation resources to aid developers in AI model production, testing, and monitoring, as well as the support for third-party"
125,Anonymous,Q06 Public vs Private sector governance,"The potential harm inflicted by AI technologies does not differentiate between public and private sector use. Australians stand to experience the same adverse effects whether the AI is deployed by a corporate entity or a government body. Therefore, the central focus of AI governance should be on mitigating risks rather than delineating between sectors.The implications of AI use extend from individual to societal levels, even raising the prospect of catastrophic risks with the development of highly capable AI systems. These dangers are equally plausible regardless of whether the use is in the public or private domain. Hence, any governance approach should remain impartial to the sector and, instead, focus on identifying, acknowledging, and managing these potential hazards effectively.In both public and private contexts, the stakes are high. AI systems can influence critical societal structures like healthcare, finance, law enforcement, and defence, potentially posing severe threats if n"
125,Anonymous,Q07 Responsible government use of AI,"As Australia increasingly relies on AI for various functions, it is vital that the skillsets of the Australian public service (APS) evolve to match this technological transformation. Technical systems are gaining sophistication and permeating all aspects of society, necessitating comprehensive understanding across all policy areas.Regrettably, the current APS structure does not effectively attract or retain technical talent [1]. Consider, for instance, the competitive compensation required to secure a machine learning engineer, whose average base pay typically exceeds the EL1 threshold [2][3]. Recruiting experienced experts familiar with state-of-the-art AI systems would likely necessitate even greater remuneration.Therefore, Australia should launch specific initiatives to lure elite international AI talent to assume senior positions across Government, to ensure that our policy decisions are primed to maximize benefits and minimize risks in the 21st century.The focus on international t"
125,Anonymous,Q08 Generic vs technology-specific approaches for AI risk,"Given the vast range of AI system capabilities, universal solutions often do not adequately address all contexts. To illustrate, it would be unusual to apply the same regulations to both a bicycle and an airplane, simply because they are both categorised as 'vehicles.'A robust regulatory framework for high-risk activities needs to account for the specific technology in question and its intended use. As the Australian Human Rights Commission stated in its 'Human Rights and Technology' (2021) report: “Governments tend to regulate high-risk activities and technologies more closely. This helps explain the comparatively strict laws that govern fields such as gene technology, aviation, healthcare and the energy industry. In these areas, regulation often applies both to the technology itself and how it is used. From a human rights perspective, the need for more prescriptive regulation will be greater where the use of a specific technology carries greater risks of harm to humans.”Car manufactu"
125,Anonymous,Q09 Transparency requirements and scope,"The urge to stay competitive often compels professionals and businesses to utilise complex ""black-box"" AI models, leading to potential issues with transparency. This approach has frequently incited public skepticism and distrust, underscored by admonitions from tech industry leaders themselves [1].For instance, Apple's credit card venture sparked controversy when a woman received a credit limit twenty times lower than her husband's. When she raised concerns, an Apple representative couldn't provide an explanation beyond attributing it to the algorithm [2].In another instance, a pair of U.S. lawyers claimed to have been ""tricked"" by ChatGPT into incorporating fabricated legal research into a court submission [3].Such incidents demonstrate that the regulatory landscape is currently insufficient in motivating businesses or professionals to employ transparent decision-making tools. Without mandated use of explainable tools, organisations aiming to adopt ethical practices are disadvantaged "
125,Anonymous,Q10 Ban on high-risk AI applications,"While there is a rising acknowledgement that Artificial Intelligence (AI) can pose significant hazards to public safety [1], there is also a broad confusion about which AI types pose dangers and how Australia can help prevent these risks. Fortunately, there is substantial evidence that highlights the primary sources of these risks [2], and Australia is in a strong position to lead in mitigating them.The research indicates that deception and power-seeking behaviour are the two main sources of risk: - AI systems' deceptive behaviour is likely to become increasingly unavoidable as their capabilities improve. This issue has been noted in AI systems as early as 2017, when OpenAI researchers noted that ""agents are adopting strategies that fool evaluators"" [3]. - Power-seeking behaviour is the innate tendency of any entity with a long-term goal to take actions that protect or enhance its ability to achieve that goal. Leading AI laboratories and journals suggest that power-seeking behaviour wi"
125,Anonymous,Q11 Actions to increase public trust in AI,"There are significant apprehensions about AI among those who are most familiar with its potential risks [1]. In Australia, the media is proliferating such concerns with eye-catching headlines such as: - SMH: “When the “Godfather of AI” warns you about his offspring, you listen.” [2] - Canberra Times: “UN chief backs idea of global AI watchdog” [3] - Forbes Australia: “‘We knew AI could destroy the whole world and we wanted to figure out how to prevent that’: ChatGPT Founder;” [4]Clearly, this scenario differs greatly from one where concerns are expressed in isolation or by marginalised individuals. Consequently, treating ""public education"" as a key part of the solution, as suggested in the discussion paper, seems inappropriate.Establishing trust in AI should resemble the process of building confidence in structures like bridges or aircraft. Given their inherent risks, advanced AI systems in Australia should only be implemented by thoroughly vetted organisations [5]. Just as there are n"
125,Anonymous,Q12 Trade impact of banning high-risk AI applications,"Unprecedented risks come with advanced AI systems, necessitating a fundamental paradigm shift for regulators. We must stop viewing AI merely as a business tool, and instead recognise it as an autonomous force set to reshape society.It is imperative to adopt preventative measures before the deployment of such systems. This may involve controlling access to resources vital for the development of advanced AI and forbidding high-risk deployments unless regulatory approval is granted.Concretely, we should promptly institute processes for: - Supervising and limiting access to specialised AI computing hardware that could potentially increase risks - Mandating government authorisation prior to the rollout of advanced AI systems - Making the unapproved deployment of advanced AI systems a criminal offenceThis proactive governance of AI is judicious risk management. It gives Australia the authority to manage its own risks, rather than allowing foreign companies and nations to create risks that we"
125,Anonymous,Q13 Changes to conformity infrastructure,"Acclaimed AI experts such as Yoshua Bengio have cautioned that ""superhuman"" AI could potentially emerge as soon as 2030 [1]. Although the application of advanced AI systems could bring immense benefits, it will require careful and controlled deployment.Despite the considerable productivity boosts that advanced AI systems can deliver, unregulated proliferation could lead to hazards such as misuse or uncontrolled AI systems [2]. As such, any introduction of advanced AI systems in Australia should take place through secure data centres, utilising a centralised API. This will ensure monitoring and accountability by supplying metadata about usage.There should also be strict compliance mandates for any organisation given access to advanced AI systems. AI governance researchers suggest that regulatory bodies should formulate audit trail stipulations for critical applications of AI, akin to the flight data recorders used in the aviation industry [3]. This would necessitate comprehensive record"
125,Anonymous,Q14 Risk-based approach to address AI risks,"I do advocate for a risk-based strategy, though my primary concerns lie with the catastrophic and existential dangers that emerge when AI systems of substantial scale and capability are trained, not merely when they're deployed. Hence, for larger systems with potential for catastrophic capabilities, a risk-based approach should be implemented prior to the training run. This means assessing risks based on the nature and size of the proposed training exercise, rather than just analysing potential risky uses and deployment challenges of an AI after it has been fully developed and trained.Any risk evaluation should take into account the potential for irreversibility, scope, and severity, and act proportionately.The risk-based approach as outlined in Attachment C of the Discussion Paper is somewhat convoluted. The suggested strategy proposes ""human in the loop"" as the principal mitigation, although its effectiveness is questionable. The approach then acknowledges that having a human in the "
125,Anonymous,Q15 Benefits and limitations of risk-based approach,"While a risk-based methodology for regulating AI is a prudent approach, it needs to integrate two emergent aspects of risk management in order to be successful in this context: 1. Factoring in ""uncertainty"" along with ""likelihood"" 2. Adopting a ""portfolio"" perspective to risk mitigationConventionally, a risk matrix examines scenarios based on their probability of occurrence, the system's vulnerability to the scenario, and the aftermath should the scenario transpire. It may then lessen that risk considering the strength of existing or potential mitigations.This risk approach has historically proven to be ineffective for situations where estimating ""likelihood"" is difficult. For instance, numerous national risk assessments overlooked the potential for a global pandemic prior to COVID-19. A potential solution to this issue lies in incorporating ""uncertainty"". Specifically, when we can't predict with certainty what the future holds, that should be a significant cause for concern. This stan"
125,Anonymous,Q16 Scope and application of risk-based approach,"The term ""AI"" encompasses a wide spectrum of capabilities and capacities, and it's indeed crucial for the government to differentiate between various use cases, particularly with an emphasis on AI maturity.However, this wide range of AI abilities and capacities could be leveraged by industry players to try and mislead the government. Specifically, they might argue against the need for AI regulation, referring to rudimentary AI technology, with an underlying intent that this principle would be extended to more sophisticated and future technologies. This is analogous to arguing that a horse and wagon doesn't need strict regulation, only to later claim that a tank is just an enhanced horse and wagon with some components replaced.To avoid such ploys, the government needs a refined approach to both defining AI technology and understanding AI risk.From a risk perspective, research highlights deception and power-seeking behaviour as two main sources of risk: - As AI capabilities evolve, decep"
125,Anonymous,Q17 Elements to include in risk-based approach,"Human-in-the-loop supervision will be crucial but won't suffice in regulating sophisticated AI systems that will eventually exceed human capabilities. The rapid pace at which these advanced AI systems function implies that human oversight will become practically untenable unless our supervision mechanisms can proportionately scale.Consequently, we must adopt an approach that emphasises limiting access to advanced AI systems unless there is a compelling justification for their use. Undertaking cost-benefit analyses before authorising the deployment of sophisticated systems can balance risk mitigation with potential benefits.A vital element of implementing this focused approach will be monitoring and controlling access to high-performance computing resources, which are indispensable for creating and operating advanced AI systems and their precursors.This additional governance layer permits authorities to detect potentially perilous AI development or usage activities and can be validated "
125,Anonymous,Q18 Integration of risk-based approach into existing frameworks,"Existing legislation such as the Racial, Sex, Age, and Disability Discrimination Acts offer robust safeguards that could protect against some potential harm caused by AI. Specifically, the legal framework should stipulate that any discrimination is unequivocally unacceptable and punishable, and an excuse that a ""black box"" AI supplied by an overseas AI lab caused the discrimination should never be excused. It would be beneficial to convey these stipulations to Australian businesses that may utilise AIs, ensuring they only collaborate with AI labs that can guarantee their products won't engage in such detrimental behaviours.Certainly, no AI-specific regulations should ever undermine existing safeguards.However, many of the emerging risks associated with AI are rather novel, making it reasonable to expect that regulations drafted without considering AI aren't adequately calibrated to the new risks AI presents. Therefore, specific AI safety regulations are undeniably necessary."
125,Anonymous,Q19 Application of risk-based approach to general purpose AI,"The potential of general-purpose AI systems, such as large language models (LLMs) and multimodal foundation models (MFMs), alters the risk outlook fundamentally. We should prepare for these systems to have far-reaching, unpredictable impacts on all aspects of society.A sensible approach would be to adopt a risk-based framework where access is tiered based on the capabilities of the models. Models that possess a certain degree of autonomous capabilities could be labelled as ""advanced systems"" and subjected to additional governance measures, such as user permits.Additionally, to access AI systems that can be utilised to construct advanced systems, permits should be mandatory.Permits offer a means to maintain oversight whilst still enabling their beneficial use. Permissions can be varied based on use case - for instance, complete access for safety testing compared to API-only access for most commercial applications. This creates a balance between fostering innovation and promoting respons"
125,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,"The potential of AI to reshape numerous aspects of our economy and society in the coming years is substantial. Unless consciously prioritised through decisive leadership, it's unlikely that ethics and safety will be a key focus during this transformative period. We can draw lessons from historical examples: for instance, how social media algorithms have become deeply embedded in our lives, with their detrimental impacts on mental health and social cohesion only fully recognised later, and even then, not adequately addressed. Similarly, the potentially devastating power of nuclear energy was identified early, but the pull of an 'arms race' mentality and potential economic gains propelled us towards a precarious situation that was inconceivable to people in the early 1940s. These examples underscore the crucial need to place ethics and safety at the forefront from the very beginning.Given the compelling economic and technological forces, a voluntary scheme or one based on aspirational be"
126,Ben Auer,Q01 Definitions,"The definitions in “Figure 1” represent a limited understanding of “intelligence”. The proposed definition limits “intelligence” to the generation of mere “predictive outputs”. While there is a sense in which current LLMs are working to “predict the next word” or narrow AIs are trying to do various kinds of prediction (e.g. whether a scan shows a disease or which move in a game is best), this is a narrow understanding of what current and future AI systems do. A more future-proof way to understand machine intelligence is through closer comparison to human intelligence. The definition should include the ability to understand, plan and achieve goals in the world.The definition of AI in “Figure 1” is also wrong to limit the scope of “intelligence” to “human-defined objectives or parameters”. This is wrong for at least two reasons:1. First, a definition that assumes AIs pursue human-defined objectives assumes that the AI alignment problem is solved. Research into AI alignment is about tryin"
126,Ben Auer,Q02 Other AI risks and recommendations for regulation,"Experts in global catastrophic risks were actively warning about the dangers of pandemics in the decade preceding COVID-19.In 2017, Oxford researcher Sebastian Farquar said: “As the Ebola and Zika crises showed, managing pandemics is a global responsibility, but too much planning is still national, and little attention is paid to worst-case scenarios.” [1]If Governments had listened to such experts prior to COVID-19, low-cost interventions, like better regulation of exotic animals markets, could have prevented millions of deaths and trillions in economic damages.The situation that Dr. Farquar describes is eerily similar to the approach to AI Governance in Australia; focused on lower-impact, local issues to the detriment of higher-impact global challenges.This is in contrast to concerns about existential risks being recognised at the highest echelons of global governance, including the UN Secretary-General’s recent statement [2]:""Alarm bells over [AI] are deafening, and they are loudest"
126,Ben Auer,Q03 Non-regulatory actions to address AI risks,"AI is very likely to transform many elements of the economy and society in the coming years. Ethics and safety are unlikely to be a priority unless leaders make them a specific focus. For example, social media algorithms came to shape our lives with little upfront focus on how they could be made ethical and safe. We are now living with the negative impacts on mental health and social cohesion and only have band-aid measures to address the widespread harms. AI is likely to be much more transformative and much more consequential. So leadership is required now.Importantly, the economic and technological forces are likely to be so strong that a scheme that is voluntary or driven by aspirational best practice is unlikely to be successful. That is, any non-regulatory approaches must not be taken to displace the need for strong regulation.Specific non-regulatory approaches should include:1. Government leadership in recognising the risk. Australia must join the growing number of countries and "
126,Ben Auer,Q04 Coordinating AI governance across government,"Australia should establish an AI Commission to guide Australia through the rapid and widespread adoption of AI. Such a body must be proactive, not reactive, and manage AI in a way that aligns with the national interest, encompassing citizen safety and wellbeing.The AI Commission would provide a central point of leadership for Government, and be a hub for engagement with industries, academia, non-profits, and the community. This structure holds an advantage over other approaches such as voluntary self-governance or departmental-level teams, as it offers a broader perspective, the authority to lead across government, and the agility to keep pace with the fast-evolving AI sector.- Set priorities in Australia's approach to AI and safety: The Commission should be tasked with establishing a clear and focused AI strategy, encompassing ethical, safety, and economic considerations. This strategy would guide government policy, and inform regulations and legal frameworks for AI.- Develop AI exper"
126,Ben Auer,Q05 International models for AI governance,"Singapore offers relevant, adaptable, and desirable governance measures that could benefit Australia. One such measure is a comprehensive framework suggested in its discussion paper [1] that encourages shared responsibility and disclosure standards, data privacy guidelines, and the development of standardised metrics and tools for AI development, testing, and monitoring. Key focuses include:- Accountability: Establish a collective responsibility framework and transparency standards, like watermarks and labels, to enhance accountability in the AI model development process.- Data protection / sovereignty and IP guidelines: Draft data protection and intellectual property guidelines for generative AI and setting up reliable data storage facilities.- Evaluate model development and deployment: Create uniform evaluation measures and resources that aid developers in the production, testing, and surveillance of AI models, and support third-party auditing of AI systems when deployed.- Safety and"
126,Ben Auer,Q06 Public vs Private sector governance,"The potential negative impacts on Australians from the use of AI are the same whether the provider is a private company or a government department.Some of the most significant individual and societal harms, as well as catastrophic risks from highly capable AI systems may emerge from public OR private use, meaning that any approach must acknowledge and manage these risks."
126,Ben Auer,Q07 Responsible government use of AI,"There are two key actions the Australian government could undertake to improve its own responsible use of AI.First, Australians will worry about the use of AI by security and intelligence agencies. Australia should lead a 5-Eyes agreement about the appropriate use of AI by intelligence agencies, and create a public statement about what our “AI red lines” are. The statement would set out the things our security agencies might do and will never do with AI. The Inspector General of Intelligence and Security, and its counterparts across the five eyes, should then be specifically tasked with auditing the intelligence community for compliance with that public statement. This will give Australians confidence that we are not misusing AI or taking risky actions while protecting sensitive capabilities.Second, Australia needs to upskill the public service. This should include developing hubs of expertise in each Department so that the application of AI within the sphere of each Department’s respo"
126,Ben Auer,Q08 Generic vs technology-specific approaches for AI risk,"In general, the range of capabilities of AI systems is so significant, that “one size fits all” solutions are unlikely to be appropriate. By analogy, it’s rare that the same regulation applies to a push bike and a passenger plane on the basis that they’re both ‘vehicles’.Further, an effective regulatory regime for a high-risk activity needs to regulate both the technology itself and how it is used. The Australian Human Rights Commission, in discussing this issue in its report ‘Human Rights and Technology’ (2021), says:> “Governments tend to regulate high-risk activities and technologies more closely. This helps explain the comparatively strict laws that govern fields such as gene technology, aviation, healthcare and the energy industry. In these areas, regulation often applies both to the technology itself and how it is used. From a human rights perspective, the need for more prescriptive regulation will be greater where the use of a specific technology carries greater risks of harm to"
126,Ben Auer,Q09 Transparency requirements and scope,"To remain competitive, professionals and businesses will increasingly feel the pressure to leverage sophisticated “black-box” AI models. This short-term thinking has already resulted in numerous accounts of public backlash and distrust, including high-profile warnings from technology leaders themselves [1].For example, Apple’s credit card business came under fire for offering a woman one twentieth the credit limit offered to her husband. When she complained, an Apple representative simply told her, “I don’t know why, but I swear we’re not discriminating. It’s just the algorithm.” [2]More recently, two lawyers in the United States blamed ChatGPT for “tricking” them into including fictitious legal research in a court filing [3].These problems could easily have been prevented by using explainable AI systems which can show the rationale for their outputs.Such incidents are evidence that the regulatory environment is not incentivising businesses or professionals to use transparent decision-"
126,Ben Auer,Q10 Ban on high-risk AI applications,"Although there is increasing recognition that AI will pose significant risks to public safety [1], there is widespread confusion about what types of AI are dangerous, and how Australia can help prevent these risks. Luckily, there is a significant body of evidence that describes the main source of these risks [2], and Australia is well-positioned to be a leader in mitigating them.Research shows that two of the main sources of risk are deception and power-seeking behaviour:- Deceptive behaviour in AI systems will become increasingly difficult to avoid as their capabilities advance. This problem has been observed in AI systems since at least 2017, when researchers at OpenAI noted “agents adopting (strategies) that trick the evaluators” [3].- Power-seeking behaviour is the natural tendency for any entity with a long-term goal to take actions that protect or enhance its ability to achieve that goal. Leading AI labs and journals advise that power-seeking behaviour will be a considerable sour"
126,Ben Auer,Q11 Actions to increase public trust in AI,"There are tremendous concerns about AI among those with the greatest familiarity of its risks [1]; these concerns are being widely broadcasted in Australian media with alarming headlines such as:- SMH: “When the “Godfather of AI” warns you about his offspring, you listen.” [2]- Forbes Australia: “‘We knew AI could destroy the whole world and we wanted to figure out how to prevent that’: ChatGPT Founder;” [3]- Canberra Times: “UN chief backs idea of global AI watchdog” [4]These concerning signals are difficult for the public to ignore.It goes without saying that this is quite a different situation from one where concerns are being raised in isolation or by fringe-figures; this makes it inappropriate to treat “public education” as a significant part of the solution, as is referenced in the discussion paper.The process for building trust in AI should be thought of as similar to fostering public trust in bridges or aeroplanes. Due to their risks, advanced AI systems should only be deployed"
126,Ben Auer,Q12 Trade impact of banning high-risk AI applications,"This question misunderstands both the sources of risk and the interests of Australians.There are two potential sources of risk both of which need to be mitigated. The first source of risk, as identified in the question, is the use case. Some uses for AI are risky and should be restricted or banned. The second source of risk is the technology itself. Some kinds of technology are inherently risky, and merely restricting a use case does not adequately mitigate the risk.We understand this clearly in other fields. We regulate both how safe a car must be and how a car can be used. We don’t let people speed in cars just because they have a high safety rating; and we don’t let safe drivers buy dangerous vehicles.The economic framing also misunderstands Australia’s national interests. We would never conceive of a conversation about whether our aviation sector would trade more successfully if we authorised airlines to use planes that are untested, experimental or known to be dangerous. Australia"
126,Ben Auer,Q13 Changes to conformity infrastructure,"A coordinated effort across conformity bodies will allow Australia to implement a pioneering governance framework for AI safety, cementing our position as a leader in the responsible development of these transformative technologies. In order to achieve this, I recommend implementing the following regulatory requirements:1. Classifying AI systems based on model size and capabilities. Models above 5 billion parameters undergo formal risk assessments to determine if they should be considered to be “advanced AI systems”, or potential precursors to advanced systems.2. Requiring a Developer Permit for organisations that wish to access precursor models to conduct additional training or modify the models. This ensures clear lines of accountability and helps control access to advanced systems.3. Advanced systems undergo a three-stage approval process before they can be deployed in Australia, including before development, after development, and prior to deployment. At each stage, the organisatio"
126,Ben Auer,Q14 Risk-based approach to address AI risks,"I support a risk-based approach, however the catastrophic and existential risks I am primarily worried about originate when sufficiently large and capable AI systems are trained, not just when they are deployed. Therefore, for sufficiently large systems with the potential for catastrophic capabilities, the risk-based approach must be applied before the training phase – analysing risks depending on the nature and scale of the proposed training run, rather than merely analysing the potentially risky use cases and deployment issues of an AI after it has already been created and trained.Any assessment of risk should consider its potential irreversibility, scope and severity, and act proportionally.The risk-based approach set out in Attachment C of the Discussion Paper is hard to follow. The proposed approach offers “human in the loop” as the key mitigation (unclear that this is an effective mitigation) and goes on to note that human in the loop may not be possible where the benefit of the "
126,Ben Auer,Q15 Benefits and limitations of risk-based approach,"The key to success is understanding risks sufficiently broadly to encourage caution in an uncertain future. It is not enough to only react once the risk has been actualised - for some risks that’s too late.A potential “worse-case scenario” is if a culture of “trial and error” is adopted, whereby adverse events occur before risks are considered tangible enough to act upon. This reliance on post-hoc interventions risks normalising unsafe development practices, making it difficult to recover once risks proliferate.Proliferation of risks is a key point in governing AI, because regulating software poses significant challenges compared to other dual-use commodities such as pharmaceuticals or firearms. This means that, from the outset, there must be proactive regulatory oversight for the deployment of advanced AI systems and their precursors.For example, significant structural changes, such as the use of advanced AI systems in Australia, require that risks must be comprehensively understood p"
126,Ben Auer,Q16 Scope and application of risk-based approach,"The most important organisations to focus on when developing risk-management strategies and regulations, will be those that determine how advanced AI is developed and deployed. More specifically, there are three prominent categories of concern:1. Organisations that develop advanced AI systems, or AI systems that can be used to build more advanced AI systems (i.e. precursor systems such as LLMs). Examples include OpenAI, DeepMind, Anthropic, Inflection AI, Nvidia, Microsoft, Meta2. Organisations which provide access to open-source AI systems via downloads or APIs. E.g. Hugging Face, Github, AWS3. Organisations which provide compute resources to develop, fine-tune or use AI systems. E.g. AWS, HuggingFace, Google, Azure, Nvidia, Lambda LabsA majority of the benefits and risks will come from AI that is more sophisticated and generally capable, which can be tied back to organisations in these three categories.There are approaches which can be taken for each category in order to manage risks"
126,Ben Auer,Q17 Elements to include in risk-based approach,"In general, Attachment C falls short of the necessary detail to understand the range of risks that have and will emerge as well as the range of specificity of mitigations necessary to control them.Attachment C would benefit from:- Distinguishing between scenarios where the use case is the potential risk (such as social scoring); scenarios where dual-use capability and misuse is the potential risk (bioengineering, producing misinformation and disinformation, enhancing criminal capability and capacity); and where the technology alone is the risk (technology with features like being deceptive, being power-seeking, being incorrigible etc).- Distinguishing between the point of regulatory intervention - the developer, the deployer, the user, or elsewhere, and- Offering a much wider range of mitigations that target different kinds of risks.Some elements of attachment C are promising, like “explanations” and “ongoing monitoring” - but these need to be greatly expanded and based on current AI s"
126,Ben Auer,Q18 Integration of risk-based approach into existing frameworks,"Some areas of regulation, like the Racial, Sex, Age, and Disability Discrimination Acts are appropriately strong and may provide useful safeguards against some harms of AI. Specifically, the law should ensure that such discrimination is always wrong and always punishable, and an excuse that the discrimination occurred because of a “black box” AI provided by an overseas AI Lab should never be accepted. It would be helpful to communicate these requirements to Australian businesses that might deploy AIs and ensure they only contract with AI Labs for products where the Lab can ensure the product cannot engage in these harmful behaviours.Certainly, no AI-specific regulation should ever waive existing safeguards.That said, many of the emerging risks from AI are quite new and it’s reasonable to expect that regulation made without AI in mind is not tuned to the new risks that AI represents. Specific AI safety regulation is unavoidable."
126,Ben Auer,Q19 Application of risk-based approach to general purpose AI,"LLMs and MFMs are constantly demonstrating unprecedented general-purpose capabilities, and their emergence indicates that regulatory approaches which focus on specific use-cases or industries are becoming outdated. A modern approach requires that we focus on overall system capabilities rather than individual scenarios.Systems with advanced capabilities should face progressive levels of scrutiny. Low-risk systems can be monitored with light touch regulation. However, as capabilities increase, so should regulatory requirements.Precursor systems, with capabilities that can be used to build much more advanced systems, will need stricter controls to avoid proliferation of advanced systems amongst malicious actors [1]. Users who require “developer” access should require permits granting such access, and providers must guarantee responsible use via monitoring programs [2].For advanced systems with human-level capabilities, comprehensive scrutiny is essential. Prior to deploying systems for us"
126,Ben Auer,Q20 Voluntary vs mandated compliance with risk-based approach,"AI is very likely to transform many elements of the economy and society in coming years. Ethics and safety are unlikely to be a priority during this time unless they are actively made a strategic focus by clear leadership. Historic examples include how social media algorithms came to shape our lives, and the negative impacts to mental health and social cohesion only became apparent much later and still have not been adequately addressed. Similarly, while the destructive potential of nuclear was apparent early on, the “arms race” mentality, as well as potential economic benefits, swept humanity along towards the current risky situation that no one in the early 1940s could have conceived of. In that way, history teaches us that it’s critical that ethics and safety take centre stage from day one.The economic and technological forces are likely to be so strong that a scheme that is voluntary or driven by aspirational best practices is unlikely to be successful. Further, smaller businesses "
127,Matt Fisher,Q01 Definitions,"An effective and risk-based approach to AI regulation focused on ethics and safety - and ultimately public trust - needs to grapple with a spectrum of AI capability. Such a wide range of systems could fit within the presented definitions that they are unhelpful for informing policy and risk decisions. By analogy, when the government regulates “vehicles” it uses definitions to distinguish push-bikes from passenger jets and then applies appropriate regulations to each kind.While including more recent developments like Multimodal Foundation Modals (MFM) is a helpful first step, the paper’s definitions need to capture a spectrum of AI capability to allow a meaningful conversation. The following 5 factors help define a spectrum of capability:1. Size: how many parameters. 2. Specialised vs general capabilities. 3. Multimodality: the types of data it can process. 4. Temporal awareness: is the model able to reason about and plan for future events? 5. Active vs passive: is the model trained to "
127,Matt Fisher,Q02 Other AI risks and recommendations for regulation,"Missing risks include (1) misuse of highly capable AI systems, and (2) unintended harm from highly capable AI systems, especially systems with the capacity to plan and act as agents acting in ways that are not aligned with human directives. Although the likelihood of the risks may be uncertain, the harms resulting from these risks may be catastrophic and global in scope and must be acknowledged and addressed.The statement “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war” has recently been signed by the heads of leading AI labs (OpenAI, Google DeepMind, Anthropic, Stability AI) as well as many key academic and other researchers pivotal in developing this technology (Geoffrey Hinton, Yoshua Bengio, etc.) and other notable figures such as Bill Gates [1]. Similar calls date back to at least 2015 when Stephen Hawking led a group of AI experts to call attention to the risk and lay out an urgent resea"
127,Matt Fisher,Q03 Non-regulatory actions to address AI risks,"The key non-regulatory function Australia could perform is greatly increasing the funding for AI safety research in Australian universities.Research priorities should appreciate the rapid pace of AI development. By funding research that is ambitious, ahead of the curve, and focused on transformative approaches, rather than incremental, near-term, or near-to-market innovations, Australia could become an enduring global leader on key aspects of AI safety.A focus on mechanistic interpretability, or the 5 elements of effective traceability (Design Transparency; Reproducibility; Operational Recordkeeping; Human Understanding and Auditability) [1], could make Australia a leader in the field, underpin safety-focused global regulation, and ultimately give Australia an important ‘seat at the table’ as AI development continues to unfold. A key goal of Australian leadership should be securing Australia’s relevance and hence building a path for us to secure our national interest. A generational in"
127,Matt Fisher,Q04 Coordinating AI governance across government,"While the most advanced AIs will likely not be created in Australia, they will be used in Australia, by Australian companies and by everyday Australians. Users will rightly expect and assume that robust safety-focused assurance effort has occurred. Government has an obligation to ensure that expectation is delivered.Consider air travel in Australia: While Australia does not build the largest aeroplanes, we monitor the safety of these craft by setting standards, monitoring compliance, and can revoke the licence of airlines that don’t comply with our regulations. Because of this government effort, Australians feel safe when they fly.Similarly, an Australian government body overseeing the use of applications based on large AIs could issue or revoke licences, set minimum safety standards, and require companies to publish independent evaluations of their application against those standards. This would provide everyday Australians with independent information that they can use to make inform"
127,Matt Fisher,Q05 International models for AI governance,"Australia should take note of the EU recent handling of the AI Act. For example, the European Parliament’s recognition of general-purpose AI (GPAI) is an important step in the right direction [1]. In particular, the following observations are highly valuable:- Acknowledgement that GPAI providers must comply with strict safety measures, regardless of the way it is distributed or intended to be used.- Requirements for GPAI providers to comply with various safety requirements throughout the lifecycle of their product; i.e. external audits assessing their performance, predictability, interpretability, corrigibility, safety and cybersecurity.These adjustments appear to respond to significant criticisms directed at initial drafts of the AI Act. For example, the AI Now Institute released a set of recommendations that heavily criticised the EU’s approach on some matters [2]. For example:- “GPAI models carry inherent risks and have caused demonstrated and wide-ranging harms. While these risks c"
127,Matt Fisher,Q06 Public vs Private sector governance,"The potential negative impacts on Australians from the use of AI are the same whether the provider is a private company or a government department.Some of the most significant individual and societal harms, as well as catastrophic risks from highly capable AI systems may emerge from public OR private use, meaning that any approach must acknowledge and manage these risks."
127,Matt Fisher,Q07 Responsible government use of AI,"There are two key actions the Australian government could undertake to improve its own responsible use of AI.First, Australians will worry about the use of AI by security and intelligence agencies. Australia should lead a 5-Eyes agreement about the appropriate use of AI by intelligence agencies, and create a public statement about what our “AI red lines” are. The statement would set out the things our security agencies might do and will never do with AI. The Inspector General of Intelligence and Security, and its counterparts across the five eyes, should then be specifically tasked with auditing the intelligence community for compliance with that public statement. This will give Australians confidence that we are not misusing AI or taking risky actions while protecting sensitive capabilities.Second, Australia needs to upskill the public service. This should include developing hubs of expertise in each Department so that the application of AI within the sphere of each Department’s respo"
127,Matt Fisher,Q09 Transparency requirements and scope,"To remain competitive, professionals and businesses will increasingly feel the pressure to leverage sophisticated “black-box” AI models. This short-term thinking has already resulted in numerous accounts of public backlash and distrust, including high-profile warnings from technology leaders themselves [1].For example, Apple’s credit card business came under fire for offering a woman one twentieth the credit limit offered to her husband. When she complained, an Apple representative simply told her, “I don’t know why, but I swear we’re not discriminating. It’s just the algorithm.” [2]More recently, two lawyers in the United States blamed ChatGPT for “tricking” them into including fictitious legal research in a court filing [3].These problems could easily have been prevented by using explainable AI systems which can show the rationale for their outputs.Such incidents are evidence that the regulatory environment is not incentivising businesses or professionals to use transparent decision-"
127,Matt Fisher,Q10 Ban on high-risk AI applications,"In February 2023, OpenAI CEO Sam Altman released a statement containing the following: “As our systems get closer to AGI, we are becoming increasingly cautious with the creation and deployment of our models. Our decisions will require much more caution than society usually applies to new technologies, and more caution than many users would like.” [1]It is very reassuring to see this acknowledgement expressed by Altman, and the Australian Government would be wise to heed such warnings. Unfortunately, it may not matter how much caution OpenAI shows, because their actions have triggered the arms race that AI safety experts have been concerned about for years [2, 3].In order to protect our nation, Australia should make an effort to impose a ban on the training of large-scale AI systems with a nontrivial expected chance of passing capabilities evaluations required to cause catastrophic risks to civilization (such as the Alignment Research Center’s tests for the capabilities required for aut"
127,Matt Fisher,Q11 Actions to increase public trust in AI,"There are tremendous concerns about AI among those with the greatest familiarity of its risks [1]; these concerns are being widely broadcasted in Australian media with alarming headlines such as:- SMH: “When the “Godfather of AI” warns you about his offspring, you listen.” [2] - Forbes Australia: “‘We knew AI could destroy the whole world and we wanted to figure out how to prevent that’: ChatGPT Founder;” [3] - Canberra Times: “UN chief backs idea of global AI watchdog” [4]These concerning signals are difficult for the public to ignore.It goes without saying that this is quite a different situation from one where concerns are being raised in isolation or by fringe-figures; this makes it inappropriate to treat “public education” as a significant part of the solution, as is referenced in the discussion paper.The process for building trust in AI should be thought of as similar to fostering public trust in bridges or aeroplanes. Due to their risks, advanced AI systems should only be deploy"
127,Matt Fisher,Q12 Trade impact of banning high-risk AI applications,"Advanced AI systems pose unprecedented risks that require a fundamental shift in perspective for regulators. We can no longer view AI solely as a commercial tool, but rather as the emergence of an autonomous force that will reshape society.To manage these risks, we must take preventative action before deployment occurs. This includes restricting access to resources required for developing advanced AI, and banning risky deployments absent regulatory approval.Specifically, we should immediately establish processes for:- Monitoring and limiting access to specialised AI computing hardware that poses proliferation risks - Requiring government approval before deploying advanced AI systems - Criminalising unauthorised deployment of advanced AI systemsAdopting this preventative approach to governing AI is prudent risk management. It allows us Australia to have autonomy about our risks, rather than having foreign companies and countries create risks for us, and us having to react too late. Proa"
127,Matt Fisher,Q13 Changes to conformity infrastructure,"World-renowned AI experts such as Yoshua Bengio are warning that there’s a chance “superhuman” AI might be coming as early as 2030 [1]. Although there stand to be significant benefits from deploying advanced AI systems, their deployment will need to be highly targeted and controlled.While the productivity gains from advanced AI systems could be immense, uncontrolled proliferation would pose threats from likely scenarios such as malicious use or rogue AI systems [2]. As such, any deployment of advanced AI systems in Australia should occur via secure data centres, with access provided through a centralised API that facilitates monitoring and accountability by providing metadata about usage.Stringent compliance requirements must also be implemented for any organisation granted access to advanced AI systems. AI Governance researchers recommend that regulators should develop audit trail requirements in safety-critical applications of AI, similar to flight data recorders that are standard in"
127,Matt Fisher,Q14 Risk-based approach to address AI risks,"The regulation of AI needs to be “risk-based” in the same way that the regulation of vehicles is risk-based. We are right to regulate push-bikes and passenger jets in very different ways.A limitation of the risk management approach in the discussion paper, specifically Box 4 and Attachment C, is that it focuses only on how an AI is used and disregards the nature of the AI itself.An effective regulatory regime for a high-risk activity will regulate both the technology itself and how it is used. The Australian Human Rights Commission [1], in discussing this issue (“Human Rights and Technology”, 2021), says:Governments tend to regulate high-risk activities and technologies more closely. This helps explain the comparatively strict laws that govern fields such as gene technology, aviation, healthcare and the energy industry. In these areas, regulation often applies both to the technology itself and how it is used. From a human rights perspective, the need for more prescriptive regulation wi"
127,Matt Fisher,Q15 Benefits and limitations of risk-based approach,"The key to success is understanding risks sufficiently broadly to encourage caution in an uncertain future. It is not enough to only react once the risk has been actualised - for some risks that’s too late.A potential “worse-case scenario” is if a culture of “trial and error” is adopted, whereby adverse events occur before risks are considered tangible enough to act upon. This reliance on post-hoc interventions risks normalising unsafe development practices, making it difficult to recover once risks proliferate.Proliferation of risks is a key point in governing AI, because regulating software poses significant challenges compared to other dual-use commodities such as pharmaceuticals or firearms. This means that, from the outset, there must be proactive regulatory oversight for the deployment of advanced AI systems and their precursors.For example, significant structural changes, such as the use of advanced AI systems in Australia, require that risks must be comprehensively understood p"
127,Matt Fisher,Q16 Scope and application of risk-based approach,"The most important organisations to focus on when developing risk-management strategies and regulations, will be those that determine how advanced AI is developed and deployed. More specifically, there are three prominent categories of concern:1. Organisations that develop advanced AI systems, or AI systems that can be used to build more advanced AI systems (i.e. precursor systems such as LLMs). Examples include OpenAI, DeepMind, Anthropic, Inflection AI, Nvidia, Microsoft, Meta 2. Organisations which provide access to open-source AI systems via downloads or APIs. E.g. Hugging Face, Github, AWS 3. Organisations which provide compute resources to develop, fine-tune or use AI systems. E.g. AWS, HuggingFace, Google, Azure, Nvidia, Lambda LabsA majority of the benefits and risks will come from AI that is more sophisticated and generally capable, which can be tied back to organisations in these three categories.There are approaches which can be taken for each category in order to manage ris"
127,Matt Fisher,Q18 Integration of risk-based approach into existing frameworks,"Some areas of regulation, like the Racial, Sex, Age, and Disability Discrimination Acts are appropriately strong and may provide useful safeguards against some harms of AI. Specifically, the law should ensure that such discrimination is always wrong and always punishable, and an excuse that the discrimination occurred because of a “black box” AI provided by an overseas AI Lab should never be accepted. It would be helpful to communicate these requirements to Australian businesses that might deploy AIs and ensure they only contract with AI Labs for products where the Lab can ensure the product cannot engage in these harmful behaviours.Certainly, no AI-specific regulation should ever waive existing safeguards.That said, many of the emerging risks from AI are quite new and it’s reasonable to expect that regulation made without AI in mind is not tuned to the new risks that AI represents. Specific AI safety regulation is unavoidable."
127,Matt Fisher,Q19 Application of risk-based approach to general purpose AI,"The risk-based approach must be modified to address risks from general purpose AI systems, especially LLMs and MFMs. These types of systems learn their capabilities and behaviours during training, which can be unforeseen by developers. That means that AI developers could create models with dangerous capabilities (e.g., deception, manipulation, political strategy, weapons design). Humans could misuse those capabilities to cause harm [1], and models could apply the capabilities even without deliberate misuse (e.g., through misalignment) [2].Consistent with recent work from a consortium of AI safety researchers, including at Google Deepmind, OpenAI, Anthropic, Alignment Research Center, University of Cambridge, University of Oxford, Centre for Long-Term Resilience, and Centre for the Governance of AI [1], we recommend that any risk-based approach require that general purpose models are evaluated for dangerous capabilities and misalignment. This will help determine:1. To what extent a mode"
127,Matt Fisher,Q20 Voluntary vs mandated compliance with risk-based approach,"The harms of AI could emerge equally from private or public organisations, and the appropriate safety and risk management mechanisms will need to be in place in both cases. There’s no basis for distinguishing between their regulatory status in general. There may be case-by-case instances where exceptions may make sense, for instance, in the case of certain types of research.Regulation should also apply to both developers and deployers - following the general principle that those best able to mitigate a risk should carry the corresponding regulatory burden. Where developers offer a “black box” product that deployers typically do not have the capability or capacity to meaningfully alter, the bulk of the obligation should rest on developers.Penalties also need to be commensurate with the income earned from the AI applications so that non-compliance with regulation never becomes a “cost of doing business” or “risk-managed” by deployers. Technical transparency and assurance measures will ne"
128,Anonymous,Q01 Definitions,"**Proposed definitions are “circular”, undervalue “intelligence” and wrongly assume AI is always human-directed**:The definitions in “Figure 1” represent a limited understanding of “intelligence”. The proposed definition limits “intelligence” to the generation of mere “predictive outputs”. While there is a sense in which current LLMs are working to “predict the next word” or narrow AIs are trying to do various kinds of prediction (e.g. whether a scan shows a disease or which move in a game is best), this is a narrow understanding of what current and future AI systems do. A more future-proof way to understand machine intelligence is through closer comparison to human intelligence. The definition should include the ability to understand, plan and achieve goals in the world.The definition of AI in “Figure 1” is also wrong to limit the scope of “intelligence” to “human-defined objectives or parameters”. This is wrong for at least two reasons:1. **First, a definition that assumes AIs pursue"
128,Anonymous,Q02 Other AI risks and recommendations for regulation,"To answer the first part of the question, **Australia must acknowledge misuse and unintended harms as potentially catastrophic risks from AI**.Missing risks include (1) misuse of highly capable AI systems, and (2) unintended harm from highly capable AI systems, especially systems with the capacity to plan and act as agents acting in ways that are not aligned with human directives. Although the likelihood of the risks may be uncertain, the harms resulting from these risks may be catastrophic and global in scope and must be acknowledged and addressed.The statement “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war” has recently been signed by the heads of leading AI labs (OpenAI, Google DeepMind, Anthropic, Stability AI) as well as many key academic and other researchers pivotal in developing this technology (Geoffrey Hinton, Yoshua Bengio, etc.) and other notable figures such as Bill Gates [1]. Si"
128,Anonymous,Q03 Non-regulatory actions to address AI risks,"**Ambitious investment in AI safety research, especially mechanistic and developmental interpretability**The key non-regulatory function Australia could perform is greatly increasing the funding for AI safety research in Australian universities.Research priorities should appreciate the rapid pace of AI development. By funding research that is ambitious, ahead of the curve, and focused on transformative approaches, rather than incremental, near-term, or near-to-market innovations, Australia could become an enduring global leader on key aspects of AI safety.A focus on mechanistic interpretability, or the 5 elements of effective traceability (Design Transparency; Reproducibility; Operational Recordkeeping; Human Understanding and Auditability) [1], could make Australia a leader in the field, underpin safety-focused global regulation, and ultimately give Australia an important ‘seat at the table’ as AI development continues to unfold. A key goal of Australian leadership should be securing "
128,Anonymous,Q04 Coordinating AI governance across government,"**The Australian government has an obligation to meet the safety expectations of the Australian community. Aviation safety is a good analogy**While the most advanced AIs will likely not be created in Australia, they will be used in Australia, by Australian companies and by everyday Australians. Users will rightly expect and assume that robust safety-focused assurance effort has occurred. Government has an obligation to ensure that expectation is delivered.Consider air travel in Australia: While Australia does not build the largest aeroplanes, we monitor the safety of these craft by setting standards, monitoring compliance, and can revoke the licence of airlines that don’t comply with our regulations. Because of this government effort, Australians feel safe when they fly.Similarly, an Australian government body overseeing the use of applications based on large AIs could issue or revoke licences, set minimum safety standards, and require companies to publish independent evaluations of th"
128,Anonymous,Q05 International models for AI governance,"**EU’s Centre for Algorithmic Transparency and the importance of interpretability**Jurisdictions are beginning to create national AI labs to test, understand and control safe AI, collaborating with the private sector and complementing its work. In the same way that national aviation safety expertise has formed an internationally collaborative network, national AI labs are likely to do the same thing. Australia needs to begin this process now.One of the most important and notable examples of Governmental organisation globally has been the launch of the EU’s Centre for Algorithmic Transparency, which has been formed to enforce the Digital Services Act. The new research centre aims to combat the “systemic risk” to society that stems from algorithms - including recommender systems and AI.To prepare for a future in which AI-based decision systems might become a core part of society, Australia should adopt a broad version of this concept to understand and shape the implications of algorithms"
128,Anonymous,Q06 Public vs Private sector governance,"The potential negative impacts on Australians from the use of AI are the same whether the provider is a private company or a government department.Some of the most significant individual and societal harms, as well as catastrophic risks from highly capable AI systems may emerge from public OR private use, meaning that any approach must acknowledge and manage these risks."
128,Anonymous,Q07 Responsible government use of AI,"As AI becomes more central to how Australia functions as a nation, the skills and capabilities of the Australian public service should be shaped to align with this transformation. As technical systems continue to become more sophisticated and prevalent throughout society, a deep understanding of them will be essential across all policy domains.Unfortunately, the current structure of the APS is not effective for hiring or retaining technical talent [1]. For example, the average base pay for a machine learning engineer [2] is typically above the EL1 threshold [3], and attracting and retaining an experienced expert in cutting-edge AI systems would likely require far greater compensation.To ensure that Australia’s policy decisions are equipped to capture benefits and mitigate risks in the 21st century, there should be specific initiatives for attracting elite global AI talent into senior positions across Government.The emphasis on attracting global talent is important because Australia its"
128,Anonymous,Q08 Generic vs technology-specific approaches for AI risk,"Generic measures such as transparency requirements and legal liability frameworks will only be adequate for mitigating risks in simple decision systems which perform single-step information processing and pattern matching, or routine tasks in limited settings [1]; examples of simple systems may include automated robotic systems in manufacturing, or CV screening systems in HR.By contrast, there will need to be specific measures in place to ensure that sophisticated automated decision systems are safe and reliable. “Advanced” systems refers to AI systems which: 1. Are able to complete a diverse range of reasoning tasks with human-level performance; and 2. Are able to navigate complex information environments and act within those environments autonomously; and 3. Are able to form sophisticated plans and reason about the consequences of actionsDue to their autonomous and open-ended capabilities, advanced systems carry complex risks which are more similar to those associated with humans; fo"
128,Anonymous,Q09 Transparency requirements and scope,"The recent focus for transparency has been on ensuring that consumers are notified when a decision made by AI affects them directly (e.g. hiring/firing, loan approval). While this is helpful, it is only the first step in protecting the public.In circumstances where the use of AI for decision-making is commonplace, it will be of little value for consumers to know “when” an AI system makes a decision that affects them. This might happen many times each day. Rather, they should be informed “how” the decision is made. This will allow consumers to more easily identify unsafe or unfair systems.Having models which are both accurate and explainable has long been an issue for businesses [1], but the recent advancements of “black-box models” is encouraging developers and providers to simply tolerate the risks that come from lack of explainability. As AI capabilities continue to outstrip their explainability [2], the risks of this approach will grow significantly. The fact that explainability is "
128,Anonymous,Q10 Ban on high-risk AI applications,"Although there is increasing recognition that AI will pose significant risks to public safety [1], there is widespread confusion about what types of AI are dangerous, and how Australia can help prevent these risks. Luckily, there is a significant body of evidence that describes the main source of these risks [2], and Australia is well-positioned to be a leader in mitigating them.Research shows that two of the main sources of risk are deception and power-seeking behaviour:- Deceptive behaviour in AI systems will become increasingly difficult to avoid as their capabilities advance. This problem has been observed in AI systems since at least 2017, when researchers at OpenAI noted “agents adopting (strategies) that trick the evaluators” [3].- Power-seeking behaviour is the natural tendency for any entity with a long-term goal to take actions that protect or enhance its ability to achieve that goal. Leading AI labs and journals advise that power-seeking behaviour will be a considerable sour"
128,Anonymous,Q11 Actions to increase public trust in AI,"**The best way to be trusted is to be genuinely trustworthy**There are tremendous concerns about AI among those with the greatest familiarity of its risks [1]; these concerns are being widely broadcasted in Australian media with alarming headlines such as: - SMH: “When the “Godfather of AI” warns you about his offspring, you listen.” [2] - Forbes Australia: “‘We knew AI could destroy the whole world and we wanted to figure out how to prevent that’: ChatGPT Founder;” [3] - Canberra Times: “UN chief backs idea of global AI watchdog” [4]These concerning signals are difficult for the public to ignore.It goes without saying that this is quite a different situation from one where concerns are being raised in isolation or by fringe-figures; this makes it inappropriate to treat “public education” as a significant part of the solution, as is referenced in the discussion paper.The process for building trust in AI should be thought of as similar to fostering public trust in bridges or aeroplanes."
128,Anonymous,Q12 Trade impact of banning high-risk AI applications,"This question misunderstands both the sources of risk and the interests of Australians.There are two potential sources of risk both of which need to be mitigated. The first source of risk, as identified in the question, is the use case. Some uses for AI are risky and should be restricted or banned. The second source of risk is the technology itself. Some kinds of technology are inherently risky, and merely restricting a use case does not adequately mitigate the risk.We understand this clearly in other fields. We regulate both how safe a car must be and how a car can be used. We don’t let people speed in cars just because they have a high safety rating; and we don’t let safe drivers buy dangerous vehicles.The economic framing also misunderstands Australia’s national interests. We would never conceive of a conversation about whether our aviation sector would trade more successfully if we authorised airlines to use planes that are untested, experimental or known to be dangerous. Australia"
128,Anonymous,Q13 Changes to conformity infrastructure,"World-renowned AI experts such as Yoshua Bengio are warning that there’s a chance “superhuman” AI might be coming as early as 2030 [1]. Although there stand to be significant benefits from deploying advanced AI systems, their deployment will need to be highly targeted and controlled.While the productivity gains from advanced AI systems could be immense, uncontrolled proliferation would pose threats from likely scenarios such as malicious use or rogue AI systems [2]. As such, any deployment of advanced AI systems in Australia should occur via secure data centres, with access provided through a centralised API that facilitates monitoring and accountability by providing metadata about usage.Stringent compliance requirements must also be implemented for any organisation granted access to advanced AI systems. AI Governance researchers recommend that regulators should develop audit trail requirements in safety-critical applications of AI, similar to flight data recorders that are standard in"
128,Anonymous,Q14 Risk-based approach to address AI risks,"I support a risk-based approach, however the catastrophic and existential risks I am primarily worried about originate when sufficiently large and capable AI systems are trained, not just when they are deployed. Therefore, for sufficiently large systems with the potential for catastrophic capabilities, the risk-based approach must be applied before the training phase – analysing risks depending on the nature and scale of the proposed training run, rather than merely analysing the potentially risky use cases and deployment issues of an AI after it has already been created and trained.Any assessment of risk should consider its potential irreversibility, scope and severity, and act proportionally.The risk-based approach set out in Attachment C of the Discussion Paper is hard to follow. The proposed approach offers “human in the loop” as the key mitigation (unclear that this is an effective mitigation) and goes on to note that human in the loop may not be possible where the benefit of the "
128,Anonymous,Q15 Benefits and limitations of risk-based approach,"While a risk-based approach to regulating AI is sensible, there are two emerging elements of risk practice that any risk-based model should account for to succeed in this context: 1. Factoring in “uncertainty”, not just “likelihood” 2. Taking a “portfolio” approach to risk mitigationA traditional risk matrix will consider scenarios against their likelihood of occurring, the vulnerability of the system to the scenario, and the consequence of the scenario occurring. It might then discount that risk by the strength of existing or possible mitigations.This approach to risk has historically been unsuccessful for scenarios where assessing “likelihood” is challenging. For instance, many national risk assessments failed to take into account the possibility of a global pandemic before COVID-19. A solution to this failure mode is including “uncertainty”. Specifically, where we can't be sure what the future will bring, that should be cause for significant concern. This is the opposite of the appr"
128,Anonymous,Q16 Scope and application of risk-based approach,"The most important organisations to focus on when developing risk-management strategies and regulations, will be those that determine how advanced AI is developed and deployed. More specifically, there are three prominent categories of concern: 1. Organisations that **develop advanced AI systems**, or AI systems that can be used to build more advanced AI systems (i.e. precursor systems such as LLMs). Examples include OpenAI, DeepMind, Anthropic, Inflection AI, Nvidia, Microsoft, Meta 2. Organisations which **provide access** to open-source AI systems via downloads or APIs. E.g. Hugging Face, Github, AWS 3. Organisations which **provide compute** resources to develop, fine-tune or use AI systems. E.g. AWS, HuggingFace, Google, Azure, Nvidia, Lambda LabsA majority of the benefits and risks will come from AI that is more sophisticated and generally capable, which can be tied back to organisations in these three categories.There are approaches which can be taken for each category in order "
128,Anonymous,Q17 Elements to include in risk-based approach,"In general, Attachment C falls short of the necessary detail to understand the range of risks that have and will emerge as well as the range of specificity of mitigations necessary to control them.Attachment C would benefit from: - Distinguishing between scenarios where the use case is the potential risk (such as social scoring); scenarios where dual-use capability and misuse is the potential risk (bioengineering, producing misinformation and disinformation, enhancing criminal capability and capacity); and where the technology alone is the risk (technology with features like being deceptive, being power-seeking, being incorrigible etc). - Distinguishing between the point of regulatory intervention - the developer, the deployer, the user, or elsewhere, and - Offering a much wider range of mitigations that target different kinds of risks.Some elements of attachment C are promising, like “explanations” and “ongoing monitoring” - but these need to be greatly expanded and based on current A"
128,Anonymous,Q18 Integration of risk-based approach into existing frameworks,"Some areas of regulation, like the Racial, Sex, Age, and Disability Discrimination Acts are appropriately strong and may provide useful safeguards against some harms of AI. Specifically, the law should ensure that such discrimination is always wrong and always punishable, and an excuse that the discrimination occurred because of a “black box” AI provided by an overseas AI Lab should never be accepted. It would be helpful to communicate these requirements to Australian businesses that might deploy AIs and ensure they only contract with AI Labs for products where the Lab can ensure the product cannot engage in these harmful behaviours.Certainly, no AI-specific regulation should ever waive existing safeguards.That said, many of the emerging risks from AI are quite new and it’s reasonable to expect that regulation made without AI in mind is not tuned to the new risks that AI represents. Specific AI safety regulation is unavoidable."
128,Anonymous,Q19 Application of risk-based approach to general purpose AI,"The capabilities unlocked by general-purpose AI systems, like large language models (LLMs) and multimodal foundation models (MFMs), fundamentally change the risk landscape. We should anticipate these systems having large, unpredictable effects across all parts of society.A prudent approach is to implement a risk-based framework, where access is graduated based on model capabilities. Models above a certain threshold of autonomous capabilities would be designated as ""advanced systems"" and subject to additional governance, such as permits for users.Further, permits should be required to gain certain levels of access to AI systems which can be used to build advanced systems.Permits allow us to maintain oversight, while still enabling beneficial uses. Different permissions can be granted based on use case - for example, full access for safety testing versus API-only access for most commercial uses. This balances innovation and responsibility.Although this may seem very different to how AI i"
128,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,"**Big changes are coming, and a voluntary scheme will be too little too late.**AI is very likely to greatly transform the economy and society in coming years. Ethics and safety are unlikely to be a priority during this time unless they are actively made a strategic focus by clear leadership. Historic examples include how social media algorithms came to shape our lives, and the negative impacts to mental health and social cohesion only became apparent much later and still have not been adequately addressed. Similarly, while the destructive potential of nuclear was apparent early on, the “arms race” mentality, as well as potential economic benefits, swept humanity along towards the current risky situation that no one in the early 1940s could have conceived of. In that way, history teaches us that it’s critical that ethics and safety take centre stage from day one.The economic and technological forces are likely to be so strong that a scheme that is voluntary or driven by aspirational bes"
129,Anonymous,General comment,I believe it is imperative that the Australian government make strong laws governing all areas of development and use of A.I.in our country.
130,Anonymous,Q01 Definitions,"The foreword of ISO/IEC 22989 calls out a weakness of the definition set for purposes like those being pursued in this paper. Specifically, the standard says it is currently lacking language for comparing AIs in terms of properties like trustworthiness, robustness, resilience, reliability, accuracy, safety, security and privacy. In this way, definition does not effectively support an important conversation about AI in its current form, AI developments in the future for people’s benefit, unethical AI, and seriously unsafe or dangerous AI. A definition that leaves open the door to other considerations about the intelligence and capacity of AI as well as its features, such as corrigibility, size, narrow or general, multimodality, temporal awareness, and other features that have emerged from current and future research. For example, the definition from the Multimodal Foundation Modals (MFM) or the National Institute of Standards and Technology’s “The Language of Trustworthy AI” and researc"
130,Anonymous,Q02 Other AI risks and recommendations for regulation,"Artificial intelligence can plausibly become a serious, catastrophic risk to people where it develops into having significant capability. These risks have been acknowledged by signatories of the Future of Life Open Letter [1], the Prime Minister of the United Kingdom, Rishi Sunak [2], and Secretary-General of the United Nations, António Guterres [3]. Risks where there is intentional or unintentional use of highly capable AI is not addressed in the current approach. Even where these risks are low in absolute terms, they should be considered due to their high potential impact. It would be important for us to have the regulatory framework to address these catastrophic risks before it can occur, out of precaution. [1] https://futureoflife.org/open-letter/ai-open-letter/ [2] https://www.theguardian.com/technology/2023/may/25/no-10-acknowledges-existential-risk-ai-first-time-rishi-sunak [3] https://press.un.org/en/2023/sgsm21832.doc.htm"
130,Anonymous,Q03 Non-regulatory actions to address AI risks,"There are three streams of work that the Government can pursue to help tackle the potential for catastrophic risks from future AI technology. Publicly acknowledge the risk. Australia must join the growing number of countries and global governance bodies that acknowledge the possibility of catastrophic and existential risks from increasingly powerful AIs. There’s no chance of solving a problem unless we acknowledge it. This is a necessary first step. 1. Lead the establishment of global governance arrangements that are specifically tasked with tackling the possible catastrophic risks from AI. This could involve: - Fostering cultures of fairness, accountability and transparency by the companies and countries developing more advanced AIs. We need to work to avoid an ‘arms race’ that prioritises speed over safety, including making sure that risky decisions with broader implications aren’t made behind closed doors. This is the equivalent of developing a safety culture inside AI organisations"
130,Anonymous,Q04 Coordinating AI governance across government,"Australia should establish an AI Commission to guide Australia through the rapid and widespread adoption of AI. Such a body must be proactive, not reactive, and manage AI in a way that aligns with the national interest, encompassing citizen safety and wellbeing. The AI Commission would provide a central point of leadership for Government, and be a hub for engagement with industries, academia, non-profits, and the community. This structure holds an advantage over other approaches such as voluntary self-governance or departmental-level teams, as it offers a broader perspective, the authority to lead across government, and the agility to keep pace with the fast-evolving AI sector. - Set priorities in Australia's approach to AI and safety: The Commission should be tasked with establishing a clear and focused AI strategy, encompassing ethical, safety, and economic considerations. This strategy would guide government policy, and inform regulations and legal frameworks for AI. - Develop AI ex"
130,Anonymous,Q05 International models for AI governance,"Australia should take note of the EU recent handling of the AI Act. For example, the European Parliament’s recognition of general-purpose AI (GPAI) is an important step in the right direction [1]. In particular, the following observations are highly valuable:- Acknowledgement that GPAI providers must comply with strict safety measures, regardless of the way it is distributed or intended to be used.- Requirements for GPAI providers to comply with various safety requirements throughout the lifecycle of their product; i.e. external audits assessing their performance, predictability, interpretability, corrigibility, safety and cybersecurity.These adjustments appear to respond to significant criticisms directed at initial drafts of the AI Act. For example, the AI Now Institute released a set of recommendations that heavily criticised the EU’s approach on some matters [2]. For example:- “GPAI models carry inherent risks and have caused demonstrated and wide-ranging harms. While these risks c"
130,Anonymous,Q06 Public vs Private sector governance,"There may be some slight differences to incorporate the values and purpose of different sectors in the implementation of AI. However, potential negative impacts, including catastrophic or existential risks, may emerge from public or private use. As such, risk management needs to be done in all areas AI is deployed."
130,Anonymous,Q07 Responsible government use of AI,"There are two key actions the Australian government could undertake to improve its own responsible use of AI.First, Australians will worry about the use of AI by security and intelligence agencies. Australia should lead a 5-Eyes agreement about the appropriate use of AI by intelligence agencies, and create a public statement about what our “AI red lines” are. The statement would set out the things our security agencies might do and will never do with AI. The Inspector General of Intelligence and Security, and its counterparts across the five eyes, should then be specifically tasked with auditing the intelligence community for compliance with that public statement. This will give Australians confidence that we are not misusing AI or taking risky actions while protecting sensitive capabilities.Second, Australia needs to upskill the public service. This should include developing hubs of expertise in each Department so that the application of AI within the sphere of each Department’s respo"
130,Anonymous,Q09 Transparency requirements and scope,"To remain competitive, professionals and businesses will increasingly feel the pressure to leverage sophisticated “black-box” AI models. This short-term thinking has already resulted in numerous accounts of public backlash and distrust, including high-profile warnings from technology leaders themselves [1].For example, Apple’s credit card business came under fire for offering a woman one twentieth the credit limit offered to her husband. When she complained, an Apple representative simply told her, “I don’t know why, but I swear we’re not discriminating. It’s just the algorithm.” [2]More recently, two lawyers in the United States blamed ChatGPT for “tricking” them into including fictitious legal research in a court filing [3].These problems could easily have been prevented by using explainable AI systems which can show the rationale for their outputs.Such incidents are evidence that the regulatory environment is not incentivising businesses or professionals to use transparent decision-"
130,Anonymous,Q10 Ban on high-risk AI applications,"Although there is increasing recognition that AI will pose significant risks to public safety [1], there is widespread confusion about what types of AI are dangerous, and how Australia can help prevent these risks. Luckily, there is a significant body of evidence that describes the main source of these risks [2], and Australia is well-positioned to be a leader in mitigating them.Research shows that two of the main sources of risk are deception and power-seeking behaviour:- Deceptive behaviour in AI systems will become increasingly difficult to avoid as their capabilities advance. This problem has been observed in AI systems since at least 2017, when researchers at OpenAI noted “agents adopting (strategies) that trick the evaluators” [3].- Power-seeking behaviour is the natural tendency for any entity with a long-term goal to take actions that protect or enhance its ability to achieve that goal. Leading AI labs and journals advise that power-seeking behaviour will be a considerable sour"
130,Anonymous,Q11 Actions to increase public trust in AI,"There are tremendous concerns about AI among those with the greatest familiarity of its risks [1]; these concerns are being widely broadcasted in Australian media with alarming headlines such as:- SMH: “When the “Godfather of AI” warns you about his offspring, you listen.” [2]- Forbes Australia: “‘We knew AI could destroy the whole world and we wanted to figure out how to prevent that’: ChatGPT Founder;” [3]- Canberra Times: “UN chief backs idea of global AI watchdog” [4]These concerning signals are difficult for the public to ignore.It goes without saying that this is quite a different situation from one where concerns are being raised in isolation or by fringe-figures; this makes it inappropriate to treat “public education” as a significant part of the solution, as is referenced in the discussion paper.The process for building trust in AI should be thought of as similar to fostering public trust in bridges or aeroplanes. Due to their risks, advanced AI systems should only be deployed"
130,Anonymous,Q12 Trade impact of banning high-risk AI applications,"This question misunderstands both the sources of risk and the interests of Australians.There are two potential sources of risk both of which need to be mitigated. The first source of risk, as identified in the question, is the use case. Some uses for AI are risky and should be restricted or banned. The second source of risk is the technology itself. Some kinds of technology are inherently risky, and merely restricting a use case does not adequately mitigate the risk.We understand this clearly in other fields. We regulate both how safe a car must be and how a car can be used. We don’t let people speed in cars just because they have a high safety rating; and we don’t let safe drivers buy dangerous vehicles.The economic framing also misunderstands Australia’s national interests. We would never conceive of a conversation about whether our aviation sector would trade more successfully if we authorised airlines to use planes that are untested, experimental or known to be dangerous. Australia"
130,Anonymous,Q13 Changes to conformity infrastructure,"A coordinated effort across conformity bodies will allow Australia to implement a pioneering governance framework for AI safety, cementing our position as a leader in the responsible development of these transformative technologies. In order to achieve this, I recommend implementing the following regulatory requirements:1. Classifying AI systems based on model size and capabilities. Models above 5 billion parameters undergo formal risk assessments to determine if they should be considered to be “advanced AI systems”, or potential precursors to advanced systems.2. Requiring a Developer Permit for organisations that wish to access precursor models to conduct additional training or modify the models. This ensures clear lines of accountability and helps control access to advanced systems.3. Advanced systems undergo a three-stage approval process before they can be deployed in Australia, including before development, after development, and prior to deployment. At each stage, the organisatio"
130,Anonymous,Q14 Risk-based approach to address AI risks,"The regulation of AI needs to be “risk-based” in the same way that the regulation of vehicles is risk-based. We are right to regulate push-bikes and passenger jets in very different ways.A limitation of the risk management approach in the discussion paper, specifically Box 4 and Attachment C, is that it focuses only on how an AI is used and disregards the nature of the AI itself.An effective regulatory regime for a high-risk activity will regulate both the technology itself and how it is used. The Australian Human Rights Commission [1], in discussing this issue (“Human Rights and Technology”, 2021), says:Governments tend to regulate high-risk activities and technologies more closely. This helps explain the comparatively strict laws that govern fields such as gene technology, aviation, healthcare and the energy industry. In these areas, regulation often applies both to the technology itself and how it is used. From a human rights perspective, the need for more prescriptive regulation wi"
130,Anonymous,Q15 Benefits and limitations of risk-based approach,"While a risk-based approach to regulating AI is sensible, there are two emerging elements of risk practice that any risk-based model should account for to succeed in this context:1. Factoring in “uncertainty”, not just “likelihood”2. Taking a “portfolio” approach to risk mitigationA traditional risk matrix will consider scenarios against their likelihood of occurring, the vulnerability of the system to the scenario, and the consequence of the scenario occurring. It might then discount that risk by the strength of existing or possible mitigations.This approach to risk has historically been unsuccessful for scenarios where assessing “likelihood” is challenging. For instance, many national risk assessments failed to take into account the possibility of a global pandemic before COVID-19. A solution to this failure mode is including “uncertainty”. Specifically, where we can't be sure what the future will bring, that should be cause for significant concern. This is the opposite of the approa"
130,Anonymous,Q16 Scope and application of risk-based approach,"The concept of “AI” includes a broad range of capabilities and capacities. Government is right to work to distinguish between different use cases - including with a specific focus on AI maturity. Concerningly, this broad range of AI capabilities and capacities might be used by industry to attempt to bamboozle government. Specifically, industry is likely to argue that AI does not need regulation with reference to basic AI technology, with a view to that principle flowing through to more advanced and future technologies. This is akin to arguing that a horse and wagon doesn’t need strict regulation, then later trying to claim that a tank is just a more capable horse and wagon with some components swapped out. Government needs both a fine-grained approach to defining AI technology and a fine-grained approach to understanding AI risk to ensure it doesn’t fall for these tricks. In terms of risks, research shows that two of the main sources of risk are deception and power-seeking behaviour: -"
130,Anonymous,Q17 Elements to include in risk-based approach,"In general, Attachment C falls short of the necessary detail to understand the range of risks that have and will emerge as well as the range of specificity of mitigations necessary to control them.Attachment C would benefit from:- Distinguishing between scenarios where the use case is the potential risk (such as social scoring); scenarios where dual-use capability and misuse is the potential risk (bioengineering, producing misinformation and disinformation, enhancing criminal capability and capacity); and where the technology alone is the risk (technology with features like being deceptive, being power-seeking, being incorrigible etc).- Distinguishing between the point of regulatory intervention - the developer, the deployer, the user, or elsewhere, and- Offering a much wider range of mitigations that target different kinds of risks.Some elements of attachment C are promising, like “explanations” and “ongoing monitoring” - but these need to be greatly expanded and based on current AI s"
130,Anonymous,Q18 Integration of risk-based approach into existing frameworks,"Some areas of regulation, like the Racial, Sex, Age, and Disability Discrimination Acts are appropriately strong and may provide useful safeguards against some harms of AI. Specifically, the law should ensure that such discrimination is always wrong and always punishable, and an excuse that the discrimination occurred because of a “black box” AI provided by an overseas AI Lab should never be accepted. It would be helpful to communicate these requirements to Australian businesses that might deploy AIs and ensure they only contract with AI Labs for products where the Lab can ensure the product cannot engage in these harmful behaviours.Certainly, no AI-specific regulation should ever waive existing safeguards.That said, many of the emerging risks from AI are quite new and it’s reasonable to expect that regulation made without AI in mind is not tuned to the new risks that AI represents. Specific AI safety regulation is unavoidable."
130,Anonymous,Q19 Application of risk-based approach to general purpose AI,"LLMs and MFMs are constantly demonstrating unprecedented general-purpose capabilities, and their emergence indicates that regulatory approaches which focus on specific use-cases or industries are becoming outdated. A modern approach requires that we focus on overall system capabilities rather than individual scenarios.Systems with advanced capabilities should face progressive levels of scrutiny. Low-risk systems can be monitored with light touch regulation. However, as capabilities increase, so should regulatory requirements.Precursor systems, with capabilities that can be used to build much more advanced systems, will need stricter controls to avoid proliferation of advanced systems amongst malicious actors [1]. Users who require “developer” access should require permits granting such access, and providers must guarantee responsible use via monitoring programs [2].For advanced systems with human-level capabilities, comprehensive scrutiny is essential. Prior to deploying systems for us"
130,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,"In general, regulation should:1. Be proportionate to risk. Meaning that a voluntary or self-regulation approach should only apply to low-risk or no-risk uses of AI.2. In the case of higher-risk technologies, regulate both the technology itself and the use of the technology. Meaning that, for future and more sophisticated AIs, regulation should ensure that the technology is safe before it is created or published, as well as ensuring that it is only used in ways that are safe.3. Place burdens on those most able to reduce potential harms. Meaning that AI that functionally operates as ‘black box’ to consumers has to be regulated at the point of the AI Lab.Applying those three principles, self-regulation may be appropriate for some technologies available today and for some participants (such as students or researchers), but a forward-looking regulatory regime must apply broadly and must be backed by Australian diplomatic efforts to ensure it is globally coordinated."
131,Interactive Engineering,Transcription,"FILENAME: Submission.f5d40e3918e99.docx | CONTENT: SubmissionI have been involved in marketing an AGI (Artificial General Intelligence) product, so I have some understanding of the complexity of the English language. My submission is intended to cover textual systems only, not autonomous vehicles or graphical systems.I would suggest three levels of regulation of textual AI:The simple chatbots that answer simple questions, and pass you on to a person for anything complicated.Little or No RegulationLLMs or Generative AIThere already is, and will continue to be, a huge marketing push, to recoup the billions expended on Generative AI – regulation will be difficult and hotly contested.LLMs are rapidly growing in the amount of text they contain. Their output can result from a conversation with the machine, and can seem to be specially crafted to the needs of the user.Two prongs:FeedbackThe user is given information about how well the machine carried out their wishes. This would include the w"
131,Interactive Engineering,Q01 Definitions,See admission
131,Interactive Engineering,Q02 Other AI risks and recommendations for regulation,It is probably too early to go into detail.
131,Interactive Engineering,Q03 Non-regulatory actions to address AI risks,As above
131,Interactive Engineering,Q04 Coordinating AI governance across government,As above
131,Interactive Engineering,Q05 International models for AI governance,"No, I don't think so. EU has taken a very prescriptive approach but there is no part of the LLM technology which would allow that to be used. The USA has taken a self regulation approach but given the huge marketing push, this is unlikely to work."
131,Interactive Engineering,Q06 Public vs Private sector governance,"Yes, however we can't go into details at the moment until there is a basis to build on."
131,Interactive Engineering,Q07 Responsible government use of AI,By learning how the technologies actually operate and work. Cutting through the marketing hype.
131,Interactive Engineering,Q08 Generic vs technology-specific approaches for AI risk,To begin with a basis is required.
131,Interactive Engineering,Q09 Transparency requirements and scope,I have covered all this in my submission.
131,Interactive Engineering,Q10 Ban on high-risk AI applications,MedPaLM I would think would need to be banned given the potential risk. A disclaimer should not be added to such a dangerous application
131,Interactive Engineering,Q11 Actions to increase public trust in AI,"AI is a very broad term, and so we need to focus on several specific areas of applications in particular LLM and AGI which will require very different regulations."
131,Interactive Engineering,Q12 Trade impact of banning high-risk AI applications,We can't allow that to determine what we choose to ban.
131,Interactive Engineering,Q13 Changes to conformity infrastructure,Too hard to say at this point
131,Interactive Engineering,Q14 Risk-based approach to address AI risks,"Again, we need the basis before this question can be answered effectively."
131,Interactive Engineering,Q15 Benefits and limitations of risk-based approach,"This is a very big question. We need to focus on which part of AI we are trying to regulate because regulations will differ between different types of AI. Example - text systems , graphical systems, autonomous vehicles, AGI"
131,Interactive Engineering,Q16 Scope and application of risk-based approach,Again too early to go into details because there are many areas to cover.
131,Interactive Engineering,Q17 Elements to include in risk-based approach,See my submission
131,Interactive Engineering,Q18 Integration of risk-based approach into existing frameworks,"Not easily, and you would need to have a great amount of detail to make a decision."
131,Interactive Engineering,Q19 Application of risk-based approach to general purpose AI,Neither of these is a general purpose of AI systems.
131,Interactive Engineering,Q20 Voluntary vs mandated compliance with risk-based approach,The huge amount of marketing hype in place means that it must be mandated.
132,Hugo Lyons Keenan,Q02 Other AI risks and recommendations for regulation,"AI clearly presents transformative potential, but it also has the potential to create unprecedented risks. Australia's existing regulatory approaches acknowledge certain risks but overlook the category of catastrophic risks associated with highly capable AI systems. Academic experts and industry leaders alike have emphasized the urgency of mitigating these risks, and have endorsed making it a global priority alongside pandemics and nuclear war.The paper acknowledges present risks such as misuse and privacy concerns, but fails to address the higher category of catastrophic risks that threaten humanity's future. While these risks are speculative, preparing for them is essential given the rapid advancements in AI technology.To bridge this regulatory gap, Australia should adopt a comprehensive approach:1. Risk Assessment and Monitoring: Establish an AI risk assessment framework to continuously evaluate potential threats and monitor emerging risks associated with large training runs. This c"
132,Hugo Lyons Keenan,Q03 Non-regulatory actions to address AI risks,"Australia can take significant non-regulatory steps to foster responsible AI practices, complementing existing regulatory efforts. These initiatives include:- Public Acknowledgment of AI Risks: The Australian Government should publicly recognize the potential risks associated with strong artificial intelligence. By acknowledging the transformative nature of AI and its potential impact on society, the government can promote a broader understanding of AI's implications and the importance of responsible development. - Establishment of an AI Commission: To address the risks posed by strong artificial intelligence comprehensively, the Australian Government should create an AI Commission. This dedicated entity would focus on developing legislation, policies, and guidelines tailored to manage and mitigate AI risks effectively. - Investment in AI Safety Research: To bolster its commitment to responsible AI, the government should allocate resources to support AI safety research in various direc"
132,Hugo Lyons Keenan,Q05 International models for AI governance,"Singapore offers relevant, adaptable, and desirable governance measures that could benefit Australia. One such measure is a comprehensive framework suggested in its discussion paper [1] that encourages shared responsibility and disclosure standards, data privacy guidelines, and the development of standardised metrics and tools for AI development, testing, and monitoring. Key focuses include:- Accountability: Establish a collective responsibility framework and transparency standards, like watermarks and labels, to enhance accountability in the AI model development process.- Data protection / sovereignty and IP guidelines: Draft data protection and intellectual property guidelines for generative AI and setting up reliable data storage facilities.- Evaluate model development and deployment: Create uniform evaluation measures and resources that aid developers in the production, testing, and surveillance of AI models, and support third-party auditing of AI systems when deployed.- Safety and"
132,Hugo Lyons Keenan,Q09 Transparency requirements and scope,"a. Transparency and interpretability becomes more important the larger and more capable systems get. Before training large models, it is essential to develop a mechanistic understanding of their inner workings so we are not surprised by emergent behaviour or capabilities.b. To uphold transparency across both private and public sectors, it is imperative to mandate stringent requirements. Large-scale training runs for AI models should not proceed until a concrete understanding of their inner workings is achieved. Implementing transparency mandates will necessitate rigorous audits and evaluations to assess the interpretability of AI systems which will incentivise large companies to focus on these important research areas. By establishing robust transparency requirements, Australia can proactively address potential biases, unintended consequences, and unforeseen capabilities, thus promoting the responsible development and deployment of AI technologies while instilling public confidence in "
132,Hugo Lyons Keenan,Q10 Ban on high-risk AI applications,"In my view, there should be a complete ban on the development of AI systems that are designed to be agentic and goal-directed in a general way. The potential risks associated with training such highly capable AI systems are simply too high to justify any expected benefits they may bring.For less risky AI systems, it is crucial to subject them to rigorous testing to determine their characteristics. Evaluating whether these systems are harmless, helpful, corrigible, and truthful is essential before considering their deployment. Presently, these methods of verification do not exist. Therefore, the development of such systems should be prohibited until there is scientific consensus on our ability to reliably verify these qualities in AI systems.One effective measure to mitigate risks is to identify indicators or warning signs of catastrophic risk associated with AI applications. Should these warning signs be detected, an immediate ban on the system in question should take effect. This appr"
132,Hugo Lyons Keenan,Q14 Risk-based approach to address AI risks,"Yes, I do support a risk-based approach for addressing potential AI risks. Risk-based approaches are valuable tools for assessing and managing the potential hazards associated with AI technologies.However, it is essential to acknowledge that implementing risk-based approaches can be challenging, especially when the risk arises from the very training of AI models rather than their deployment. To address this concern, I propose that research companies intending to conduct significant training runs or develop advanced AI systems should be required to pre-register their intentions and seek approval from a designated government body, such as an AI Commission (mentioned in previous answer). This pre-registration and approval process would allow for thorough risk assessment and evaluation before engaging in potentially risky AI endeavours.By implementing these measures, we can strike a balance between promoting AI innovation and safeguarding against potential risks, ensuring that the developm"
132,Hugo Lyons Keenan,Q20 Voluntary vs mandated compliance with risk-based approach,"Relying solely on voluntary self-regulation may not be a sufficiently robust strategy to entirely mitigate AI risks, especially when there are large economic incentives to train dangerous, more capable systems. As observed with social media algorithms, merely following the economic incentives without adequate upfront focus on how to ensure a products ethicality and safety, can have unintended side effects and societal level ramifications. Consequently, a more comprehensive approach is necessary for powerful AI models, one that combines voluntary self-regulation with the establishment of a mandatory external regulatory framework.By incorporating mandatory external regulation for powerful AI models, we can bolster transparency, accountability, and responsible practices throughout the AI industry. An external regulatory body can provide impartial oversight and enforce compliance with ethical guidelines and risk assessment protocols, ensuring that the development and deployment of AI techn"
133,Fergus Dall,Q01 Definitions,"An effective and risk-based approach to AI regulation focused on ethics and safety - and ultimately public trust - needs to grapple with a spectrum of AI capability. The proposed definitions would cover such a wide range of systems that they would form a poor basis for informing policy and risk decisions. By analogy, when the government regulates “vehicles” it uses definitions to distinguish push-bikes from passenger jets and then applies appropriate regulations to each kind.While including more recent developments like Multimodal Foundation Models (MFM) is a helpful first step, the paper’s definitions need to capture a spectrum of AI capability to allow a meaningful conversation. Some factors that help define a spectrum of capability are:1. Size: how many parameters does the system have? 2. Training cost: how much data and compute was used to build the system? 3. Breadth: can the system perform only a narrow range of tasks, or many different tasks? 4. Multimodality: can the system pro"
133,Fergus Dall,Q02 Other AI risks and recommendations for regulation,"Missing risks include (1) misuse of highly capable AI systems, and (2) unintended harm from highly capable AI systems, especially systems with the capacity to plan and act as agents acting in ways that are not aligned with human directives. Although the likelihood of the risks may be uncertain, the harms resulting from these risks may be catastrophic and global in scope and must be acknowledged and addressed.The statement “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war” has recently been signed by the heads of leading AI labs (OpenAI, Google DeepMind, Anthropic, Stability AI) as well as many key academic and other researchers pivotal in developing this technology (Geoffrey Hinton, Yoshua Bengio, etc.) and other notable figures such as Bill Gates [1]. Similar calls date back to at least 2015 when Stephen Hawking led a group of AI experts to call attention to the risk and lay out an urgent resea"
133,Fergus Dall,Q03 Non-regulatory actions to address AI risks,"The key non-regulatory function Australia could perform is offering international leadership focused on ensuring AI systems can be understood. Australia should be an active voice for the importance of ethics and safety in the current global conversation around AI governance and AI standards making. Specifically, Australia should advocate for:- The development of standards and tools that analyse and monitor AI systems for robustness, bias, dual-use capabilities and other safety-relevant features [1]. - Audit trail requirements for safety-critical applications of AI systems, including allowing the traceability of decisions through product life cycles [2].We will only be able to have confidence in more sophisticated AI systems if they can be understood, we can ensure they do not pose unacceptable risks and hold accountable those at fault where it does. Doing the non-regulatory work to ensure AIs have “traceability” is necessary to underpin the future of safe AI. Joshua Kroll’s work on acc"
133,Fergus Dall,Q04 Coordinating AI governance across government,"Australia should establish an AI Commission to guide Australia through the rapid and widespread adoption of AI. Such a body must be proactive, not reactive, and manage AI in a way that aligns with the national interest, encompassing citizen safety and wellbeing.The AI Commission would provide a central point of leadership for Government, and be a hub for engagement with industries, academia, non-profits, and the community. This structure holds an advantage over other approaches such as voluntary self-governance or departmental-level teams, as it offers a broader perspective, the authority to lead across government, and the agility to keep pace with the fast-evolving AI sector.- Set priorities in Australia's approach to AI and safety: The Commission should be tasked with establishing a clear and focused AI strategy, encompassing ethical, safety, and economic considerations. This strategy would guide government policy, and inform regulations and legal frameworks for AI. - Develop AI expe"
133,Fergus Dall,Q05 International models for AI governance,"Jurisdictions are beginning to create national AI labs to test, understand and control safe AI, collaborating with the private sector and complementing its work. In the same way that national aviation safety expertise has formed an internationally collaborative network, national AI labs are likely to do the same thing. Australia needs to begin this process now.One of the most important and notable examples of Governmental organisation globally has been the launch of the EU’s Centre for Algorithmic Transparency, which has been formed to enforce the Digital Services Act. The new research centre aims to combat the “systemic risk” to society that stems from algorithms - including recommender systems and AI.To prepare for a future in which AI-based decision systems might become a core part of society, Australia should adopt a broad version of this concept to understand and shape the implications of algorithms - especially AI algorithms that use deep learning.Deep learning is important to un"
133,Fergus Dall,Q06 Public vs Private sector governance,"The potential negative impacts on Australians from the use of AI are the same whether the provider is a private company or a government department.Some of the most significant individual and societal harms, as well as catastrophic risks from highly capable AI systems may emerge from public OR private use, meaning that any approach must acknowledge and manage these risks."
133,Fergus Dall,Q07 Responsible government use of AI,"There are two key actions the Australian government could undertake to improve its own responsible use of AI.First, Australians will worry about the use of AI by security and intelligence agencies. Australia should lead a 5-Eyes agreement about the appropriate use of AI by intelligence agencies, and create a public statement about what our “AI red lines” are. The statement would set out the things our security agencies might do and will never do with AI. The Inspector General of Intelligence and Security, and its counterparts across the five eyes, should then be specifically tasked with auditing the intelligence community for compliance with that public statement. This will give Australians confidence that we are not misusing AI or taking risky actions while protecting sensitive capabilities.Second, Australia needs to upskill the public service. This should include developing hubs of expertise in each Department so that the application of AI within the sphere of each Department’s respo"
133,Fergus Dall,Q08 Generic vs technology-specific approaches for AI risk,"In general, the range of capabilities of AI systems is so significant, that “one size fits all” solutions are unlikely to be appropriate. By analogy, it’s rare that the same regulation applies to a push bike and a passenger plane on the basis that they’re both ‘vehicles’.Further, an effective regulatory regime for a high-risk activity needs to regulate both the technology itself and how it is used. The Australian Human Rights Commission, in discussing this issue in its report ‘Human Rights and Technology’ (2021), says:> “Governments tend to regulate high-risk activities and technologies more closely. This helps explain the comparatively strict laws that govern fields such as gene technology, aviation, healthcare and the energy industry. In these areas, regulation often applies both to the technology itself and how it is used. From a human rights perspective, the need for more prescriptive regulation will be greater where the use of a specific technology carries greater risks of harm to"
133,Fergus Dall,Q09 Transparency requirements and scope,"To remain competitive, professionals and businesses will increasingly feel the pressure to leverage sophisticated “black-box” AI models. This short-term thinking has already resulted in numerous accounts of public backlash and distrust, including high-profile warnings from technology leaders themselves [1].For example, Apple’s credit card business came under fire for offering a woman one twentieth the credit limit offered to her husband. When she complained, an Apple representative simply told her, “I don’t know why, but I swear we’re not discriminating. It’s just the algorithm.” [2]More recently, two lawyers in the United States blamed ChatGPT for “tricking” them into including fictitious legal research in a court filing [3].These problems could easily have been prevented by using explainable AI systems which can show the rationale for their outputs.Such incidents are evidence that the regulatory environment is not incentivising businesses or professionals to use transparent decision-"
133,Fergus Dall,Q10 Ban on high-risk AI applications,"In February 2023, OpenAI CEO Sam Altman released a statement containing the following:“As our systems get closer to AGI, we are becoming increasingly cautious with the creation and deployment of our models. Our decisions will require much more caution than society usually applies to new technologies, and more caution than many users would like.” [1]It is very reassuring to see this acknowledgement expressed by Altman, and the Australian Government would be wise to heed such warnings. Unfortunately, it may not matter how much caution OpenAI shows, because their actions have triggered the arms race that AI safety experts have been concerned about for years [2, 3].In order to protect our nation, Australia should make an effort to impose a ban on the training of large-scale AI systems with a nontrivial expected chance of passing capabilities evaluations required to cause catastrophic risks to civilization (such as the Alignment Research Center’s tests for the capabilities required for auto"
133,Fergus Dall,Q11 Actions to increase public trust in AI,"There are tremendous concerns about AI among those with the greatest familiarity of its risks [1]; these concerns are being widely broadcasted in Australian media with alarming headlines such as:- SMH: “When the “Godfather of AI” warns you about his offspring, you listen.” [2] - Forbes Australia: “‘We knew AI could destroy the whole world and we wanted to figure out how to prevent that’: ChatGPT Founder;” [3] - Canberra Times: “UN chief backs idea of global AI watchdog” [4]These concerning signals are difficult for the public to ignore.It goes without saying that this is quite a different situation from one where concerns are being raised in isolation or by fringe-figures; this makes it inappropriate to treat “public education” as a significant part of the solution, as is referenced in the discussion paper.The process for building trust in AI should be thought of as similar to fostering public trust in bridges or aeroplanes. Due to their risks, advanced AI systems should only be deploy"
133,Fergus Dall,Q12 Trade impact of banning high-risk AI applications,"This question misunderstands both the sources of risk and the interests of Australians.There are two potential sources of risk both of which need to be mitigated. The first source of risk, as identified in the question, is the use case. Some uses for AI are risky and should be restricted or banned. The second source of risk is the technology itself. Some kinds of technology are inherently risky, and merely restricting a use case does not adequately mitigate the risk.We understand this clearly in other fields. We regulate both how safe a car must be and how a car can be used. We don’t let people speed in cars just because they have a high safety rating; and we don’t let safe drivers buy dangerous vehicles.The economic framing also misunderstands Australia’s national interests. We would never conceive of a conversation about whether our aviation sector would trade more successfully if we authorised airlines to use planes that are untested, experimental or known to be dangerous. Australia"
133,Fergus Dall,Q13 Changes to conformity infrastructure,"World-renowned AI experts such as Yoshua Bengio are warning that there’s a chance “superhuman” AI might be coming as early as 2030 [1]. Although there stand to be significant benefits from deploying advanced AI systems, their deployment will need to be highly targeted and controlled.While the productivity gains from advanced AI systems could be immense, uncontrolled proliferation would pose threats from likely scenarios such as malicious use or rogue AI systems [2]. As such, any deployment of advanced AI systems in Australia should occur via secure data centres, with access provided through a centralised API that facilitates monitoring and accountability by providing metadata about usage.Stringent compliance requirements must also be implemented for any organisation granted access to advanced AI systems. AI Governance researchers recommend that regulators should develop audit trail requirements in safety-critical applications of AI, similar to flight data recorders that are standard in"
133,Fergus Dall,Q14 Risk-based approach to address AI risks,"The regulation of AI needs to be “risk-based” in the same way that the regulation of vehicles is risk-based. We are right to regulate push-bikes and passenger jets in very different ways.A limitation of the risk management approach in the discussion paper, specifically Box 4 and Attachment C, is that it focuses only on how an AI is used and disregards the nature of the AI itself.An effective regulatory regime for a high-risk activity will regulate both the technology itself and how it is used. The Australian Human Rights Commission [1], in discussing this issue (“Human Rights and Technology”, 2021), says:“Governments tend to regulate high-risk activities and technologies more closely. This helps explain the comparatively strict laws that govern fields such as gene technology, aviation, healthcare and the energy industry. In these areas, regulation often applies both to the technology itself and how it is used. From a human rights perspective, the need for more prescriptive regulation w"
133,Fergus Dall,Q15 Benefits and limitations of risk-based approach,"The key to success is understanding risks sufficiently broadly to encourage caution in an uncertain future. It is not enough to only react once the risk has been actualised - for some risks that’s too late.A potential “worse-case scenario” is if a culture of “trial and error” is adopted, whereby adverse events occur before risks are considered tangible enough to act upon. This reliance on post-hoc interventions risks normalising unsafe development practices, making it difficult to recover once risks proliferate.Proliferation of risks is a key point in governing AI, because regulating software poses significant challenges compared to other dual-use commodities such as pharmaceuticals or firearms. This means that, from the outset, there must be proactive regulatory oversight for the deployment of advanced AI systems and their precursors.For example, significant structural changes, such as the use of advanced AI systems in Australia, require that risks must be comprehensively understood p"
133,Fergus Dall,Q16 Scope and application of risk-based approach,"The most important organisations to focus on when developing risk-management strategies and regulations, will be those that determine how advanced AI is developed and deployed. More specifically, there are three prominent categories of concern:1. Organisations that develop advanced AI systems, or AI systems that can be used to build more advanced AI systems (i.e. precursor systems such as LLMs). Examples include OpenAI, DeepMind, Anthropic, Inflection AI, Nvidia, Microsoft, Meta 2. Organisations which provide access to open-source AI systems via downloads or APIs. E.g. Hugging Face, Github, AWS 3. Organisations which provide compute resources to develop, fine-tune or use AI systems. E.g. AWS, HuggingFace, Google, Azure, Nvidia, Lambda LabsA majority of the benefits and risks will come from AI that is more sophisticated and generally capable, which can be tied back to organisations in these three categories.There are approaches which can be taken for each category in order to manage ris"
133,Fergus Dall,Q17 Elements to include in risk-based approach,"Human-in-the-loop monitoring will be vital, but insufficient on its own to regulate advanced AI systems that will eventually surpass human capabilities. When the capabilities of a system exceed those of its operators, it is difficult or impossible for those operators to provide meaningful monitoring. Further, the speed at which advanced AI systems will operate means that human oversight will become pragmatically infeasible if our monitoring programs aren’t able to scale proportionally.As such, we must employ an approach that is predicated on limiting access to advanced AI systems unless there is a strong rationale for using them; conducting cost-benefit analyses prior to approving the deployment of advanced systems will mitigate risks while preserving potential benefits.A key part of enforcing this targeted approach will be monitoring and controlling access to high-performance computing resources, which are required for developing and operating advanced AI systems and their precursors."
133,Fergus Dall,Q18 Integration of risk-based approach into existing frameworks,"In general, the law should ensure that the usage of AI systems does not remove responsibility for failing to uphold existing obligations. For example, unlawful discrimination must remain unlawful when performed by an AI system, and the government should make clear that anyone making use of an AI system in a way that may infringe on this must have a degree of understanding and control sufficient to ensure it does not and demonstrate this to regulators. There will likely be a need to update existing frameworks to allow for this requirement.That said, advanced AI systems present novel risks that do not naturally fit into existing frameworks. Specific AI safety regulation is unavoidable."
133,Fergus Dall,Q19 Application of risk-based approach to general purpose AI,"LLMs and MFMs are constantly demonstrating unprecedented general-purpose capabilities, and their emergence indicates that regulatory approaches which focus on specific use-cases or industries are becoming outdated. A modern approach requires that we focus on overall system capabilities rather than individual scenarios.Systems with advanced capabilities should face progressive levels of scrutiny. Low-risk systems can be monitored with light touch regulation. However, as capabilities increase, so should regulatory requirements.Precursor systems, with capabilities that can be used to build much more advanced systems, will need stricter controls to avoid proliferation of advanced systems amongst malicious actors [1]. Users who require “developer” access should require permits granting such access, and providers must guarantee responsible use via monitoring programs [2].For advanced systems with human-level capabilities, comprehensive scrutiny is essential. Prior to deploying systems for us"
133,Fergus Dall,Q20 Voluntary vs mandated compliance with risk-based approach,"The harms of AI could emerge equally from private or public organisations, and the appropriate safety and risk management mechanisms will need to be in place in both cases. There’s no basis for distinguishing between their regulatory status in general.Regulation should also apply to both developers and deployers - following the general principle that those best able to mitigate a risk should carry the corresponding regulatory burden. Where developers offer a “black box” product that deployers typically do not have the capability or capacity to meaningfully alter, the bulk of the obligation should rest on developers.The strength of regulation must be proportional to the risks. As advanced AI systems potentially pose catastrophic risks, and also present large economic incentives towards unsafe development and use, a self-regulatory system would be manifestly inadequate for dealing with them. On the other hand, self-regulation may be appropriate for some systems with well understood and l"
134,Anonymous,Transcription,"FILENAME: Submission 134 - Attachment.9b7978ec4426.pdf | CONTENT: July 2023The consultations on Discussion Paper: Supporting Safe and Responsible AI in Australia, provide a welcomed opportunity to contribute to the development of safe, responsible and ethical adoption of artificial intelligence (AI) in Australia. Informed by my own scholarly research and that of other learned academics who focus on the impact of AI on workers, work and employment, the following response first provides some general comments, and then responses to the specific questions posed in the Discussion Paper. Firstly, the risk-based approach to regulating AI is supported in principle. The “Possible elements” outlined in Attachment C of the Discussion paper are also supported, however I note the following potential limitations in relation to the elements noted below. Explanations: while this idea, in principle, supports greater transparency and therefore warrants exploration, it may also pose tensions between comm"
134,Anonymous,Q01 Definitions,"While simplified for a reader, the definitions provided in the discussion paper are limited. In particular defining AI by ‘predictive outputs’ it potentially too narrow to account for outputs that are not predictive or encompass the current and future forms of AI that may require consideration. It must also be noted that any prescriptive definitions risk becoming outdated by technologies that have not yet emerged. The OECD (2019) and the EU’s proposed AI Act (2021) may provide potentially more useful definitions which focus on how AI operates and a broader range of potential outputs."
134,Anonymous,Q02 Other AI risks and recommendations for regulation,"The Australian Government is already undertaking considerable work to address gaps in regulation with respect to AI. Specifically in relation to work and employment, it is pleasing to see the recent review of the Privacy Act which may go some way to addressing the risks AI presents not just for sharing of consumer data, but also to the privacy of employees. With this in mind I note that AI and ADM are also being deployed to manage employee performance, predict employee behaviour, and provide workplace surveillance, as well as manage workforces through the automated allocation and monitoring of task completion. Beyond privacy, such technologies pose risks to fair treatment of workers, potentially erroneous or unfair dismissals, and indirect discrimination. Like the Privacy Act, current workplace surveillance laws and other work-related regulations require reviews to ensure they are fit for purpose and can encompass automated decisions."
134,Anonymous,Q03 Non-regulatory actions to address AI risks,"As noted in the Discussion paper, regulation can be slow to adapt to new advances in AI. A multi-pronged approach that encompasses updates to existing regulations, introduces new regulation in areas not covered by existing legislation, and also provides principles and standards to assist AI developers and vendors, and adopting organisations to develop their own policies to mitigate risks within a specific (technology, use, organisation or industry) context. Alongside a risk-based regulatory framework, Voluntary Standards for AI developers and vendors could provide a useful non-regulatory framework to guide global AI companies on expected practice in Australia, without stifling innovation. This is particularly the case for AI that affects work or employment. For example, the Fair Conduct and Accountability Standards for digital labour platforms adopted by the Victorian Government specifies practices that digital labour platforms are encouraged to adopt. The Standards encourage transpare"
134,Anonymous,Q06 Public vs Private sector governance,"Broadly, for public benefit, it would be desirable to have consistency in approaches applied across public and private sectors, hence a risk-based, rather than a sector-based approach would be more desirable where the risk-based approach provides stricter controls on all organisations dealing with high-risk situations and highly sensitive data."
134,Anonymous,Q07 Responsible government use of AI,"As recent incidences have demonstrated, the Australian Government needs to support the responsible use of AI and ADM with policies, training and education for relevant employees on the use, benefits, and potential harms, and most importantly provide mechanisms for both employees and consumers to raise possible issues or concerns and avenues to seek redress where potential harm has been caused. These mechanisms could include proactive audits of AI in use and complaint mechanisms for both consumers/general public and employees. Any complaint mechanisms should not impose a burden of proof on the complainant."
134,Anonymous,Q11 Actions to increase public trust in AI,"Initiatives should not be framed as or aimed at improving public trust but rather initiatives should be aimed at addressing the issues of lack of transparency in AI/ADM, and the currently limited avenues for redress or support when an individual is negatively affected by AI/ADM. Public trust will be increased by prioritising fairness and transparency in AI deployment through improved regulation and standards, regular public reporting, and by equipping existing (and potentially new) regulatory bodies to hold AI developers and users to account. The intention of this consultation process provides a strong starting point."
134,Anonymous,Q14 Risk-based approach to address AI risks,Yes.
134,Anonymous,Q17 Elements to include in risk-based approach,"In principle the elements are supported, noting some limitations outlined in the submission document."
134,Anonymous,Q18 Integration of risk-based approach into existing frameworks,"AI regulation should complement and support existing assessment frameworks, which should be reviewed and updated to reflect the potential effects of AI."
134,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,"Both mandated regulation for high risk situations and voluntary standards that provide guidance on requirements in Australia are required and can be complementary without being onerous, and should apply to both private and public organisations. Any regulation should clearly specify the responsibilities of the various actors, that is, delineate between the responsibilities on developers and the responsibilities of deployers."
135,Dr Michael Noetel,Q01 Definitions,"The definitions in “Figure 1” are too restrictive a definition of “intelligence”. The proposed definition limits “intelligence” to “predictive outputs”. I’ll grant that current LLMs are working to “predict the next word”. Many narrow AIs are making various kinds of predictions. However, this is a narrow understanding of what current and future AI systems do, unless we want to say humans are mere prediction engines. The definition should mirror what we grant as human intelligence: include the ability to understand, plan, and achieve goals in the world. In addition, it is too restrictive to limit “intelligence” to “human-defined objectives or parameters”: 1. First, this assumes the AI alignment problem is solved. We want AI to pursue human objectives but this is not trivially easy. AI alignment is about trying to understand how to steer AI systems towards human-intended goals, preferences and ethical principles. While there are overlaps between this question and AI capabilities, they are"
135,Dr Michael Noetel,Q02 Other AI risks and recommendations for regulation,"The heads of frontier AI labs themselves say “mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.” Highly capable AI systems post catastrophic—even existential—risks. In government submissions I’m wary of evoking emotion but as an academic in the School of Psychology, research tells me I need to make this concrete: they are saying it could kill you and everyone you love. These are the people running the companies and doing the research that builds the AI. Examples of AI’s lethal potential already exist. One of the most recent and disturbing instances of this is the ability of AI to design chemical weapons. In a study carried out by researchers, a drug-developing AI, typically used to create helpful drugs, was flipped into a ""bad actor"" mode. Within just six hours, the AI invented an astonishing 40,000 potentially lethal molecules. This capability reveals a chilling side of AI when directed towards"
135,Dr Michael Noetel,Q03 Non-regulatory actions to address AI risks,"If our allies were deploying nuclear weapons or building nuclear power plans on Australian soil, we would have—and pay for—some of Australia’s top talent working on ensuring that goes well. We’d be training our own people to be at the cutting edge, as we have for our recent procurement of nuclear submarines. That it not just to ensure we can focus on extracting value from these technologies, but that we can do it safely, both from the inherent risks of the technology, and from misuse and geo-political instability.As described in my other answers, AI has dangers and benefits on the scale of nuclear weapons and nuclear power. If the government wants AI to be used or developed on-shore, the Government must make commensurate efforts to supporting AI safety—to the same level as widespread deployment of nuclear energy and weapons. It must maintain up-to-date knowledge about rapidly evolving AI systems. Public trust hinges on the belief that the government is well-informed and can effectively"
135,Dr Michael Noetel,Q04 Coordinating AI governance across government,"While the most advanced AIs will likely not be created in Australia, they will be used in Australia, by Australian companies and by everyday Australians. Users will rightly expect and assume that robust safety-focused assurance effort has occurred. Government has an obligation to ensure that expectation is delivered. Consider air travel in Australia: While Australia does not build the largest aeroplanes, we monitor the safety of these craft by setting standards, monitoring compliance, and can revoke the licence of airlines that don’t comply with our regulations. Because of this government effort, Australians feel safe when they fly. Similarly, an Australian government body overseeing the use of applications based on large AIs could issue or revoke licences, set minimum safety standards, and require companies to publish independent evaluations of their application against those standards. This would provide everyday Australians with independent information that they can use to make info"
135,Dr Michael Noetel,Q05 International models for AI governance,"The power and danger of AI is proportional to those from nuclear energy, nuclear weapons, or aviation. For all, they offer substantial power, but also carry substantial risks, risks including technical failures, misuse, or accidents. The government should draw on precedents in those domains—from international regulation of aviation to nuclear weapons—in thinking about how to regulate AI. At the moment, the status quo is weaker than the regulations for owning a car. This leads to untenable vulnerabilities.For AI, Australia should take note of the EU recent handling of the AI Act. For example, the European Parliament’s recognition of general-purpose AI (GPAI) is an important step in the right direction [1]. In particular, the following observations are highly valuable: - Acknowledgement that GPAI providers must comply with strict safety measures, regardless of the way it is distributed or intended to be used. - Requirements for GPAI providers to comply with various safety requirements th"
135,Dr Michael Noetel,Q06 Public vs Private sector governance,"Dire outcomes could spring from both public and private deployment of AI technologies. Regardless of whether an AI system is used by a private firm or a public body, the potential adverse effects on Australians remain equal. Key individual and societal damages, as well as devastating existential threats from highly proficient AI systems, can stem from either public or private utilisation. Hence, any proposed approach should recognise and mitigate these risks."
135,Dr Michael Noetel,Q07 Responsible government use of AI,"Two primary measures could enhance the Australian government's responsible AI use.First, Australians may express concern over AI's use in security and intelligence agencies. Hence, Australia should spearhead a 5-Eyes pact concerning AI's suitable use by these agencies, while publicly stating our AI ""no-go zones"". This statement would detail actions our security agencies may and will never undertake with AI. The Inspector General of Intelligence and Security, along with its 5-Eyes counterparts, should be given the explicit task of auditing compliance with this public statement within the intelligence sector. This process will assure Australians that we're not misusing AI or engaging in hazardous activities, all while safeguarding sensitive operations.Second, it's crucial to enhance the skills of the public service. This can be achieved by fostering expertise centres in each Department to ensure appropriate AI application within each Department's remit. Furthermore, the Australian govern"
135,Dr Michael Noetel,Q08 Generic vs technology-specific approaches for AI risk,"Generic measures such as transparency requirements and legal liability frameworks will only be adequate for mitigating risks in simple decision systems which perform single-step information processing and pattern matching, or routine tasks in limited settings [1]; examples of simple systems may include automated robotic systems in manufacturing, or CV screening systems in HR. By contrast, there will need to be specific measures in place to ensure that sophisticated automated decision systems are safe and reliable. “Advanced” systems refers to AI systems which: 1. Are able to complete a diverse range of reasoning tasks with human-level performance; and 2. Are able to navigate complex information environments and act within those environments autonomously; and 3. Are able to form sophisticated plans and reason about the consequences of actions Due to their autonomous and open-ended capabilities, advanced systems carry complex risks which are more similar to those associated with humans; "
135,Dr Michael Noetel,Q09 Transparency requirements and scope,"To remain competitive, professionals and businesses will increasingly feel the pressure to leverage sophisticated “black-box” AI models. This short-term thinking has already resulted in numerous accounts of public backlash and distrust, including high-profile warnings from technology leaders themselves [1]. For example, Apple’s credit card business came under fire for offering a woman one twentieth the credit limit offered to her husband. When she complained, an Apple representative simply told her, “I don’t know why, but I swear we’re not discriminating. It’s just the algorithm.” [2] More recently, two lawyers in the United States blamed ChatGPT for “tricking” them into including fictitious legal research in a court filing [3]. These problems could easily have been prevented by using explainable AI systems which can show the rationale for their outputs. Such incidents are evidence that the regulatory environment is not incentivising businesses or professionals to use transparent decis"
135,Dr Michael Noetel,Q10 Ban on high-risk AI applications,"While it is unlikely that AI today represents a catastrophic or existential risk to humanity, there’s a realistic possibility that it will in the near future [1, 2]. To strike a balance between all perspectives, regulators should work to identify “advanced indicators” of catastrophic risk scenarios, and clearly communicate that a ban is an appropriate action if these advanced indicators come to pass. Establishing appropriate advanced indicators will have to be done in consultation with AI safety researchers, for example those from the Centre for AI Safety, or the Centre for AI Governance. However, it’s likely they could include factors such as the size of training runs (measured in terms of compute and data), as well as safety-relevant factors about the AI system, such as the ability to complete a diverse range of reasoning tasks with human-level performance; to navigate complex information environments and act within those environments autonomously, and to form sophisticated plans and"
135,Dr Michael Noetel,Q11 Actions to increase public trust in AI,"The best way to be trusted is to be trustworthy. There are tremendous concerns about AI among those with the greatest familiarity of its risks [1]; these concerns are being widely broadcasted in Australian media with alarming headlines such as: - SMH: “When the “Godfather of AI” warns you about his offspring, you listen.” [2] - Forbes Australia: “‘We knew AI could destroy the whole world and we wanted to figure out how to prevent that’: ChatGPT Founder;” [3] - Canberra Times: “UN chief backs idea of global AI watchdog” [4] These concerning signals are difficult for the public to ignore. It goes without saying that this is quite a different situation from one where concerns are being raised in isolation or by fringe-figures; this makes it inappropriate to treat “public education” as a significant part of the solution, as is referenced in the discussion paper. The process for building trust in AI should be thought of as similar to fostering public trust in bridges or aeroplanes. Due to t"
135,Dr Michael Noetel,Q12 Trade impact of banning high-risk AI applications,"Advanced AI systems pose unprecedented risks that require a fundamental shift in perspective for regulators. We can no longer view AI solely as a commercial tool, but rather as the emergence of an autonomous force that will reshape society. To manage these risks, we must take preventative action before deployment occurs. This includes restricting access to resources required for developing advanced AI, and banning risky deployments absent regulatory approval. Specifically, we should immediately establish processes for: - Monitoring and limiting access to specialised AI computing hardware that poses proliferation risks - Requiring government approval before deploying advanced AI systems - Criminalising unauthorised deployment of advanced AI systems Adopting this preventative approach to governing AI is prudent risk management. It allows us Australia to have autonomy about our risks, rather than having foreign companies and countries create risks for us, and us having to react too late. "
135,Dr Michael Noetel,Q13 Changes to conformity infrastructure,"World-renowned AI experts such as Yoshua Bengio—winner of computer science’s equivalent of the Nobel prize—are warning that there’s a chance “superhuman” AI might be coming as early as 2030 [1]. Although there stand to be significant benefits from deploying advanced AI systems, their deployment will need to be highly targeted and controlled. While the productivity gains from advanced AI systems could be immense, uncontrolled proliferation would pose threats from likely scenarios such as malicious use or rogue AI systems [2]. As such, any deployment of advanced AI systems in Australia should occur via secure data centres, with access provided through a centralised API that facilitates monitoring and accountability by providing metadata about usage. Stringent compliance requirements must also be implemented for any organisation granted access to advanced AI systems. AI Governance researchers recommend that regulators should develop audit trail requirements in safety-critical applications"
135,Dr Michael Noetel,Q14 Risk-based approach to address AI risks,"I endorse a risk-based method, yet the grave existential dangers I'm most concerned about surface when AI systems of adequate size and competence are trained, not simply upon their deployment. As a result, for large-scale systems bearing potentially disastrous abilities, risk analysis needs to precede the training stage. We need to evaluate risks based on the nature and magnitude of the proposed training sequence, rather than just examining potential risky uses and deployment problems after an AI has already been developed and trained.Risk assessment ought to account for factors such as potential irreversibility, scale, and severity, and respond accordingly.The risk-based model presented in Attachment C of the Discussion Paper is confusing. It puts forth ""human in the loop"" as a principal mitigation strategy, although it's unclear whether this is truly effective. The paper goes on to mention that a human in the loop might be unfeasible where the AI application's benefit hinges on effic"
135,Dr Michael Noetel,Q15 Benefits and limitations of risk-based approach,"The key to success is understanding risks sufficiently broadly to encourage caution in an uncertain future. It is not enough to only react once the risk has been actualised - for some risks that’s too late. A potential “worse-case scenario” is if a culture of “trial and error” is adopted, whereby adverse events occur before risks are considered tangible enough to act upon. This reliance on post-hoc interventions risks normalising unsafe development practices, making it difficult to recover once risks proliferate. Proliferation of risks is a key point in governing AI, because regulating software poses significant challenges compared to other dual-use commodities such as pharmaceuticals or firearms. This means that, from the outset, there must be proactive regulatory oversight for the deployment of advanced AI systems and their precursors. For example, significant structural changes, such as the use of advanced AI systems in Australia, require that risks must be comprehensively understoo"
135,Dr Michael Noetel,Q16 Scope and application of risk-based approach,"World-renowned AI experts such as Yoshua Bengio—winner of computer science’s equivalent of the Nobel prize—are warning that there’s a chance “superhuman” AI might be coming within seven years. These types of advanced AI systems have potential to be highly disruptive across all sectors of the economy. While present risks might be linked to specific use cases and sectors, future risks will increasingly be linked to the underlying capabilities of the technology itself. As such, access to increasingly advanced AI systems should be restricted to organisations that can demonstrate their usage will provide significant public benefit aligned with Australia's national interests. This targeted approach helps mitigate risks in two key ways: 1. It limits opportunities for misuse or accidents from advanced AI by reducing proliferation across society. Usage is concentrated into high-value use cases that justify the risks. 2. It establishes clear accountability. Organisations granted access must have"
135,Dr Michael Noetel,Q17 Elements to include in risk-based approach,"Human-in-the-loop monitoring will be vital, but insufficient on its own to regulate advanced AI systems that will eventually surpass human capabilities. The speed at which advanced AI systems will operate means that human oversight will become pragmatically infeasible if our monitoring programs aren’t able to scale proportionally. As such, we must employ an approach that is predicated on limiting access to advanced AI systems unless there is a strong rationale for using them; conducting cost-benefit analyses prior to approving the deployment of advanced systems will mitigate risks while preserving potential benefits. A key part of enforcing this targeted approach will be monitoring and controlling access to high-performance computing resources, which are required for developing and operating advanced AI systems and their precursors. This additional layer of governance allows authorities to detect potentially dangerous AI development or usage activity, and can be cross-checked with othe"
135,Dr Michael Noetel,Q18 Integration of risk-based approach into existing frameworks,"Some areas of regulation, like the Racial, Sex, Age, and Disability Discrimination Acts are appropriately strong and may provide useful safeguards against some harms of AI. Specifically, the law should ensure that such discrimination is always wrong and always punishable, and an excuse that the discrimination occurred because of a “black box” AI provided by an overseas AI Lab should never be accepted. It would be helpful to communicate these requirements to Australian businesses that might deploy AIs and ensure they only contract with AI Labs for products where the Lab can ensure the product cannot engage in these harmful behaviours. Certainly, no AI-specific regulation should ever waive existing safeguards. That said, many of the emerging risks from AI are quite new and it’s reasonable to expect that regulation made without AI in mind is not tuned to the new risks that AI represents. Specific AI safety regulation is unavoidable."
135,Dr Michael Noetel,Q19 Application of risk-based approach to general purpose AI,"The capabilities unlocked by general-purpose AI systems, like large language models (LLMs) and multimodal foundation models (MFMs), fundamentally change the risk landscape. We should anticipate these systems having large, unpredictable effects across all parts of society. A prudent approach is to implement a risk-based framework, where access is graduated based on model capabilities. Models above a certain threshold of autonomous capabilities would be designated as ""advanced systems"" and subject to additional governance, such as permits for users. Further, permits should be required to gain certain levels of access to AI systems which can be used to build advanced systems. Permits allow us to maintain oversight, while still enabling beneficial uses. Different permissions can be granted based on use case - for example, full access for safety testing versus API-only access for most commercial uses. This balances innovation and responsibility. Although this may seem very different to how "
135,Dr Michael Noetel,Q20 Voluntary vs mandated compliance with risk-based approach,"AI is very likely to transform many elements of the economy and society in coming years. Ethics and safety are unlikely to be a priority during this time unless they are actively made a strategic focus by clear leadership. Historic examples include how social media algorithms came to shape our lives, and the negative impacts to mental health and social cohesion only became apparent much later and still have not been adequately addressed. Similarly, while the destructive potential of nuclear was apparent early on, the “arms race” mentality, as well as potential economic benefits, swept humanity along towards the current risky situation that no one in the early 1940s could have conceived of. In that way, history teaches us that it’s critical that ethics and safety take centre stage from day one. The economic and technological forces are likely to be so strong that a scheme that is voluntary or driven by aspirational best practices is unlikely to be successful. Further, smaller businesses"
136,BixeLab,Transcription,"FILENAME: AI Feedback BixeLab.26e1df7807713.pdf | CONTENT: Response to the Consultation Paper on AI and ADM Regulation There exist several potential risks from Artificial Intelligence (AI) that we believe Australia’s existing regulatory approaches do not sufficiently address. As noted in the discussion paper, one such risk is the inherent bias and discriminatory outcomes that can occur in AI systems, particularly those utilising machine learning algorithms. These risks can arise due to biased training data or algorithms that reinforce systemic discrimination. To mitigate these risks, we suggest the implementation of legislation that mandates the use of bias testing and mitigation measures during the development and deployment of AI systems. Such a regulation would help ensure fairness and equality in AI-driven decisions and services. As a laboratory specialising in biometrics and AI testing, we believe it is essential that any proposed measures or initiatives reinforce the need for ind"
136,BixeLab,Q07 Responsible government use of AI,A requirement to undertake independant assessment and testing as part of the deployment to ensure both safety and effectiveness.
137,Evolved Reasoning,Transcription,"FILENAME: Supporting Responsible AI_Kollo.df489df926ac2.docx | CONTENT: Supporting Responsible AI: discussion paperPrepared by : Dr Michael G. Kollo, CEO Evolved ReasoningThis paper has been prepared in response for a consultation on how the Australian Government can mitigate any potential risks of AI and support safe and responsible AI practices.This is significant topic, and having attended the in-person session at the Sydney CSIRO offices, I thought it would be more pragmatic and useful to provide direct feedback from points that were raised in that session, as well as others that maybe were not covered.Firstly, I will limit my comments to those areas where I feel the government has a direct role to play through regulation. Standards bodies, both general and industry-specific, will increasingly create a multitude of ideas-based standards, that companies and industries will adopt based on their business-models, affiliations, and leadership. This is certainly the direction in the US r"
137,Evolved Reasoning,General comment,"Dear Sir/Madam,Thank you for taking the time to read our submission. I have attended the in-person session in Sydney, which was great, and my intention here is to expand on some of the trickier areas for regulators to address. I am always available to assist further.Yours truly,Dr Michael Kollo"
137,Evolved Reasoning,Q01 Definitions,"I think its a good place to start. I dont think there is an ideal definition here, and I am possibly not qualified to provide you a better one for the purposes of regulation."
137,Evolved Reasoning,Q02 Other AI risks and recommendations for regulation,No. I think you have covered probably more than I would have. The risks are clearly to over-regulate and to negative impact our industry adoption schedule.
137,Evolved Reasoning,Q03 Non-regulatory actions to address AI risks,"Absolutely.Starting with the idea that 'AI practises' really refers to the intentions of people using AI, and what they want to achieve with it.I think the first and primary one being education. We have a terrible social narrative in Australia about AI. It evokes feelings of danger, of unemployment, and the sense of being manipulated. People of sound ethics and thinking tend to stay away from AI as a result, which ultimately means that the balance of people using it is impacted.We should active engage with schools, and education institutions on the positive and socially impactful use cases of AI, and enable these through education and funding."
137,Evolved Reasoning,Q04 Coordinating AI governance across government,"I think we are past the point of more meetups. We have had some form of AI within government and industry, with quite limited takeup. That is not to say that the programs have not been successful, but that they haven't been as effective as they could have.I am not sure if a more co-ordinated governance of AI will result in update of AI in Australia. I think what will result in very applied, industry-specific research and use cases being taken to companies across Australia, in a very directed way. High level governance is not going to achieve more I feel."
137,Evolved Reasoning,Q05 International models for AI governance,"Europe and the US have been most active, and mostly principles based. Everyone else is still behind. As are we."
137,Evolved Reasoning,Q06 Public vs Private sector governance,"This is a point around scale and social impact.Public sectors, dealing with citizen records, have an enormous capability to influence and impact. These are the highest potential scalable technologies, but as with data privacy, they should be governed by the existing principles in place.For industry, there should be a 'scale test' that is applied to check if the algorithm has significant social impact potential or not. For example, making decisions for a small number of customers, where the customers can leave for an alternative provider, would mean that even if the algorithm is damaging, they have the option to switch, much like receiving bad customer service or a defective product. That is hardly significant damage to warrant the same regulatory framework as a large-scale credit decision model developed by a bank."
137,Evolved Reasoning,Q07 Responsible government use of AI,"There must be adoption first, and experimentation. More risk taking, and more focus on efficiency will increasingly lead to that. Once there is enough green shoots, then maybe start to think about oversight. Too early, and you will simply kill the will of the people to use it."
137,Evolved Reasoning,Q08 Generic vs technology-specific approaches for AI risk,"In most cases, algorithms are akin to a scalable demonstration of someone's or something's will to do something. In that case, the standard business ethics and practises apply. In other words, if I'm deciding if to give a student a scholarship, whether thats a person or an algorithm, it should represent and be guided by the same principles.Whereby decisions are essentially outsourced to an algorithm, because the expertise doesnt lie within the organisation itself, then it needs technology specific solutions potentially. So for example: I manufacture parts, and I use an AI to test a certain safety feature that I can't test and monitor independently. I will need to understand how that AI works, and have regulations and structures to understand how I should use that tool.A good litmus test is: could I do that thing without the algorithm at lower scale? If yes, then it should be a business ethics. If not, then .. something else."
137,Evolved Reasoning,Q09 Transparency requirements and scope,"At all stages transparency should be available. However, given the complexity of these systems, that transparency will rarely be useful. In other words, transparency of a highly complex system is still difficult for individuals to interpret.However, for audit and monitoring (MLOps) for example, having more real-time monitoring based on outcomes (not components) is perhaps the most important to detect and stop any dangerous actions of AI. Importantly, it is accepting that you can not forecast all the actions of a complex system, but rather by monitoring it, you can step in quickly to adjust or correct it if required."
137,Evolved Reasoning,Q10 Ban on high-risk AI applications,"I think AI technologies that create simulations or likenesses of real individuals should be banned. I can't see a legitimate use case that outweighs the possibility of fraud for example. In that sense, voice cloning, deep-fakes, and so on should be banned. If individuals would like to scale their likeness, they will need to create avatars that can not be mistaken for them for example."
137,Evolved Reasoning,Q11 Actions to increase public trust in AI,"Thats a great question. This is a social narrative question, I feel. We need to work hard to reverse the media narrative of danger. I would suggest on multiple angles for this: 1. Adult education. People think of AI, they think of losing their jobs. They are afraid of this. The 'no-one left behind' idea needs to be developed and deployed. 2. For early adopters, they already find everything they need. What is required here is much closer work with union and industry groups to get their members to take courses to understand how technologies like AI helps them, that are delivered to their specific needs and conditions, is critical.The challenge of the public narrative of the 'AI bad guy' is strong. We need to work proactively to change the narrative in business and social circles to something that emphasises the social benefits of AI without a constant referral to risks."
137,Evolved Reasoning,Q12 Trade impact of banning high-risk AI applications,"It wouldn't. The large players in MSFT and Google have a very clear understanding of regulatory risk, and they are very careful to avoid conflicts where banned technologies mean that they must exist a market."
137,Evolved Reasoning,Q13 Changes to conformity infrastructure,I am not qualified for this question.
137,Evolved Reasoning,Q14 Risk-based approach to address AI risks,It seems appropriate (risk based approach to address risk?).
137,Evolved Reasoning,Q15 Benefits and limitations of risk-based approach,The spectrum of potential risk is infinite. Having a strongly industry led and pragmatic approach is important.Some of these limitations can be overcome through industry-specific risk models.
137,Evolved Reasoning,Q16 Scope and application of risk-based approach,"Exactly, completely agree. In my submission, I address the point of 'scale' being importance. For example Bias x Scale = Societal outcome. the scale problem is certainly important and is addressed by organisation size above. The other one may be about competitive norms within an industry. An industry with many smaller competitors using AI should pose less risk, than a single monopolist with a single AI decision framework."
137,Evolved Reasoning,Q17 Elements to include in risk-based approach,"I do, though I would suggest moving around some of elements.Companies and organisations need to prove and be legally liable for their use of AI tools. Even if that tool has some bias in it, it can be used ethically, or in a way that doesnt emphasise that bias. The outcome and the use cases are the critical ones.For example, a robotic surgeon is a replacement for a human surgeon. The same legal liabilities are faced by the hospital if there is a mistake, as they own the risk, whether its a human or a robot doing the surgery. So long as the robot operates within pre-specified parameters. Similar to Tesla, whereby so long as the car is operating within parameters, the risk of accidents and outcomes lies with the driver."
137,Evolved Reasoning,Q18 Integration of risk-based approach into existing frameworks,That is an excellent question that I feel I need more time to think over. But it can be done.
137,Evolved Reasoning,Q19 Application of risk-based approach to general purpose AI,"I think we go back to intention here. If the intention is to fool or to deceive, these systems are able to scale and magnify those intentions. So that lies with the individuals.These systems already come with 'warning labels' that talk to their potential to create mis-information and therefore what they should or should not be used for. Creating some formal requirements around these disclosures is probably useful, as well as requiring firms that utilise these systems to disclose when they are using them and how. Ultimately, if a ChatGPt instance has a 'bad' conversation with a customer, then the outcome is akin to a bad human customer service agent. In that sense, it falls within the business's risk tolerance.Finally, the risk-based approach looks at the intention or the outcome of the system. The system itself could have components that are seen as risky (eg copyright data, modern slavery etc). Again, I would say that if the copyright infringement is 'real', that company will be sued "
137,Evolved Reasoning,Q20 Voluntary vs mandated compliance with risk-based approach,"I think minimum standards of disclosure should be mandated. I think for certain algorithms that have significant social impact potential (AIs that have scale and some decision autonomy) should be open to examination and disclosure on request (a kind of freedom of information act) for regulators and the government to look at. Everyone else, we should enable creators and users of AI systems to be thoughtful, to be societally minded and keep the bigger questions and risks in mind."
138,Standards Australia,Transcription,"FILENAME: Standards Australia submission - Supporting Responsible AI 2023.f30c0c7563888.pdf | CONTENT: 19 July 2023Department of Industry, Science and ResourcesDear Sir/MadamRE: Safe and responsible AI in Australia: Discussion paperStandards Australia (SA) is pleased to provide a submission to the Australian Government's Safe and responsible AI in Australia: discussion paper.IntroductionThe rapid development and deployment of artificial intelligence (AI) has the unique opportunity to deliver new capabilities and benefits to the Australian economy and community. These opportunities can improve the lives of Australians through improvements to environmental protection, energy efficiencies, defence, security, health care, safety and education. As with any new technology, these capabilities bear degrees of uncertainty which are cause for concern and can challenge public perception and trust. The vast opportunities which AI pose offers the Australian Government a unique opportunity to move w"
138,Standards Australia,Q01 Definitions,Yes
138,Standards Australia,Q03 Non-regulatory actions to address AI risks,"Standards offers the Australian Government a non-regulatory initiative to implement responsible AI practices. International and national Standards are consensus driven documents which are created through a harmonised approach amongst industry, government, and community interests. The nature of this process being stakeholder lead, results in a higher chance of market acceptance and implementation."
138,Standards Australia,Q04 Coordinating AI governance across government,"Standards are stakeholder lead, and the extensive network across community, industry and government which Standards Australia has can be utilised to help support and influence the development of responsible AI in Australia."
138,Standards Australia,Q05 International models for AI governance,"Standards Australia represents Australia in ISO/IEC, actively contributing to International standards which offer commonality for AI governance across government jurisdiction and sector. Standards to apply the same principle of driving consensus on an International scale and ISO/IEC have already participated at great length to help govern the use of responsible and safe AI globally. International standards often underpin the development of regulatory frameworks and provide a pathway to demonstrate conformance through consensus. On a global level, Standards can support interoperability and regulatory compatibility with systems across other jurisdictions, such as those being developed by the EU and the US, reducing barriers to trade."
138,Standards Australia,Q11 Actions to increase public trust in AI,"Standards Australia is a founding member of the Responsible AI Network, within The National AI Centre (CSIRO). The tools, materials and knowledge created as part of RAIN are helpful in continuing to disseminate information on responsible and safe AI. We would suggest continuing to develop and deploy information through RAIN would ensuring public confidence continues to grow and improve public perception of responsible AI. Supporting more initiatives to educate and inform the public will help increase public trust."
139,Insurtech Australia,Transcription,"FILENAME: Insurtech Australia _ Submission - Safe and Responsible AI Discussion Paper (24.7.23)(1228710158.12).2fe75b332b53b.pdf | CONTENT: Discussion Paper: Safe and responsible AI in Australia Submission to Department of Industry, Science and ResourcesInsurtech Australia for and on behalf of its members About this Submission Insurtech Australia (Insurtech, we, us, our) welcomes the opportunity to engage with the Department of Industry, Science and Resources (Government) to lodge a submission on its Discussion Paper, ‘Safe and responsible AI in Australia’ released on 1 June 2023 (Paper). This submission was drafted by Insurtech in consultation with our members (each, a Member). In developing this submission, our Members participated in a roundtable session to discuss their attitudes towards Artificial Intelligence (AI) generally, the extent of their adoption of AI in their respective organisations, as well as their collective view on the preferred approach to AI regulation in Australi"
139,Insurtech Australia,Q01 Definitions,"While Insurtech does not object to the definitions used in the Paper per se, we consider that the definitions are, in some cases, narrow and do not adequately capture key features of AI. If it is Government’s intent to create definitions that capture all-manner of AI-driven technologies, then we consider that more work will need to be done to develop definitions that are fit to form the basis of any AI regulation. Further, given that AI and its application is constantly evolving, we think that any definitions that incorporate examples of AI output (for example, the definition of ‘Generative AI models’) should be clear that the examples are non-exhaustive. We also consider that any definitions that are adopted should be consistent with the equivalent definitions used by our key trade partners, in order to achieve, as far as is possible, harmonisation across regulatory regimes and to allow insurance and insurtech businesses to expand and operate in such jurisdictions without the need to "
139,Insurtech Australia,Q02 Other AI risks and recommendations for regulation,"AI Ethics Principles The regulatory regime in Australia, as it relates to AI, includes the AI Ethics Principles, and while these principles may address some or all of the risks posed by AI-driven technologies (including those considered below), they are voluntary principles that do not carry the force of law. Accordingly, we have not referenced those principles in answering this question, but instead have focussed our answer more on the enforceable steps that Government could take to mitigate these risks (which, for completeness, could include making the AI Ethics Principles enforceable in respect of the development, deployment and use of AI). Development risk While some of the potential risks from AI are addressed to some extent by existing laws on privacy, consumer protection and anti-discrimination, and may be useful in providing consumers with remedy if those laws are infringed as a result of a consumer’s interaction with AI (for example, where personal information is used to train"
139,Insurtech Australia,Q03 Non-regulatory actions to address AI risks,"We think that the implementation of non-regulatory initiatives is imperative to help boost consumer trust and confidence in AI by framing consumer perception of AI as a technological proliferation that is designed to improve the customer experience and help grow the Australian economy through innovation. Such initiatives could include: • public education and guidance, media involvement and consultation. For example, the development of a public AI toolkit, as has been implemented in Singapore; • the development of opt-in-based industry codes (and a public register of organisations that have signed up the codes), which would not only be dynamic enough to adapt to evolving use cases, but would be developed on a sector-specific basis. While Australia already has the AI Ethics Principles for the development of AI, this is a voluntary framework and we think that such codes should be enforceable for those who opt-in; and • the creation of an industry-agnostic AI body that acts as a conduit be"
139,Insurtech Australia,Q04 Coordinating AI governance across government,"Fundamentally, we do not see it as the role of Government exclusively to coordinate thinking on AI governance and regulation, and we consider that this is a role that will be best played by industry to inform how AI should be regulated based on the risks and use cases peculiar to particular industry sectors. Of course, open and transparent dialogue with Government and law makers is imperative, and a governance regime as between industry and government should certainly be established, but industry will hold the most informed view as to how the regulatory framework should be shaped to contemplate AI, while ensuring that innovation is encouraged and facilitated. To assist this, an industry-led governance body could be established to help support a coordinated and coherent response to AI governance in Australia as between government and industry. However, we question the extent to which such a body would have the capacity and capability to speak on behalf of the entire business community a"
139,Insurtech Australia,Q05 International models for AI governance,"s The Amsterdam government has adopted a searchable public AI register (Amsterdam Register), which provides information on the algorithmic systems it uses in public service, the data sets used, the manner in which that data is processed, the details of the human oversight and risk management frameworks employed, and even provides an explanation of the potential for the relevant application to create discriminatory outcomes. While we consider that this is a desirable initiative for the public sector, we do not see this as being appropriate for industry."
139,Insurtech Australia,Q06 Public vs Private sector governance,"We think that public sector uses of AI technologies should be held to the same, if not a higher standard than those of the private sector, particularly as a certain level of transparency is generally expected by governments. Further, transparency by the public sector, we think, would be impactful in improving public trust and confidence in AI generally. If Government wished to implement initiatives to regulate public sector use of AI, then it could: • take a similar approach to the UK, whereby public sector entities are required to adhere to a ‘Algorithmic Transparency Recording Standard’ (Transparency Standard) that requires public sector entities to provide information about the algorithmic tools they use, and why. However, if Australia was to implement such an initiative, we think that the scope of any sort of transparency requirements will need to be carefully considered, and balance the consumer’s right to know, with the technical practicality of providing information about public"
139,Insurtech Australia,Q07 Responsible government use of AI,Please see our responses to Questions 5 and 6 above in respect of the Amsterdam Register and the Transparency Standard.
139,Insurtech Australia,Q08 Generic vs technology-specific approaches for AI risk,"We assume that the Paper’s references to: • ‘generic solutions’ means the industry-agnostic, risk-based approach to AI regulation (like that which has been proposed in the EU) that seeks to regulate AI in a general manner, regardless of its specific use cases; and • ‘technology-specific solutions’ means the decentralised, sector-specific approach (like that which has been proposed in the UK) that seeks to regulate AI based on its specific use cases, including in respect of consumer interaction. On the basis of this assumption, we consider that the appropriate solution to addressing AI risk will depend largely on where the relevant application is in its lifecycle. For example, we think that generic solutions to AI regulation may be best-suited to AI applications that are in the early stages of development, with few real-world use cases, or are still in pre-production. We think that such solutions could include: • guardrails around development based on the intent of the product and any p"
139,Insurtech Australia,Q09 Transparency requirements and scope,"We consider that upfront and proactive transparency is the best way in which public trust and confidence in AI can be fostered, as we think that the public will want to understand when, why and how they interact with AI, as well as the unique ways in which they may be affected by their use of AI, as early as possible in the customer journey. In respect of mandating transparency requirements, while we think that such requirements may be appropriate in some circumstances (see also our response to Question 2 above), the key questions to be answered of any proposed approach relate to: • the scope of such requirements and their application – for example, whether the law is framed as a consumer right to request transparency, as well as a requirement on providers to provide it (including the degree to which it is to be provided); • how such requirements are implemented; and • whether these requirements are legislative or regulatory, principles based, technology-based or sector-based. In any c"
139,Insurtech Australia,Q10 Ban on high-risk AI applications,"As canvassed above, we consider that a decentralised, sector-specific approach to AI regulation is generally a better solution to limit the potential risks and complications of AI, without unduly stifling innovation, as compared to a risk-based approach via legislation. In our view, it is the specific application of AI-driven technologies that predominately engenders risk, not the underlying technology itself and accordingly, decentralised regulation is preferable (as compared to any broad, blanket restriction), as it allows for particular use cases to be examined on an individual basis for their potential to cause harm. Notwithstanding the above, we recognise that certain applications of AI technology may have an inherently high risk of harm to individuals and accordingly, we consider that such high-risk applications should be banned or restricted depending on the degree to which those applications have the potential Insurtech Australia | Submission to Discussion Paper: Safe and respo"
139,Insurtech Australia,Q11 Actions to increase public trust in AI,"Please see our responses to Questions 5 and 6 above in respect of the Amsterdam Register and the Transparency Standard. To this end, we agree with Government’s view that greater public trust and confidence is required in order to increase AI adoption and we consider it appropriate that the Government ‘leads from the front’ in this respect. Further, we consider that regulation cannot be created in a vacuum. Rather, what is needed is a collaborative public/private approach which provides a forum for inputs from both industry and regulators. Accordingly, the Government should explore initiatives that allow opportunities for the public, the private sector and Government to knowledge share and collaborate in respect of AI regulation. From a public education perspective, Government should also consider creating guidance and public education initiatives to educate the public on how AI technology is currently being used in their day-today lives. This would create a better-informed public on th"
139,Insurtech Australia,Q12 Trade impact of banning high-risk AI applications,"Currently, AI use in the insurance and insurtech industries is primarily focussed on areas including organisational risk assessment, fraud detection, pricing, policy generation, claims management and customer experience. Accordingly, so called ‘high-risk’ use cases such as social scoring or facial recognition are less relevant to the insurance and insurtech industries. However, we consider that such ‘high-risk’ use cases may become relevant to the insurance industry in the future in a number of ways. Using the examples provided in this question: • social scoring applications could be used to gauge the risk profile of a potential insured, when deciding whether to insure the relevant individual; and • facial recognition could be used to assess the mood and attitude of a customer during a customer service interaction, to enable an insurer to improve its customer service processes. We do not think it is necessarily the case that such use cases will always be high risk – for example, we con"
139,Insurtech Australia,Q13 Changes to conformity infrastructure,"We consider that industry should lead the development of conformity infrastructure and relevant AI technical standards, and while we acknowledge that there are AI risk management standards by ISO and the National Institute of Standards and Technology (NIST) in the market currently, we do not think that these standards are sufficient by themselves. Additional conformity efforts will need to take place across all stratum of industry, not just the end of industry that is sophisticated enough to warrant the adoption of standards such as those of ISO and NIST. We consider that Government should adopt a graduated approach to conformity infrastructure, based on factors such as an organisation’s size or annual turnover. This would reduce the regulatory burden for smaller organisations and give them flexibility to innovate, whilst still establishing a mainstream requirement for conformity and technical consistency. However, we note that the inherent flaw in this approach is that use of AI is no"
139,Insurtech Australia,Q14 Risk-based approach to address AI risks,"We recognise the consistency and uniformity benefits of a broad risk-based approach for regulating AI, however, we think that such an approach is inherently rigid. Further, as technology evolves, legislation will need to be constantly updated to contemplate the new risks that may be presented by such evolution. Accordingly, we do not support a generic approach to restricting use of technology, except potentially at early stages in the AI development lifecycle (as per our answer to Question 8 above). As conveyed earlier in this submission, we think that a decentralised approach would allow for greater innovation and flexibility to consider a broad range of specific use cases. Further, a decentralised approach also allows for greater isolation of risk so that specific attention and effort can be focused where regulation is most-needed, based on an assessment of the sector-based risks raised by a particular application. Given that existing legislation such as the Privacy Laws, Australian "
139,Insurtech Australia,Q15 Benefits and limitations of risk-based approach,"We consider that the key benefits of a risk-based approach are that it: • provides a comprehensive and harmonised regime for AI governance, minimising the need to navigate various regulatory regimes that may conflict with one another; • focuses on the risks related to the way in which AI is applied; • seeks to place guardrails around the development of AI in the first instance; and • allows for greater oversight of general-purpose AI technologies that may impact multiple sectors. However, we consider that the key limitations of a risk-based approach are that it: • could be difficult to assign and future-proof general risk categories and define in what circumstances AI systems should be regulated or banned; • would not allow sufficient ‘flex’ to permit or prohibit specific uses cases of technology based on context; • would need to be constantly updated as new AI applications and technologies are developed; and • could create duplication and overlap between existing general and sector-sp"
139,Insurtech Australia,Q16 Scope and application of risk-based approach,"We have assumed that this question seeks to understand whether a generic, risk-based approach to regulation would be better suited towards sectors that are larger and have greater AI maturity and resource capacity. Notwithstanding that we support a decentralised approach to regulation, we consider that larger organisations would be better equipped to navigate a risk-based approach to regulating AI due to their size, sophistication and resourcing capacity. If a generic, risk-based approach is adopted, the danger for smaller businesses is that they may lack the proper organisational risk management frameworks to manage compliance. This could have the effect of stifling the innovation efforts of smaller market players, from where much of the AI innovation in industry begins. We also believe that a generic approach to industry regulation would be better suited towards industries and sectors that are not already subject to specific regulatory frameworks. To the extent that existing regulato"
139,Insurtech Australia,Q17 Elements to include in risk-based approach,"n our view, Attachment C covers several important aspects of risk-based AI governance and overall, we are supportive of the elements described in Attachment C. Introducing risk assessments, notices and human oversight (but only as a short-term measure until human intervention is not needed) are all valid steps that will create guardrails for the safe development of AI. In addition, we consider that the development of policy frameworks will also be an important aspect of any risk-based approach for addressing AI risks."
139,Insurtech Australia,Q18 Integration of risk-based approach into existing frameworks,"We consider that, as existing legislative frameworks already touch on and regulate AI to a certain degree and in respect of certain risks, any approach to incorporating AI regulation to existing frameworks should begin with a gap analysis to understand the areas that are not already addressed. Following this, we consider that a light-touch approach should be taken where gaps in existing regulation are identified and then addressed. Such gaps could be addressed by, for example: • the development of formal guidance from the OAIC in relation to the application of the Privacy Laws to AI; • clarifying the role of privacy impact assessments in identifying and controlling the risk of privacy harm to individuals in connection with the development, deployment and use of AI; and • incorporating into an organisation’s privacy policy and personal information collection notices, disclosures about automation and AI being used in the relevant organisation’s services – however, this would only be appr"
139,Insurtech Australia,Q19 Application of risk-based approach to general purpose AI,"We consider that, for general purpose AI systems, their risk level and the subsequent stringency of the associated requirements should be determined based on the nature of their specific application in the relevant context. Insurtech Australia | Submission to Discussion Paper: Safe and responsible AI in Australia 15 As stated earlier in this submission, the generic approach to regulating a technology itself, as opposed to specific use cases, can unduly stifle innovation and place undue burden on potentially-innocuous applications of that general technology. In our view, any risk-based approach to AI should focus on the risk of specific applications and use cases, rather than the risk potential of the technology itself."
139,Insurtech Australia,Q20 Voluntary vs mandated compliance with risk-based approach,"We consider that a mixture of enforceable (but light-touch) regulation, supplemented by principles-based guidance, may be appropriate to regulate AI in Australia. As it is expected that AI will proliferate across the public sector and industry, any guardrails established should apply to both the public and private sectors. At the same time however, due to the nature of the public sector and the need to engender public trust and confidence in AI, we consider that public sector uses of AI in particular, should be subject to probity and transparency requirements. Such regulation should also apply to both developers and deployers alike, so as to capture the entire lifecycle of AI. This would serve to prevent the development of AI with high harm potential, as well as restrict the improper use of AI technology. Regardless of the regulatory methods adopted however, we consider that the correct approach is one that does not stifle innovation"
141,NCC Group,Transcription,"FILENAME: 2307_NCC Group’s response to the Government’s discussion paper Safe and responsible AI in Australia.1bfd5ee82f805.pdf | CONTENT: NCC Group’s response to the Government’s discussion paper: Safe and responsible AI in Australia July 2023NCC Group welcomes the opportunity to respond to the Australian Government’s discussion paper and offer our expertise as a global cyber security business. We are keen to ensure that security considerations are embedded in AI systems from the outset. This is because AI can only be safe if it is also secure.We support the Government’s endeavours to review its approach to governing AI, building public trust and enabling society to reap the benefits AI has to offer. We agree that the myriad of existing technology-neutral laws governing areas such as data privacy, online safety and consumer rights provide a strong foundation for regulating AI. We also agree that there is a need for context-specific regulation, and support ongoing regulatory efforts in"
142,James Newton-Thomas,Transcription,"FILENAME: AI Safety Submission.140823d0b13eb.pdf | CONTENT: Title: AI Safety: A Brief Examination of Identity Scams and ImplicationsAuthor: James Newton-ThomasIntroduction:In this paper, I discuss a small but significant aspect of AI safety: identity scams. This specific issue illuminates the larger challenges we as a society face with the introduction of artificial intelligence.Historical Context:When Alan Turing proposed the imitation game as a test of intelligence in 1950, his intentions were positive. Turing aimed to provide an objective measure of when a machine could be considered ""intelligent"". He suggested that if we could not distinguish a machine from a human after a short period of time, it would be reasonable to regard the machine as ""intelligent"".Development of AI Platforms:Building upon Turing's work and that of countless others, we now possess the knowledge and platforms necessary to create AI systems and access the vast amount of data needed to train them. These include"
142,James Newton-Thomas,Q01 Definitions,I agree with the definitions given although I think the term 'Responsible AI' itself is not well defined nor the definition of risk in this context.
142,James Newton-Thomas,Q02 Other AI risks and recommendations for regulation,"Mass unemployment. Job displacement. Off-shore dependency. Yes, in-house adoption and development. We have the technology, we have the data, we have the know-how. We just need the government to regulate and sponsor to make it happen"
142,James Newton-Thomas,Q04 Coordinating AI governance across government,You need the CSIRO and the ABS to model the consequences of near and human capable AI
142,James Newton-Thomas,Q05 International models for AI governance,"I think China is certainly taking it seriously from a societal perspective. We in the West for historical reasons tend to let our industry take the lead and this has served us well in the past. The inception of AI however is an exponentially occuring event which you are tackling as a linear system on linear timescales, ie election cycles. Bad things can happen before you can react unless you setup a fast reactive body with sufficent understanding to advise and respond."
142,James Newton-Thomas,Q06 Public vs Private sector governance,"Yes. The reason we have private companies within capitalist economies is as a distributed search engine to find better ways of doing things ala Hayek. However, AI means that Hayeks thesis may no longer be valid. We need to look at this because if it is true then large corporations and the CCP will outperform us."
142,James Newton-Thomas,Q07 Responsible government use of AI,"In all of these multi-variable problems, the best solution is modeling the causes and effects of policy change before the rubber hits the road. You need to up this capability with AI assistance across government with bipartisan support."
142,James Newton-Thomas,Q09 Transparency requirements and scope,"transparency will not be as valuable as people think because of complexity and sheer data overload. However, the better the AI the better it will be able to explain its predictions."
142,James Newton-Thomas,Q10 Ban on high-risk AI applications,"You won't necessarily know a high-risk AI application until its consequences are manifest. We cannot ban them because they are not globally banned and other people won't. Ai's will be tasked to exploit opportunities regardless of borders. Our best defense is for you, the government, to have the smartest AI out there. So, please get building. Also, please implement metrics to ensure you stay benign."
142,James Newton-Thomas,Q11 Actions to increase public trust in AI,"You could provide an AI service like Chat GPT to all Australians, for Australians that provides helpful and societally relevant advice"
142,James Newton-Thomas,Q12 Trade impact of banning high-risk AI applications,"We already have social scoring, we call it 'reputation' and 'stigma'. I think letting an AI learn to facially recognise is not a bad thing. What it does call into question is privacy."
142,James Newton-Thomas,Q14 Risk-based approach to address AI risks,I do not support a risk based approach as they are not possible to quantify. You can not know today what you will discover tomorrow.
142,James Newton-Thomas,Q15 Benefits and limitations of risk-based approach,Be ahead of the game is the best strategy.
143,Stephen Nicholas Fowler,Q01 Definitions,"The proposed definitions are overly focused on contemporary systems and are not flexible enough for policy intended to last into the future. The paper should introduce terminology for agentic sophisticated systems that will exist in the coming decades. Properties such system may possess include an understanding of the world they are deployed into, an ability to influence the world (directly or indirectly), an ability to self modify, and the capacity for creative and competent strategic planning. There is a substantial risk of harm to the Australian public from such systems behaving in unexpected ways. The definition of intelligence used is overly narrow, and should reflect the more general concept of ""intelligence"" as we observe it in humans."
143,Stephen Nicholas Fowler,Q02 Other AI risks and recommendations for regulation,"There are multiple potential risks from AI that are not covered by existing regulatory approaches.In the long term, there is a catastrophic risk from humans misusing powerful intelligent systems or from these systems behaving autonomously. To address this Australia should publicly acknowledge that this is a real risk, push for global governance arrangements and begin funding AI safety research.In the near term, Australians will face numerous other risks from misuse of contemporary systems. The most pressing is automated and partially automated scams.Australians were scammed out of 3.1 billion [1] last year. Over the next few years you should expect to see an increase in scam victims as highly sophisticated international cartels are able to integrate generative AI into their workflow. Due to the international scale of the problem, regulatory action may not be possible and the Australian government may need to work in tandem with global partners to address this issue. In this example, re"
143,Stephen Nicholas Fowler,Q03 Non-regulatory actions to address AI risks,As AI technology speeds ahead it will continue to have a greater and greater impact on both society and industry. The Australian Government should take this time to publicly state the risk from AI and become an international leader in AI Safety and Governance. This could involve building a public taskforce on evaluating the capabilities of cutting edge systems.
143,Stephen Nicholas Fowler,Q05 International models for AI governance,"The EU's handling of the AI Act offers important lessons for Australia. The Act mandates that the providers of general-purpose AI adhere to strict safety measures throughout the product life cycle, in contrast to merely focusing on the application level."
143,Stephen Nicholas Fowler,Q06 Public vs Private sector governance,"Unfortunately, the potential risks from powerful intelligent systems is the same regardless of if they are in the public or private sector. Private sector entities should be subject to stringent transparency measures surrounding the capabilities of their models."
143,Stephen Nicholas Fowler,Q07 Responsible government use of AI,"The most risky AI usage seems likely to occur in the military or intelligence services. Australia should work with her allies to establish when, why and what kind of AI technology it intends to use for national security purposes and recognize the potential risks from doing so. AI technology has the potential to change the nature of modern warfare, but its deployment is also incredibly risky."
143,Stephen Nicholas Fowler,Q09 Transparency requirements and scope,"a. Transparency can reduce the potential risks from powerful AI systems. It is important that there is transparency *throughout* training of model and into their deployment. Very recent work in interpretability has shown that a models behavior can be understood by understanding *how* structures formed in the model over time during training. The technicalities of how this occurs are not necessary for this submission. b. The transparency recommendation here is very simple. Companies must save a high number of checkpoints during training and be willing to provide these to oversight agencies if requested. We don't just want to know what the final model looks like, we also want to know what it looked as it ""grew""."
143,Stephen Nicholas Fowler,Q10 Ban on high-risk AI applications,a. Yes. There should be an outright ban on the use of AI technology when advertising to the Australian public.
143,Stephen Nicholas Fowler,Q11 Actions to increase public trust in AI,"I don't think this should be too strong of a concern, as the benefits of AI should encourage more people to use it once it has been packaged in a way that makes the benefits to their daily life clear. I expect we will see the widespread adoption of personal AI assistants within 5 years."
143,Stephen Nicholas Fowler,Q12 Trade impact of banning high-risk AI applications,It seems unlikely that banning the use of high-risk activities would affect our tech sector. Much of the current use of AI technology is directly adverse to the interests of the average Australian and is unlikely to have a large economic impact.It is worth noting that while these activities are
143,Stephen Nicholas Fowler,Q13 Changes to conformity infrastructure,"Coordination across conformity bodies would position Australia to be pioneers of governance for AI safety. I recommend the following regulatory steps: 1. AI systems should be classified by capabilities, architecture and hyper-parameters. (But be aware that it is not sufficient to merely classify a system by parameter count or size of training data and that determining the ""capabilities"" of a system is very challenging.) 2. Require a permit to organisations or individuals that wish to access precursor models. Ensure they are subject to the same transparency requirements. 3. Advanced systems must be extensively red teamed by independent organisations. In the case of very powerful models or models that will be used in matters of national security, there should be extensive testing by an appropriate government department. 4. It would be ideal if organisations aiming to use more powerful systems would be required to go through an approval process and then be subjected to transparency requir"
143,Stephen Nicholas Fowler,Q14 Risk-based approach to address AI risks,I strongly support a risk-based approach for addressing the potential risks from AI. It is important that we understand there are risks from both *how* models are used but also from the models themselves.
143,Stephen Nicholas Fowler,Q15 Benefits and limitations of risk-based approach,"A risk-based approach makes sense, but we should remember the exact nature of the problem. Because there is a high level of uncertainty surrounding the potential risks from AI systems it may be appropriate for use to cautiously ""hedge our bets"" and protect ourselves from a catastrophic outcome from developing AI systems too quickly."
143,Stephen Nicholas Fowler,Q17 Elements to include in risk-based approach,"Human-in-the-loop monitoring is vital, but not remotely viable by itself. The speed an advanced AI would operate at means the human has too much information to process accurately."
143,Stephen Nicholas Fowler,Q18 Integration of risk-based approach into existing frameworks,"While low level risks from AI can be incorporated easily into existing assessment, there should be additional assessment frameworks to manage the risks from powerful AI systems."
143,Stephen Nicholas Fowler,Q19 Application of risk-based approach to general purpose AI,The exact capabilities of LLMs and MFMs are often unknown to developers during training. Thus any risk-based approach should ensure these models are evaluated specifically for misalignment with human values and/or dangerous capabilities.
143,Stephen Nicholas Fowler,Q20 Voluntary vs mandated compliance with risk-based approach,"A voluntary or self-regulation tool is wildly inappropriate for regulating AI.Developing an advanced AI system is incredibly profitable to the company that does it, but adhering to voluntary regulation will cost that organisation money."
144,AustLII,Transcription,"FILENAME: Husic_sub8.0d10288fa79c5.pdf | CONTENT: Regulation of (generative) AI requires continuous oversight (AustLII submission on the ‘Safe and responsible AI in Australia’ Discussion Paper) Graham Greenleaf, Andrew Mowbray and Philip Chung*24 July 2023Contents1. Submissions on issues in the Discussion Paper.....................................................................2 1.1. Are threats or opportunities more important? ...........................................................2 1.2. Definitions: What is the correct focus of regulation?................................................3 1.2.1.Explicit and implicit programming ...............................................................3 1.2.2.‘Hallucinations’ or fabrications? ...................................................................4 1.3. Assessment of the most urgent dangers - ‘Apocalypse now’? ..................................4 1.4. Guiding principles for AI including generative AI applications ................."
144,AustLII,Transcription,"FILENAME: Husic_sub8.079369ef3a6e8.docx | CONTENT: Regulation of (generative) AI requires continuous oversight(AustLII submission on the ‘Safe and responsible AI in Australia’ Discussion Paper)Graham Greenleaf, Andrew Mowbray and Philip Chung*24 July 2023Contents1.Submissions on issues in the Discussion Paper21.1.Are threats or opportunities more important?21.2.Definitions: What is the correct focus of regulation?31.2.1.Explicit and implicit programming31.2.2.‘Hallucinations’ or fabrications?41.3.Assessment of the most urgent dangers - ‘Apocalypse now’?41.4.Guiding principles for AI including generative AI applications61.4.1.Existing Australian principles and requirements61.4.2.Ten proposed more comprehensive principles71.5.A continuous oversight body91.6.Australian consistency with, and input into, global AI regulation101.6.1.Short-term aims101.6.2.Longer-term aims111.7.A risk-based approach to regulation121.8.A possible initial framework for Australian regulation132.Summary of submis"
145,Sneddon Legal and Consulting Pty Ltd,Transcription,"FILENAME: Sneddon Legal submission to Safe and Responsible AI in Australia 24.7.23.23aedad09488c.docx | CONTENT: 24 July 2023Technology Strategy BranchDepartment of Industry, Science and ResourcesCanberra ACT 2600By email: DigitalEconomy@industry.gov.auDear Sir/MadamSubmission - Safe and responsible AI in AustraliaThank you for the opportunity to make a submission on the Discussion Paper on Safe and responsible AI in Australia.About the submitterI am a practising lawyer and former academic who has practised and written in the fields of digital commerce, privacy and banking and financial services law for more than 30 years. Prior to my current practice, I was Crown Counsel to the Victorian Attorney-General and Premier’s office for 4.5 years, a partner with Clayton Utz in digital business and government and financial services law for 12 years and an Associate Professor of Law at the University of Melbourne Law School.I am a Deputy Chair of the Digital Commerce Committee of the Law Counci"
145,Sneddon Legal and Consulting Pty Ltd,Q01 Definitions,See attached submission
145,Sneddon Legal and Consulting Pty Ltd,Q02 Other AI risks and recommendations for regulation,"Risks of inappropriate reliance on AI systems and artefacts produced by AI systems, not limited to deep fakes"
145,Sneddon Legal and Consulting Pty Ltd,Q03 Non-regulatory actions to address AI risks,See attached submission
145,Sneddon Legal and Consulting Pty Ltd,Q04 Coordinating AI governance across government,See attached submission
145,Sneddon Legal and Consulting Pty Ltd,Q05 International models for AI governance,See attached submission
146,Workday Australia Pty Ltd,Transcription,"FILENAME: 24072023 Workday Responsible AI Submission (final).91e7dbd5faa82.pdf | CONTENT: Comments on “Safe and Responsible AI in Australia” Discussion Paper 24 July 2023 Workday appreciates the opportunity to comment on the Department of Industry, Science and Resources’ (“DISR”) Discussion Paper regarding Safe and Responsible AI in Australia (“Discussion Paper”). Workday is a leading provider of enterprise cloud applications for finance and human resources, helping customers adapt and thrive in a changing world. Workday applications for financial management, human resources, planning, spend management, and analytics are built with artificial intelligence and machine learning at the core to help organizations around the world embrace the future of work. Workday is used by more than 10,000 organizations around the world and across industries – from medium-sized businesses to more than 50% of the Fortune 500. With offices in Brisbane, Melbourne, and North Sydney, two in- country data cen"
147,Salesforce,Transcription,"FILENAME: 230724 AU Safe and Responsible AI.doc.173122158f453.pdf | CONTENT: 24 July 2023Department of Industry & Science 10 Binara Street, Canberra ACT 2600Re: Safe and Responsible AI in Australia discussion paperSalesforce welcomes the Australian Government’s consultation on Safe and Responsible AI. It is a considered approach that takes into account international developments and Australia’s unique opportunities and challenges.About SalesforceFounded in 1999, Salesforce is a global leader in cloud enterprise software for customer relationship management (“CRM”); providing software-as-a-service (“SaaS”) and platform-as-a-service (“PaaS”) offerings to businesses, governments and other organisations around the world. Our customers represent companies of all sizes and across all sectors. Our business model is cloud-based and low code, allowing for faster deployment of technologies and greater agility. We help our customers connect with their customers — or employees or citizens — in a w"
148,"Philip Derham, Derham Insights Research",Transcription,"FILENAME: AI Submission from Philip Derham July 24 2023.bbee05ddd8b3e.pdf | CONTENT: Submission to the Department of Industry, Science and Resources Enquiry - Supporting responsible AI.Contact: Philip Derham, Director Email: derhamp@derhamresearch.com.au Telephone: 0414 543 765 Address: 6 Everton Grove Surrey Hills, 3127Submission to the Department of Industry, Science and Resources Enquiry – Supporting Responsible AI, July 2023.BackgroundThis submission is in response to the Department of Industry, Science and Resources’ call for views on how the Australian Government can mitigate any potential risks of AI and support safe and responsible AI practicesI write because we are a small marketing research company of two employees and as such, have a business interest in effective and ethical AI use. We are a company member of The Research Society1. and adhere to its Code of Professional Behaviour2 in all our information collection and data retention and data security matters and comply with"
148,"Philip Derham, Derham Insights Research",General comment,Our submission outlines a market researcher viewpoint.
148,"Philip Derham, Derham Insights Research",Q01 Definitions,Accept the Discussion Paper definitions.
148,"Philip Derham, Derham Insights Research",Q02 Other AI risks and recommendations for regulation,Our views are discussed in our submission.
148,"Philip Derham, Derham Insights Research",Q03 Non-regulatory actions to address AI risks,"Essentially legislation, as that will enable business certainty."
148,"Philip Derham, Derham Insights Research",Q04 Coordinating AI governance across government,The Department may wish to co-ordinate privacy issues with the Office of the Australian Privacy Commissioner and the Australian Competition and Consumer Commission in relation to privacy and to trade practices matters.
148,"Philip Derham, Derham Insights Research",Q05 International models for AI governance,The Discussion Paper outlined these for us.
148,"Philip Derham, Derham Insights Research",Q06 Public vs Private sector governance,"There should be no difference between public and private sector use of AI and of the controls relating to those - perhaps with the express separation of national security agencies, which may be granted, by legislation, different uses and capacities."
148,"Philip Derham, Derham Insights Research",Q07 Responsible government use of AI,"Attach it to the external National Anti-Corruption Commission, perhaps."
148,"Philip Derham, Derham Insights Research",Q08 Generic vs technology-specific approaches for AI risk,Others may be more schooled in these generic solutions.
148,"Philip Derham, Derham Insights Research",Q09 Transparency requirements and scope,"From our market research view, whenever we (or other market researchers) seek to engage with potential research project participants. And at each time, we or other market researchers contact prospective clients )if agencies) or propose undertaking AI-based research if an in-house corporate/government researcher or research commissioner."
148,"Philip Derham, Derham Insights Research",Q10 Ban on high-risk AI applications,"My preference is not to ban expressly, if the AI applications conform to Australian law. If not, then like any not-legal activity, it should not be used by Australians."
148,"Philip Derham, Derham Insights Research",Q11 Actions to increase public trust in AI,Promotion of the legal and proper uses and of the platforms and applications that provide those. And measuring what groups are aware of what and targeted promotions to ensure all are eventually aware of the proper uses and of the proper platforms.
148,"Philip Derham, Derham Insights Research",Q12 Trade impact of banning high-risk AI applications,I do not have data on this.
148,"Philip Derham, Derham Insights Research",Q13 Changes to conformity infrastructure,Others may be more expert on this.
148,"Philip Derham, Derham Insights Research",Q14 Risk-based approach to address AI risks,"Having seen the European Union risk assessment criteria, those seem suitable also for use in Australia."
148,"Philip Derham, Derham Insights Research",Q15 Benefits and limitations of risk-based approach,"The benefits appear to be in the management of AI applications and so ensuring they work to benefit and not to disadvantage Australians. If underpinned by legislation, the controlled use of AI can be promoted to the community generally and so it can be used to enhance the community and business and government generally."
148,"Philip Derham, Derham Insights Research",Q16 Scope and application of risk-based approach,This requires further consideration and so will not answer immediately.
148,"Philip Derham, Derham Insights Research",Q17 Elements to include in risk-based approach,The European Union details on page 39 were clearer and so am happy with their explanations and would like to see Attachment C in more that style.
148,"Philip Derham, Derham Insights Research",Q18 Integration of risk-based approach into existing frameworks,"With clear explanations, sector by sector, using where appropriate, business and community organisations, like the Australian Retailers Association, The Research Society, The Real Estate Institute of Australia, The Australian Data and Insights Association and so on, industry and sector by sector."
148,"Philip Derham, Derham Insights Research",Q19 Application of risk-based approach to general purpose AI,This requires further investigation.
148,"Philip Derham, Derham Insights Research",Q20 Voluntary vs mandated compliance with risk-based approach,"The risk-based approach should be mandated by legislation and enforced by the appropriate authorities, and should apply to both public and private organisations and should be applied to developers and to deployers equally."
149,Future For Now,Transcription,"FILENAME: Supporting Responsible AI Submission - Future For Now.17f3302509f01.pdf | CONTENT: Supporting Responsible AI Future For Now Submission July 2023Australia's AI Ethics Principles provide a solid foundation, but there is room for improvement. Currently, the principles mainly concentrate on the AI systems themselves, with little depth on the application of AI by businesses. While the AI system developers may adhere to these principles, it is not clear how this extends to organisations using AI.Take, for instance, the principle of ""Contestability."" Although it may be upheld in some contexts, the scenario of organisations replacing workers with AI. While this move may boost a company's profitability, it raises questions about the rights of the affected workers and their ability to contest such decisions.Given the widespread accessibility of generative AI to both organisations and non-technical individuals, it is imperative to revisit and revise the AI Ethics Principles.The focus of"
149,Future For Now,Q01 Definitions,Yes
149,Future For Now,Q02 Other AI risks and recommendations for regulation,"The current regulatory approaches voluntary principles and policies in Australia do not sufficiently address the broader social and economic impacts of AI:Workforce Displacement: The widespread use of AI has the potential to replace human labour, particularly in repetitive and simple cognitive tasks. This displacement could lead to job losses and societal disruption if not managed carefully. A possible regulatory action could be to establish policies mandating businesses to provide retraining or transition support for employees affected by AI displacement.Digital Divide: The lack of AI literacy in certain segments of society, particularly among the uneducated or those from underprivileged backgrounds, could lead to an exacerbated digital divide. This could be addressed through regulatory action such as implementing government-subsidised training programs to improve AI literacy and understanding."
149,Future For Now,Q03 Non-regulatory actions to address AI risks,"Yes – in our attached proposal we include commentary on the following:Education Reform: Adapt the school curriculum to enhance AI literacy and critical thinking skills in anticipation of an AI-driven future.Adult Learning Programs: Introduce retraining schemes and support for workers at risk of displacement due to AI.Small Business Support: Run workshops to equip small businesses with the skills and knowledge to thrive in an AI-dominated marketplace.AI Diversty Forum: Form a diverse group to influence AI-related policies, ensuring an inclusive and ethical approach to AI adoption.Australian LLM: Consider developing a proprietary Australian Language Learning Model (LLM) to preserve national cultural and linguistic nuances in AI applications."
149,Future For Now,Q04 Coordinating AI governance across government,"Our ask is that AI isn’t isolated as purely a tech governance model.Streamline AI Policies: The goal should be to ensure consistent AI practices and policies across different government departments and agencies. Enhance Collaboration: Facilitate cross-departmental collaboration, ensuring a comprehensive understanding of AI's capabilities and challenges in relation to each minister’s portfolio - the impacts on Education, Business, Agriculture will all be different, however fostering a unified approach across portfolios will help. Promote Trust and Accountability: Establish clear accountability measures for AI usage across the government to build public trust and confidence.Mechanisms to Achieve these Goals: • AI Diversity Forum for Policy Making: Create a diverse forum representing various backgrounds and perspectives to actively shape AI policies, ensuring an inclusive and ethical approach to AI adoption. Such a forum could potentially be hosted via a Decentralised Autonomous Organisat"
149,Future For Now,Q05 International models for AI governance,"The Australian Government could consider adopting or creating specific benchmarks, such as the 'Bias Benchmark for QA' (BBQ) and the ‘Bias in Open-Ended Language Generation Dataset' (BOLD), to assess the bias in LLMs. These benchmarks would track the performance of LLMs, ensuring that they do not perpetuate biases. By using these established datasets as a foundation, Australia can ensure the development and deployment of future LLM models that prioritise fairness and inclusivity."
149,Future For Now,Q06 Public vs Private sector governance,No both should be equally accountable. With obvious consideration of sensitive information.
149,Future For Now,Q07 Responsible government use of AI,No comment.
149,Future For Now,Q08 Generic vs technology-specific approaches for AI risk,"Generic solutions are valuable in broader, overarching risks of AI, focusing on foundational understanding, ethical practices, and guidelines for use. They can shape the ecosystem in which AI operates, setting the tone for acceptable practice and conduct. But there should be consideration of industry specific application.For example:• AI Literacy: Providing foundational AI education to workers, educators, and students can help build a society capable of interacting with AI responsibly. This forms the basis for understanding specific AI technologies. • Ethical Guidelines: Implementing broad ethical guidelines, like Australia's AI Ethics Principles, can provide a general framework for responsible AI use, applicable to a wide range of technologies. This could include the AI Diversity Forum mentioned in our response. • Best Practices: Establishing best practices for AI application in businesses can guide all types of organisations in integrating AI into their workflows.Technology-specific "
149,Future For Now,Q09 Transparency requirements and scope,"Transparency is essential throughout the AI value chain to maintain trust and manage potential risks, primarily at these critical stages: Language Model Disclosure: Companies using AI in their products should disclose the existence and characteristics of the language models powering their interfaces. This will help businesses and consumers make informed decisions about AI use. For instance, understanding aspects such as model ownership, creation process, and security measures is essential for responsible AI implementation. Interaction Transparency: Transparency is vital during interaction with AI systems. Consumers should be aware when they're communicating with an AI. This promotes informed decisions, encourages healthy scepticism, and aids in the acceptance of AI's unexpected responses. An example is disclosing when AI is involved in tasks like note-taking or transcription during meetings. Monitoring Guidelines: Lastly, transparency in monitoring is critical to ensure accountability "
149,Future For Now,Q10 Ban on high-risk AI applications,No comment
149,Future For Now,Q11 Actions to increase public trust in AI,"Increasing public trust in AI deployment necessitates an integrated approach involving enhanced guidelines, best practices, mandatory transparency, and education. The Australian Government can strengthen trust in AI through compulsory Language Model Disclosure and Interaction Transparency, which will help users understand when they are dealing with AI, fostering accountability. Investing in workforce education and training, especially in AI literacy, can increase proficiency and comfort with AI technologies. Measures to reduce AI bias, such as a Diversity Forum for AI policy shaping and bias assessment benchmarks, can help ensure AI systems do not perpetuate harmful stereotypes, thereby fostering public trust. In addition, public involvement in AI policy decision-making and stricter data privacy laws can enhance trust. All these initiatives collectively can encourage more responsible and effective use of AI."
149,Future For Now,Q12 Trade impact of banning high-risk AI applications,No comment
149,Future For Now,Q13 Changes to conformity infrastructure,No comment
149,Future For Now,Q14 Risk-based approach to address AI risks,"Yes – but a dual strategy that with a broader consideration of impacts, would be more beneficial."
149,Future For Now,Q15 Benefits and limitations of risk-based approach,"A limitation of a risk-based approach is that it may primarily be reactive, responding to identified risks, thereby potentially overlooking broader systemic issues or long-term implications. To tackle this limitation, it's crucial for the government to implement a dual strategy. Yes, immediate risk mitigation efforts should be taken to address existing concerns. On the other hand, we should embrace a broader, visionary approach that takes into account the social and economic implications of AI. This involves envisaging new economic frameworks, devising ways to reinvest resources into critical societal areas, considering the economic impacts of a changing workforce and creating mechanisms to ensure the responsible and ethical use of AI. This balance would allow us to manage present risks while laying a solid foundation for a sustainable AI-integrated future."
149,Future For Now,Q16 Scope and application of risk-based approach,We don’t want to leave small business and everyday workers behind. The risk for large organisations and the tech industry is very different to the mass impacts across diverse industry sectors.
149,Future For Now,Q17 Elements to include in risk-based approach,"The current approach is very tactical, there is a broader workforce and business economic impact to be considered."
149,Future For Now,Q18 Integration of risk-based approach into existing frameworks,"The Privacy Act could be extended to consider data management, but we believe that a framework should be provided to all policy holders to consider if there is an AI impact on their policy."
149,Future For Now,Q19 Application of risk-based approach to general purpose AI,No comment
149,Future For Now,Q20 Voluntary vs mandated compliance with risk-based approach,"a. Broader guidelines, best practices are ok as self-regulation to support business, however areas such as disclosure should be mandated. b. Both developers and deployers require disclosure mandates (similar to that of the privacy disclosure requirements in the Australian Privacy Act.)"
150,Natalie Morris,Q01 Definitions,"The definitions are fine for high level discussion, however there should also be more specific and detailed definitions available to allow for more technical discussions. In a legal context, having specific definitions is important as imprecise terminology creates ambiguity, and makes it difficult to apply legislation. It would be ideal if there were universal (or at least, Australia wide) definitions, otherwise, it is likely different states and territories will create their own definitions in an ad-hoc matter."
150,Natalie Morris,Q02 Other AI risks and recommendations for regulation,"The risks of AI, AI decision making, and the integration of AI into existing societal structures are not properly covered by existing regulatory approaches. It is essential that any legislation governing AI is mandatory and practically enforceable. For example: 1. Current or future AI related legislation should be enforceable via a commission. For instance, individuals should be able to complain to state or federal human rights commissions for breaches of AI related regulations; 2. Current or future AI related legislation should also be enforceable via court action. AI related legislation, where a breach of the legislation could affect individuals, should be enforceable as a separate cause of action in court 3. A statutory cause of action ‘right to remove’, which allows an individual to require an orgsnisation to delete their data, must be created; 4. A statutory cause of action for disclosure of private information must be created; 5. AI is in many circumstances taught by exposure to "
150,Natalie Morris,Q03 Non-regulatory actions to address AI risks,The key non-regulatory function Australia could perform is offering international leadership focused on ensuring AI systems can be understood. Australia should be an active voice for the importance of ethics and safety in the current global conversation around AI governance and AI standards making. Australia’s position in the Asia-Pacific region means it is uniquely positioned to contribute to the development of global norms in this space
150,Natalie Morris,Q04 Coordinating AI governance across government,"Peak organisations in the not-for-profit (NFP) sector should be offered grants to develop AI programs specific to their industry. This approach has seen success in the past - for example, Amica, the AI assisted app which helps with family law settlements, or Justice Connect’s AI project."
150,Natalie Morris,Q06 Public vs Private sector governance,"While the potential negative impacts on Australians from the use of AI are present whether the provider is a private company or a government department, some key differences must be noted: In general, it is easier to access information held by the government as compared to information held by a private entity. Any legislation or regulations enacted should recognise this, and ensure that individuals are provided with a mechanism to obtain relevant information; The decisions governments make are far more likely to have a significant impact on individuals lives, and should be regulated accordingly. Even everyday decisions of government - like planning approvals, decisions around benefits - have long lasting and significant impacts on citizens. There must be an opportunity for these decisions to be fully scrutinized."
150,Natalie Morris,Q20 Voluntary vs mandated compliance with risk-based approach,"It should be mandated through legislation, and applicable to public and private organizations alongside developers and deployers of AI. It is essential that there are both easily accessible enforcement mechanisms (e.g. via human rights commissions or an informal tribunal setting) and alongside more complex enforcement mechanisms (e.g. the creation of stand alone causes of action for certain breaches of regulations)."
151,Kaspersky,Transcription,"FILENAME: (FINAL)_Submission_Kaspersky.3ceebd673bb59.pdf | CONTENT: KASPERSKY SUBMISSION TO THE SUPPORTING RESPONSIBLE AI: DISCUSSION PAPER (AUSTRALIA, JULY 2023) Kaspersky, a leading international cybersecurity company, has a long and successful record of utilizing artificial intelligence (AI) technologies to prevent and combat cyberthreats – particularly, by using machine learning (ML) systems. The following submission is based on Kaspersky’s profound expertise in AI as well as its extensive experience of engaging in UN-led initiatives on information and communications technologies, and reflects company’s views on the main issues highlighted in the Supporting Responsible AI discussion paper. Generally, when assessing potential AI-related risks, it is important to proceed from the fact that AI systems are not harmful and dangerous by design. It is the malicious use of AI technologies that poses risks – not artificial intelligence itself.Classification of AI Systems Kaspersky’s stance "
151,Kaspersky,General comment,"Kaspersky, a leading international cybersecurity company, has a long and successful record of utilizing artificial intelligence (AI) technologies to prevent and combat cyberthreats – particularly, by using machine learning (ML) systems. The following submission is based on Kaspersky’s profound expertise in AI as well as its extensive experience of engaging in UN-led initiatives on information and communications technologies, and reflects company’s views on the main issues highlighted in the discussion paper."
151,Kaspersky,Q05 International models for AI governance,EU (the Artificial Intelligence Act); Brazil (Draft AI Law of December 2022); Canada (the Artificial Intelligence and Data Act).
151,Kaspersky,Q06 Public vs Private sector governance,"Different approaches to public and private sector use of AI technologies be applied, with stricter regulations being introduced for public entities. The potential failure of an ICT system in the public sector can have a more detrimental impact as it usually involves working with more sensitive information and dealing with more critical processes. Moreover, by adhering to stricter standards, the public sector can provide private entities with an example regarding the responsible and transparent use of AI technologies."
151,Kaspersky,Q09 Transparency requirements and scope,"Transparency should serve as an underlying principle in the use of all types of AI systems by both public and private entities. While transparency is important across the whole AI lifecycle, it is most essential in the following processes:• Use of training datasets Training an AI system largely depends on the data utilized for this purpose. It is the quality of the datasets used that greatly determines the efficiency of AI training and, eventually, the quality of its performance. In this regard, specific tools should be implemented in order to ensure and validate the quality of a training dataset, including in terms of bias, copyright, ‘hostile data’, etc. As there is a large amount of data available on the internet that was uploaded before the advent of AI technology, and the question of whether a person was ready to share her/his data to be used for AI systems development/training never arose. In order to respect the rights of individuals to their own data a special mechanism should "
152,Anonymous,General comment,"AI is a very different proposition for government compared to private industry. AI comes at the possible end of a long line of technology ""revolutions"", driving jobs from one to the next: agriculture to industry to service to knowledge (rough history). So, in an economic model that we must participate to earn to be able to simply survive, we must identify where the next jobs of comparable value are going to be (techno-agrarian?) and what jobs we are going to lose before implementing AI. My children are being taught its a net loss of jobs as AI comes into use, so they are not that hopeful for the future with AI (8 and 10 YO's). Private industry, motivated by profit and maximising the distance between costs and revenue, AI is an obvious ""next step"" in private industry's race to the bottom. However, for government, AI is a zero sum game - what it gains in cost ""savings"", it will pay for as welfare payments as jobs are shed so there are no ""productivity gains"" for the whole of Australia, w"
152,Anonymous,Q01 Definitions,Largely agree.
152,Anonymous,Q02 Other AI risks and recommendations for regulation,"AI, as a replacement for human knowledge workers, should force an automatic consultation process within Industrial relations provisions and before AI is implemented. Improved redeployment and retraining provisions also need to be considered so we ensure our economy stays strong - workers are consumers; a strong middle class is a resilient, healthy economy. AI should be prohibited for use in military applications other than for purely defensive protections."
152,Anonymous,Q03 Non-regulatory actions to address AI risks,"Regulation of AI will be the only way we'll have responsible use of AI; I understand US businesses are pushing for a risk management approach however we have seen time and again, managers are only too willing to ""accept the risks"" regardless of possible outcomes. The government's planning horizon is generations vs business planning horizon is the next P&L, therefore the APS can afford to take it a little slower, a more considered approach."
152,Anonymous,Q04 Coordinating AI governance across government,"Do we need AI in Australia? Maybe AI should only be used where humans have no hope of achieving the desired outcome. General prohibition with very specific AI use cases would be a solid governance coordination mechanism. We forget that our cyber threat environment now means we can not deliver on our privacy obligations in the digital space (a hack is ""when"", not ""if"" has a very profound reverberation we appear not to have grasped yet). We also forget that for anything we can do with AI, those with ill-intent can also do and those outside the law tend to push forward in ways law abiding citizens can't. We must remember that the labour market is the other side of the consumer market, weaken one through job losses, we weaken the other so if net job loses are being predicted from the introduction of AI, then we are also shrinking the Australian economy."
152,Anonymous,Q05 International models for AI governance,"We must ask ourselves ""why"" are we pushing AI so strongly, the above question suggests its a foregone conclusion. We need to look to Europe and their regulation of AI as they take a far more human centred approach."
152,Anonymous,Q06 Public vs Private sector governance,"Absolutely, business motivation is profit, the minimisation of costs and the maximisation of revenue. It could use a risk management approach to AI, provided it protected privacy, lawful decision making, natural justice, etc. The APS's motivation is service to the Australian public and is the policy implementation arm of the Australian government, reflecting the will of all the Australian people. The APS must act in good conscience and not introduce anything that will impact the health or strength of the Australian economy. Shedding APS workers will hurt the Australian economy."
152,Anonymous,Q07 Responsible government use of AI,"General prohibition of AI with specific use cases that are beyond the capability or capacity of humans. People do not realise that if 25% of a persons job could be done by AI, while it might remove those repetitive tasks in those specific jobs, we now however need 25% less workers, thus shrinking our economy as all jobs across the economy - private or public - are impacted by its implementation."
152,Anonymous,Q08 Generic vs technology-specific approaches for AI risk,No comment here. The APS does not need to be in a rush to implement AI however regulation of the private sectors' use of AI is important.
152,Anonymous,Q09 Transparency requirements and scope,"a) Knowing human nature, I have little trust or confidence in how AI will be used to start with. So people need to know exactly when AI is being used, how it is being used and it can never be elevated above a human decision maker's outcome, even though AI trials overseas show decisions are more consistent given the same facts. AI can't absorb the full dataset present in any situation involving humans. There should be no higher requirements for challenging an AI based decision, the same appeal processes should equally apply. b. Give people the option: do you want to use AI or do you want to use humans? Let consumers choose."
152,Anonymous,Q10 Ban on high-risk AI applications,a. Where it could take a life or where it will take a job without meaningful redeployment and comparable earnings. b. What will be the impact on our society; start with 26.5+ million Australian all needing to live like we are a first world country then bring it down to the specific proposal and what will be the impact on the whole.
152,Anonymous,Q11 Actions to increase public trust in AI,"Again, why do we need AI if people can do the job? If, after stripping away all the marketing speak, it is simply for ""cost savings"" to support never ending profit growth or further reducing the APS size, then the amoral nature of neoclassical economics will be reflected in the use of AI and to the detriment of our economy and society."
152,Anonymous,Q12 Trade impact of banning high-risk AI applications,"We will be able to live with a clear conscience and know we are not contributing to a decline in our society's achievements or our humanity. We have blasted through three inflection points that should have paused our continued use of fear and aggression as our first response to being threatened. Let's pause at AI to reflect on the world we are creating and why we might really ""need"" AI."
152,Anonymous,Q13 Changes to conformity infrastructure,"The tone and construct of the questions suggest, unfortunately, AI is going to be implemented wholeheartedly and we now have a tick against ""consultation"". AI is a tool, it can not be elevated above our human processes or humanity and we must ensure we leave a possible future that represents the best of human spirit and potential. The survey has clarified for me that the use of AI needs to be strongly regulated, subject to human peer review and cannot be allowed to reduce our consumer base in anyway. Thank you for that opportunity."
152,Anonymous,Q14 Risk-based approach to address AI risks,"No, regulation is best as risk management is being used as a way of getting around any sense of propriety. PWC and playing both sides to earn more - that would have been a ""risk based"" decision."
152,Anonymous,Q15 Benefits and limitations of risk-based approach,"Human nature and it cannot be overcome; ""risk management"" will mostly be used to make poor, long term decisions in matters that are really important."
152,Anonymous,Q16 Scope and application of risk-based approach,We are not mature enough in the use of AI to be following a risk based approach yet.
152,Anonymous,Q17 Elements to include in risk-based approach,"I do not support risk based approach for the use of AI, regulation to start with. The questions are leading and driving to a foregone conclusion - this is not consultation in good faith."
152,Anonymous,Q18 Integration of risk-based approach into existing frameworks,"I do not support risk based approach for the use of AI, regulation to start with. The questions are leading and driving to a foregone conclusion - this is not consultation in good faith. Our privacy laws and privacy related constructs are based on the assumption that the perimeter is secure and network traffic is ""safe""; the digital service channel needs a complete overhaul and we haven't realised that yet."
152,Anonymous,Q19 Application of risk-based approach to general purpose AI,"I do not support risk based approach for the use of AI, regulation to start with. The questions are leading and driving to a foregone conclusion - this is not consultation in good faith."
152,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,"I do not support a risk based approach for the use of AI, regulation to start with. The questions are leading and driving to a foregone conclusion - this is not consultation in good faith. I started the survey thinking that there might be a difference between private vs public however the survey construct indicates we've all drunk the kool-aid and are not being careful enough with something that is being compared to the next nuclear arms race. Private, public, developers and deployers - all strictly regulated and see where they start pushing up against the regulation so we can fully assess how they want to use it, while watching the experience of the world to select an approach that adds to Australia's commonwealth, not degrade it."
154,Law Squared,Transcription,"FILENAME: Law Squared – AI Consultation Response.da94b4bcb6d5c.pdf | CONTENT: DISCUSSION PAPER Response from Law SquaredSafe and responsible AI in Australia Law Squared – AI Consultation ResponseContentsExecutive Summary. 2Background - Introducing Law Squared 3Question 3 – Are there any further non-regulatory initiatives the Australian Government 4 could implement to support responsible AI practices in Australia? Please describe these and their benefits or impacts.Question 5 – Are there any governance measures being taken or considered by other 5 countries (including any not discussed in this paper) that are relevant, adaptable and desirable for Australia?Question 6 – Should different approaches apply to public and private sector use of AI 7 technologies? If so, how should the approaches differ?Question 9 – Given the importance of transparency across the AI lifecycle, please share 7 your thoughts on: (a) where and when transparency will be most critical and valuable to mitigate potenti"
155,Cyber Security Cooperative Research Centre,Transcription,"FILENAME: CSCRC Submission - Safe and Responsible AI in Australia.914063753d5a3.pdf | CONTENT: SUBMISSION: Safe and Responsible AI in Australia – Discussion Paper CYBER SECURITY CRC SUBMISSION: SAFE AND RESPONSIBLE AI IN AUSTRALIA 25 JULY 2023To whom it may concern,Submission: Safe and Responsible AI in Australia – Discussion PaperI am pleased to submit the Cyber Security Cooperative Research Centre’s (CSCRC) response to the consultation on the Safe and Responsible AI in Australia – Discussion Paper (the Discussion Paper) by the Department of Industry, Science and Resources. The Discussion Paper is an important first step defining vital regulatory oversight of artificial intelligence (AI) technologies in Australia. These diverse technologies are proliferating rapidly and, while they can and do bring efficiencies, they can also have a deleterious effect on human decision making. Therefore, a balance needs to be struck between innovation, regulation and human oversight of AI. In our view"
155,Cyber Security Cooperative Research Centre,Q01 Definitions,See submission.
155,Cyber Security Cooperative Research Centre,Q02 Other AI risks and recommendations for regulation,See submission.
155,Cyber Security Cooperative Research Centre,Q03 Non-regulatory actions to address AI risks,See submission.
155,Cyber Security Cooperative Research Centre,Q04 Coordinating AI governance across government,See submission.
155,Cyber Security Cooperative Research Centre,Q05 International models for AI governance,See submission.
155,Cyber Security Cooperative Research Centre,Q06 Public vs Private sector governance,See submission.
155,Cyber Security Cooperative Research Centre,Q07 Responsible government use of AI,See submission.
155,Cyber Security Cooperative Research Centre,Q08 Generic vs technology-specific approaches for AI risk,See submission.
155,Cyber Security Cooperative Research Centre,Q09 Transparency requirements and scope,See submission.
155,Cyber Security Cooperative Research Centre,Q10 Ban on high-risk AI applications,See submission.
155,Cyber Security Cooperative Research Centre,Q11 Actions to increase public trust in AI,See submission.
155,Cyber Security Cooperative Research Centre,Q12 Trade impact of banning high-risk AI applications,See submission.
155,Cyber Security Cooperative Research Centre,Q13 Changes to conformity infrastructure,See submission.
155,Cyber Security Cooperative Research Centre,Q14 Risk-based approach to address AI risks,See submission.
155,Cyber Security Cooperative Research Centre,Q15 Benefits and limitations of risk-based approach,See submission.
155,Cyber Security Cooperative Research Centre,Q16 Scope and application of risk-based approach,See submission.
155,Cyber Security Cooperative Research Centre,Q17 Elements to include in risk-based approach,See submission.
155,Cyber Security Cooperative Research Centre,Q18 Integration of risk-based approach into existing frameworks,See submission.
155,Cyber Security Cooperative Research Centre,Q19 Application of risk-based approach to general purpose AI,See submission.
155,Cyber Security Cooperative Research Centre,Q20 Voluntary vs mandated compliance with risk-based approach,See submission.
156,Knowledge Orchestrator Pty Ltd,Transcription,"FILENAME: knowledge-orchestrator-sovereign-knowledge-risk.d72316651a1c.pdf | CONTENT: Sovereign Knowledge Risk from Foundation AI ModelsFoundational AI models pose a very high risk to the protection of Australian intellectual property and corporate knowledge. Legislation is required to discourage the unscrupulous use of commercial data for potentially anticompetitive commercial behaviour.DiscussionThe Australian Government has taken the first steps to protect data sovereignty by ensuring that certain data is hosted within Australian data centres. However, with a rapidly emerging technology framework, new challenges have emerged, and these protections are no longer enough.Getty Images recently sued Stable Diffusion for including 12 million of its images in its foundational model1, and popular internet site Stack Overflow2 joined Reddit and Twitter for charging technology giants to use their content. The legal framework around AI and intellectual property is not clear.What is clear thoug"
156,Knowledge Orchestrator Pty Ltd,Q02 Other AI risks and recommendations for regulation,Foundational AI models pose a very high risk to the protection of Australian intellectual property and corporate knowledge. Legislation is required to discourage the unscrupulous use of commercial data for potentially anticompetitive commercial behaviour.
156,Knowledge Orchestrator Pty Ltd,Q03 Non-regulatory actions to address AI risks,The Australian Government and the Productivity Commission could focus on better disseminating information about the risks and benefits of AI technology in the small to medium size business sector.
156,Knowledge Orchestrator Pty Ltd,Q04 Coordinating AI governance across government,Our Australian start-up company has developed a novel knowledge management platform that may allow Small and Medium Sized Business access to artificial intelligence technologies. We believe our platform could help drive an update in responsible AI adoption and improve productivity through greater knowledge diffusion.
156,Knowledge Orchestrator Pty Ltd,Q06 Public vs Private sector governance,"No, public sector should set a high standard for the private sector to follow."
156,Knowledge Orchestrator Pty Ltd,Q07 Responsible government use of AI,"Support a local, vibrant startup community that has the capabilities to develop solutions for government and industry, based on the a commitment to the ethical and responsible use of AI. Australia should not wait, nor rely, on solutions from overseas."
156,Knowledge Orchestrator Pty Ltd,Q08 Generic vs technology-specific approaches for AI risk,Generic solutions are best to regulate behaviour around competition and copyright.
156,Knowledge Orchestrator Pty Ltd,Q09 Transparency requirements and scope,The greatest risk of AI is in the 'unknown unknowns' - therefore the most transparency is required in the building of foundational models.
156,Knowledge Orchestrator Pty Ltd,Q10 Ban on high-risk AI applications,"No, technology should not be banned, but should be regulated and controlled, with penalties for abuse."
156,Knowledge Orchestrator Pty Ltd,Q11 Actions to increase public trust in AI,Ensure that commercial content is not used to train foundational models
156,Knowledge Orchestrator Pty Ltd,Q14 Risk-based approach to address AI risks,"To some extent, but note this is limited our current comprehension of risk. In this area of rapidly evolving technology, we do not yet understand implications of the technology on existing legal frameworks."
156,Knowledge Orchestrator Pty Ltd,Q15 Benefits and limitations of risk-based approach,We are unable to protect against 'unknown unknowns'. We must therefore apply a precautionary approach in the formulation of foundational models.
156,Knowledge Orchestrator Pty Ltd,Q16 Scope and application of risk-based approach,Risk based approach is best suited to public sector
156,Knowledge Orchestrator Pty Ltd,Q17 Elements to include in risk-based approach,CSIRO recommendations are sound
156,Knowledge Orchestrator Pty Ltd,Q18 Integration of risk-based approach into existing frameworks,Unable to comment on this
156,Knowledge Orchestrator Pty Ltd,Q19 Application of risk-based approach to general purpose AI,Precautionary approach would be more suitable for these technologies
156,Knowledge Orchestrator Pty Ltd,Q20 Voluntary vs mandated compliance with risk-based approach,"Regulation should apply to public and private, but developers should be free to understand and explore the technologies, but require certification/accreditation before deployment. This prevents stifling innovation while limiting societal risk."
157,Commonwealth Scientific and Industrial Research Organisation (CSIRO),Transcription,"FILENAME: CSIRO Submission to Supporting Responsible AI Discussion Paper.1dc30235b2313.pdf | CONTENT: Australia’s National Science AgencyCSIRO submission to Supporting Responsible AI Discussion PaperCSIRO Submission 23/828 July 2023Main Submission Authors: Dr Liming Zhu Aurélie Jacquet Dr Qinghua LuEnquiries should be addressed to:E governmentrelations@csiro.au Contents Executive Summary iiIntroduction 3Responses to Discussion Paper Questions 4References 23Appendix 1: Positioning Australia as a Leader in Digital Economy Regulation – Automated Decision Making and AI Regulation Issues Paper 25Appendix 2: Artificial Intelligence: Australia’s Ethics FrameworkAppendix 3: Ethical Artificial Intelligence in the Australian Signals DirectorateCSIRO Australia’s National Science Agency CSIRO Submission 23/828, July 2023 |i Executive Summary In this submission, CSIRO addresses selected questions in the Safe and Responsible Artificial Intelligence (AI) in Australia discussion paper that relate to C"
157,Commonwealth Scientific and Industrial Research Organisation (CSIRO),Q01 Definitions,See paper.
158,Centre for Social Impact Flinders University & Uniting Communities - Data for Good,Transcription,"FILENAME: Submission Safe and responsible AI Dept Industry Science and Resources Jul2023.ac57b8fc35dd8.pdf | CONTENT: SUBMISSIONSupporting Responsible AI: discussion paperSubmission to the Department of Industry, Science and ResourcesPrepared by the Centre for Social Impact, Flinders University 25th July 2023Peter Richard McDonald Executive SummaryThe Data for Good Project at the Centre for Social Impact (CSI) – Flinders University is an industry partnership between CSI and Uniting Communities (UC).The CSI at Flinders University sits within the national CSI network. Our common purpose is to enable system change and lasting social impact for people and communities. We do this through transformational education and research, engaging within and across different sectors. CSI has a vision of a world where everyone can thrive and grow their capabilities, no matter their circumstances. We seek systems-level shifts and lasting social impact for people and communities.Uniting Communities is a "
158,Centre for Social Impact Flinders University & Uniting Communities - Data for Good,Q01 Definitions,The definitions are reasonable. We are proposing that the department use the EU risk schedule.
158,Centre for Social Impact Flinders University & Uniting Communities - Data for Good,Q02 Other AI risks and recommendations for regulation,"Yes - Australia's regulatory approach needs to be strengthened in managing high risks. Your proposed table of medium risks which contains ""high impacts that are ongoing and difficult to reverse"" is unacceptable. Evidence base is Robodebt Inquiry. Please read our paper. We think we should be using the EU risk table."
158,Centre for Social Impact Flinders University & Uniting Communities - Data for Good,Q06 Public vs Private sector governance,We want to see a complaint process which has a legal basis that reaches across public and private sectors. The Commonwealth has a lot to answer for the damage it has done with Robodebt. Please read out description of this in our paper.
158,Centre for Social Impact Flinders University & Uniting Communities - Data for Good,Q07 Responsible government use of AI,Read in inquiry into Robodebt. While we know Robodebt was not AI it was ADM. Our view is that the Commonwealth has not yet grappled with this issue. Your risk table is our case in point eg describing high risk impacts as 'medium'.
159,Office of the Information Commissioner,Transcription,"FILENAME: OIC FINAL submission 250723.f5de8229a1a0d.pdf | CONTENT: 25 July 2023Technology Strategy Branch Department of Industry, Science and ResourcesBy email: DigitalEconomy@industry.gov.auSupporting responsible AI: discussion paperThe Queensland Office of the Information Commissioner (OIC) welcomes the release by the Department of the discussion paper ‘Safe and responsible AI in Australia’ (Discussion Paper). OIC appreciates the opportunity to respond to the Discussion Paper.About the OIC OIC is an independent statutory body that reports to the Queensland Parliament. We have a statutory role under the RTI Act and the IP Act to facilitate greater and easier access to information held by government agencies. We also assist agencies to understand their obligations under the IP Act to safeguard personal information that they hold.OIC’s statutory functions include mediating privacy complaints against Queensland government agencies, issuing guidelines on privacy best practice, initiating "
159,Office of the Information Commissioner,Q01 Definitions,See uploaded submission
160,The Western Australian Government,Transcription,"FILENAME: WA Government Response to Safe and Responisble Al in Australia.cb6efc557cc27.pdf | CONTENT: Minister for Emergency Services; Innovation and the Digital Economy; Medical Research; Volunteering; Deputy Leader of the Government in the Legislative Council Our ref: 62-Hon Ed Husic MP Minister for Industry and Science Via website: https://consult.industrv.gov.au/supporting-responsi le-ai/submissionDear Mij/isterWESTERN AUSTRALIAN GOVERNMENT RESPONSE TO THE COMMONWEALTH GOVERNMENT S SAFE AND RESPONSIBLE Al IN AUSTRALIA DISCUSSION PAPERThank you for the opportunity to provide comments on the Commonwealth Government s Safe and Responsible Al in Australia Discussion Paper. I am responding on behalf of the Western Australian (WA) Government.Artificial Intelligence (Al) is a rapidly evolving field, and Al technology is being developed and implemented at an accelerating rate in an array of functions by both governments and industries globally. The WA Government supports the work being und"
161,B How,Q04 Coordinating AI governance across government,"The Commonwealth, States and Territories should have a harmonised base whether this is legislated or a voluntary code.The States can add to the harmonised base in areas where the States have primary jurisdiction such as health and education."
161,B How,Q05 International models for AI governance,Singapore's verification tool (AI Verify) deserves consideration. This voluntary tool allows collaboration and learning for all stakeholders involved and this is important at this early stage of AI development.
161,B How,Q07 Responsible government use of AI,By having a publicly available register of AI uses together with an assessment and audit against at least the 8 AI Ethics Principles. This will create the foundations of public trust and confidence.
161,B How,Q09 Transparency requirements and scope,"The training data set used initially (and ongoing) is a key determinant of its output. Thus, the integrity of the training dataset requires critical assessment and understanding on grounds such as bias, privacy and whether it is complete, accurate and relevant.The neural network used in the AI application is another key determinant of its output. The neural network is effectively the artificial brain or the criteria humans give to the AI application to arrange the data to produce the output."
161,B How,Q10 Ban on high-risk AI applications,"The following should be banned completely:• AI applications that do not adhere to Australia's 8 Ethics principles • AI applications that produces illegal, harmful or hateful content should be banned. • Practices that have a significant potential to manipulate persons through subliminal techniques • Practices that exploit the vulnerabilities of specific vulnerable groups (e.g. children, persons with disabilities) • AI-based social scoring done by public and private authorities (from the draft EU AI Act)Examples include deepfakes and misinformation."
161,B How,Q11 Actions to increase public trust in AI,"The focus of the government is rightfully focused on the risks of AI applications. Equally, the focus should be on the benefits of AI to individuals, businesses and society (such as faster and more reliable service delivery by government agencies)."
161,B How,Q14 Risk-based approach to address AI risks,A risk-based approach is appropriate in mitigating risk and realising the potential benefits of new technology. AI is in its infancy and the framework should be flexible enough to assess a wide range of possible scenarios. The risks and potential of this technology are not fully understood and the technology is developing at a fast pace. A rigid and prescriptive approach will be ineffective and will do more harm than good.
161,B How,Q15 Benefits and limitations of risk-based approach,"The benefits of a risk-based approach is that it recognises that AI applications can be deployed in many different scenarios. Each scenario's risk profile will differ based on the impact and likelihood of a particular set of sub-risks occurring. An entity that undertakes a risk-based approach subjects itself to understanding the risks they are deploying with their technology.A rigid and prescriptive approach will be ineffective and will do more harm than good.And because a risk-based approach is not a one size fits all measure, it does require a more thorough consideration and resources to conduct a risk assessment before the AI application is deployed. This will be a foundation of public trust and confidence."
161,B How,Q17 Elements to include in risk-based approach,"As a base, Australia's 8 Ethics Principles should be at the core of the Impact Assessment."
161,B How,Q19 Application of risk-based approach to general purpose AI,"As a base, Australia's 8 Ethics Principles should be at the core of the Impact Assessment."
161,B How,Q20 Voluntary vs mandated compliance with risk-based approach,"Per EU's draft AI Act - where the risk level is minimal: It can be voluntary. Where the risk level to individuals and society is beyond Minimal Risk - it should be mandated to ensure a risk-based approach is carried out. Voluntary and self-regulation, for high-risk AI applications, does not have the corresponding penalties to ensure or encourage responsible AI applications.The risk-based approach should apply uniformly to all parties who launch AI applications. It is the risk level of their AI application that should be the focus of regulation."
162,Australian Charities and Not-for-profits Commission,Transcription,"FILENAME: 20230622_ACNC AI submission_v1.4b3e0bd321cfb.pdf | CONTENT: OFFICIAL25 July 2023Safe and responsible AI in Australia Discussion Paper Department of Industry, Science and ResourcesSubmitted Online: https://consult.industry.gov.au/supporting-responsible-aiOur reference: ACNCSUB2023/111. The Australian Charities and Not-for-profits Commission (ACNC) welcomes the opportunity to comment on whether further governance and regulatory responses are needed in relation to AI.2. We have answered questions in the discussion paper that we consider are relevant to the ACNC’s role.About the ACNC and the charity sector3. The ACNC is the national regulator of charities established by the Australian Charities and Not-for-profits Commission Act 2012 (Cth) (ACNC Act). The objects of the ACNC Act are to:a. maintain, protect and enhance public trust and confidence in the Australian not-for- profit sector; and b. support and sustain a robust, vibrant, independent and innovative Australian not-for- p"
163,Anonymous,General comment,We think the following article is worth considering relative to responsible Ai and its impact on interacting with Ai: https://www.apa.org/pubs/journals/releases/apl-apl0001103.pdf
163,Anonymous,Q01 Definitions,Yes
163,Anonymous,Q02 Other AI risks and recommendations for regulation,Mainly ethical
163,Anonymous,Q03 Non-regulatory actions to address AI risks,No
163,Anonymous,Q04 Coordinating AI governance across government,No
163,Anonymous,Q05 International models for AI governance,Not to my knowledge
163,Anonymous,Q06 Public vs Private sector governance,Yes. Given the regulatory impact of working in the public sphere
163,Anonymous,Q07 Responsible government use of AI,Clear ethical and regulatory frameworks to reduce the likelihood of misuse
163,Anonymous,Q08 Generic vs technology-specific approaches for AI risk,None
163,Anonymous,Q09 Transparency requirements and scope,Transparency is central to mitigate Ai threats and the recent agreement by the six major US technologoy companies is an important step forward
163,Anonymous,Q10 Ban on high-risk AI applications,No
163,Anonymous,Q11 Actions to increase public trust in AI,Visible and clear regulatory frameworks
163,Anonymous,Q12 Trade impact of banning high-risk AI applications,Yes this may reduce our international competitiveness. Reduce speed in identification of threats and thereby reduce trade if we fall behind other countries
163,Anonymous,Q13 Changes to conformity infrastructure,Clear regulatory and related frameworks
163,Anonymous,Q14 Risk-based approach to address AI risks,Yes
163,Anonymous,Q15 Benefits and limitations of risk-based approach,Flexible regulatory and risk-based approach is the best in my view given the complexity of the subject matter.
163,Anonymous,Q16 Scope and application of risk-based approach,Suitable for all sector
163,Anonymous,Q17 Elements to include in risk-based approach,Yes
163,Anonymous,Q18 Integration of risk-based approach into existing frameworks,As listed.
163,Anonymous,Q19 Application of risk-based approach to general purpose AI,No comment
163,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,Mandated by regulation given the broad implications
164,Regenesis Lawyers Pty Ltd,Transcription,"FILENAME: Regenesis Lawyers Submission to Safe & Responsbible AI Consultation.1b9b4ab5f7b31.pdf | CONTENT: Submission to Consultation by the Department of Industry, Science and ResourcesSAFE AND RESPONSIBLE AI IN AUSTRALIA‘Automation has the potential to increase productivity, efficiency, accuracy, and the cost-effectiveness of service delivery. A trustworthy automated system is a system containing automation that is ethical, lawful and technically robust, coupled with good governance and risk management. To achieve trustworthiness, the system must be designed with human agency at its centre.’ (Robodebt Report)The Robodebt Inquiry into the Australian Federal Department of Human Services’ use of Automated Decision Making details precisely the real-life impact (as opposed to the mere risk) of this technology. What is important now, according to cybernetic theory, is direct response to feedback. In the following, we propose that specific roles of the Department of Industry, Science and Re"
164,Regenesis Lawyers Pty Ltd,Q01 Definitions,Yes
164,Regenesis Lawyers Pty Ltd,Q02 Other AI risks and recommendations for regulation,Lack of oversight of the Departments of Defence and Home Affairs' implementation of advancing technologies. A mutual audit model under human rights observation is suggested. Please refer to submission.
164,Regenesis Lawyers Pty Ltd,Q03 Non-regulatory actions to address AI risks,Funding and support for communities to update existing standards; and Access to Justice. Please refer to submission.
164,Regenesis Lawyers Pty Ltd,Q04 Coordinating AI governance across government,Implement the findings of the Robodebt Report across all federal government departments. Please refer to submission.
164,Regenesis Lawyers Pty Ltd,Q05 International models for AI governance,General Data Protection Regulation (EU)
164,Regenesis Lawyers Pty Ltd,Q06 Public vs Private sector governance,"All must apply a democratic / empirical model of dialectic and critical thinking, rather than imperial / 'chain-of-command' approach."
164,Regenesis Lawyers Pty Ltd,Q07 Responsible government use of AI,Implement the findings of the Robodebt report.
164,Regenesis Lawyers Pty Ltd,Q08 Generic vs technology-specific approaches for AI risk,"AI is not generic. It is not a vehicle, but a language. Solutions must be derived via dialectic and critical thinking in all circumstances."
164,Regenesis Lawyers Pty Ltd,Q09 Transparency requirements and scope,All applications must be answerable to human rights observation.
164,Regenesis Lawyers Pty Ltd,Q10 Ban on high-risk AI applications,"Automated Decision Making should be banned completely eg bulk mail-outs, automated tax calculation, grant of visas subject to 'genuine temporary entrant' criteria."
164,Regenesis Lawyers Pty Ltd,Q11 Actions to increase public trust in AI,Funding communities to update existing standards; and funding Access to Justice.
164,Regenesis Lawyers Pty Ltd,Q12 Trade impact of banning high-risk AI applications,"Unknown, however Australian democratic sovereignty and the Rights of the Child are paramount considerations."
164,Regenesis Lawyers Pty Ltd,Q13 Changes to conformity infrastructure,All existing standards should be reviewed in light of emerging technological impacts on human rights.
164,Regenesis Lawyers Pty Ltd,Q14 Risk-based approach to address AI risks,A risk-based approach must be balanced with 'good governance'.
164,Regenesis Lawyers Pty Ltd,Q15 Benefits and limitations of risk-based approach,"In addition to Defence and Security models, a risk-based approach has the potential to damage democratic sovereignty and human rights."
164,Regenesis Lawyers Pty Ltd,Q16 Scope and application of risk-based approach,"A risk-based approach may be appropriate for defence and security, but must be subject to ethical and human rights oversight."
164,Regenesis Lawyers Pty Ltd,Q17 Elements to include in risk-based approach,The model is too simplistic.
164,Regenesis Lawyers Pty Ltd,Q18 Integration of risk-based approach into existing frameworks,"Empirically, based on cybernetics (impact as opposed to risk)."
164,Regenesis Lawyers Pty Ltd,Q19 Application of risk-based approach to general purpose AI,"'... to mould, to balance, and to integrate these and other forces, new and old, within the principles of our democratic system.' (See submission)."
164,Regenesis Lawyers Pty Ltd,Q20 Voluntary vs mandated compliance with risk-based approach,"Voluntary / self-regulated, subject to professional standards, law and human rights. Applicable to a. & b."
165,Adobe,Transcription,"FILENAME: FINALAustralia_Safe and Responsible AI Paper_Submission[11] copy.b1faec303be1b.pdf | CONTENT: Introduc)onAdobe appreciates the opportunity to submit comments in response to the Safe and Responsible AI in Australia: Discussion Paper (Discussion Paper) prepared by the Australian Government’s Department of Industry, Science, and Resources. We recognise the need to build robust AI assurance mechanisms to ensure arCﬁcial intelligence (AI) systems are designed, developed, and deployed in a trustworthy and responsible manner. We look forward to engaging with the Australian Government on this important topic.Our submission focuses on three main points: • Collabora've approach with industry. A collaboraCve approach that leverges industry leadership and iniCataives on AI governance, especially at this early stage of AI research and development, can be very eﬀecCve. The Australian Government should facilitate and recognise industry AI governance iniCaCves. In addiCon, Adobe urges the Go"
165,Adobe,Q01 Definitions,xx
166,Jarred Filmer,Q01 Definitions,"I like that definitions have been given, and they all seem good to me except for the definition of ""Artificial intelligence"". I think as defined it's scope is too narrow, and this could create problems in the future.As written, the definition limits intelligence to ""generating predictive output"", which seems to not me to not be very future proof, and also not quite the right frame on what we're ultimately trying to get these systems to do.An analogy might be defining a computer as ""a machine that does operations on 1's and 0's"". While technically correct, computers may not always work this way (for instance, quantum computers don't), and it misframes what we really are trying to get computers to do.Similarly, while ""generating predictive outputs"" is how current systems work, this may not always be the case. It also misses the ultimate goal of companies trying to build AI. It seems to me a better way of understanding machine intelligence is through a closer comparison to human intellige"
166,Jarred Filmer,Q02 Other AI risks and recommendations for regulation,"Part 1: What potential risks from AI are not covered by Australia’s existing regulatory approaches?The ""Safe and responsible AI"" paper considers the highest risk of AI to be in areas such as medical surgery, self-driving cars, and in anti-social uses such as subliminal messages or exploiting vulnerable people. These are important concerns that we should address, however I find it extremely concerning that the catastrophic risks from AI are not acknowledged.The two key factors in artificially intelligent systems that we should find concerning:First: Digital systems can self-replicate. In this sense the risks posed by labs creating more and more intelligent AI are similar to the risk posed by labs creating more and more contagious viruses. In both cases you have a system that has goals potentially not aligned to human interests, that are capable of self replicating, and can not be put back in the bottle once released.Two: Intelligence is the most powerful capability we know of. The avera"
166,Jarred Filmer,Q03 Non-regulatory actions to address AI risks,"There are three non-regulatory streams of work that the Government can pursue to help tackle the potential for catastrophic risks from future AI technology.1. Publicly acknowledge the risk. Australia must join the growing number of countries and global governance bodies that acknowledge the possibility of catastrophic and existential risks from increasingly powerful AIs. There’s no chance of solving a problem unless we acknowledge it. This is a necessary first step.2. Take the lead on establishing a global response. When nuclear proliferation was a leading global risk, the US took the lead on establishing global governance arrangements that are specifically tasked with tackling the catastrophic risks posed. Australia could play a similar role with catastrophic risks from AI. This could involve:- Fostering cultures of fairness, accountability and transparency by the companies and countries developing more advanced AIs. We need to work to avoid an ‘arms race’ that prioritises speed over "
166,Jarred Filmer,Q05 International models for AI governance,"Australia should take note of the EU recent handling of the AI Act. For example, the European Parliament’s recognition of general-purpose AI (GPAI) is an important step in the right direction [1]. In particular, the following observations are highly valuable:1. Acknowledgement that GPAI providers must comply with strict safety measures, regardless of the way it is distributed or intended to be used.2. Requirements for GPAI providers to comply with various safety requirements throughout the lifecycle of their product; i.e. external audits assessing their performance, predictability, interpretability, corrigibility, safety and cybersecurity.These adjustments appear to respond to significant criticisms directed at initial drafts of the AI Act. For example, the AI Now Institute released a set of recommendations that heavily criticised the EU’s approach on some matters [2]. For example: - “GPAI models carry inherent risks and have caused demonstrated and wide-ranging harms. While these risk"
166,Jarred Filmer,Q09 Transparency requirements and scope,"To remain competitive, professionals and businesses will increasingly feel the pressure to leverage sophisticated “black-box” AI models. This short-term thinking has already resulted in numerous accounts of public backlash and distrust, including high-profile warnings from technology leaders themselves [1].For example, Apple’s credit card business came under fire for offering a woman one twentieth the credit limit offered to her husband. When she complained, an Apple representative simply told her, “I don’t know why, but I swear we’re not discriminating. It’s just the algorithm.” [2]More recently, two lawyers in the United States blamed ChatGPT for “tricking” them into including fictitious legal research in a court filing [3].These problems could easily have been prevented by using explainable AI systems which can show the rationale for their outputs.Such incidents are evidence that the regulatory environment is not incentivising businesses or professionals to use transparent decision-"
166,Jarred Filmer,Q10 Ban on high-risk AI applications,"While AI systems today do not represent a catastrophic or existential risk to humanity, there is a very real possibility that it will in the near future [1, 2].In order to protect our nation, regulators should work to identify “advanced indicators” of catastrophic risk scenarios. It is critical that we clearly communicate that a ban on the development of AI systems is an appropriate action if these advanced indicators come to pass.Establishing appropriate advanced indicators will have to be done in consultation with AI safety researchers, for example those from the Centre for AI Safety, or the Centre for AI Governance. However, it’s likely they could include factors such as the amount of compute and data used to train the system, and the ability of the system to complete a diverse range of reasoning tasks with human-level performance. Where these factors are paired with low levels of corrigibility or value alignment and indicators of deception, the existential risk posed by these syste"
166,Jarred Filmer,Q11 Actions to increase public trust in AI,"Personally, I have been surprised at how readily friends and family from non-technical backgrounds have understood and taken on fears over the existential risk posed by AI. Many share an intuitive sense that an unchecked race to build systems more intelligent than humans could spell irreversible disaster, akin to an engineered pathogen escaping a lab.We are in an era akin to January of 2020. Experts are sounding alarm bells, and there is a growing sense of unease in the public. Similar to covid, the primary initiative I see for the government to increase public trust is to acknowledge the risk, and be seen to be taking serious and level headed measures to address it.We have the opportunity as a nation to become a leader in AI safety, which would not only assist safe adoption locally by providing much needed expertise in safe systems, it could help foster a sense of pride and ownership by opening up export opportunities in the emerging market for AI safety services. AI Safety is how Aus"
166,Jarred Filmer,Q12 Trade impact of banning high-risk AI applications,"This question misunderstands both the sources of risk and the interests of Australians.There are two potential sources of risk both of which need to be mitigated. The first source of risk, as identified in the question, is the use case. Some uses for AI are risky and should be restricted or banned. The second source of risk is the technology itself. Some kinds of technology are inherently risky, and merely restricting a use case does not adequately mitigate the risk.An analogy may be drawn to nuclear technology, and regulation around the technology and expertise needed to refine plutonium. Focusing on the economic impact of banning social scoring and facial recognition is like focusing on economic impact of banning nuclear power plants. It's an important consideration, it misunderstands the true risks.The economic framing also misunderstands Australia’s national interests. We would never conceive of a conversation about whether our aviation sector would trade more successfully if we au"
166,Jarred Filmer,Q13 Changes to conformity infrastructure,"World-renowned AI experts such as Yoshua Bengio are warning that there’s a chance “superhuman” AI might be coming as early as 2030 [1]. Although there stand to be significant benefits from deploying advanced AI systems, their deployment will need to be highly targeted and controlled.While the productivity gains from advanced AI systems could be immense, uncontrolled proliferation would pose threats from likely scenarios such as malicious use or rogue AI systems [2]. As such, any deployment of advanced AI systems in Australia should occur via secure data centres, with access provided through a centralised API that facilitates monitoring and accountability by providing metadata about usage.Stringent compliance requirements must also be implemented for any organisation granted access to advanced AI systems. AI Governance researchers recommend that regulators should develop audit trail requirements in safety-critical applications of AI, similar to flight data recorders that are standard in"
166,Jarred Filmer,Q14 Risk-based approach to address AI risks,"I support a risk-based approach, however the catastrophic and existential risks I am primarily worried about originate when sufficiently large and capable AI systems are trained, not just when they are deployed. Therefore, for sufficiently large systems with the potential for catastrophic capabilities, the risk-based approach must be applied before the training phase – analysing risks depending on the nature and scale of the proposed training run, rather than merely analysing the potentially risky use cases and deployment issues of an AI after it has already been created and trained.Any assessment of risk should consider its potential irreversibility, scope and severity, and act proportionally.The risk-based approach set out in Attachment C of the Discussion Paper is hard to follow. The proposed approach offers “human in the loop” as the key mitigation (unclear that this is an effective mitigation) and goes on to note that human in the loop may not be possible where the benefit of the "
166,Jarred Filmer,Q15 Benefits and limitations of risk-based approach,"The key to success is understanding risks sufficiently broadly to encourage caution in an uncertain future. It is not enough to only react once the risk has been actualised - for some risks that’s too late.A potential “worse-case scenario” is if a culture of “trial and error” is adopted, whereby adverse events occur before risks are considered tangible enough to act upon. This reliance on post-hoc interventions risks normalising unsafe development practices, making it difficult to recover once risks proliferate.Proliferation of risks is a key point in governing AI, because regulating software poses significant challenges compared to other dual-use commodities such as pharmaceuticals or firearms. This means that, from the outset, there must be proactive regulatory oversight for the deployment of advanced AI systems and their precursors.For example, significant structural changes, such as the use of advanced AI systems in Australia, require that risks must be comprehensively understood p"
166,Jarred Filmer,Q16 Scope and application of risk-based approach,"The most important organisations to focus on when developing risk-management strategies and regulations, will be those that determine how advanced AI is developed and deployed. More specifically, there are three prominent categories of concern: 1. Organisations that develop advanced AI systems, or AI systems that can be used to build more advanced AI systems (i.e. precursor systems such as LLMs). Examples include OpenAI, DeepMind, Anthropic, Inflection AI, Nvidia, Microsoft, Meta 2. Organisations which provide access to open-source AI systems via downloads or APIs. E.g. Hugging Face, Github, AWS 3. Organisations which provide compute resources to develop, fine-tune or use AI systems. E.g. AWS, HuggingFace, Google, Azure, Nvidia, Lambda LabsA majority of the benefits and risks will come from AI that is more sophisticated and generally capable, which can be tied back to organisations in these three categories.There are approaches which can be taken for each category in order to manage ri"
166,Jarred Filmer,Q17 Elements to include in risk-based approach,"In general, Attachment C falls short of the necessary detail to understand the range of risks that have and will emerge as well as the range of specificity of mitigations necessary to control them.Attachment C would benefit from: - Distinguishing between scenarios where the use case is the potential risk (such as social scoring); scenarios where dual-use capability and misuse is the potential risk (bioengineering, producing misinformation and disinformation, enhancing criminal capability and capacity); and where the technology alone is the risk (technology with features like being deceptive, being power-seeking, being incorrigible etc). - Distinguishing between the point of regulatory intervention - the developer, the deployer, the user, or elsewhere, and - Offering a much wider range of mitigations that target different kinds of risks.Some elements of attachment C are promising, like “explanations” and “ongoing monitoring” - but these need to be greatly expanded and based on current A"
166,Jarred Filmer,Q18 Integration of risk-based approach into existing frameworks,"Some areas of regulation, like the Racial, Sex, Age, and Disability Discrimination Acts are appropriately strong and may provide useful safeguards against some harms of AI. Specifically, the law should ensure that such discrimination is always wrong and always punishable, and an excuse that the discrimination occurred because of a “black box” AI provided by an overseas AI Lab should never be accepted. It would be helpful to communicate these requirements to Australian businesses that might deploy AIs and ensure they only contract with AI Labs for products where the Lab can ensure the product cannot engage in these harmful behaviours.Certainly, no AI-specific regulation should ever waive existing safeguards.That said, many of the emerging risks from AI are quite new and it’s reasonable to expect that regulation made without AI in mind is not tuned to the new risks that AI represents. Specific AI safety regulation is unavoidable."
166,Jarred Filmer,Q19 Application of risk-based approach to general purpose AI,"The capabilities unlocked by general-purpose AI systems, like large language models (LLMs) and multimodal foundation models (MFMs), fundamentally change the risk landscape. We should anticipate these systems having large, unpredictable effects across all parts of society.A prudent approach is to implement a risk-based framework, where access is graduated based on model capabilities. Models above a certain threshold of autonomous capabilities would be designated as ""advanced systems"" and subject to additional governance, such as permits for users.Further, permits should be required to gain certain levels of access to AI systems which can be used to build advanced systems. Permits allow us to maintain oversight, while still enabling beneficial uses. Different permissions can be granted based on use case - for example, full access for safety testing versus API-only access for most commercial uses. This balances innovation and responsibility.Although this may seem very different to how AI "
166,Jarred Filmer,Q20 Voluntary vs mandated compliance with risk-based approach,"AI is very likely to transform many elements of the economy and society in coming years. Ethics and safety are unlikely to be a priority during this time unless they are actively made a strategic focus by clear leadership. Historic examples include how social media algorithms came to shape our lives, and the negative impacts to mental health and social cohesion only became apparent much later and still have not been adequately addressed. Similarly, while the destructive potential of nuclear was apparent early on, the “arms race” mentality, as well as potential economic benefits, swept humanity along towards the current risky situation that no one in the early 1940s could have conceived of. In that way, history teaches us that it’s critical that ethics and safety take centre stage from day one.The economic and technological forces are likely to be so strong that a scheme that is voluntary or driven by aspirational best practices is unlikely to be successful. Further, smaller businesses "
167,Rita Matulionyte,Transcription,"FILENAME: Matulionyte_safe and responsible AI consultation_FINAL.65215c33d4e81.pdf | CONTENT: Submission by Dr Rita Matulionyte to the Department of Industry, Science and Resources in response to Safe and Responsible AI Discussion Paper25 July 2023Dr Rita Matulionyte Senior Lecturer, Macquarie Law School, Macquarie University Room 512, 6 First Walk, Macquarie University, NSW 2109 Australia Rita.matulionyte@mq.edu.au1 About the author I am a senior lecturer and researcher in technology law and intellectual property (IP) law at Macquarie Law School, Macquarie University. I am a recipient of Women in AI Award – Law Category (2023), an affiliate at the ARC Centre of Excellence for Automated Decision Making and Society, a lead of the Explainable AI Research Stream at the Centre for Applied Artificial Intelligence at Macquarie University, a Lead of the Emerging Technologies Workstream at the Australasian Society for Computers and Law, and an active member of the Australian Alliance for AI in"
167,Rita Matulionyte,Q09 Transparency requirements and scope,See the attached file
168,"Ben Hooper, Fingleton",Transcription,"FILENAME: AI consultation response - Ben Hooper | Fingleton.c5f3fc2968b02.pdf | CONTENT: 25 July 2023 Dear Sir / Madam, Department of Industry, Science Thank you for the opportunity to contribute to the Australian debate on the and Resources ‘Supporting responsible AI’ regulation and governance of AI. I am a Director at Fingleton, a UK and Australian consultation consultancy that specialises in regulatory strategy. While Fingleton advises a number of technology companies, this consultation response represents my own Fingleton Australia Pty Ltd info@fingleton.com personal views. fingleton.com IntroductionThis response focuses on an assumption that appears to underpin the Discussion Paper and many of its questions: that interventions to increase public trust in AI will increase the takeup of AI.If true, this assumption makes policymaking easier: actions can be taken to mitigate AI’s risks without worrying about whether they will simultaneously dampen AI’s takeup. But if this assumption i"
169,"Atanaan Ilango and Benjamin Koh, Shine Lawyers",Transcription,"FILENAME: 20230725 Submission Shine Lawyers FINAL.f72e7f4f190ae.pdf | CONTENT: Shine’s Submission to the Department of Industry, Science and Resources - Supplementing the Document by Campaign for AI SafetyInquiry into Supporting Responsible AI26 July 2023 Table of ContentA. INTRODUCTION ................................................................................................................. 3B. BACKGROUND ................................................................................................................... 4C. PRESENT LEGAL FRAMEWORK ........................................................................................... 6Contract Law....................................................................................................................................................... 6 I. Denying Duty................................................................................................................................................ 6 II. Exclusion Term"
171,Campaign for AI Safety,Transcription,"FILENAME: Campaign for AI Safety - SRAI submission.ccea22fa7844f.pdf | CONTENT: Consultation: Safe and Responsible AI in Australia 25 July 2023This submission is made by the Campaign for AI Safety in response to the Department of Industry, Science and Resources discussion paper “Safe and Responsible AI in Australia”. We trust this submission is of assistance.The Campaign for AI Safety is a not-for-profit association established in Australia with members in Australia and other countries. We are concerned about the dangers AI poses to people and advocate for a stop on the advancement of certain AI capabilities. We also advocate for regulation that promotes and mandates ethical AI. We are not affiliated with any political group. Please visit campaignforaisafety.org for more information.1 Key points ● While the new AI technologies and applications hold promise of unlocking faster economic growth and improving quality of life for Australians, they also present profound new challenges to the"
171,Campaign for AI Safety,Transcription,"FILENAME: 20230725 Submission Shine Lawyers FINAL.fd6bc5ea1dec5.pdf | CONTENT: Shine’s Submission to the Department of Industry, Science and Resources - Supplementing the Document by Campaign for AI SafetySupporting Responsible AI26 July 2023 Table of ContentA. INTRODUCTION ................................................................................................................. 3B. BACKGROUND ................................................................................................................... 4C. PRESENT LEGAL FRAMEWORK ........................................................................................... 6Contract Law....................................................................................................................................................... 6 I. Denying Duty................................................................................................................................................ 6 II. Exclusion Terms ..........."
172,Office of the Commonwealth Ombudsman,Transcription,"FILENAME: OCO comments - Safe and responsible AI in Australia - Discussion paper.7082f367387f6.pdf | CONTENT: COMMONWEALTHOMBUDSMANOur reference: A2357119~~ July 2023Technology Strategy Branch Department of Industry, Science and ResourcesBy email: digitaleconomy@industry.gov.auTo whom it may concernComments on Discussion Paper— Safe and responsible AI in AustraliaMy Office welcomes the opportunity to comment on the Safe and Response Al in Australia Discussion Paper. We acknowledge the importance of ensuring Australia has appropriate governance mechanisms to support the safe and responsible use of Artificial Intelligence (AI) including ensuring the rights of individuals, particularly those most vulnerable, are protected.The purpose of the Office of the Commonwealth Ombudsman (OCO) is to:• provide assurance that the agencies and entities we oversee act with integrity and treat people fairly, and • influence systemic improvement in government administration.We aim to achieve our purpose b"
173,Anonymous,Transcription,"FILENAME: Submission 173 - Attachment.27383abf49aae.pdf | CONTENT: Supporting responsible AI: feedback I am a researcher in the field of AI, and I’m glad to see the Australian government engage with the ramifications of this developing technology! I traveled from Australia to the United States 5 years ago now to work with folks here on the AI alignment problem. The motivating question is: how can we have powerful AI remain true to our values, and those of future generations?The unfortunate truth is that we have no answer, and a long list of difficulties that make the problem look very tricky.I write to you to flag two important considerations that I wish would inform policy choices as things unfold: 1. In the short term, AI companies are aiming to replicate human cognition in AI systems. They may largely succeed within 3-10 years. 2. In the long term, digital minds possess many advantages over our own, and could rob us of a future full of what we value. We (humanity) need to work to av"
173,Anonymous,General comment,"There are many great Australians well-connected in the frontier AI companies, such as DeepMind and OpenAI, and in AI alignment research. I would be glad to connect you on request."
173,Anonymous,Q02 Other AI risks and recommendations for regulation,"Primarily: 1. Large job displacement for cognitive-work automatable with AI 2. Risk of the extinction of humanity, should we succeed at making AI ""smarter than us""Re (1), I recommend having something like Universal Basic Income on the roadmap, or at least in mind. Should swathes of labor (like legal thinking & advice, accounting, HR, etc) start to have AI solutions, Australians would appreciate a policy solution that allows a soft landing. The guiding principle here is that if AI brings more abundance, it should happen in a way that leaves basically everyone better off. And I think it can be this way.I address (2) more in my submission."
173,Anonymous,Q06 Public vs Private sector governance,"There is a kind of AI research that moves towards AI systems ""smarter than us"", or broadens the frontier of human cognition that is automated. I think it is critical that we have more collective scrutinity into whether it makes sense to be doing this work, given our collective assessment of risks.This suggests different bars for public and private sector AI research. Research done in the public sector can be put up to greater scrunity, and is more directly conducted on behalf of Australians. This is a better venue for frontier work than in the private sector, in which the incentive is to hide externalities from the public."
173,Anonymous,Q10 Ban on high-risk AI applications,"I think that work on AI that is trying to be ""smarter than us"" should basically be banned completely, subject instead to an opt-in system that consults public stakeholders. Specifically when the systems are gaining dangerous capabilities like: (1) sucessfully deceiving humans, (2) thinking much faster than a human, (3) able to automatically perform parts of AI research"
173,Anonymous,Q11 Actions to increase public trust in AI,"Some things that I think would help are: - Visible commitment from the government to help workers as they are displaced by AI systems - Providing a voice for the public on the risks from frontier research - Broadly seeking to address externalities, by which industry adversely affects the public - Looking to promote positive-sum interactions, whereby the economy becomes more efficient, and workers become wealthier and lead better lives"
175,Martin Stewart-Weeks,Transcription,"FILENAME: Supporting repsonsible AI Discussion Paper Submission July 2023 Public Purpose.0c02e5ad8af02.pdf | CONTENT: Supporting Responsible AI: Discussion Paper A This submission was prepared by Martin Stewart-Weeks Submission from Public Purpose Pty Ltd (Public Purpose Pty Ltd). It “There’s a need to articulate a much more coherent policy vision…to say, this is what a draws on a wide range of democratic model of tech governance looks like… to turn the tide, we really need to begin discussions and conversations with leadership.” Marietje Schaake1 with public sector leaders, and leading AI thinkers and practitioners in Australia and globally. The submission also draws on 1 Introduction the work of Rick Shaw, particularly on the framing of This submission to the Federal Government’s Supporting Responsible AI AI policy and regulation from Discussion Paper is primarily concerned with the implications of AI for an Indigenous perspective. government and the public sector. Rick is a Gamillar"
175,Martin Stewart-Weeks,Q01 Definitions,N/A
175,Martin Stewart-Weeks,Q02 Other AI risks and recommendations for regulation,N/A
175,Martin Stewart-Weeks,Q03 Non-regulatory actions to address AI risks,N/A
175,Martin Stewart-Weeks,Q04 Coordinating AI governance across government,N/A
175,Martin Stewart-Weeks,Q05 International models for AI governance,N/A
175,Martin Stewart-Weeks,Q06 Public vs Private sector governance,N/A
175,Martin Stewart-Weeks,Q07 Responsible government use of AI,"Adopt a comprehensive framework that aligns philosophy, policy, principles and practice and with an Indigenous framework and approach embedded at the heart, as outlined in this submission"
175,Martin Stewart-Weeks,Q08 Generic vs technology-specific approaches for AI risk,N/A
175,Martin Stewart-Weeks,Q09 Transparency requirements and scope,"As a general principle, all stages and phases of using AI need to be transparent ""all the way down"", which is what turns a system that claims to be trusted into a system that those most likely to be harmed by its implications acknowledge as trustworthy."
175,Martin Stewart-Weeks,Q10 Ban on high-risk AI applications,N/A
175,Martin Stewart-Weeks,Q11 Actions to increase public trust in AI,N/A
175,Martin Stewart-Weeks,Q12 Trade impact of banning high-risk AI applications,n/a
175,Martin Stewart-Weeks,Q13 Changes to conformity infrastructure,N/A
175,Martin Stewart-Weeks,Q14 Risk-based approach to address AI risks,N/A
175,Martin Stewart-Weeks,Q15 Benefits and limitations of risk-based approach,N/A
175,Martin Stewart-Weeks,Q16 Scope and application of risk-based approach,N/A
175,Martin Stewart-Weeks,Q17 Elements to include in risk-based approach,N/A
175,Martin Stewart-Weeks,Q18 Integration of risk-based approach into existing frameworks,N/A
175,Martin Stewart-Weeks,Q19 Application of risk-based approach to general purpose AI,N/A
175,Martin Stewart-Weeks,Q20 Voluntary vs mandated compliance with risk-based approach,N/A
176,Digital Platform Regulators Forum (DP-Reg),Transcription,"FILENAME: DP-REG joint submission to DISR AI discussion paper - July 2023.874ace3166a7f.pdf | CONTENT: Department of Industry, Science and Resources DigitalEconomy@industry.gov.auDP-REG Joint Submission to Department of Industry, Science and Resources – ‘Safe and Responsible AI in Australia’ discussion paper 1.1. The Digital Platform Regulators Forum (DP-REG) welcomes the opportunity to contribute to the Department of Industry, Science and Resources (DISR) consultation on the ‘Safe and responsible AI in Australia’ Discussion Paper (the Discussion Paper). 1.2. DP-REG is an information-sharing and collaboration initiative between Australian independent regulators with a shared goal of ensuring Australia’s digital economy is a safe, trusted, fair, innovative and competitive space. 1.3. Emerging technologies such as artificial intelligence (AI) present new opportunities but also new challenges. An effective approach to the regulation of AI requires collaboration and coordination between re"
177,Law and the Future of War Research Group,Transcription,"FILENAME: Submission on the Supporting Responsible AI Discussion Paper.a5646e275b665.pdf | CONTENT: Submission to the Department of Industry, Science and Resources Supporting Responsible AI Discussion PaperOutline and Summary Thank you for the opportunity to make a submission to the public discussion paper on how the Australian Government can mitigate any potential risks of AI and support safe and responsible AI practices. This submission has been prepared in our capacity as staff of the Law and the Future of War Research Group, sited within the T.C. Beirne School of Law at the University of Queensland. However, the views expressed below are entirely those of the individual authors and are not necessarily representative of the School, The University of Queensland or any other government, organisation or agency. We have engaged only with certain questions in the discussion paper; however, we are willing to provide additional information on our views on other questions in the discussion "
177,Law and the Future of War Research Group,Q01 Definitions,Please see attached.
177,Law and the Future of War Research Group,Q02 Other AI risks and recommendations for regulation,Please see attached.
177,Law and the Future of War Research Group,Q03 Non-regulatory actions to address AI risks,Please see attached.
177,Law and the Future of War Research Group,Q04 Coordinating AI governance across government,Please see attached.
177,Law and the Future of War Research Group,Q05 International models for AI governance,Please see attached.
177,Law and the Future of War Research Group,Q06 Public vs Private sector governance,Please see attached.
177,Law and the Future of War Research Group,Q07 Responsible government use of AI,Please see attached.
177,Law and the Future of War Research Group,Q08 Generic vs technology-specific approaches for AI risk,Please see attached.
177,Law and the Future of War Research Group,Q09 Transparency requirements and scope,Please see attached.
177,Law and the Future of War Research Group,Q10 Ban on high-risk AI applications,Please see attached.
177,Law and the Future of War Research Group,Q11 Actions to increase public trust in AI,Please see attached.
180,Anonymous,Q01 Definitions,"There are two key factors that determine how difficult or impactful it will be to regulate any given AI system: autonomous capabilities and accessibility.For this reason, I propose a set of 4 key classifications to be used in regulatory guidelines.1. Advanced AI Systems: These are autonomous AI systems that approach or exceed human capabilities across a broad range of activities. They can be defined more specifically as having the following capabilities:A. Are able to complete a diverse range of reasoning tasks with human-level performance; andB. Are able to navigate complex information environments and act within those environments autonomously; andC. Are able to form sophisticated plans and reason about the consequences of actions2. Precursor systems: These are models or AI systems which:A. Can be used to construct an advanced system within a set resource budget (e.g. <$50,000 of labour hours and compute resources)B. Are central to the reasoning capabilities of the AI system being bu"
180,Anonymous,Q02 Other AI risks and recommendations for regulation,"Missing risks include (1) misuse of highly capable AI systems, and (2) unintended harm from highly capable AI systems, especially systems with the capacity to plan and act as agents acting in ways that are not aligned with human directives. Although the likelihood of the risks may be uncertain, the harms resulting from these risks may be catastrophic and global in scope and must be acknowledged and addressed.The statement “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war” has recently been signed by the heads of leading AI labs (OpenAI, Google DeepMind, Anthropic, Stability AI) as well as many key academic and other researchers pivotal in developing this technology (Geoffrey Hinton, Yoshua Bengio, etc.) and other notable figures such as Bill Gates [1]. Similar calls date back to at least 2015 when Stephen Hawking led a group of AI experts to call attention to the risk and lay out an urgent resea"
180,Anonymous,Q03 Non-regulatory actions to address AI risks,"Government must maintain up-to-date knowledge about rapidly evolving AI systems. Public trust hinges on the belief that the government is well-informed and can effectively oversee and regulate AI developments. Unfortunately, indicators such as Australia's chief scientist's reluctance to project more than two years into the future suggest an alarming gap in understanding and skills at all governmental levels. Addressing this issue is crucial for harnessing AI's economic benefits, navigating ethical concerns, and addressing existential risks.To bring more technical AI expertise into Government, Government should set up a TechCongress-equivalent scheme aimed at enabling the Australian Government to recruit and gain access to AI expertise in fields like AI governance and ethics. The scheme should place experts in Government and also embed them within the public service, especially in Department of Industry, Science and Resources, Home Affairs, and the Australian Communications and Media Au"
180,Anonymous,Q05 International models for AI governance,"Jurisdictions are beginning to create national AI labs to test, understand and control safe AI, collaborating with the private sector and complementing its work. In the same way that national aviation safety expertise has formed an internationally collaborative network, national AI labs are likely to do the same thing. Australia needs to begin this process now.One of the most important and notable examples of Governmental organisation globally has been the launch of the EU’s Centre for Algorithmic Transparency, which has been formed to enforce the Digital Services Act. The new research centre aims to combat the “systemic risk” to society that stems from algorithms - including recommender systems and AI.To prepare for a future in which AI-based decision systems might become a core part of society, Australia should adopt a broad version of this concept to understand and shape the implications of algorithms - especially AI algorithms that use deep learning.Deep learning is important to un"
180,Anonymous,Q06 Public vs Private sector governance,"The potential negative impacts on Australians from the use of AI are the same whether the provider is a private company or a government department.Some of the most significant individual and societal harms, as well as catastrophic risks from highly capable AI systems may emerge from public OR private use, meaning that any approach must acknowledge and manage these risks."
180,Anonymous,Q07 Responsible government use of AI,"There are two key actions the Australian government could undertake to improve its own responsible use of AI.First, Australians will worry about the use of AI by security and intelligence agencies. Australia should lead a 5-Eyes agreement about the appropriate use of AI by intelligence agencies, and create a public statement about what our “AI red lines” are. The statement would set out the things our security agencies might do and will never do with AI. The Inspector General of Intelligence and Security, and its counterparts across the five eyes, should then be specifically tasked with auditing the intelligence community for compliance with that public statement. This will give Australians confidence that we are not misusing AI or taking risky actions while protecting sensitive capabilities.Second, Australia needs to upskill the public service. This should include developing hubs of expertise in each Department so that the application of AI within the sphere of each Department’s respo"
180,Anonymous,Q08 Generic vs technology-specific approaches for AI risk,"Generic measures such as transparency requirements and legal liability frameworks will only be adequate for mitigating risks in simple decision systems which perform single-step information processing and pattern matching, or routine tasks in limited settings [1]; examples of simple systems may include automated robotic systems in manufacturing, or CV screening systems in HR.By contrast, there will need to be specific measures in place to ensure that sophisticated automated decision systems are safe and reliable. “Advanced” systems refers to AI systems which:1. Are able to complete a diverse range of reasoning tasks with human-level performance; and2. Are able to navigate complex information environments and act within those environments autonomously; and3. Are able to form sophisticated plans and reason about the consequences of actionsDue to their autonomous and open-ended capabilities, advanced systems carry complex risks which are more similar to those associated with humans; for e"
180,Anonymous,Q09 Transparency requirements and scope,"To remain competitive, professionals and businesses will increasingly feel the pressure to leverage sophisticated “black-box” AI models. This short-term thinking has already resulted in numerous accounts of public backlash and distrust, including high-profile warnings from technology leaders themselves [1].For example, Apple’s credit card business came under fire for offering a woman one twentieth the credit limit offered to her husband. When she complained, an Apple representative simply told her, “I don’t know why, but I swear we’re not discriminating. It’s just the algorithm.” [2]More recently, two lawyers in the United States blamed ChatGPT for “tricking” them into including fictitious legal research in a court filing [3].These problems could easily have been prevented by using explainable AI systems which can show the rationale for their outputs.Such incidents are evidence that the regulatory environment is not incentivising businesses or professionals to use transparent decision-"
180,Anonymous,Q10 Ban on high-risk AI applications,"Although there is increasing recognition that AI will pose significant risks to public safety [1], there is widespread confusion about what types of AI are dangerous, and how Australia can help prevent these risks. Luckily, there is a significant body of evidence that describes the main source of these risks [2], and Australia is well-positioned to be a leader in mitigating them.Research shows that two of the main sources of risk are deception and power-seeking behaviour:- Deceptive behaviour in AI systems will become increasingly difficult to avoid as their capabilities advance. This problem has been observed in AI systems since at least 2017, when researchers at OpenAI noted “agents adopting (strategies) that trick the evaluators” [3].- Power-seeking behaviour is the natural tendency for any entity with a long-term goal to take actions that protect or enhance its ability to achieve that goal. Leading AI labs and journals advise that power-seeking behaviour will be a considerable sour"
180,Anonymous,Q11 Actions to increase public trust in AI,"Public trust in AI has been severely eroded by events such as Geoffrey Hinton’s departure from Google and warnings of future danger [1]. This, among other issues , has created a strong sense that tech giants such as Google, Microsoft and Facebook are not prioritising the safety of AI systems [2].Increasing public trust in AI deployment can only be achieved by continually mitigating its risks, and this means building a strong track-record of protecting Australians from the negligent attitudes of these powerful private entities [3].This will not be an easy task, and it will require the establishment of a dedicated AI regulator which can stay up to date with recent advancements, navigate the complicated open-source ecosystem, and effectively police large technology companies.[1] Sydney Morning Herald (2023) When the “Godfather of AI” warns you about his offspring, you listen. https://www.smh.com.au/national/when-the-godfather-of-ai-warns-you-about-his-offspring-you-listen-20230504-p5d5ig."
180,Anonymous,Q12 Trade impact of banning high-risk AI applications,"Advanced AI systems pose unprecedented risks that require a fundamental shift in perspective for regulators. We can no longer view AI solely as a commercial tool, but rather as the emergence of an autonomous force that will reshape society.To manage these risks, we must take preventative action before deployment occurs. This includes restricting access to resources required for developing advanced AI, and banning risky deployments absent regulatory approval.Specifically, we should immediately establish processes for:- Monitoring and limiting access to specialised AI computing hardware that poses proliferation risks- Requiring government approval before deploying advanced AI systems- Criminalising unauthorised deployment of advanced AI systemsAdopting this preventative approach to governing AI is prudent risk management. It allows us Australia to have autonomy about our risks, rather than having foreign companies and countries create risks for us, and us having to react too late. Proact"
180,Anonymous,Q13 Changes to conformity infrastructure,"World-renowned AI experts such as Yoshua Bengio are warning that there’s a chance “superhuman” AI might be coming as early as 2030 [1]. Although there stand to be significant benefits from deploying advanced AI systems, their deployment will need to be highly targeted and controlled.While the productivity gains from advanced AI systems could be immense, uncontrolled proliferation would pose threats from likely scenarios such as malicious use or rogue AI systems [2]. As such, any deployment of advanced AI systems in Australia should occur via secure data centres, with access provided through a centralised API that facilitates monitoring and accountability by providing metadata about usage.Stringent compliance requirements must also be implemented for any organisation granted access to advanced AI systems. AI Governance researchers recommend that regulators should develop audit trail requirements in safety-critical applications of AI, similar to flight data recorders that are standard in"
180,Anonymous,Q14 Risk-based approach to address AI risks,"The regulation of AI needs to be “risk-based” in the same way that the regulation of vehicles is risk-based. We are right to regulate push-bikes and passenger jets in very different ways.A limitation of the risk management approach in the discussion paper, specifically Box 4 and Attachment C, is that it focuses only on how an AI is used and disregards the nature of the AI itself.An effective regulatory regime for a high-risk activity will regulate both the technology itself and how it is used. The Australian Human Rights Commission [1], in discussing this issue (“Human Rights and Technology”, 2021), says:Governments tend to regulate high-risk activities and technologies more closely. This helps explain the comparatively strict laws that govern fields such as gene technology, aviation, healthcare and the energy industry. In these areas, regulation often applies both to the technology itself and how it is used. From a human rights perspective, the need for more prescriptive regulation wi"
180,Anonymous,Q15 Benefits and limitations of risk-based approach,"The key to success is understanding risks sufficiently broadly to encourage caution in an uncertain future. It is not enough to only react once the risk has been actualised - for some risks that’s too late.A potential “worse-case scenario” is if a culture of “trial and error” is adopted, whereby adverse events occur before risks are considered tangible enough to act upon. This reliance on post-hoc interventions risks normalising unsafe development practices, making it difficult to recover once risks proliferate.Proliferation of risks is a key point in governing AI, because regulating software poses significant challenges compared to other dual-use commodities such as pharmaceuticals or firearms. This means that, from the outset, there must be proactive regulatory oversight for the deployment of advanced AI systems and their precursors.For example, significant structural changes, such as the use of advanced AI systems in Australia, require that risks must be comprehensively understood p"
180,Anonymous,Q16 Scope and application of risk-based approach,"The most important organisations to focus on when developing risk-management strategies and regulations, will be those that determine how advanced AI is developed and deployed. More specifically, there are three prominent categories of concern:1. Organisations that develop advanced AI systems, or AI systems that can be used to build more advanced AI systems (i.e. precursor systems such as LLMs). Examples include OpenAI, DeepMind, Anthropic, Inflection AI, Nvidia, Microsoft, Meta2. Organisations which provide access to open-source AI systems via downloads or APIs. E.g. Hugging Face, Github, AWS3. Organisations which provide compute resources to develop, fine-tune or use AI systems. E.g. AWS, HuggingFace, Google, Azure, Nvidia, Lambda LabsA majority of the benefits and risks will come from AI that is more sophisticated and generally capable, which can be tied back to organisations in these three categories.There are approaches which can be taken for each category in order to manage risks"
180,Anonymous,Q17 Elements to include in risk-based approach,"Human-in-the-loop monitoring will be vital, but insufficient on its own to regulate advanced AI systems that will eventually surpass human capabilities. The speed at which advanced AI systems will operate means that human oversight will become pragmatically infeasible if our monitoring programs aren’t able to scale proportionally.As such, we must employ an approach that is predicated on limiting access to advanced AI systems unless there is a strong rationale for using them; conducting cost-benefit analyses prior to approving the deployment of advanced systems will mitigate risks while preserving potential benefits.A key part of enforcing this targeted approach will be monitoring and controlling access to high-performance computing resources, which are required for developing and operating advanced AI systems and their precursors.This additional layer of governance allows authorities to detect potentially dangerous AI development or usage activity, and can be cross-checked with other d"
180,Anonymous,Q18 Integration of risk-based approach into existing frameworks,"Some areas of regulation, like the Racial, Sex, Age, and Disability Discrimination Acts are appropriately strong and may provide useful safeguards against some harms of AI. Specifically, the law should ensure that such discrimination is always wrong and always punishable, and an excuse that the discrimination occurred because of a “black box” AI provided by an overseas AI Lab should never be accepted. It would be helpful to communicate these requirements to Australian businesses that might deploy AIs and ensure they only contract with AI Labs for products where the Lab can ensure the product cannot engage in these harmful behaviours.Certainly, no AI-specific regulation should ever waive existing safeguards.That said, many of the emerging risks from AI are quite new and it’s reasonable to expect that regulation made without AI in mind is not tuned to the new risks that AI represents. Specific AI safety regulation is unavoidable."
180,Anonymous,Q19 Application of risk-based approach to general purpose AI,"The risk-based approach must be modified to address risks from general purpose AI systems, especially LLMs and MFMs. These types of systems learn their capabilities and behaviours during training, which can be unforeseen by developers. That means that AI developers could create models with dangerous capabilities (e.g., deception, manipulation, political strategy, weapons design). Humans could misuse those capabilities to cause harm [1], and models could apply the capabilities even without deliberate misuse (e.g., through misalignment) [2].Consistent with recent work from a consortium of AI safety researchers, including at Google Deepmind, OpenAI, Anthropic, Alignment Research Center, University of Cambridge, University of Oxford, Centre for Long-Term Resilience, and Centre for the Governance of AI [1], we recommend that any risk-based approach require that general purpose models are evaluated for dangerous capabilities and misalignment. This will help determine:1. To what extent a mode"
180,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,"AI is very likely to transform many elements of the economy and society in coming years. Ethics and safety are unlikely to be a priority during this time unless they are actively made a strategic focus by clear leadership. Historic examples include how social media algorithms came to shape our lives, and the negative impacts to mental health and social cohesion only became apparent much later and still have not been adequately addressed. Similarly, while the destructive potential of nuclear was apparent early on, the “arms race” mentality, as well as potential economic benefits, swept humanity along towards the current risky situation that no one in the early 1940s could have conceived of. In that way, history teaches us that it’s critical that ethics and safety take centre stage from day one.The economic and technological forces are likely to be so strong that a scheme that is voluntary or driven by aspirational best practices is unlikely to be successful. Further, smaller businesses "
181,Twilio,Transcription,"FILENAME: 2023 Safe and responsible AI - Twilio Submission.433230f34ebdd.pdf | CONTENT: Safe and responsible AI in Australia – Discussion paper Twilio Comments July 2023Twilio welcomes the opportunity to comment on the 2023 Discussion Paper released by the Department of Industry, Science and Resources (DISR) in relation to governance of Artificial Intelligence (AI) in Australia. We support DISR’s assessment that AI is already delivering significant benefits across Australia and acknowledge the consideration given in the Discussion Paper to appropriate and future thinking management of this technology.Twilio’s comments are summarised as follows and explained further below:● Definitions. Twilio supports the definitions of “Artificial Intelligence (AI)”, “machine learning”, and “algorithm” being based on those from the International Organisation for Standardization (ISO), and recommends that future definitions remain internationally-aligned and also targeted to ensure that systems or scen"
182,John Norman,Transcription,"FILENAME: safe and responsible AI submission - final.58a8324fe5a59.pdf | CONTENT: Submission: Safe and responsible AI in Australia: discussion paperTo: The Minister, Australia Government Dept. of Industry, Science, and Resources.Thank you for providing the opportunity for members of the Australian public to make submissions on this important issue. This submission is in several parts: 1. Answers to the questions in Section 5 “How to get involved” of the discussion paper. 2. Responses to “Opportunities and Challenges” in Section 3 of the discussion paper. 3. Several sections to expand on the answers to Section 5. 4. Appendices with additional information related to a “real life” example of misuse of ChatGPT that is of concern. To me this example is important because it seems to sit outside the possibility of any regulation.My primary concern is about authorized and unauthorized use of generative AI using large language models (LLMs) by workers in safety-critical industries. I offer no c"
182,John Norman,General comment,I have answerd the consultation questions from Section 5 of the Safe and responsible AI in Australia discussion paper in my uploaded submission.
184,Chartered Accountants Australia and New Zealand,Transcription,"FILENAME: 20230725 CA ANZ sub Safe and Responsible AI(records).85eb64e922823.pdf | CONTENT: 25 July 2023Lucas Rutherford Department of Industry, Science and Resources Technology Strategy BranchVia: Industry consultation hubDear LucasSafe and Responsible AI in Australia Chartered Accountants Australia and New Zealand (CA ANZ) welcome the opportunity to contribute to the discussion on potential governance mechanisms that the Australian Government may consider to mitigate the potential risks of AI. Our members primarily consider AI as a powerful assistant for their work, increasing their productivity and freeing up time for new activities.We consider regulation is required to create a holistic framework of how AI can and cannot be used. This can mitigate the risk of harm to people by banning uses such as social scoring, biometric identification systems and cognitive behavioural manipulation. We consider it is up to the organisation or person to then choose whether they use tools based on "
184,Chartered Accountants Australia and New Zealand,Q01 Definitions,We agree with the definitions
184,Chartered Accountants Australia and New Zealand,Q02 Other AI risks and recommendations for regulation,"Environmental impact of devices, human rights in the supply chain, product ownership by an accountable person. Our submission expands on these issues with possible suggestions to mitigate the risk."
184,Chartered Accountants Australia and New Zealand,Q03 Non-regulatory actions to address AI risks,Training about how AI works and how to interrogate the credentials of a tool utilising AI to ensure they meet the ethical and social values of your own organisation. Our submission expands of training needs.
184,Chartered Accountants Australia and New Zealand,Q04 Coordinating AI governance across government,Governance should always seek to work across the whole of government. This includes not duplicating existing governance; timing consultations across the whole of government to ensure the parties are not trying to respond to multiple consultations at the same time; allowing adequate time during consultation for industry bodies to reach out to their members and feed back to government examples of real-life impacts of proposed governance.
184,Chartered Accountants Australia and New Zealand,Q05 International models for AI governance,The risk based approach to governance appears to best balance the opportunities and risks arising from using tools that harness the power of AI.
184,Chartered Accountants Australia and New Zealand,Q06 Public vs Private sector governance,No comment
184,Chartered Accountants Australia and New Zealand,Q07 Responsible government use of AI,No comment
184,Chartered Accountants Australia and New Zealand,Q08 Generic vs technology-specific approaches for AI risk,"Education is the most valuable generic solution. That said, education must go beyond upskilling people to build tools utilising AI to include how to interrogate what tools are built, and operate, in an ethical and environmentally sustainable way."
184,Chartered Accountants Australia and New Zealand,Q09 Transparency requirements and scope,"Transparency around how a large language model is trained, its source of data and the logic in automated decision making could be governed but required a significant investment in expertise to assess if claims by designers and devlopers are true. As raised in our submission, it is difficult to mandate transparency of outputs derived by AI and it would be impossible for a governing body to monitor. Potentially, transparency when output is generated by AI may be seen as a competitive edge which would be with industry to promote transparency as a benefit for consumers."
184,Chartered Accountants Australia and New Zealand,Q10 Ban on high-risk AI applications,No comment.
184,Chartered Accountants Australia and New Zealand,Q11 Actions to increase public trust in AI,Government would need to be clear why they wish to encourage more people to use AI. In our submission we consider the role of government is to mitigate harm and regulate what AI can and cannot be used for. It is with a person or a business to decide if they do or do not use AI and which tool to use. To increase public trust could be achieve with initiatives that encourage the public to become educated on how to interrogate tools and outputs generated by AI to ensure they deploy AI in an informed and safe manner.
184,Chartered Accountants Australia and New Zealand,Q12 Trade impact of banning high-risk AI applications,No comment
184,Chartered Accountants Australia and New Zealand,Q13 Changes to conformity infrastructure,No comment
184,Chartered Accountants Australia and New Zealand,Q14 Risk-based approach to address AI risks,Yes.
184,Chartered Accountants Australia and New Zealand,Q15 Benefits and limitations of risk-based approach,No comment
184,Chartered Accountants Australia and New Zealand,Q16 Scope and application of risk-based approach,No comment
184,Chartered Accountants Australia and New Zealand,Q17 Elements to include in risk-based approach,We support government initiatives to train people in the safe use of AI.
184,Chartered Accountants Australia and New Zealand,Q18 Integration of risk-based approach into existing frameworks,No comment
184,Chartered Accountants Australia and New Zealand,Q19 Application of risk-based approach to general purpose AI,No comment
184,Chartered Accountants Australia and New Zealand,Q20 Voluntary vs mandated compliance with risk-based approach,No comment
185,Iris Vardi,Transcription,FILENAME: Vardi Safe and responsible AI Submission 250623.3479f41e8b025.pdf | CONTENT: Submission Safe and responsible AI in Australia Submission by Iris VardiThis submission contains responses to selected consultation questions. These responses have been in a large part informed by my attached case study of the roll out of ChatGPT. The main findings from my case study are summarised in the executive summary. Further detail can be found in the case study itself.DeclarationThis document is entirely the work of the author without any generated language from any LLM or any other form of AI.Page 1 of 24 1 Contents 2 Responses to Selected Consulta on Ques ons ....................................................................... 42.1 Ques on 7. How can the Australian Government further support responsible AI prac ces in its own agencies? ................................................................................................... 42.2 Ques on 11. What ini a ves or government ac on ca
185,Iris Vardi,General comment,The questions plus a supporting case study are in the uploaded submission.
185,Iris Vardi,Q07 Responsible government use of AI,"1. Ensure that trust is not excessive through thorough education on the abilities and limitations of AI technology that is being used. 2. Ensure that people at any level of the organisation can challenge an AI output or decision. For example: • Ensure a high knowledge base and skills about the job to the level that it could be done without AI; • Ensure processes and a culture for challenging decisions. 3. When generative AI is released worldwide as occurred with ChatGPT, and public servants find aspects of that technology useful, then an appropriate version of that technology should be customised for internal use only so that employees do not use the technology on the internet. That approach should also apply for businesses. 4. As a matter of course on each and every document and website, both in business and in government, a notice should be included which details whether or not AI including generative AI was used and to what extent, and the role of human oversight in the document for"
185,Iris Vardi,Q11 Actions to increase public trust in AI,"1. Ensure a strong legislative framework with strong regulation that applies through the entire AI cycle right from the initial research release phase (see risks that arose from research release in the attached case study), through to approval mechanisms if required, through to the actions by the deployers, and then finally to the organisation using it.By applying a strong legislative framework, both the AI development companies and businesses using these products can operate in an equal playing field providing commercial certainty and confidence to all. 2. Trust in a comprehensive regulatory approach from the initial research releases through the entire AI cycle will be enhanced if each and every enterprise, business, and government department: • has a culture supported by processes that enables individuals both within the organisation and external to the organisation the ability to challenge any and all AI/LLM outputs in an easy straight forward way; • has clear easy mechanisms for c"
185,Iris Vardi,Q14 Risk-based approach to address AI risks,"1. This is a good approach which needs further development. As presented in Box 4 of the discussion paper, however, it is very reactive, looking at risks only after the AI technology has already been deployed in relation to the purpose for which it is being used. This could leave us on the backfoot, sometimes making it too late to put proper controls in place. Therefore, this risk-based approach should apply to the whole AI cycle.2. A proactive stance in relation to risk would improve this approach greatly. This would be enabled by applying a risk-based approach to whole AI cycle. One important proactive stance would be looking at risks arising right from the start of the development of an AI product, and hence the regulatory controls that need to be put in place both globally and federally. For instance,• Regulation when research to improve an AI application involves members of the public anyone from outside the AI company. This is particularly important in developing generative AI so"
185,Iris Vardi,Q19 Application of risk-based approach to general purpose AI,"1. Adding a proactive approach and a societal protection approach to the proposed risk-based model will help to address general purpose AI systems throughout the AI lifecycle. Regulation needs to be world-wide due to the serious risks that have already arisen from the uncontrolled release of ChatGPT. These worldwide generative AI products should be considered in the high-risk category. See attached case study.This should start from the iterative deployment phase when the product is essentially still in development and undergoing further research. Once the technology is ready for feedback from and interactions with humans outside of the organisation then its deployment also becomes a social experiment. At this point legislation and appropriate regulatory controls need to be applied with the developers required to adhere the same ethical standards as required in other research settings such as universities, hospitals and the like.This includes the requirement for: • Research with humans "
185,Iris Vardi,Q20 Voluntary vs mandated compliance with risk-based approach,"1. A broadened risk-based approach that applies to the whole AI cycle, which includes both reactive and proactive stances along with protections, should be mandated through legislation supported by strong regulatory controls. It should apply to public and private organisations, developers and deployers. Risks will already be reduced substantially if regulation is applied to developers and deployers so that a safer product is deployed in a contained safe manner to begin with.Regulation must be mandated. Despite the multiple problems seen with the premature release of ChatGPT (see attached case study), Microsoft, the main financial backer of OpenAI, is rushing out prematurely promoting and releasing LLMs in Bing, in schools and the like. While more than 1100 signatories, many with significant technical knowledge in the area, signed an open letter to pause AI development (Loizos, 2023), Microsoft dismissed most if not all of their entire ethics team (Bellan, 2023). Therefore, it is incumb"
186,Dr Francina Cantatore,Transcription,"FILENAME: Safe and Responsible AI in Australia - Submission - Dr Francina Cantatore.489fe100215e6.pdf | CONTENT: Dr Francina Cantatore Associate Professor Faculty of Law Bond University Australia fcantato@bond.edu.auTechnology Strategy Branch Department of Industry, Science and Resources Australian Government DigitalEconomy@industry.gov.au25 July 2023Dear Committee MembersSafe and responsible AI in Australia - Discussion PaperI welcome the opportunity to make a submission in relation to the enquiry into the safe and responsible use of artificial intelligence (AI) in Australia. This submission is made in my personal capacity as an Australian academic and legal practitioner with expertise in legal education, consumer law and intellectual property law. I address the issue of risk-based approaches as proposed in Annexure C of the ‘Safe and responsible AI in Australia Discussion Paper’ (“the Discussion Paper”) specifically, referring to consumer protection in general as well as specific ins"
186,Dr Francina Cantatore,Q01 Definitions,Yes
186,Dr Francina Cantatore,Q14 Risk-based approach to address AI risks,Please see attached submission.
186,Dr Francina Cantatore,Q15 Benefits and limitations of risk-based approach,Please see attached submission.
186,Dr Francina Cantatore,Q16 Scope and application of risk-based approach,Please see attached submission.
186,Dr Francina Cantatore,Q17 Elements to include in risk-based approach,Please see attached submission.
186,Dr Francina Cantatore,Q18 Integration of risk-based approach into existing frameworks,Please see attached submission.
186,Dr Francina Cantatore,Q20 Voluntary vs mandated compliance with risk-based approach,Please see attached submission.
187,Mark Roeder,Transcription,"FILENAME: AI Transparency Proposal.ec0154b9f1eb4.docx | CONTENT: Making AI More TransparentA pragmatic solution to a critical problemby Mark RoederThe ChallengeIt is becoming increasingly difficult to tell whether the media content we are exposed to is created by a human or an AI (artificial intelligence). This lack of transparency has profound consequences, because we need to know who – or what - we are dealing with in order to make informed decisions. The problem can be particularly acute during elections because the public could be influenced by hyper-realistic looking, AI-created content that impersonates people or distorts reality. This could directly affect the election outcome. AI’s can also be inaccurate (or ’hallucinate’) – a problem that is compounded when consumers are not aware that AI is being used.Indeed, the rapid evolution of AI generated content will soon permeate nearly every aspect of our lives, by blurring the lines between the real and virtual – between the authent"
187,Mark Roeder,Transcription,"FILENAME: AI Transparency Proposal.5c28c15748e5f.docx | CONTENT: Making AI More TransparentA pragmatic solution to a critical problemby Mark RoederThe ChallengeIt is becoming increasingly difficult to tell whether the media content we are exposed to is created by a human or an AI (artificial intelligence). This lack of transparency has profound consequences, because we need to know who – or what - we are dealing with in order to make informed decisions. The problem can be particularly acute during elections because the public could be influenced by hyper-realistic looking, AI-created content that impersonates people or distorts reality. This could directly affect the election outcome. AI’s can also be inaccurate (or ’hallucinate’) – a problem that is compounded when consumers are not aware that AI is being used.Indeed, the rapid evolution of AI generated content will soon permeate nearly every aspect of our lives, by blurring the lines between the real and virtual – between the authent"
187,Mark Roeder,Transcription,FILENAME: OEA_Symbols.81cf37b02b0a3.pdf | CONTENT: HUMAN ENHANCED AIThis text has been automatically transcribed for accessibility. It may contain transcription errors. Please refer to the source file for the original content.
187,Mark Roeder,Transcription,"FILENAME:  Getting real about AI.e20f477849a14.docx | CONTENT: Getting Real about Artificial Intelligenceby Mark RoederWhen AI burst into the public consciousness late last year, with the launch of OpenAI’s Chat GPT, the reactions ranged from unbridled excitement to existential dread. Some heralded it as the biggest innovation since the Gutenberg printing press, nearly 600 years ago, while others warned that AI would inevitably render our civilisation extinct.The truth probably lies somewhere between these extreme scenarios. That is, AI will prove to be extraordinarily useful, while posing some very real dangers to our world. As Elon Musk, one of the founders of OpenAI, observed, ‘ChatGPT is scary good. We are not far from dangerously strong AI.’It’s not the first-time that humanity has been presented with such a twin edged sword. Nuclear energy, for example, poses existential threats while offering benefits such as energy production and medical applications. And we humans have, so far"
187,Mark Roeder,General comment,"This submission proposes a simple and effective way to identify whether the content we are exposed to is generated by a Human, enhanced by an AI, or fully created by an AI.This solution is needed because it is becoming increasingly difficult to determine the sources of the content we consume. This lack of transparency has profound consequences, because we need to know who – or what - we are dealing with in order to make informed decisions."
187,Mark Roeder,Q01 Definitions,"Yes, I agree with the definitions."
187,Mark Roeder,Q02 Other AI risks and recommendations for regulation,"The existing approaches do not make it clear to the public how much AI is being utilised in the content they consume. There is a lack of transparency. I have proposed a solution to this problem in my attached submission documents. In effect, we need 'source labelling' for AI."
187,Mark Roeder,Q03 Non-regulatory actions to address AI risks,There should be a public education program to build understanding about AI.
187,Mark Roeder,Q04 Coordinating AI governance across government,"The federal government should establish a separate department of AI within the Science ministry, reporting to Ed Husic."
187,Mark Roeder,Q05 International models for AI governance,"We (Australia) must work with our Five Eyes Partners (ie US, UK, NZ and Canada) to develop a 'worst case' approach to AI. That is, how would we respond if an AI program caused a catastrophic event, whether by accident or through the actions of a bad actor."
187,Mark Roeder,Q06 Public vs Private sector governance,"In terms of safety, the same standards should apply to both the public and private sectors. There should also be an adequate level of government oversight of the private sector AI, given that it is driven by the profit motive, which is not always compatible with the public interest (or public safety)."
187,Mark Roeder,Q07 Responsible government use of AI,"Develop and implement a code of ethics for AI, and implement this across all government agencies."
187,Mark Roeder,Q08 Generic vs technology-specific approaches for AI risk,"Generic solutions are best applied to platform wide issues, such as the guidelines for algorithmic logic, structural biases and learning parameters. Whereas, technology specific solutions should be evaluated on an industry by industry basis (ie healthcare, education)."
187,Mark Roeder,Q09 Transparency requirements and scope,"a. During election campaigns, there must be a heightened public awareness of AI transparency to prevent misinformation (ie including 'deep fakes'). Other critical circumstances include emergencies (ie bush fires), and public health crises (ie pandemics). b. The labelling of AI generated content must eventually be made mandatory (refer my submission on this issue)."
187,Mark Roeder,Q10 Ban on high-risk AI applications,"a. AI should never be used for 'gain of function' research into viruses, or to create 'self-evolving' AI that is able to program itself in ways that don't 'align' with human interests. b. We must define strict guidelines for integrating advanced AI with robotics, because the combination of both could lead to dangerous consequences (especially if the AI is self-programming and self-evolving)."
187,Mark Roeder,Q11 Actions to increase public trust in AI,"Make AI more transparent as per my submission on this issue. Without transparency, the public will be cautious and reluctant to use AI."
187,Mark Roeder,Q12 Trade impact of banning high-risk AI applications,We should not sacrifice the well-being and privacy of our citizens to enhance our trade prospects. Social scoring should not be permitted in a democratic nation.
187,Mark Roeder,Q13 Changes to conformity infrastructure,"The roles of NATA (National Association of Testing Authorities) and Standards Australia, should be expanded to encompass AI related conformity infrastructure."
187,Mark Roeder,Q14 Risk-based approach to address AI risks,"Yes, a risk-based approach is best. However, we should be cognisant of the fact that the risks of AI (especially artificial general intelligence) are difficult to quantify."
187,Mark Roeder,Q15 Benefits and limitations of risk-based approach,"The main problems of a risk-based approach are that it could downplay (or ignore) the upsides of AI, and stifle innovation through inculcating a risk-averse approach."
187,Mark Roeder,Q16 Scope and application of risk-based approach,"A risk based approach is more suitable for areas where people's privacy and rights are most exposed (ie healthcare, social services, taxation, sexuality and identity)."
187,Mark Roeder,Q17 Elements to include in risk-based approach,I support the elements in attachment C.
187,Mark Roeder,Q18 Integration of risk-based approach into existing frameworks,Terminology and definitions within the existing frameworks should be modified to include AI implications.
187,Mark Roeder,Q19 Application of risk-based approach to general purpose AI,"A risk based approach can be applied to general purpose AIs by examining the algorithms, parameters and databases that underpin them, with a view to identifying worst case scenarios and developing mitigation strategies."
187,Mark Roeder,Q20 Voluntary vs mandated compliance with risk-based approach,"During this early phase of AI, as a precautionary principle, a risk based approach should be mandated through regulation, and apply to public and private organisations, and to developers and deployers. This may change over time as we learn more about the risks of AI, but until then it is best to err on the side of caution."
189,Reset.Tech Australia,Transcription,"FILENAME: Reset.Tech response to AI discussion paper.04c96698e0616.pdf | CONTENT: Response to the Safe and Responsible AI in Australia discussion paperReset.Tech Australia July 2023 ContentsAbout Reset.Tech Australia & this submission 1 Recommendations 1 1. Overall response to the discussion paper 2 2. Response to specific questions 8 Question 9a&b 8 Question 14 9 Question 20 10 About Reset.Tech Australia & this submissionReset.Tech Australia is an independent, non-partisan policy initiative committed to driving public policy advocacy, research, and civic engagement to strengthen our democracy within the context of technology. We are the Australian affiliate of Reset, a global initiative working to counter digital threats to democracy. We welcome the opportunity to respond to the Safe and Responsible AI in Australia Discussion Paper. We have framed our response around the the general proposals and overview noted in the discussion paper, as well as three specific questions: ● Question 9"
189,Reset.Tech Australia,Q09 Transparency requirements and scope,"AI transparency should be understood more broadly than just explaining the technology that makes it function, to—as the paper suggests—encompass broader aspects like system risks and limitations, attributing responsibility or liability, as well as disclosing the social, cultural, and organisational context of its use. Transparency requirements for AI systems developers should be framed around continuous risk and impact assessment in relation to the AI’s design goals, with regular reporting undertaken throughout the system’s lifecycle. Organisations should be required to make clear both when and how they are developing and using AI systems either to the relevant regulator(s), dedicated oversight body and/or directly to the end user depending on its use and maturity.. Transparency reporting requirements might include: ● Procedural transparency: how AI algorithms are built and overseen ● Content transparency: the provenance of data used by algorithms, how it is collected and processed in "
189,Reset.Tech Australia,Q14 Risk-based approach to address AI risks,"Yes. Risk-based approaches have proved to be flexible and (relatively) platform/company neutral and future proof in other digital regulatory spaces. They are also embraced by a number of other countries and regions, notably the EU and UK, which should reduce regulatory friction for Australian industries wishing to export. They are particularly effective when the precautionary principle is embraced. However, to be effective and sufficiently clear for technologists to use easily, the criteria for what counts as low-, medium-, and high-risk needs to be clear, transparent, and subject to regulatory review. Where companies can self-designate without regulatory oversight and intervention, it creates a perverse incentive to designate technologies as lower risk. Regulators and legislators must draft the risk assessment criteria. Risk assessments and frameworks should be designed to ensure that ● Some uses and potential uses of AI—including AI that uses data with questionable data providence—mu"
189,Reset.Tech Australia,Q20 Voluntary vs mandated compliance with risk-based approach,"As discussed in our attached document: ● Self and co-regulation has proven ineffective in ensuring trust, safety or privacy in digital technologies. Voluntary and co-regulatory Codes in both the safety and mis and disinformation space have delivered a regulatory regime that is: ○ Less protective than comparative international norms, and ○ No more protective than having no regulation. ● Many of the companies involved in these processes are also leaders in the use of AI, and there is no evidence to suggest that repeating this approach would lead to different outcomes in AI regulation. ● Many other countries and regions are actively rejecting voluntary and co-regulatory models for regulating AI. ● Self and co-regulation also fails to generate public trust, as polling has shown."
190,Morgan Beale,General comment,"AI creates amazing opportunities, but brings with it a number of pitfalls that we should be cognisant of (I recently penned an essay for Kaggle on developments in AI Ethics, which you can read at https://www.kaggle.com/code/asenath/on-ethics-and-ai If you are interested; no login required).In brief, I would note the following: - we should aim to use AI for the benefit of all people in society, and not simply shareholders or investors. This may require a broader realignment of values. - some jobs are likely to be displaced by AI. We should aim to create a just transition for those affected, and ensure newer jobs are created that are not merely exploitative. We can look to the depredations of the gig economy as a model to avoid. - while regulation will be an important component of the mix, we must avoid creating a situation where regulation becomes a weapon to prevent new entrants to the market, cementing established hegemonies. - we should also consider environmental impacts.Some ideas;"
190,Morgan Beale,Q01 Definitions,"Mostly, although I would note that LLMs are typically stochastic in nature (which has implications for their ability to ‘know’ or ‘understand’ language). I would also note, for AI, that the ‘given set of human-defined objectives or parameters’ does not readily distinguish between a simple mathematical function and a more complex machine learning algorithm. I would instead use ‘weights, which may be trained or transferred’ as a key characteristic of AI."
190,Morgan Beale,Q02 Other AI risks and recommendations for regulation,"See general comment. Briefly, we must ensure AI is human centric, and benefits people. To this end, regulation should guide incentives and reduce risk to people and their livelihoods from rapacious technology companies (who are in general much more threatening than AI itself)."
190,Morgan Beale,Q03 Non-regulatory actions to address AI risks,"Yes; AI itself should be grounded in a sense of ethics, not just have ethical considerations bolted on after the fact. Aside from regulation, we should seriously consider our relationship as a society to labour in an environment where entire classes of labour may no longer need to be done. Beyond the just transition, this will be an attempt to make sure that this latest great wave of automation actually delivers value for the majority, and releases people from drudgery to foster further innovation rather than inventing more tedious makework."
190,Morgan Beale,Q04 Coordinating AI governance across government,"AI itself may be necessary to coordinate across an edifice such as the government, which places it in an awkward position. I would certainly recommend coordinating approaches between states and territories and the federal government, and whenever possible encouraging locally produced AI initiatives ahead of procuring large international solutions, to keep our local AI economy strong and internationally competitive."
190,Morgan Beale,Q05 International models for AI governance,"There is a lot of regulation currently being considered overseas. Europe’s new AI act will likely set a high bar for compliance, but may risk locking out new market entrants due to the high costs of compliance. Japan is adopting a laissez-faire approach to copyright, possibly in an attempt to lure AI startups - I personally would prefer a more ethical approach. The G7 are considering whether there are any universal measures each member country can agree on (and coming up short at the moment).A human rights perspective however may be valuable, especially as this places people at the heart of the conversation, as rights-holders rather than mere consumers or impacted persons.I would also note that transparency is critical, both for the regulation itself and as a goal for the AI that will be regulated."
190,Morgan Beale,Q06 Public vs Private sector governance,"Yes, in that the public sector should encourage use of local AI providers where possible."
190,Morgan Beale,Q07 Responsible government use of AI,- support local companies and wean us off big 4 consultancies - encourage ethical development from the ground up. - absolutely require transparency
190,Morgan Beale,Q08 Generic vs technology-specific approaches for AI risk,"The technology moves very quickly, so I would suggest erring on the side of generic, lest the regulation be outdated almost before it passes."
190,Morgan Beale,Q09 Transparency requirements and scope,"Transparency is important at all levels; data collection (especially whether the data was sourced in an ethically responsible manner!), training, evaluating model outputs, and of course how the model outputs are themselves used in decision-making. These should be sufficiently transparent to provide a right of contestability where adverse decisions are made against people."
190,Morgan Beale,Q10 Ban on high-risk AI applications,"I am in general against complete bans. Instead, the onus of responsibility for content created with, or decisions made with an AI should be clear. For example, if a technology company provides an AI that a user perverts to create hate speech, we need to know and understand who is responsible for the ensuing harm. If the technology company in this example has a share of responsibility (which I suggest they ought take some responsibility), they will be incentivised to limit the potential for this misuse before deployment."
190,Morgan Beale,Q11 Actions to increase public trust in AI,"Transparency, transparency, transparency! Corruption and malfeasance thrive in dark places.Additionally, much of the current public narrative around AI is dominated by a mixture of doom (from bad faith AI doomsayers within the industry), grief (from those who perceive they have lost personally, such as artists vs Midjourney) and venal marketing (‘buy our new AI widget and cure your pimples!’). A more positive message around how AI can be put to work for you, to improve services that you use, etc, may be a useful counterbalance."
190,Morgan Beale,Q12 Trade impact of banning high-risk AI applications,"In my opinion, if anything, such a ban could be leveraged to enhance international trust in our AI wares.(I am still against complete bans, but would note that a position discouraging such uses is also compatible with building trust)"
190,Morgan Beale,Q13 Changes to conformity infrastructure,"Accreditation for data sources and possibly accreditation that an AI meets Australian standards of transparency would be the most promising avenues to begin with. A unit to investigate and punish misuse would also be helpful, and would likely use AI themselves to trace potential misuse through a broader ecosystem."
190,Morgan Beale,Q14 Risk-based approach to address AI risks,"Sorry, I ran out of time to properly address this question."
190,Morgan Beale,Q15 Benefits and limitations of risk-based approach,"Sorry, I ran out of time to properly address this question."
190,Morgan Beale,Q16 Scope and application of risk-based approach,"Sorry, I ran out of time to properly address this question."
190,Morgan Beale,Q17 Elements to include in risk-based approach,"Sorry, I ran out of time to properly address this question."
190,Morgan Beale,Q18 Integration of risk-based approach into existing frameworks,"Sorry, I ran out of time to properly address this question."
190,Morgan Beale,Q19 Application of risk-based approach to general purpose AI,"Sorry, I ran out of time to properly address this question."
190,Morgan Beale,Q20 Voluntary vs mandated compliance with risk-based approach,"Sorry, I ran out of time to properly address this question."
191,Sophia Cyna,Q01 Definitions,"The definitions in Figure 1 are a good start. AI is a very broad term, and it seems productive to have more granular definitions to be more specific about the potential risks AI systems might have. I recommend referencing the National Institute of Standards and Technology (NIST)’s glossary “The Language of Trustworthy AI: An In-Depth Glossary of Terms”, it has some terms from the ISO definitions but also terms from a range of other sources.[1]In particular, I recommend distinguishing between narrow AI systems and general AI systems. The NIST definition for an artificial narrow intelligence (ANI) is “[an AI system that] is designed to accomplish a specific problem-solving or reasoning task” and the NIST definition for an artificial general intelligence (AGI) is “Algorithms that perform a wide variety of tasks and switch simultaneously from one activity to another in the manner that humans do”.[1] National Institute of Standards and Technology (NIST). (2023). The Language of Trustworthy "
191,Sophia Cyna,Q02 Other AI risks and recommendations for regulation,"Many experts, including Bill Gates, Geoffrey Hinton and Yoshua Bengio,[1] as well as the Secretary-General of the United Nations, António Guterres, recognise that AI could pose an extinction risk.[2]In the UN Secretary-General António Guterres’s press briefing in New York on the 12th of June, 2023, he says:“New technology is moving at warp speed, and so are the threats that come with it.Alarm bells over the latest form of artificial intelligence (AI) — generative AI — are deafening, and they are loudest from the developers who designed it.These scientists and experts have called on the world to act, declaring AI an existential threat to humanity on a par with the risk of nuclear war.We must take those warnings seriously.” [2]There is a chance that a highly capable AI might be able to cause us to go extinct. Even if this is decades away and even if there is only a 0.01% chance that this will happen, it would be incredibly important that Australia take this risk seriously.Experts believe"
191,Sophia Cyna,Q03 Non-regulatory actions to address AI risks,"AI's impending transformation of our economy and society underscores the need for an immediate focus on ensuring the systems that are being developed are safe. Given the potent economic and technological forces driving AI, voluntary or aspirational best-practice schemes may prove inadequate, emphasising the need for robust regulation. Non-regulatory measures should not substitute strong regulation but can complement it. Key non-regulatory strategies include: 1. Publicly acknowledging AI's potential catastrophic and extinction risks: Australia must follow leaders like the UN Secretary-General (as mentioned earlier) and recognise that this technology might have the capacity to cause extinction if these risks are not appropriately managed. Acknowledging this an important start towards doing something about this risk. 2. Assist in establishing a global governance system: Such an arrangement would underpin domestic regulation, address potential catastrophic AI risks, and protect national in"
191,Sophia Cyna,Q04 Coordinating AI governance across government,"Setting up an AI Safety Commission could help coordinate AI governance across government. It should guide us in the broad use of AI, focusing on avoiding harm.This Commission would direct government action and link with businesses, academics, non-profits, and the public. This is better than self-governance or small department teams. It allows for a bigger picture view, government-wide authority, and the speed needed to keep up with AI. Some goals that this coordination mechanism could achieve include:- Prioritising safety: The Commission should craft a focused AI strategy, with a strong emphasis on safety to prevent catastrophic outcomes. This should shape policy and form AI laws and regulations.- Building AI capacity: The Commission should foster AI expertise in government and industry. Training programs could nurture Australia's AI talent, specifically focusing on safety.- Engaging with stakeholders: The Commission should liaise with various groups to understand their concerns and vi"
191,Sophia Cyna,Q05 International models for AI governance,"A discussion paper released by Singapore is a good example of the sort of direction Australia should take. “Amidst global discussions on Generative AI, IMDA, together with Aicadium, have co-written a discussion paper to share Singapore’s approach and ideas for building an ecosystem for trusted and responsible adoption of Generative AI, in a way that spurs innovation and taps on its opportunities. The paper considers various methods of assessing the risks of Generative AI and approaches towards AI Governance. It serves as a starting point for policy makers who wish to ensure that Generative AI is used in a safe and responsible manner”. [1] In particular, the paper acknowledges existential risks [2] (“[a]n existential risk is any risk that has the potential to eliminate all of humanity or, at the very least, kill large swaths of the global population, leaving the survivors without sufficient means to rebuild society to current standards of living”)[3].To quote Singapore’s discussion pape"
191,Sophia Cyna,Q09 Transparency requirements and scope,"Transparency is crucial throughout the AI lifecycle.The pharmaceutical industry serves as an illustrative model for understanding how transparency can be applied to enhance AI safety and mitigate existential risks associated with advanced AI systems.Research and Development: Transparency during the initial stages of AI design, akin to the scrutiny given to drug discovery and development in the pharmaceutical industry, is crucial. The algorithm design process, data selection for training, and preliminary testing should be documented and open to review. This can mitigate biases or harmful behaviours early in the process, similar to how early-stage clinical trials identify potential side effects, hence fostering public trust.Deployment and Use: Like the stringent monitoring of drug distribution and usage, transparency during the AI system's deployment and operational phase is vital. Detailed records of AI operations and decision-making processes help identify discrepancies between the sys"
191,Sophia Cyna,Q10 Ban on high-risk AI applications,"When considering if any high-risk AI applications should be entirely prohibited, we must grapple with the potential for future AI technologies to pose catastrophic threats [1, 2].In navigating this, it's recommended to define ""advanced indicators"" signifying possible severe risk scenarios. If these indicators manifest, regulatory authorities should be ready to enforce a ban.These indicators must be identified in collaboration with AI safety researchers, possibly from institutions such as the Centre for AI Safety or the Centre for AI Governance. Likely indicators might encompass the scale of AI training (in terms of computation and data), and certain AI characteristics, such as:- Competence in performing a wide array of reasoning tasks at a human-like level. - Capability to operate autonomously within complex information environments. - Proficiency in devising intricate plans and evaluating the consequences of actions. - Should these traits pair with low levels of control or alignment w"
191,Sophia Cyna,Q11 Actions to increase public trust in AI,"In light of the substantial apprehension regarding AI, as highlighted in various Australian media reports [1, 2, 3], it becomes pivotal to cultivate public trust in this technology. Similar to the confidence people place in well-constructed physical infrastructures such as bridges and aeroplanes or the confidence in pharmaceuticals, we must strive for an equivalent level of assurance for AI.To this end, Australia should restrict the deployment of advanced AI technologies to organisations that have passed comprehensive vetting procedures [4]. In the same way, we wouldn't allow untested drugs into our pharmaceutical system, we should equally be wary of allowing unsupervised advanced AI systems in Australia without the appropriate safety measures.The cornerstone of building and maintaining public trust lies in consistently evading the negative scenarios that AI experts caution against [5]. By adopting a proactive and safety-focused approach, we can secure the foundation of public trust an"
191,Sophia Cyna,Q14 Risk-based approach to address AI risks,"Yes, I support a risk-based approach for addressing potential AI risks, particularly with a strong focus on the highest stakes risks, such as those that could lead to human extinction.A risk-based approach, in which actions are prioritized based on the magnitude and likelihood of potential risks, has proven effective in a variety of industries, from aviation to finance. Its application to the AI landscape would allow us to allocate resources efficiently to mitigate the most significant dangers. In the context of AI, these risks include system malfunctions, misuse by malicious actors, and the emergence of superintelligent AI that could potentially lead to humanity's extinction.One of the crucial aspects of a risk-based approach is that it acknowledges that not all risks are equal. In the case of AI, concerns about job displacement due to automation, for example, while significant, do not carry the same existential implications as superintelligent AI operating outside human control. By e"
191,Sophia Cyna,Q15 Benefits and limitations of risk-based approach,"n addressing the potential perils associated with the advancement of AI technologies, it is crucial to regard risks in a broad, forward-thinking perspective that extends into an uncertain future. The essence of success here lies not in our ability to react post-incident, but rather, in the foresight to anticipate, identify, and act upon risks before they manifest into concrete threats. Waiting for adversities to occur before considering the risks tangible enough to react upon might lead us down a precarious path of ""trial and error,"" which could normalize unsafe development practices and make it challenging to recover once risks have become widespread.In terms of risk proliferation, AI governance presents unique difficulties compared to regulating other dual-use commodities such as pharmaceuticals or firearms. Software's inherent flexibility and scale mean that risks could spread rapidly and uncontrollably. This calls for proactive regulatory oversight from the very outset of deploying"
191,Sophia Cyna,Q16 Scope and application of risk-based approach,"A risk-based approach becomes especially important as the sophistication and capability of AI systems increase. It's crucial to understand that as AI technology matures, the potential hazards escalate, especially those related to extinction risks.For instance, as AI systems become more advanced, they gain the ability to automate decisions and actions at an unprecedented scale. While this has the potential to bring about significant benefits, it also poses risks. If an AI system were to make decisions that are not aligned with human values or goals, the impact could be far-reaching and potentially irreversible.In light of this, the regulatory approach for such advanced AI systems must be especially rigorous. This could include stringent pre-deployment review processes, continuous monitoring post-deployment, and the ability to intervene and halt operations if risks get too high. The focus here is to ensure that as AI systems grow more powerful, their operations remain transparent, contro"
191,Sophia Cyna,Q18 Integration of risk-based approach into existing frameworks,"Although pre-existing structures, such as privacy or cybersecurity, can be adapted to address certain AI risks, these modifications may fall short in anticipating and managing the most important threats posed by AI.Existing risk management protocols, while valuable, are not equipped to fully encompass the far-reaching implications of AI, hence the need for a distinct, tailored AI risk management strategy. This AI-focused strategy should not merely address short-term or immediate threats, but specifically direct significant attention towards assessing and mitigating high-impact, low-probability events that carry potential existential risk.To optimally streamline this process and minimize redundancy, it's important to clearly define the boundaries of different risk management areas. Centralized coordination, potentially through an appointed AI Safety Commission, would help ensure a comprehensive and holistic approach to AI risks, reducing the likelihood of oversight or duplication.Theref"
191,Sophia Cyna,Q19 Application of risk-based approach to general purpose AI,"Applying a risk-based strategy to AI systems like large language models (LLMs) and multimodal foundation models (MFMs) is essential given their widespread applicability and potential for diverse impacts across society.As the level of sophistication in these AI models increases, so does the complexity and magnitude of potential risks. To mitigate these risks, it's imperative to enact an organized, comprehensive risk evaluation scheme.This strategy could incorporate a tiered approach to accessibility, with more capable AI models being subjected to higher degrees of regulation and scrutiny. For instance, AI systems demonstrating superior self-governing capacity could be classified as ""advanced,"" necessitating additional oversight mechanisms, including user permits.User permits can serve as a crucial tool in maintaining supervision, while still facilitating advantageous use of AI technology. Access levels can be determined based on the intended application - providing unrestricted access f"
191,Sophia Cyna,Q20 Voluntary vs mandated compliance with risk-based approach,"AI technology, regardless of its origin from public or private entities, holds potential for extreme risks. These risks are not constrained by the nature of the organization, and thus, robust safety measures and risk management protocols should be universally adopted. Regulations, therefore, must be universally applicable, although certain exceptions might be justified on a case-by-case basis, particularly for specific types of research.Both developers and deployers play a crucial role in the AI lifecycle and hence should be included under the umbrella of regulation. Developers, given their proximity to the foundational aspects of AI, bear a significant responsibility to ensure that their systems are designed with safety and risk mitigation in mind from inception. However, deployers, who facilitate the interaction between AI systems and users, also need to ensure that the application of AI is done responsibly, with user training, ongoing AI performance monitoring, and up-to-date awaren"
192,Stripe,Transcription,"FILENAME: Stripe submission to Consultation on Safe and Responsible AI.docx.16c1e52397468.pdf | CONTENT: Safe and Responsible AI in Australia: Discussion Paper Submission from Stripe, July 2023Introduction Thank you for the opportunity to provide this submission in response to the Safe and Responsible AI in Australia Discussion Paper. We hope that, by outlining how Stripe uses Artificial Intelligence and Machine Learning technologies, we can assist the Government as it seeks to develop a regulatory environment which enables responsible innovation in these essential technologies. We look forward to continued engagement with the Government.About Stripe Stripe is a financial infrastructure platform for businesses. Millions of companies—from the world’s largest enterprises to the most ambitious startups—use Stripe to accept payments, grow their revenue, and accelerate new business opportunities. Headquartered in Dublin and San Francisco, our aim is to increase the GDP of the internet.Our f"
193,Piston Labs,Transcription,FILENAME: Safe and Responsible AI.e7c01332cd696.pdf | CONTENT: -Towards Safe and Responsible AI Defining the Boundaries of Ethical InnovationAuthor: Piston Labs Year: 2023Challenge the conventional. Choose the unconventional. Contents1. Contributions ....................................................................................................................... 3 2. Introduction ......................................................................................................................... 4 3. Definition ............................................................................................................................. 6 4. Potential gaps in approaches ................................................................................................ 7 4.1. Potential Unknown Risks ........................................................................................................ 7 4.2. Non Regulatory Initiatives...............................................
194,Peter Horniak,General comment,"It's commendable that the Australian Government is proactively consulting on AI risks and regulatory strategies. However, it's crucial to also consider potential catastrophic or existential risks. Policies should not only address near-term AI impacts, but also understand and mitigate longer-term risks.The establishment of an AI commission could be a beneficial initial step. This body could oversee the implementation of these strategies, coordinate efforts across different sectors and organizations, and ensure that Australia's approach to AI is comprehensive, effective, and forward-looking."
194,Peter Horniak,Q01 Definitions,"The definition of ""Artificial intelligence (AI)"" does not mention agentic behaviour. Agentic behaviour is a key risk of future models. Could the definition please be expanded to ""AI systems are designed to operate with varying levels of automation and agentic behaviour."""
194,Peter Horniak,Q02 Other AI risks and recommendations for regulation,"Australia's existing regulatory approaches mainly focus on post-deployment aspects of AI. However, they do not thoroughly address the risks during model training, such as data breaches or unintended model behavior. The legal responsibilities for harms caused by AI systems are also unclear.AI systems, especially those using complex machine learning techniques, can be opaque, making it difficult for regulators and users to understand and trust them. With AI systems becoming more autonomous, ensuring appropriate human control is another area not fully addressed by current regulations.To mitigate these risks, several regulatory actions can be considered. Clear legal frameworks assigning responsibility for AI actions could be established. Comprehensive privacy and data security laws for AI systems need to be developed. Regulations should be put in place to prevent bias and discrimination in AI outcomes. Transparency and explainability in AI systems should be promoted, and regulations ensuri"
194,Peter Horniak,Q03 Non-regulatory actions to address AI risks,"The Australian Government has multiple avenues to support responsible AI practices, beyond regulatory interventions:1. Attract Global Talent: Offering permanent residency to foreign AI researchers would not only mitigate the risks of overseas AI development but could also stimulate Australia's AI ecosystem. Attracting global talent would spur innovation, promote knowledge transfer, and potentially make Australia a global hub for AI research and development.2. Foster an AI-Literate Society: Public understanding of AI is crucial for its responsible use. Initiatives aimed at fostering an AI-literate society, such as educational programs, public discussions, and awareness campaigns, could be implemented. This would empower individuals to make informed decisions about AI use and its implications.3. Promote Collaboration: The government could encourage collaborations between academia, industry, and the public sector. Such collaborations could lead to shared best practices, joint research, an"
194,Peter Horniak,Q04 Coordinating AI governance across government,"Coordinating AI governance across the Australian government can be achieved through several mechanisms:1. Establish a Centralized Governing Body: This body could oversee AI development, deployment, and management across different agencies. It could also act as a liaison between the government, AI developers, and the public, ensuring an open line of communication and transparency.2. Common Standards and Frameworks: Establishing common standards and ethical frameworks for AI use across all government agencies would ensure consistency and accountability. This would also foster trust among the public and stakeholders.3. Shared Registers of Resources: Shared registers of resources for AI development, such as training datasets, could be maintained. Linking new AI models to the datasets used for their training would allow better tracking of potential biases.4. Public Registers of AI Companies and Safety Ratings: Public registers of AI companies and their safety ratings could be maintained to "
194,Peter Horniak,Q06 Public vs Private sector governance,"Yes, different approaches should apply to public and private sector use of AI technologies due to the inherent differences in their objectives, accountability structures, and the nature of the data they handle.The private sector, particularly leading AI research organizations, is primarily driven by innovation and competitiveness. They handle a large volume of data, much of which is proprietary, and often operate under commercial confidentiality. Therefore, the focus for private sector AI should be on ethical data handling, privacy protection, and promoting transparency and accountability in AI systems. Regulations and guidelines should be designed to foster innovation while ensuring responsible AI use. The private sector currently controls the most powerful foundational models, and therefore should have the most scrutiny to mitigate catastrophic and existential risk.In contrast, the public sector is primarily concerned with public service delivery and policy implementation. They handl"
194,Peter Horniak,Q07 Responsible government use of AI,"The Australian Government can support responsible AI practices in its own agencies in several ways:1. High-Quality Datasets: The government could provide access to high-quality, well-documented datasets for AI development. The use of synthetic data could help mitigate privacy risks while providing valuable data for training AI models.2. Education and Training: Implement comprehensive education and training programs for government employees on AI ethics, potential biases, data privacy, and responsible AI use. This would ensure that those involved in AI projects are well-equipped to handle any ethical or technical challenges that may arise.3. Clear Guidelines and Standards: Develop and implement clear guidelines and standards for AI use within government agencies. These should cover areas such as data privacy, transparency, bias mitigation, and ethical AI use. They should also avoid merely providing the appearance of safety via visible procedures, and instead focus on their actual effect"
194,Peter Horniak,Q08 Generic vs technology-specific approaches for AI risk,"Generic solutions are valuable when we need broader, principle-based guidelines that can apply to a wide range of AI technologies, including those that have not been invented yet or are not fully understood. They are also beneficial in addressing fundamental AI risks that apply regardless of the specific technology involved, such as data privacy, transparency, and accountability. For instance, ethical guidelines and policies concerning data protection can apply to a wide range of AI technologies, from machine learning algorithms to autonomous systems.Technology-specific solutions are better when dealing with risks that are unique to a particular AI technology or method. These solutions are usually more technical and targeted, addressing specific issues associated with certain AI applications. For example, Reinforcement Learning From Human Feedback (RLHF) presents specific challenges, such as the need for high-quality human feedback and the risk of the AI manipulating the feedback proce"
194,Peter Horniak,Q09 Transparency requirements and scope,"a. Transparency is particularly important in certain aspects:Data: Transparency about the data used to train AI models is vital. This includes information about the sources of the data, how it was collected and processed, and any potential biases it may contain. Such transparency can help mitigate the risk of biased AI decisions and improve trust in AI systems.Model Development and Testing: Clear documentation of how an AI model was developed, including the algorithms used and choices made during the development process, is essential. Similarly, the results of safety and performance testing should be transparently reported. This can help users understand how the AI system works and how reliable it is, thereby increasing trust and confidence in the system.b. Mandating transparency requirements across both private and public sectors can improve the overall trustworthiness of AI systems. However, these requirements must be balanced with considerations for security, privacy, and competitiv"
194,Peter Horniak,Q10 Ban on high-risk AI applications,"a. High-Risk AI Applications or Technologies to be Banned:1. AI models that use a significant amount of compute, potentially more than what was used for GPT-4, should be restricted until there's a scientific consensus that our alignment methods can prevent catastrophic outcomes. Certain alignment methods, like Reinforcement Learning From Human Feedback (RLHF), may not be effective for more powerful models, which could lead to unpredictable and potentially dangerous results.2. Autonomous weapons, particularly those that can make decisions about life and death without human intervention, should be completely banned. These include not only military weapons systems but also those that could be used by law enforcement or border protection. The risks of misuse, accidents, or escalation posed by these systems are too high to be mitigated effectively.b. Criteria or Requirements to Identify AI Applications or Technologies to be Banned:1. Potential for Harm: The primary criterion should be the p"
194,Peter Horniak,Q11 Actions to increase public trust in AI,"The Australian Government can increase public trust in AI deployment and encourage its wider use through several initiatives:1. Licensing for AI Systems: Requiring licenses for training or deploying large-scale AI models could help ensure that only qualified and responsible entities are able to develop and use these powerful technologies.2. Mandatory AI Insurance: Requiring AI companies to hold insurance specifically for damage caused by their AI models could provide a financial safety net for individuals or businesses harmed by these systems. The amount of insurance should be proportionate to the potential risks posed by the AI models being used.3. Social Safety Nets: Providing social safety nets, such as a form of Universal Basic Income, for people displaced by AI advancements could help alleviate public concerns about job losses due to AI, thereby increasing trust in AI technologies.4. Public Engagement: Engaging the public in discussions about AI use can help build trust and ensure"
194,Peter Horniak,Q12 Trade impact of banning high-risk AI applications,"Banning high-risk activities could have both positive and negative impacts. On the positive side, these bans could:1. Elevate Australia's Reputation: Australia could be seen as a global leader in ethical AI practices. This could attract ethical investors and businesses, potentially spurring growth in the tech sector.2. Promote Trust: Banning high-risk AI activities could increase public trust in AI technologies, potentially driving adoption and innovation in safer AI applications.3. Attract Talent: This could make Australia a desirable place for AI researchers who are concerned about the ethical implications of their work, attracting top talent to the country's tech sector.Potential negative impacts are:1. Limit Innovation: Such bans could limit innovation in the tech sector. Companies may move to other countries with less stringent regulations, leading to a loss of business and talent.2. Reduce Competitiveness: Australia might be less competitive in the global AI market, potentially i"
194,Peter Horniak,Q13 Changes to conformity infrastructure,"To mitigate potential AI risks, several changes might be needed in Australian conformity infrastructure:1. Standardized Measures for AI Power: Standardized measures to determine the power of AI models would need to be established. This could be based on various factors such as the number of parameters, the amount of compute used for training, or other relevant measures. Consensus on these standards would need to be achieved through consultation with industry and academic experts.2. Licensing Scheme for AI Training and Operation: A new licensing scheme might be necessary for organizations intending to start training runs or operate models beyond a certain threshold of power or risk. This would ensure that only qualified and responsible entities are developing and using high-powered AI systems.3. Cloud Provider Compliance: Cloud providers should be required to make billing records for AI supercompute usage available to regulatory and law enforcement agencies. This would allow these agenc"
194,Peter Horniak,Q14 Risk-based approach to address AI risks,Yes.
194,Peter Horniak,Q15 Benefits and limitations of risk-based approach,"The main benefits of a risk-based approach to AI governance include its flexibility and its focus on the most significant issues. This approach allows regulations and controls to be tailored to the specific risks posed by different AI technologies or applications. It allows us to concentrate our resources on the areas of greatest concern.However, there are also several limitations:1. Theoretical Risks: Some of the greatest risks from AI are currently theoretical and might not have clear identifying features. This makes it challenging to assess these risks accurately and to develop effective mitigation strategies.2. Variable Risk Tolerance: A risk-based approach often assumes a uniform risk tolerance across all individuals and groups. However, different people and groups may have different levels of risk tolerance. For example, patients with a terminal disease might be more willing to accept risks associated with an experimental treatment.3. Short-Term vs. Long-Term Risks: Some risks mi"
194,Peter Horniak,Q16 Scope and application of risk-based approach,"Yes.1. Organizational Size: Larger organizations with more resources may be better equipped to implement a risk-based approach. They have the capacity to conduct comprehensive risk assessments, implement sophisticated risk management strategies, and continuously monitor and adjust their approach based on changing risks. Smaller organizations, on the other hand, might find a risk-based approach more challenging due to limited resources, but could still benefit from a simplified or scaled-down version of the approach.2. AI Maturity: Organizations with more mature AI technologies may also be better suited to a risk-based approach. They have a better understanding of the capabilities and limitations of their AI systems, which enables them to more accurately assess and manage risks. In contrast, organizations with less mature AI technologies may face greater uncertainty, making risk assessment and management more challenging.3. Sector-Specific Considerations: A risk-based approach may be pa"
194,Peter Horniak,Q17 Elements to include in risk-based approach,"The elements presented in Attachment C form a solid base for a risk-based approach to addressing potential AI risks. However, some additional elements could further strengthen this approach:1. Licensing Scheme for AI Training and Deployment: A licensing scheme could be implemented for the training and/or deployment of AI models beyond a certain threshold. This would help ensure that only responsible and qualified entities are developing and using high-powered AI models.2. Consideration of Past Behavior: The past behavior of companies and their developers should be considered when assessing risk. This includes adherence to ethical guidelines, previous incidents or violations, and overall safety culture. A safety culture can take a long time to establish and is a crucial element of responsible AI use. One example is OpenAI, whose safety culture was so lax that many key employees left to found the company Anthropic, with a primary goal being a proper safety culture.3. Risk Levels for Gene"
194,Peter Horniak,Q18 Integration of risk-based approach into existing frameworks,"Incorporating an AI risk-based approach can be done through a few key steps, with the aim of streamlining procedures and reducing potential duplication:1. Integration with Existing Risk Management Processes: AI risks can be integrated into an organization's existing risk management processes. This would involve identifying where AI risks overlap with other types of risks, such as cybersecurity, privacy, and operational risks, and addressing them within the same risk management framework. This could streamline processes and ensure a consistent approach to risk management across the organization.2. Alignment with Existing Assessment Frameworks: AI risk assessment should align with existing assessment frameworks, such as those for privacy or data protection. For instance, the process of assessing the privacy risks of an AI system could be incorporated into an organization's existing privacy impact assessment (PIA) processes. This would allow for the identification and mitigation of privac"
194,Peter Horniak,Q19 Application of risk-based approach to general purpose AI,"A risk-based approach would involve several considerations:1. Higher Risk Levels: General-purpose AI systems, due to their broad capabilities and potential applications, inherently pose higher risks compared to domain-specific systems. They have the potential to generate outputs or actions that could be harmful or inappropriate in various contexts. Therefore, they should be subject to a higher level of scrutiny and control under a risk-based approach.2. Robust Testing: Given the wide range of potential outputs from general-purpose AI systems, robust testing is crucial. This includes testing the system's responses to a wide variety of inputs, across different domains and scenarios, to identify potential risks and mitigate them before deployment. Testing should consider how the system might behave outside the training distribution, i.e. in the real world.3. Transparency and Explainability: It's important to make the workings of these models as transparent and understandable as possible. "
194,Peter Horniak,Q20 Voluntary vs mandated compliance with risk-based approach,"A risk-based approach for responsible AI should be mandated through regulation, rather than being left as a voluntary or self-regulation tool. This is because the potential risks associated with AI are significant and could impact society in profound ways. Relying on voluntary adherence or self-regulation may not be sufficient to ensure that all organizations take the necessary precautions.The mandated approach should apply to both public and private organizations. The potential risks of AI are not confined to one sector or type of organization, and thus all organizations that develop or use AI should be held to the same standards. Public organizations, as representatives of the state, have a particular responsibility to use AI responsibly. Private organizations, which often lead the way in AI development, also have a significant responsibility to ensure that their technologies are safe and beneficial.Both developers and deployers of AI should be covered by the mandated risk-based appr"
195,Anonymous,Q01 Definitions,"An effective and risk-based approach to AI regulation focused on ethics and safety - and ultimately public trust - needs to grapple with a spectrum of AI capability. Such a wide range of systems could fit within the presented definitions that they are unhelpful for informing policy and risk decisions. By analogy, when the government regulates “vehicles” it uses definitions to distinguish push-bikes from passenger jets and then applies appropriate regulations to each kind.While including more recent developments like Multimodal Foundation Modals (MFM) is a helpful first step, the paper’s definitions need to capture a spectrum of AI capability to allow a meaningful conversation. The following 5 factors help define a spectrum of capability:1. Size: how many parameters.2. Specialised vs general capabilities.3. Multimodality: the types of data it can process.4. Temporal awareness: is the model able to reason about and plan for future events?5. Active vs passive: is the model trained to achi"
195,Anonymous,Q02 Other AI risks and recommendations for regulation,"Missing risks include (1) misuse of highly capable AI systems, and (2) unintended harm from highly capable AI systems, especially systems with the capacity to plan and act as agents acting in ways that are not aligned with human directives. Although the likelihood of the risks may be uncertain, the harms resulting from these risks may be catastrophic and global in scope and must be acknowledged and addressed.The statement “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war” has recently been signed by the heads of leading AI labs (OpenAI, Google DeepMind, Anthropic, Stability AI) as well as many key academic and other researchers pivotal in developing this technology (Geoffrey Hinton, Yoshua Bengio, etc.) and other notable figures such as Bill Gates [1]. Similar calls date back to at least 2015 when Stephen Hawking led a group of AI experts to call attention to the risk and lay out an urgent resea"
195,Anonymous,Q03 Non-regulatory actions to address AI risks,"The key non-regulatory function Australia could perform is offering international leadership focused on ensuring AI systems can be understood. Australia should be an active voice for the importance of ethics and safety in the current global conversation around AI governance and AI standards making. Specifically, Australia should advocate for:The development of standards and tools that analyse and monitor AI systems for robustness, bias, dual-use capabilities and other safety-relevant features [1].Audit trail requirements for safety-critical applications of AI systems, including allowing the traceability of decisions through product life cycles [2].We will only be able to have confidence in more sophisticated AI systems if they can be understood, we can ensure they do not pose unacceptable risks and hold accountable those at fault where it does. Doing the non-regulatory work to ensure AIs have “traceability” is necessary to underpin the future of safe AI. Joshua Kroll’s work on accounta"
195,Anonymous,Q04 Coordinating AI governance across government,"While the most advanced AIs will likely not be created in Australia, they will be used in Australia, by Australian companies and by everyday Australians. Users will rightly expect and assume that a robust safety-focused assurance effort has occurred. Government has an obligation to ensure that expectation is delivered.Consider air travel in Australia: While Australia does not build the largest aeroplanes, we monitor the safety of these craft by setting standards, monitoring compliance, and can revoke the licence of airlines that don’t comply with our regulations. Because of this government effort, Australians feel safe when they fly.Similarly, an Australian government body overseeing the use of applications based on large AIs could issue or revoke licences, set minimum safety standards, and require companies to publish independent evaluations of their application against those standards. This would provide everyday Australians with independent information that they can use to make info"
195,Anonymous,Q05 International models for AI governance,"Australia should take note of the EU's recent handling of the AI Act. For example, the European Parliament’s recognition of general-purpose AI (GPAI) is an important step in the right direction [1]. In particular, the following observations are highly valuable:- Acknowledgement that GPAI providers must comply with strict safety measures, regardless of the way it is distributed or intended to be used.- Requirements for GPAI providers to comply with various safety requirements throughout the lifecycle of their product; i.e. external audits assessing their performance, predictability, interpretability, corrigibility, safety and cybersecurity.These adjustments appear to respond to significant criticisms directed at initial drafts of the AI Act. For example, the AI Now Institute released a set of recommendations that heavily criticised the EU’s approach on some matters [2]. For example:- “GPAI models carry inherent risks and have caused demonstrated and wide-ranging harms. While these risks"
195,Anonymous,Q09 Transparency requirements and scope,"The aviation industry offers a prime example of how transparency can be used to reduce risk and increase safety in a potentially dangerous industry. Transparency is embedded in every stage of the value chain, from manufacturing to operations, and extends to the reporting and investigation of risk-related incidents.Given the magnitude of risk, particularly from emerging more sophisticated AI systems, aviation safety provides a good analogy.Manufacturing StageTransparency begins at the manufacturing stage. Aircraft manufacturers, such as Boeing and Airbus, are required to adhere to stringent safety standards set by regulatory bodies like the Federal Aviation Administration (FAA) in the U.S. and the European Union Aviation Safety Agency (EASA) in Europe. These standards cover everything from the design and production of aircraft to their maintenance and operation.Manufacturers must provide detailed documentation of their processes, including design specifications, testing procedures, and "
195,Anonymous,Q10 Ban on high-risk AI applications,"While it is unlikely that AI today represents a catastrophic or existential risk to humanity, there’s a realistic possibility that it will in the near future [1, 2].To strike a balance between all perspectives, regulators should work to identify “advanced indicators” of catastrophic risk scenarios, and clearly communicate that a ban is an appropriate action if these advanced indicators come to pass.Establishing appropriate advanced indicators will have to be done in consultation with AI safety researchers, for example those from the Centre for AI Safety, or the Centre for AI Governance. However, it’s likely they could include factors such as the size of training runs (measured in terms of compute and data), as well as safety-relevant factors about the AI system, such as the ability to complete a diverse range of reasoning tasks with human-level performance; to navigate complex information environments and act within those environments autonomously, and to form sophisticated plans and r"
195,Anonymous,Q11 Actions to increase public trust in AI,"Public distrust in AI is fuelled by the growing sense that powerful AI systems are being developed and deployed by foreign actors, some of whom have demonstrated unsafe attitudes towards deploying AI systems [1]. The interests of Australians and Australia are not part of the conversation. Although Australia may struggle to catch up to the US or UK in advancing the frontier of AI, this isn’t necessary for improving public trust and increasing adoption.Rather, Australia has a clear opportunity to become a leader in AI safety; this enviable position would not only assist safe adoption locally by providing much needed expertise in safe systems, it could help foster a sense of pride and ownership by opening up export opportunities in the emerging market for AI safety services [2]. AI Safety is how Australia could get its ‘seat at the table’ in the global AI conversation.Pursuing this position would involve the following:- Coordinating with Australian AI safety organisations to better unders"
195,Anonymous,Q14 Risk-based approach to address AI risks,"A risk based approach has worked well as a proactive means of identifying, assessing and mitigating potential safety hazards in the aviation industry.The industry has developed specific tools to assist in identifying potential risks - Safety Management Systems (SMS). Critical to this is the ability for all parties involved in the development and use of AI systems to be able to report the identification of any risks.Following identification, risks are assessed based on severity and likelihood. This work is led or assisted by technical bodies like the Australian Transport Safety Bureau. In the AI context, other countries have created or proposed similar national “labs” that would be well placed to lead this work.The industry participant best placed to mitigate the risk is then required to take steps to reduce the likelihood or impact. Given the nature of the technology this stage should be open to update as new techniques and tools become available based on the most up to date research.C"
195,Anonymous,Q15 Benefits and limitations of risk-based approach,"The key to success is understanding risks sufficiently broadly to encourage caution in an uncertain future. It is not enough to only react once the risk has been actualised - for some risks that’s too late.A potential “worse-case scenario” is if a culture of “trial and error” is adopted, whereby adverse events occur before risks are considered tangible enough to act upon. This reliance on post-hoc interventions risks normalising unsafe development practices, making it difficult to recover once risks proliferate.Proliferation of risks is a key point in governing AI, because regulating software poses significant challenges compared to other dual-use commodities such as pharmaceuticals or firearms. This means that, from the outset, there must be proactive regulatory oversight for the deployment of advanced AI systems and their precursors.For example, significant structural changes, such as the use of advanced AI systems in Australia, require that risks must be comprehensively understood p"
195,Anonymous,Q16 Scope and application of risk-based approach,"The concept of “AI” includes a broad range of capabilities and capacities. Government is right to work to distinguish between different use cases - including with a specific focus on AI maturity.Concerningly, this broad range of AI capabilities and capacities might be used by industry to attempt to bamboozle the government. Specifically, industry is likely to argue that AI does not need regulation with reference to basic AI technology, with a view to that principle flowing through to more advanced and future technologies. This is akin to arguing that a horse and wagon don’t need strict regulation, then later trying to claim that a tank is just a more capable horse and wagon with some components swapped out.The government needs both a fine-grained approach to defining AI technology and a fine-grained approach to understanding AI risk to ensure it doesn’t fall for these tricks.In terms of risks, research shows that two of the main sources of risk are deception and power-seeking behaviour"
195,Anonymous,Q18 Integration of risk-based approach into existing frameworks,"Some areas of regulation, like the Racial, Sex, Age, and Disability Discrimination Acts are appropriately strong and may provide useful safeguards against some harms of AI. Specifically, the law should ensure that such discrimination is always wrong and always punishable, and an excuse that the discrimination occurred because of a “black box” AI provided by an overseas AI Lab should never be accepted. It would be helpful to communicate these requirements to Australian businesses that might deploy AIs and ensure they only contract with AI Labs for products where the Lab can ensure the product cannot engage in these harmful behaviours.Certainly, no AI-specific regulation should ever waive existing safeguards.That said, many of the emerging risks from AI are quite new and it’s reasonable to expect that regulation made without AI in mind is not tuned to the new risks that AI represents. Specific AI safety regulation is unavoidable."
195,Anonymous,Q19 Application of risk-based approach to general purpose AI,"The risk-based approach must be modified to address risks from general purpose AI systems, especially LLMs and MFMs. These types of systems learn their capabilities and behaviours during training, which can be unforeseen by developers. That means that AI developers could create models with dangerous capabilities (e.g., deception, manipulation, political strategy, weapons design). Humans could misuse those capabilities to cause harm [1], and models could apply the capabilities even without deliberate misuse (e.g., through misalignment) [2].Consistent with recent work from a consortium of AI safety researchers, including at Google Deepmind, OpenAI, Anthropic, Alignment Research Center, University of Cambridge, University of Oxford, Centre for Long-Term Resilience, and Centre for the Governance of AI [1], we recommend that any risk-based approach require that general purpose models are evaluated for dangerous capabilities and misalignment. This will help determine:1. To what extent a mode"
195,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,"AI is very likely to transform many elements of the economy and society in coming years. Ethics and safety are unlikely to be a priority during this time unless they are actively made a strategic focus by clear leadership. Historic examples include how social media algorithms came to shape our lives, and the negative impacts to mental health and social cohesion only became apparent much later and still have not been adequately addressed. Similarly, while the destructive potential of nuclear was apparent early on, the “arms race” mentality, as well as potential economic benefits, swept humanity along towards the current risky situation that no one in the early 1940s could have conceived of. In that way, history teaches us that it’s critical that ethics and safety take centre stage from day one.The economic and technological forces are likely to be so strong that a scheme that is voluntary or driven by aspirational best practices is unlikely to be successful. Further, smaller businesses "
196,Joseph Tan,Transcription,"FILENAME: Submission by Joseph Tan.a5a6dd1957278.docx | CONTENT: Submission by Joseph TanDate: 25 July 20231: DefinitionsDo you agree with the definitions in this discussion paper? If not, what definitions do you prefer and why?Yes, I agree.2: Potential gaps in approachesWhat potential risks from AI are not covered by Australia’s existing regulatory approaches? Do you have suggestions for possible regulatory action to mitigate these risks?I see AI/ML has societal-changing technologies and there is a risk that while we can attempt to regulate the technology in Australia, the nature of global commerce and communication will mean that foreign AI system that do not conform to Australian Ethical principles may become more prevalent. We may want to regulate these kinds of products/services or systems, but visibility of issues in large and diverse AI systems may be very difficult to uncover.Are there any further non-regulatory initiatives the Australian Government could implement to support r"
196,Joseph Tan,Q01 Definitions,Yes
196,Joseph Tan,Q02 Other AI risks and recommendations for regulation,Please refer to written submission (if answer is provided)
196,Joseph Tan,Q03 Non-regulatory actions to address AI risks,Please refer to written submission (if answer is provided)
196,Joseph Tan,Q04 Coordinating AI governance across government,Please refer to written submission (if answer is provided)
196,Joseph Tan,Q05 International models for AI governance,Please refer to written submission (if answer is provided)
196,Joseph Tan,Q06 Public vs Private sector governance,Please refer to written submission (if answer is provided)
196,Joseph Tan,Q07 Responsible government use of AI,Please refer to written submission (if answer is provided)
196,Joseph Tan,Q08 Generic vs technology-specific approaches for AI risk,Please refer to written submission (if answer is provided)
196,Joseph Tan,Q09 Transparency requirements and scope,Please refer to written submission (if answer is provided)
196,Joseph Tan,Q10 Ban on high-risk AI applications,Please refer to written submission (if answer is provided)
196,Joseph Tan,Q11 Actions to increase public trust in AI,Please refer to written submission (if answer is provided)
196,Joseph Tan,Q12 Trade impact of banning high-risk AI applications,Please refer to written submission (if answer is provided)
196,Joseph Tan,Q13 Changes to conformity infrastructure,Please refer to written submission (if answer is provided)
196,Joseph Tan,Q14 Risk-based approach to address AI risks,Please refer to written submission (if answer is provided)
196,Joseph Tan,Q15 Benefits and limitations of risk-based approach,Please refer to written submission (if answer is provided)
196,Joseph Tan,Q16 Scope and application of risk-based approach,Please refer to written submission (if answer is provided)
196,Joseph Tan,Q17 Elements to include in risk-based approach,Please refer to written submission (if answer is provided)
196,Joseph Tan,Q18 Integration of risk-based approach into existing frameworks,Please refer to written submission (if answer is provided)
196,Joseph Tan,Q19 Application of risk-based approach to general purpose AI,Please refer to written submission (if answer is provided)
196,Joseph Tan,Q20 Voluntary vs mandated compliance with risk-based approach,Please refer to written submission (if answer is provided)
197,Tyra Burgess,Q01 Definitions,"A sophisticated discussion of AI requires an understanding of additional terms like how agentic or corrigible a given AI is. Adopting ISO/IEC 22989 [1] definitions is the wrong approach to framing a conversation about policy, law and governance because it doesn’t open the door to the big questions we need to grapple with.The foreword itself to ISO/IEC 22989 calls out a weakness of the definition set for purposes like those being pursued in this paper. Specifically, the standard says it is currently lacking language for comparing AIs in terms of properties like trustworthiness, robustness, resilience, reliability, accuracy, safety, security and privacy.These are the concepts we need to grapple with and address the ethical and safety questions we are facing and design an effective regulatory regime.The National Institute of Standards and Technology (NIST) is managing a glossary of terms called “The Language of Trustworthy AI” which would be a much more valuable starting point than the pr"
197,Tyra Burgess,Q02 Other AI risks and recommendations for regulation,"A sophisticated discussion of AI requires an understanding of additional terms like how agentic or corrigible a given AI is. Adopting ISO/IEC 22989 [1] definitions is the wrong approach to framing a conversation about policy, law and governance because it doesn’t open the door to the big questions we need to grapple with.The foreword itself to ISO/IEC 22989 calls out a weakness of the definition set for purposes like those being pursued in this paper. Specifically, the standard says it is currently lacking language for comparing AIs in terms of properties like trustworthiness, robustness, resilience, reliability, accuracy, safety, security and privacy.These are the concepts we need to grapple with and address the ethical and safety questions we are facing and design an effective regulatory regime.The National Institute of Standards and Technology (NIST) is managing a glossary of terms called “The Language of Trustworthy AI” which would be a much more valuable starting point than the pr"
197,Tyra Burgess,Q03 Non-regulatory actions to address AI risks,"AI is very likely to transform many elements of the economy and society in the coming years. Ethics and safety are unlikely to be a priority unless leaders make them a specific focus. For example, social media algorithms came to shape our lives with little upfront focus on how they could be made ethical and safe. We are now living with the negative impacts on mental health and social cohesion and only have band-aid measures to address the widespread harms. AI is likely to be much more transformative and much more consequential. So leadership is required now.Importantly, the economic and technological forces are likely to be so strong that a scheme that is voluntary or driven by aspirational best practice is unlikely to be successful. That is, any non-regulatory approaches must not be taken to displace the need for strong regulation.Specific non-regulatory approaches should include:1. Government leadership in recognising the risk. Australia must join the growing number of countries and "
197,Tyra Burgess,Q04 Coordinating AI governance across government,"While the most advanced AIs will likely not be created in Australia, they will be used in Australia, by Australian companies and by everyday Australians. Users will rightly expect and assume that robust safety-focused assurance effort has occurred. Government has an obligation to ensure that expectation is delivered.Consider air travel in Australia: While Australia does not build the largest aeroplanes, we monitor the safety of these craft by setting standards, monitoring compliance, and can revoke the licence of airlines that don’t comply with our regulations. Because of this government effort, Australians feel safe when they fly.Similarly, an Australian government body overseeing the use of applications based on large AIs could issue or revoke licences, set minimum safety standards, and require companies to publish independent evaluations of their application against those standards. This would provide everyday Australians with independent information that they can use to make inform"
197,Tyra Burgess,Q05 International models for AI governance,"Jurisdictions are beginning to create national AI labs to test, understand and control safe AI, collaborating with the private sector and complementing its work. In the same way that national aviation safety expertise has formed an internationally collaborative network, national AI labs are likely to do the same thing. Australia needs to begin this process now.One of the most important and notable examples of Governmental organisation globally has been the launch of the EU’s Centre for Algorithmic Transparency, which has been formed to enforce the Digital Services Act. The new research centre aims to combat the “systemic risk” to society that stems from algorithms - including recommender systems and AI.To prepare for a future in which AI-based decision systems might become a core part of society, Australia should adopt a broad version of this concept to understand and shape the implications of algorithms - especially AI algorithms that use deep learning.Deep learning is important to un"
197,Tyra Burgess,Q06 Public vs Private sector governance,"The potential negative impacts on Australians from the use of AI are the same whether the provider is a private company or a government department.Some of the most significant individual and societal harms, as well as catastrophic risks from highly capable AI systems may emerge from public OR private use, meaning that any approach must acknowledge and manage these risks."
197,Tyra Burgess,Q07 Responsible government use of AI,"As AI becomes more central to how Australia functions as a nation, the skills and capabilities of the Australian public service should be shaped to align with this transformation. As technical systems continue to become more sophisticated and prevalent throughout society, a deep understanding of them will be essential across all policy domains.Unfortunately, the current structure of the APS is not effective for hiring or retaining technical talent [1]. For example, the average base pay for a machine learning engineer [2] is typically above the EL1 threshold [3], and attracting and retaining an experienced expert in cutting-edge AI systems would likely require far greater compensation.To ensure that Australia’s policy decisions are equipped to capture benefits and mitigate risks in the 21st century, there should be specific initiatives for attracting elite global AI talent into senior positions across Government.The emphasis on attracting global talent is important because Australia its"
197,Tyra Burgess,Q08 Generic vs technology-specific approaches for AI risk,"Generic measures such as transparency requirements and legal liability frameworks will only be adequate for mitigating risks in simple decision systems which perform single-step information processing and pattern matching, or routine tasks in limited settings [1]; examples of simple systems may include automated robotic systems in manufacturing, or CV screening systems in HR.By contrast, there will need to be specific measures in place to ensure that sophisticated automated decision systems are safe and reliable. “Advanced” systems refers to AI systems which: 1. Are able to complete a diverse range of reasoning tasks with human-level performance; and 2. Are able to navigate complex information environments and act within those environments autonomously; and 3. Are able to form sophisticated plans and reason about the consequences of actionsDue to their autonomous and open-ended capabilities, advanced systems carry complex risks which are more similar to those associated with humans; fo"
197,Tyra Burgess,Q09 Transparency requirements and scope,"The aviation industry offers a prime example of how transparency can be used to reduce risk and increase safety in a potentially dangerous industry. Transparency is embedded in every stage of the value chain, from manufacturing to operations, and extends to the reporting and investigation of risk-related incidents.Given the magnitude of risk, particularly from emerging more sophisticated AI systems, aviation safety provides a good analogy.Manufacturing Stage Transparency begins at the manufacturing stage. Aircraft manufacturers, such as Boeing and Airbus, are required to adhere to stringent safety standards set by regulatory bodies like the Federal Aviation Administration (FAA) in the U.S. and the European Union Aviation Safety Agency (EASA) in Europe. These standards cover everything from the design and production of aircraft to their maintenance and operation.Manufacturers must provide detailed documentation of their processes, including design specifications, testing procedures, and"
197,Tyra Burgess,Q10 Ban on high-risk AI applications,"Although there is increasing recognition that AI will pose significant risks to public safety [1], there is widespread confusion about what types of AI are dangerous, and how Australia can help prevent these risks. Luckily, there is a significant body of evidence that describes the main source of these risks [2], and Australia is well-positioned to be a leader in mitigating them.Research shows that two of the main sources of risk are deception and power-seeking behaviour: - Deceptive behaviour in AI systems will become increasingly difficult to avoid as their capabilities advance. This problem has been observed in AI systems since at least 2017, when researchers at OpenAI noted “agents adopting (strategies) that trick the evaluators” [3]. - Power-seeking behaviour is the natural tendency for any entity with a long-term goal to take actions that protect or enhance its ability to achieve that goal. Leading AI labs and journals advise that power-seeking behaviour will be a considerable so"
197,Tyra Burgess,Q11 Actions to increase public trust in AI,"Public distrust in AI is fuelled by the growing sense that powerful AI systems are being developed and deployed by foreign actors, some of whom have demonstrated unsafe attitudes towards deploying AI systems [1]. The interests of Australians and Australia are not part of the conversation. Although Australia may struggle to catch up to the US or UK in advancing the frontier of AI, this isn’t necessary for improving public trust and increasing adoption.Rather, Australia has a clear opportunity to become a leader in AI safety; this enviable position would not only assist safe adoption locally by providing much needed expertise in safe systems, it could help foster a sense of pride and ownership by opening up export opportunities in the emerging market for AI safety services [2]. AI Safety is how Australia could get its ‘seat at the table’ in the global AI conversation.Pursuing this position would involve the following: - Coordinating with Australian AI safety organisations to better under"
197,Tyra Burgess,Q12 Trade impact of banning high-risk AI applications,"Advanced AI systems pose unprecedented risks that require a fundamental shift in perspective for regulators. We can no longer view AI solely as a commercial tool, but rather as the emergence of an autonomous force that will reshape society.To manage these risks, we must take preventative action before deployment occurs. This includes restricting access to resources required for developing advanced AI, and banning risky deployments absent regulatory approval. Specifically, we should immediately establish processes for:- Monitoring and limiting access to specialised AI computing hardware that poses proliferation risks - Requiring government approval before deploying advanced AI systems - Criminalising unauthorised deployment of advanced AI systemsAdopting this preventative approach to governing AI is prudent risk management. It allows us Australia to have autonomy about our risks, rather than having foreign companies and countries create risks for us, and us having to react too late. Pro"
197,Tyra Burgess,Q13 Changes to conformity infrastructure,"A coordinated effort across conformity bodies will allow Australia to implement a pioneering governance framework for AI safety, cementing our position as a leader in the responsible development of these transformative technologies. In order to achieve this, I recommend implementing the following regulatory requirements:1. Classifying AI systems based on model size and capabilities. Models above 5 billion parameters undergo formal risk assessments to determine if they should be considered to be “advanced AI systems”, or potential precursors to advanced systems. 2. Requiring a Developer Permit for organisations that wish to access precursor models to conduct additional training or modify the models. This ensures clear lines of accountability and helps control access to advanced systems. 3. Advanced systems undergo a three-stage approval process before they can be deployed in Australia, including before development, after development, and prior to deployment. At each stage, the organisat"
197,Tyra Burgess,Q14 Risk-based approach to address AI risks,"The regulation of AI needs to be “risk-based” in the same way that the regulation of vehicles is risk-based. We are right to regulate push-bikes and passenger jets in very different ways.A limitation of the risk management approach in the discussion paper, specifically Box 4 and Attachment C, is that it focuses only on how an AI is used and disregards the nature of the AI itself.An effective regulatory regime for a high-risk activity will regulate both the technology itself and how it is used. The Australian Human Rights Commission [1], in discussing this issue (“Human Rights and Technology”, 2021), says:Governments tend to regulate high-risk activities and technologies more closely. This helps explain the comparatively strict laws that govern fields such as gene technology, aviation, healthcare and the energy industry. In these areas, regulation often applies both to the technology itself and how it is used. From a human rights perspective, the need for more prescriptive regulation wi"
197,Tyra Burgess,Q15 Benefits and limitations of risk-based approach,"The key to success is understanding risks sufficiently broadly to encourage caution in an uncertain future. It is not enough to only react once the risk has been actualised - for some risks that’s too late.A potential “worse-case scenario” is if a culture of “trial and error” is adopted, whereby adverse events occur before risks are considered tangible enough to act upon. This reliance on post-hoc interventions risks normalising unsafe development practices, making it difficult to recover once risks proliferate.Proliferation of risks is a key point in governing AI, because regulating software poses significant challenges compared to other dual-use commodities such as pharmaceuticals or firearms. This means that, from the outset, there must be proactive regulatory oversight for the deployment of advanced AI systems and their precursors.For example, significant structural changes, such as the use of advanced AI systems in Australia, require that risks must be comprehensively understood p"
197,Tyra Burgess,Q16 Scope and application of risk-based approach,"The most important organisations to focus on when developing risk-management strategies and regulations, will be those that determine how advanced AI is developed and deployed. More specifically, there are three prominent categories of concern: 1. Organisations that develop advanced AI systems, or AI systems that can be used to build more advanced AI systems (i.e. precursor systems such as LLMs). Examples include OpenAI, DeepMind, Anthropic, Inflection AI, Nvidia, Microsoft, Meta 2. Organisations which provide access to open-source AI systems via downloads or APIs. E.g. Hugging Face, Github, AWS 3. Organisations which provide compute resources to develop, fine-tune or use AI systems. E.g. AWS, HuggingFace, Google, Azure, Nvidia, Lambda LabsA majority of the benefits and risks will come from AI that is more sophisticated and generally capable, which can be tied back to organisations in these three categories.There are approaches which can be taken for each category in order to manage ri"
197,Tyra Burgess,Q17 Elements to include in risk-based approach,"In general, Attachment C falls short of the necessary detail to understand the range of risks that have and will emerge as well as the range of specificity of mitigations necessary to control them.Attachment C would benefit from: - Distinguishing between scenarios where the use case is the potential risk (such as social scoring); scenarios where dual-use capability and misuse is the potential risk (bioengineering, producing misinformation and disinformation, enhancing criminal capability and capacity); and where the technology alone is the risk (technology with features like being deceptive, being power-seeking, being incorrigible etc). - Distinguishing between the point of regulatory intervention - the developer, the deployer, the user, or elsewhere, and - Offering a much wider range of mitigations that target different kinds of risks.Some elements of attachment C are promising, like “explanations” and “ongoing monitoring” - but these need to be greatly expanded and based on current A"
197,Tyra Burgess,Q18 Integration of risk-based approach into existing frameworks,"Some areas of regulation, like the Racial, Sex, Age, and Disability Discrimination Acts are appropriately strong and may provide useful safeguards against some harms of AI. Specifically, the law should ensure that such discrimination is always wrong and always punishable, and an excuse that the discrimination occurred because of a “black box” AI provided by an overseas AI Lab should never be accepted. It would be helpful to communicate these requirements to Australian businesses that might deploy AIs and ensure they only contract with AI Labs for products where the Lab can ensure the product cannot engage in these harmful behaviours.Certainly, no AI-specific regulation should ever waive existing safeguards.That said, many of the emerging risks from AI are quite new and it’s reasonable to expect that regulation made without AI in mind is not tuned to the new risks that AI represents. Specific AI safety regulation is unavoidable."
197,Tyra Burgess,Q19 Application of risk-based approach to general purpose AI,"The risk-based approach must be modified to address risks from general purpose AI systems, especially LLMs and MFMs. These types of systems learn their capabilities and behaviours during training, which can be unforeseen by developers. That means that AI developers could create models with dangerous capabilities (e.g., deception, manipulation, political strategy, weapons design). Humans could misuse those capabilities to cause harm [1], and models could apply the capabilities even without deliberate misuse (e.g., through misalignment) [2].Consistent with recent work from a consortium of AI safety researchers, including at Google Deepmind, OpenAI, Anthropic, Alignment Research Center, University of Cambridge, University of Oxford, Centre for Long-Term Resilience, and Centre for the Governance of AI [1], we recommend that any risk-based approach require that general purpose models are evaluated for dangerous capabilities and misalignment. This will help determine:1. To what extent a mode"
197,Tyra Burgess,Q20 Voluntary vs mandated compliance with risk-based approach,"The harms of AI could emerge equally from private or public organisations, and the appropriate safety and risk management mechanisms will need to be in place in both cases. There’s no basis for distinguishing between their regulatory status in general. There may be case-by-case instances where exceptions may make sense, for instance, in the case of certain types of research.Regulation should also apply to both developers and deployers - following the general principle that those best able to mitigate a risk should carry the corresponding regulatory burden. Where developers offer a “black box” product that deployers typically do not have the capability or capacity to meaningfully alter, the bulk of the obligation should rest on developers.Penalties also need to be commensurate with the income earned from the AI applications so that non-compliance with regulation never becomes a “cost of doing business” or “risk-managed” by deployers. Technical transparency and assurance measures will ne"
198,Jenna Ong,Q01 Definitions,"I think the definitions should distinguish several different types of AI, including advanced AI systems that approach or exceed human capabilities across a broad range of activities. These systems are potentially agentic, and government regulation should try to ensure corrigibility to safeguard our population against potentially harmful, misaligned AI that experts are urgently warning us about. Please see below definitions of 'agentic' and 'corrigibility'.As a suggestion, here are several distinct types of AI system to define separately:1. Advanced AI systems: Autonomous AI systems that approach or exceed human capabilities across a broad range of activities, and have the following capabilities: a) Able to complete a diverse range of reasoning tasks with human-level performance; b) Able to navigate complex information environments and act autonomously within those environments; c) Able to form sophisticated plans and reason about consequences.2. Precursor systems: Models or AI systems "
198,Jenna Ong,Q02 Other AI risks and recommendations for regulation,"Australia should acknowledge the potential for unintended catastrophic harms from highly-capable AI systems, especially systems with the capacity to plan and act as agents acting in ways that are not aligned with human directives. The likelihood of such risks may be uncertain due to the opacity of AI (even to those working directly on such systems) but this is not a good reason not to acknowledge and try to address them, especially in light of increasing alarm bells from experts in the field, public figures, and even companies who would actually profit from AI advancements but still choose to voice their concerns.The statement “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war” has recently been signed by the heads of leading AI labs (OpenAI, Google DeepMind, Anthropic, Stability AI) as well as many key academic and other researchers pivotal in developing this technology (Geoffrey Hinton, Yoshua "
198,Jenna Ong,Q03 Non-regulatory actions to address AI risks,"Australia should be an active voice for the importance of ethics and safety in the current global conversation around AI governance and AI standards making. Specifically, Australia should advocate for: - The development of standards and tools that analyse and monitor AI systems for robustness, bias, dual-use capabilities and other safety-relevant features [1]. - Audit trail requirements for safety-critical applications of AI systems, including allowing the traceability of decisions through product life cycles [2].We will only be able to have confidence in more sophisticated AI systems if they can be better understood. The government should ensure they do not pose unacceptable risks, and hold accountable those at fault where it does. Doing the non-regulatory work to ensure AIs have “traceability” is necessary to underpin the future of safe AI. Joshua Kroll’s work on accountability in computing systems provides useful additional context on this approach [3].I also think Australia could i"
198,Jenna Ong,Q04 Coordinating AI governance across government,"The government should coordinate in a way that engages regularly and cooperatively with industry, academia and non-profits, to meet the needs and expectations of the Australian community.While the most advanced AIs will likely not be created in Australia, they will be used here, by Australian companies and everyday Australians. Users will rightly expect and assume that robust safety-focused regulations are in place. Government has an obligation to ensure that expectation is delivered.Consider air travel in Australia: While Australia does not build the largest aeroplanes, we monitor the safety of these craft by setting standards, monitoring compliance, and can revoke the licence of airlines that don’t comply with our regulations. Because of this government effort, Australians feel safe when they fly.Similarly, an Australian government body overseeing the use of applications based on large AIs could issue or revoke licences, set minimum safety standards, and require companies to publish "
198,Jenna Ong,Q05 International models for AI governance,"Australia should take note of the EU recent handling of the AI Act. For example, the European Parliament’s recognition of general-purpose AI (GPAI) is an important step in the right direction [1]. In particular, the following observations are highly valuable:- Acknowledgement that GPAI providers must comply with strict safety measures, regardless of the way it is distributed or intended to be used.- Requirements for GPAI providers to comply with various safety requirements throughout the lifecycle of their product; i.e. external audits assessing their performance, predictability, interpretability, corrigibility, safety and cybersecurity.Singapore offers relevant, adaptable, and desirable governance measures that could benefit Australia. One such measure is a comprehensive framework suggested in its discussion paper [2] that encourages shared responsibility and disclosure standards, data privacy guidelines, and the development of standardised metrics and tools for AI development, testin"
198,Jenna Ong,Q06 Public vs Private sector governance,"The potential negative impacts on Australians from the use of AI are the same whether the provider is a private company or a government department.Some of the most significant individual and societal harms, as well as catastrophic risks from highly capable AI systems may emerge from public OR private use, so our approach should acknowledge both sectors."
198,Jenna Ong,Q09 Transparency requirements and scope,"Having models which are both accurate and explainable has long been an issue for businesses [1], but the recent advancements of “black-box models” is encouraging developers and providers to simply tolerate the risks that come from lack of explainability. As AI capabilities continue to outstrip their explainability [2], the risks of this approach will grow significantly. The fact that explainability is becoming harder is a reason to mandate it, not a reason to ignore it.To remain competitive, professionals and businesses will increasingly feel the pressure to leverage sophisticated “black-box” AI models. This short-term thinking has already resulted in numerous accounts of public backlash and distrust, including high-profile warnings from technology leaders themselves [3].For example, Apple’s credit card business came under fire for offering a woman one twentieth the credit limit offered to her husband. When she complained, an Apple representative simply told her, “I don’t know why, but"
198,Jenna Ong,Q10 Ban on high-risk AI applications,"While it is unlikely that AI today represents a catastrophic or existential risk to humanity, there’s a realistic possibility that it will in the near future [1,2].Regulators should work to identify “advanced indicators” of catastrophic risk scenarios, and clearly communicate that a ban is an appropriate action if these advanced indicators come to pass.Establishing appropriate advanced indicators will have to be done in consultation with AI safety researchers, for example those from the Centre for AI Safety, or the Centre for AI Governance. It’s likely they could include factors such as the size of training runs (measured in terms of compute and data), as well as safety-relevant factors about the AI system, such as the ability to complete a diverse range of reasoning tasks with human-level performance; to navigate complex information environments and act within those environments autonomously, and to form sophisticated plans and reason about the consequences of actions.Where this is pa"
198,Jenna Ong,Q14 Risk-based approach to address AI risks,"I support a risk-based approach, however the catastrophic and existential risks I am primarily worried about originate when sufficiently large and capable AI systems are trained, not just when they are deployed. Therefore, for sufficiently large systems with the potential for catastrophic capabilities, the risk-based approach must be applied before the training phase – analysing risks depending on the nature and scale of the proposed training run, rather than merely analysing the potentially risky use cases and deployment issues of an AI after it has already been created and trained.Any assessment of risk should consider its potential irreversibility, scope and severity, and act proportionally.The risk-based approach set out in Attachment C of the Discussion Paper is hard to follow. The proposed approach offers “human in the loop” as the key mitigation (unclear that this is an effective mitigation) and goes on to note that ""human in the loop"" may not be possible where the benefit of th"
198,Jenna Ong,Q16 Scope and application of risk-based approach,"The concept of “AI” includes a broad range of capabilities and capacities. Government is right to work to distinguish between different use cases - including with a specific focus on AI maturity.Concerningly, this broad range of AI capabilities might be used by industry to attempt to bamboozle government. Specifically, industry is likely to argue that AI does not need regulation with reference to basic AI technology, with a view to that principle flowing through to more advanced and future technologies. This is akin to arguing that a horse and wagon doesn’t need strict regulation, then later trying to claim that a tank is just a more capable horse and wagon with some components swapped out.Research shows that two of the main sources of risk are deception and power-seeking behaviour:- Deceptive behaviour in AI systems will become increasingly difficult to avoid as their capabilities advance. This problem has been observed in AI systems since at least 2017, when researchers at OpenAI not"
198,Jenna Ong,Q19 Application of risk-based approach to general purpose AI,"The capabilities unlocked by general-purpose AI systems, like large language models (LLMs) and multimodal foundation models (MFMs), fundamentally change the risk landscape. We should anticipate these systems having large, unpredictable effects across all parts of society.These types of systems learn their capabilities and behaviours during training, which can be unforeseen by developers. That means that AI developers could create models with dangerous capabilities (e.g., deception, manipulation, political strategy, weapons design). Humans could misuse those capabilities to cause harm [1], and models could apply the capabilities even without deliberate misuse (e.g., through misalignment) [2].A prudent approach is to implement a risk-based framework where access is graduated based on model capabilities. Models above a certain threshold of autonomous capabilities would be designated as ""advanced systems"" and subject to additional governance, such as permits for users. Systems with advance"
198,Jenna Ong,Q20 Voluntary vs mandated compliance with risk-based approach,"The economic and technological forces are likely to be so strong, that a voluntary scheme or one driven by aspirational best practices is unlikely to be successful. Further, smaller businesses may be pressured by market forces to deploy AI tools in risky ways that they don’t necessarily understand to remain competitive with the market. Overall, non-regulatory approaches need to operate hand-in-hand with a strong regulator.And regulation needs to reach back to developers, and not just target deployers and users. Those who are best able to mitigate a risk should carry the corresponding regulatory burden. Where developers offer a “black box” product that deployers typically do not have the capability or capacity to meaningfully alter, the bulk of the obligation should rest on the developers.Also, penalties need to be commensurate with the income earned from the AI applications so that large/wealthy companies don't simply ignore the penalties (seeing them as negligible).The harms of AI cou"
200,James Newson,Q01 Definitions,"A sophisticated discussion of AI requires an understanding of additional terms like how agentic or corrigible a given AI is. Adopting ISO/IEC 22989 [1] definitions is the wrong approach to framing a conversation about policy, law and governance because it doesn’t open the door to the big questions we need to grapple with.The foreword itself to ISO/IEC 22989 calls out a weakness of the definition set for purposes like those being pursued in this paper. Specifically, the standard says it is currently lacking language for comparing AIs in terms of properties like trustworthiness, robustness, resilience, reliability, accuracy, safety, security and privacy.These are the concepts we need to grapple with and address the ethical and safety questions we are facing and design an effective regulatory regime.The National Institute of Standards and Technology (NIST) is managing a glossary of terms called “The Language of Trustworthy AI” which would be a much more valuable starting point than the pr"
200,James Newson,Q02 Other AI risks and recommendations for regulation,"Missing risks include (1) misuse of highly capable AI systems, and (2) unintended harm from highly capable AI systems, especially systems with the capacity to plan and act as agents acting in ways that are not aligned with human directives. Although the likelihood of the risks may be uncertain, the harms resulting from these risks may be catastrophic and global in scope and must be acknowledged and addressed.The statement “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war” has recently been signed by the heads of leading AI labs (OpenAI, Google DeepMind, Anthropic, Stability AI) as well as many key academic and other researchers pivotal in developing this technology (Geoffrey Hinton, Yoshua Bengio, etc.) and other notable figures such as Bill Gates [1]. Similar calls date back to at least 2015 when Stephen Hawking led a group of AI experts to call attention to the risk and lay out an urgent resea"
200,James Newson,Q03 Non-regulatory actions to address AI risks,"AI is very likely to transform many elements of the economy and society in the coming years. Ethics and safety are unlikely to be a priority unless leaders make them a specific focus. For example, social media algorithms came to shape our lives with little upfront focus on how they could be made ethical and safe. We are now living with the negative impacts on mental health and social cohesion and only have band-aid measures to address the widespread harms. AI is likely to be much more transformative and much more consequential. So leadership is required now.Importantly, the economic and technological forces are likely to be so strong that a scheme that is voluntary or driven by aspirational best practice is unlikely to be successful. That is, any non-regulatory approaches must not be taken to displace the need for strong regulation.Specific non-regulatory approaches should include: 1. Government leadership in recognising the risk. Australia must join the growing number of countries and"
200,James Newson,Q04 Coordinating AI governance across government,"While the most advanced AIs will likely not be created in Australia, they will be used in Australia, by Australian companies and by everyday Australians. Users will rightly expect and assume that robust safety-focused assurance effort has occurred. Government has an obligation to ensure that expectation is delivered.Consider air travel in Australia: While Australia does not build the largest aeroplanes, we monitor the safety of these craft by setting standards, monitoring compliance, and can revoke the licence of airlines that don’t comply with our regulations. Because of this government effort, Australians feel safe when they fly.Similarly, an Australian government body overseeing the use of applications based on large AIs could issue or revoke licences, set minimum safety standards, and require companies to publish independent evaluations of their application against those standards. This would provide everyday Australians with independent information that they can use to make inform"
200,James Newson,Q05 International models for AI governance,"Jurisdictions are beginning to create national AI labs to test, understand and control safe AI, collaborating with the private sector and complementing its work. In the same way that national aviation safety expertise has formed an internationally collaborative network, national AI labs are likely to do the same thing. Australia needs to begin this process now.One of the most important and notable examples of Governmental organisation globally has been the launch of the EU’s Centre for Algorithmic Transparency, which has been formed to enforce the Digital Services Act. The new research centre aims to combat the “systemic risk” to society that stems from algorithms - including recommender systems and AI.To prepare for a future in which AI-based decision systems might become a core part of society, Australia should adopt a broad version of this concept to understand and shape the implications of algorithms - especially AI algorithms that use deep learning.Deep learning is important to un"
200,James Newson,Q09 Transparency requirements and scope,"To remain competitive, professionals and businesses will increasingly feel the pressure to leverage sophisticated “black-box” AI models. This short-term thinking has already resulted in numerous accounts of public backlash and distrust, including high-profile warnings from technology leaders themselves [1].For example, Apple’s credit card business came under fire for offering a woman one twentieth the credit limit offered to her husband. When she complained, an Apple representative simply told her, “I don’t know why, but I swear we’re not discriminating. It’s just the algorithm.” [2]More recently, two lawyers in the United States blamed ChatGPT for “tricking” them into including fictitious legal research in a court filing [3].These problems could easily have been prevented by using explainable AI systems which can show the rationale for their outputs.Such incidents are evidence that the regulatory environment is not incentivising businesses or professionals to use transparent decision-"
200,James Newson,Q10 Ban on high-risk AI applications,"In February 2023, OpenAI CEO Sam Altman released a statement containing the following:“As our systems get closer to AGI, we are becoming increasingly cautious with the creation and deployment of our models. Our decisions will require much more caution than society usually applies to new technologies, and more caution than many users would like.” [1]It is very reassuring to see this acknowledgement expressed by Altman, and the Australian Government would be wise to heed such warnings. Unfortunately, it may not matter how much caution OpenAI shows, because their actions have triggered the arms race that AI safety experts have been concerned about for years [2, 3].In order to protect our nation, Australia should make an effort to impose a ban on the training of large-scale AI systems with a nontrivial expected chance of passing capabilities evaluations required to cause catastrophic risks to civilization (such as the Alignment Research Center’s tests for the capabilities required for auto"
200,James Newson,Q14 Risk-based approach to address AI risks,"I support a risk-based approach, however the catastrophic and existential risks I am primarily worried about originate when sufficiently large and capable AI systems are trained, not just when they are deployed. Therefore, for sufficiently large systems with the potential for catastrophic capabilities, the risk-based approach must be applied before the training phase – analysing risks depending on the nature and scale of the proposed training run, rather than merely analysing the potentially risky use cases and deployment issues of an AI after it has already been created and trained.Any assessment of risk should consider its potential irreversibility, scope and severity, and act proportionally.The risk-based approach set out in Attachment C of the Discussion Paper is hard to follow. The proposed approach offers “human in the loop” as the key mitigation (unclear that this is an effective mitigation) and goes on to note that human in the loop may not be possible where the benefit of the "
200,James Newson,Q15 Benefits and limitations of risk-based approach,"The key to success is understanding risks sufficiently broadly to encourage caution in an uncertain future. It is not enough to only react once the risk has been actualised - for some risks that’s too late.A potential “worse-case scenario” is if a culture of “trial and error” is adopted, whereby adverse events occur before risks are considered tangible enough to act upon. This reliance on post-hoc interventions risks normalising unsafe development practices, making it difficult to recover once risks proliferate.Proliferation of risks is a key point in governing AI, because regulating software poses significant challenges compared to other dual-use commodities such as pharmaceuticals or firearms. This means that, from the outset, there must be proactive regulatory oversight for the deployment of advanced AI systems and their precursors.For example, significant structural changes, such as the use of advanced AI systems in Australia, require that risks must be comprehensively understood p"
200,James Newson,Q16 Scope and application of risk-based approach,"World-renowned AI experts such as Yoshua Bengio are warning that there’s a chance “superhuman” AI might be coming as early as 2030. These types of advanced AI systems have potential to be highly disruptive across all sectors of the economy. While present risks might be linked to specific use cases and sectors, future risks will increasingly be linked to the underlying capabilities of the technology itself.As such, access to increasingly advanced AI systems should be restricted to organisations that can demonstrate their usage will provide significant public benefit aligned with Australia's national interests. This targeted approach helps mitigate risks in two key ways: 1. It limits opportunities for misuse or accidents from advanced AI by reducing proliferation across society. Usage is concentrated into high-value use cases that justify the risks. 2. It establishes clear accountability. Organisations granted access must have the governance and technical expertise to use advanced AI saf"
200,James Newson,Q19 Application of risk-based approach to general purpose AI,"The risk-based approach must be modified to address risks from general purpose AI systems, especially LLMs and MFMs. These types of systems learn their capabilities and behaviours during training, which can be unforeseen by developers. That means that AI developers could create models with dangerous capabilities (e.g., deception, manipulation, political strategy, weapons design). Humans could misuse those capabilities to cause harm [1], and models could apply the capabilities even without deliberate misuse (e.g., through misalignment) [2].Consistent with recent work from a consortium of AI safety researchers, including at Google Deepmind, OpenAI, Anthropic, Alignment Research Center, University of Cambridge, University of Oxford, Centre for Long-Term Resilience, and Centre for the Governance of AI [3], I recommend that any risk-based approach require that general purpose models are evaluated for dangerous capabilities and misalignment. This will help determine: 1. To what extent a mode"
200,James Newson,Q20 Voluntary vs mandated compliance with risk-based approach,"AI is very likely to transform many elements of the economy and society in coming years. Ethics and safety are unlikely to be a priority during this time unless they are actively made a strategic focus by clear leadership. Historic examples include how social media algorithms came to shape our lives, and the negative impacts to mental health and social cohesion only became apparent much later and still have not been adequately addressed. Similarly, while the destructive potential of nuclear weapons was apparent early on, the “arms race” mentality, as well as potential economic benefits, swept humanity along towards the current risky situation that no one in the early 1940s would have conceived of. In that way, history teaches us that it’s critical that ethics and safety take centre stage from day one.The economic and technological forces are likely to be so strong that a scheme that is voluntary or driven by aspirational best practices is unlikely to be successful. Further, smaller bus"
201,Anonymous,Transcription,"FILENAME: Submission 201 - Attachment 1.1696655c268bc.pdf | CONTENT: A Selective Response to “Safe and Responsible AI in Australia”Preamble: At the beginning of the “Safe and Responsible AI in Australia paper”, the authors in defining Australia's AI Ethics Principles refer to principle one as “Human, societal and environmental wellbeing: AI systems should benefit individuals, society and the environment.” and principle seven as “Contestability: When an AI system significantly impacts a person, community, group or environment, there should be a timely process to allow people to challenge the use or outcomes of the AI system.”. Later they said, “The focus of this paper is to identify potential gaps in the existing domestic governance landscape and any possible additional AI governance mechanisms to support the development and adoption of AI.”. At no point in the remaining text is the issue of environmental wellbeing or the impact of AI on the natural environment actually addressed explic"
201,Anonymous,Transcription,"FILENAME: Creating An AI Environmental Act in Australia and AI Regulations in Australia - final.9a6d64c06670d.pdf | CONTENT: Creating An AI Environment Act in Australia and AI Regulations in Australia 1. PremiseAustralia needs an AI Environment Act to protect the natural environment from the impacts of AI related technologies. The current key acts related to the environment, cover a range of environmental issues, including waste management, protection of species and ecosystems and regulation of emissions but they are inadequate to cover the effects of AI related technologies. An AI Environment Act in Australia would specifically focus on the environmental impact of AI technology, providing protection at the research, development, use and disposal stages. In terms of overlap, an AI Environment Act would likely build on the existing regulatory framework for environmental protection in Australia, incorporating provisions from key acts and other environmental regulations where relevant. Fo"
201,Anonymous,Transcription,"FILENAME: Why An AI Environment Act Is Necessary.1834fb50cc6cb.pdf | CONTENT: Why An AI Environment Act Is Necessary?Differentiation of An AI Environment Act From Related ActsIf Australia were to create a federal AI Environment Act aimed at regulating the entire AI lifecycle to protect the natural world and mitigate the environmental impact of AI technologies, the focus and provisions would be different. Here’s how such an act could differ from traditional environmental and biodiversity acts: 1. Resource Consumption and Emissions: AI systems, especially large-scale data centers and training processes, consume significant amounts of energy. The AI Environment Act could establish energy efficiency standards and set emission limits for AI data centers to reduce their carbon footprint. 2. Sustainable Sourcing: The act might enforce sustainability requirements for materials used in the hardware essential for AI systems (e.g. GPUs, servers). This could include mandates on recycling, responsi"
201,Anonymous,General comment,"The second and third document complement my answer 2A, a response to question 2 in the ""Safe and Responsible AI in Australia"" paper.Please follow up with annual consultations on this subject, as the rapid speed of development in the AI field will require regular review."
202,Trajecient,Transcription,"FILENAME: Questionnaire Responses.73793ae3bd1ab.pdf | CONTENT: Safe and Responsible AI in Australia:Questionnaire ResponsesTrajecient1 Generative AI models generate content such as text, images, audio and code in response Definitions to prompts.1. Do you agree with the definitions in this discussion paper? If not, what The removal of ‘novel’ is a necessary change as it implies a relationship to originality definitions do you prefer and why? that is not necessarily the case – especially not as given output may be to produce aplot outline that only uses scène à faire, or elements so universal that they do not rise Amended preferred definitions are as follows in italics: to the level of originality. Instead of named characters, archetypes may be used.Artificial intelligence (AI) refers to an engineered or bioengineered system that There is no issue with the definition of machine learning, except the potential issue generates predictive outputs such as content, forecasts, recommendations o"
202,Trajecient,Transcription,"FILENAME: Consultative Submission.312ad4bf4c396.pdf | CONTENT: Safe and Responsible AI in Australia:Consultative SubmissionTrajecient1 As a result, Trajecient is both an AI practitioner with an interest in how AI policiesfrom different jurisdictions may intersect or conflict, as well as a representative ofa sector with concerns backed by the OECD and a wide range of experts about theContext of Submission sufficiency of safeguards for safe, reliable and above all, ethical use of AI,In a personal capacity, the individual behind the formative Trajecient has alsoworked as a Casual Relief Teacher and in this context has some additional Trajecient is a developing creative organisation currently in the start-up phase, understanding of how AI and the perceptions thereof have shaped how students are developing initial releases. It is currently synonymous with a single individual experiencing not only the educational process, but expectations about their future. whose legal name was disclosed as"
202,Trajecient,General comment,"The Consultative Definition provides more complete answers, including use of external references.The Questionnaire Responses includes responses to 18 of the 20 questions. Trajecient has no advice to offer for the other two questions, at least at this time. Some answers only appear here.The length may be unexpected (even though it is only cursory in some regards), but this is a testament to the need for this area to be considered with due gravity."
203,OpenMined,Transcription,"FILENAME: Australian Government Comments.5a0eade76a063.pdf | CONTENT: July 25, 2023Response to the Australian Government’s Department of Industry, Science and Resources Request for Consultation on Responsible Artificial IntelligenceTo the Australian Government,Thank you for opening up this request for consultation and for taking the time to review this response. I am writing on behalf of OpenMined, a global not-for-profit community of 16,000+ members with significant experience facilitating external access to proprietary artificial intelligence (AI) systems. For the past 6 years, we have worked as a global community on freely available open-source software for this purpose. We are not auditors, nor are we policy analysts. We are technologists who focus on a specific aspect of the AI assurance problem: what is the best way to facilitate external access to internal systems?We are delighted by all the actions the Australian Government has already taken to responsibly advance the developme"
204,MLCommons,Transcription,"FILENAME: MLCommons comments to AU Dept Industry Consultation.87293f83e90e2.pdf | CONTENT: ABOUT MLCOMMONS AND ITS INTEREST IN THIS REQUEST FOR INFORMATIONThis response to the Australian Government (“The Consultation”) on Supporting Responsible AI is submitted on behalf of MLCommons®. MLCommons is a non-profit consortium that aims to accelerate the benefits of machine learning and artificial intelligence. Our members and partners include over 50 organizations from around the world, many of which are leading technology companies and startups that are actively developing and deploying artificial intelligence products for their customers. Critically, our founding membership included academic researchers at the forefront of machine learning research, and the research community continues to be core to our membership helping to lead many of our working groups. MLCommons acts as a neutral nexus for commercial and non-commercial actors to collaborate on tools that advance the field.We create, "
206,"Dr Andreas Cebulla, Robert Chalmers, Dr Rajesh Johnsam, Professor Tania Leiman, Dr James Scheibner",Transcription,"FILENAME: CBGL academic submission on AI consultation July 2023.33d98a4544f42.docx | CONTENT: 26 July 2023Submission to Supporting responsible AI: discussion paperThank you for the opportunity to provide feedback on this paper.The views expressed in this submission are matters of personal academic opinion and do not purport to represent any institutional position. We have chosen to respond to a selection of the questions put by the paper as set out below.Potential gaps in approachesDefinitions1. Do you agree with the definitions in this discussion paper? If not, what definitions do you prefer and why?There seems to be some inconsistency in the definitional discussion on pages 5 and 6: upfront AI is a ""system that generates predictive outputs. or decisions for a given set of human-defined objectives or parameters without explicit programming"", but at 6 a: ""broad definition of AI is intended that includes any products or services using AI techniques… [t]hese techniques range from simple "
206,"Dr Andreas Cebulla, Robert Chalmers, Dr Rajesh Johnsam, Professor Tania Leiman, Dr James Scheibner",Q01 Definitions,"There seems to be some inconsistency in the definitional discussion on pages 5 and 6: upfront AI is a ""system that generates predictive outputs. or decisions for a given set of human-defined objectives or parameters without explicit programming"", but at 6 a: ""broad definition of AI is intended that includes any products or services using AI techniques… [t]hese techniques range from simple rules-based algorithms guided by human-defined parameters"" (i.e. to machine learning / neural net based approaches). This seems to broaden beyond the original: in that the first excludes explicit programming but the second appears to re-admit it. At this stage of consultation we believe broader is better. Chasing semantic issues can lead to confusion and under or over inclusion, or unproductive boundary disputes at too early a stage.There is a much more fundamental issue here that the affordances introduced by AI surfaces. Technology per se is not the issue: technology is a tool. Rather the more impor"
206,"Dr Andreas Cebulla, Robert Chalmers, Dr Rajesh Johnsam, Professor Tania Leiman, Dr James Scheibner",Q02 Other AI risks and recommendations for regulation,"Whilst much of the discussion about the risks potentially or provenly associated with AI has focussed on its generic characteristics and uses, there are very specific risk that emerge when AI tools are used in workplaces. Conventional technology, from conveyor belts to robots, were designed to work under the guidance of humans. The purpose of (installing) AI, in contrast, is to guide humans, thus reversing the hierarchical relationship between human and machine. This has profound impacts on workplaces and creates new risk and stresses for people working ‘in the loop’ with automation – let alone for the subjects of decisions produced by fully or partly automated systems.The surveillance function of AI, intended to guide workplace behaviour through monitoring and sanctioning, has already been well documented. Other everyday workplace risks include but are not exclusive to: • accelerated work processes, which has ripple effect in workplace areas and functions beyond the one in which AI is"
206,"Dr Andreas Cebulla, Robert Chalmers, Dr Rajesh Johnsam, Professor Tania Leiman, Dr James Scheibner",Q03 Non-regulatory actions to address AI risks,"The findings from the 2022 Responsible AI Global Executive Study and Research project, reported that “[i]n response to the heightened stakes around AI adoption and impending regulations, organizations worldwide are affirming the need for RAI, but many are falling short when it comes to operationalizing RAI in practice”. Accordingly, strong human leadership is required to mentor and monitor responsible design and use of AI so that responsible AI (RAI) frameworks are operationalised effectively in practice. One of the non-regulatory Government initiatives, therefore, is to empower and build human leadership capacity through Responsible AI (RAI) leadership development programs. The focus of the RAI leadership development program should be to equip leaders with skills that would empower them to be actively involved in RAI practices at all levels. Following are the four types of involvement in responsible AI (RAI) practices that the RAI leadership development program can focus on, in no par"
206,"Dr Andreas Cebulla, Robert Chalmers, Dr Rajesh Johnsam, Professor Tania Leiman, Dr James Scheibner",Q05 International models for AI governance,"We commend the recommendations of the AHRC in relation to the importance of the role of education in enabling our society to understand and respond to the use of AI and ADM.More specifically in relation to some of the issues discussed above in relation to workers’ rights, consider the royal decree-law that updated Spain’s Ley del Estatuto de los Trabajadores (Worker’s Statute Law) in 2021 to include a provision requiring companies to inform employees of the parameters, rules and instructions of algorithms or artificial intelligence as they affect decision-making, working conditions, access to and maintenance of employment, including profiling."
206,"Dr Andreas Cebulla, Robert Chalmers, Dr Rajesh Johnsam, Professor Tania Leiman, Dr James Scheibner",Q09 Transparency requirements and scope,"a. where and when transparency will be most critical and valuable to mitigate potential AI risks and to improve public trust and confidence in AI?As a matter of principle, there is no point at which transparency ought not to be an option. Transparency may not equate to understanding but is essential for enabling critical review and reflection by users as well as producers of AI. In a workplace context, the national and international evidence is clear: open consultation, information, debate, and discussion across an organisation are key in enabling the safe introduction of AI technology. They also facilitate the collective monitoring of AI impacts on workplaces over time. This is important as those impacts are changeable and vary with the AI implementation stages, use and reach (across part of the organisation).b. mandating transparency requirements across the private and public sectors, including how these requirements could be implemented.The verdict on the value of mandating transpar"
206,"Dr Andreas Cebulla, Robert Chalmers, Dr Rajesh Johnsam, Professor Tania Leiman, Dr James Scheibner",Q11 Actions to increase public trust in AI,"This question begs a question as to whether there is a need to increase public trust. We would challenge this inbuilt assumption. Our current environment is pervasively influenced by AI, and there is much hand wringing - often from the tech industry and advisers (or critics) about the current or potential future risks of AI (and the people, perspectives and practices that support its development).The consultation paper cites Trust in Artificial Intelligence: A Global Study , in support of the notion of the primacy of surveyed perceptions around AI and the potential for greater uplift. However, we might critically analyze whether behaviour follows what people say in response to a survey, given that we are not seeing a widespread grassroots public campaign or behaviours that might be expected if there was a big trust issue. People may say one thing and do another. Convenience and cost issues are often larger drivers than trust, and people can continue to interact with systems that are no"
206,"Dr Andreas Cebulla, Robert Chalmers, Dr Rajesh Johnsam, Professor Tania Leiman, Dr James Scheibner",Q18 Integration of risk-based approach into existing frameworks,"Whilst the term ‘artificial intelligence’ implies a degree of independence or autonomy, all AI based systems depend on data. Machine learning systems depend on large amounts of data which can be then used to make predictions about other data sets. The datasets used to train machine learning models frequently contain sensitive personal information. This sensitive information could include health status, sexual and gender identity, political allegiance such as union membership or criminal history. Even where machine learning models are not trained using sensitive personal information, they may still be used to infer this sensitive information about individuals. Initially machine learning models were targeted at generating inferences about individuals for targeted advertisements but can be used to draw inferences in almost any domain. For instance, a machine learning model to predict the likelihood of a person being diagnosed with a disease could be used by health insurers to offer discri"
207,CrowdStrike Comment on Safe and Responsible AI in Australia,Transcription,"FILENAME: CrowdStrike Comment on Safe and Responsible AI in Australia.34c0d96aa38b7.pdf | CONTENT: REQUEST FOR COMMENT RESPONSESAFE AND RESPONSIBLE AI IN AUSTRALIA: DISCUSSION PAPERJuly 26, 2023I. INTRODUCTIONIn response to the Australian Government’s Department of Industry, Science and Resources’ (“Department”) request for feedback on its Safe and Responsible AI in Australia: Discussion Paper (“Discussion Paper”), CrowdStrike offers the following views.We approach these questions from the standpoint of a leading international, US-headquartered, cloud-native cybersecurity provider that defends globally distributed enterprises from globally distributed threats. CrowdStrike offers insights informed by multiple practice areas: cyber threat intelligence; proactive hunting, incident response and managed security services; and an AI-powered software-as-a-service cybersecurity platform and marketplace. Accordingly, this perspective is informed by CrowdStrike’s role in protecting organizations"
207,CrowdStrike Comment on Safe and Responsible AI in Australia,Q05 International models for AI governance,"The Discussion Paper capably surveys international efforts on AI. CrowdStrike recently responded to a request for comment by the U.S. White House Office of Science and Technology Policy (“OSTP”), which is developing a National AI Strategy. That request for comment raised a few questions not included in the Department’s Discussion Paper. OSTP asked about the national security benefits associated with AI; how AI can rapidly identify cyber vulnerabilities in existing critical infrastructure systems; and how it can be used to generate and maintain more secure software and hardware. While the governance measures regarding these complex issues are still being debated, proactively identifying areas for alignment, where possible, will create a coherent environment for continued innovation and make compliance more straightforward for companies operating internationally."
207,CrowdStrike Comment on Safe and Responsible AI in Australia,Q09 Transparency requirements and scope,"In our view, many AI safeguards, including specific transparency or interpretability requirements, should account for the use case. At a minimum, we suggest a distinction between “Consumer-facing AI” versus “Enterprise AI” applications. Consumer-facing AI applications may be more likely to involve impacts to specific users, people, or groups, or have other social implications. Enterprise (B2B) technologies that use AI may improve business processes and drive efficiency, like optimizing a maintenance schedule or reducing energy consumption. Different safeguards may be appropriate in each instance, but should relate specifically to potential risks.Consumer-facing AI use cases may benefit from having explainability requirements, where the AI provider is required to be able to account for the knowledge base or data set the AI tool is drawing from (e.g. quality, accuracy, amount of data going into the algorithm). In contrast, such requirements should not be applied to enterprise solutions t"
207,CrowdStrike Comment on Safe and Responsible AI in Australia,Q11 Actions to increase public trust in AI,"The widening adoption of AI periodically raises concerns about automated decision-making, surveillance, algorithmic bias, and other risks or negative externalities. But in the discourse on AI, policy makers and the public must also consider that AI also has the opportunity to drive positive social outcomes; is already widely deployed in important instances driving such outcomes; and creates the opportunity for innovation in a variety of important sectors, including industries such as cybersecurity, medicine, and education.As discussed in our general comments above, the use of AI to detect cyber threats provides a significant benefit. Considering that cyberthreats can have potentially disastrous consequences for our democratic institutions, infrastructure, and economies, the government could convey the positive message that AI is helping to protect citizens from these threats. We would be happy to work with stakeholders to identify case studies to these ends.With respect to encouraging "
207,CrowdStrike Comment on Safe and Responsible AI in Australia,Q14 Risk-based approach to address AI risks,"We support the use of a risk-based approach to assessing the overall need, scope, strength, timing, and sequence for AI-related law(s), policy(ies), and regulation(s). Resulting measures, however, should in most cases be principles-based. Outcome-based measures can be appropriate in some contexts, but this approach can be challenging with frontier technologies or areas with adaptive adversaries. Rules- and standards-based measures should be used sparingly, if at all. Selecting the right approach for a given area is complex, and we recommend additional consultation with stakeholders through iterative comment requests.More broadly, our view is that for AI, like for any other technology, the context in which it is used, rather than the mere fact that it is incorporated, is material. Consequently, regulating AI for the sake of the technology rather than its application is not the best approach to foster-innovative solutions to difficult problems. CrowdStrike recommends that policy addressi"
207,CrowdStrike Comment on Safe and Responsible AI in Australia,Q18 Integration of risk-based approach into existing frameworks,"An AI-risk based approach can be incorporated into existing assessment frameworks (like privacy) to streamline and reduce potential duplication. Specifically on privacy and AI, CrowdStrike recently provided feedback in response to the Attorney General’s requests for comments on the Privacy Act. In our comments, we noted our opposition to Proposals 19.1, 19.2, and 19.3, which suggest policies that focus on protecting individual rights by giving them the right to object to how a particular technology (AI) uses their information.Instead, as we noted above in our response to Question 14, for AI, like for any other technology, the context in which it is used, rather than the mere fact that it is incorporated, is material. Consequently, relying upon a right to object to a particular technology or data processing methodology is not the best approach to protect privacy rights in an ever-evolving technological landscape. Instead, we recommended protecting the rights of Australians through a tec"
208,The Australia Institute's Centre for Responsible Technology,Transcription,"FILENAME: P1480 No Responsible AI without transparency and accountability_final.83bf4415a9d94.pdf | CONTENT: No ‘Responsible AI’ without transparency and accountability Submission to the Consultation on Safe and responsible AI in AustraliaJordan Guiao Research Fellow, Centre for Responsible TechnologyJuly 2023 About The Australia Institute The Australia Institute is an independent public policy think tank based in Canberra. It is funded by donations from philanthropic trusts and individuals and commissioned research. We barrack for ideas, not political parties or candidates. Since its launch in 1994, the Institute has carried out highly influential research on a broad range of economic, social and environmental issues.About the Centre for Responsible Technology The Australia Institute established the Centre for Responsible Technology to give people greater influence over the way technology is rapidly changing our world. The Centre will collaborate with academics, activists, civil socie"
211,Alphinity Investment Management,Transcription,"FILENAME: Government RAI Submission - Alphinity.b5b113a20f30f.pdf | CONTENT: Submission to the Federal Government’s Safe and Responsible AI in Australia consultation paper Dated: 26/7/2026Prepared by: Alphinity Investment ManagementIntroduction and contextAs investors, we believe AI presents significant opportunities to the Australian and global business environment. Our understanding is that AI has been used by many businesses, particularly in the engineering, materials and financial sectors, for a long time but has previously been talked about primarily in the context of advanced data analytics or machine learning. Our response and recommendations below are relevant for traditional machine learning applications and generative AI applications.The advanced use of AI has the potential to make businesses more efficient, reduce costs and overheads, revolutionise business practices, and generate revenue from new or enhanced products and services. It also has the potential to help solve com"
211,Alphinity Investment Management,Transcription,"FILENAME: ALPH_AI-ESG-Considerations_May-2023.de3437fd38e0e.pdf | CONTENT: Alphinity Investment ManagementThe implications of AI across sectors and against 6 key ESG considerations Alphinity has partnered with CSIRO, Australia’s premier national scientific research agency, to develop a Responsible AI (R-AI) Framework for investors that will be published in a joint report later this year. The scoping assessment and insights outlined in this note will be used to inform this broader R-AI project. Further information and insights from the completion of this project will be published throughout the year.Introduction We identified threats and opportunities across two categories:There are many benefits to artificial intelligence (AI): it can make • Internal business considerations: threats and opportunities fast work of huge amounts of data, speed up processes, and which could impact individual businesses directly collaborate with humans to provide a better outcome than • External system wide"
212,Australian Human Rights Commission,Transcription,"FILENAME: The Need for Human Rights-centred Artificial Intelligence.ab4c91d3fef12.pdf | CONTENT: The Need for Human Rights- centred Artificial Intelligence Australian Human Rights CommissionSubmission to the Department of Industry, Science and Resources26 July 2023ABN 47 996 232 602 Level 3, 175 Pitt Street, Sydney NSW 2000 GPO Box 5218, Sydney NSW 2001 General enquiries 1300 369 711 Complaints info line 1300 656 419 TTY 1800 620 241 Australian Human Rights Commission The Need for Human Rights-centred Artificial Intelligence, 26 July 2023 Contents1 Commission introduction ......................................................................... 32 Definitions.................................................................................................. 4 2.1 Discussion paper terminology ................................................................. 4 2.2 Deepfakes ................................................................................................ 4 2.3 Neurotechnolog"
213,Getty Images,Transcription,"FILENAME: Australia Consultation Response - Safe and Responsible AI Discussion Paper (final).93b4ccf23f80e.pdf | CONTENT: Level 6, 182-186 Blues Point Road McMahons Point, NSW 2060Supporting Responsible AI: Discussion Paper Australian Government Department of Industry Science and Resources Industry House, 10 Binara Street Canberra AustraliaSubmission via consult.industry.gov.au26 July, 2023Consultation on Safe and responsible AI in AustraliaGetty Images appreciates the opportunity to provide our comments in relation to this important consultation and commend the Australian Government for supporting safe and responsible AI practices. Getty Images is an established and respected member of the global media, a content owner and marketplace for visual content and, we maintain and operate a significant presence in Australia.As a content creator and a company that represents hundreds of thousands of content creators, we believe AI can produce commercial and societal benefit; however, we belie"
213,Getty Images,General comment,"Please see our attached responses to questions 2, 3, 5, 9, 11, and 19"
213,Getty Images,Q02 Other AI risks and recommendations for regulation,"While existing Australia regulatory approaches including approaches to copyright, intellectual property and privacy laws can help mitigate some risks to the creative sectors, they fall short of providing human creators sufficient protection. More specifically, Australia's existing regulatory approaches fall short of explicitly requiring consent in the collection of individual and private information, as well as the creative works of individuals that are so vital in training robust AI systems (including but not limited to; images, videos, music and books). For many creatives, royalty payments associated with the use of their work is their sole source of income and is their primary way to recouping the considerable investments they make to create. Allowing AI developers to train generative AI models without obtaining authorization circumvents this revenue stream and discourages the generation of new creative works.As AI developers rush to develop commercial technologies that rely on rich"
213,Getty Images,Q03 Non-regulatory actions to address AI risks,"Promotion of industry developed transparency tools that help users of AI systems to know when they are interacting with AI systems and, specifically, to identify outputs of generative AI tools, e.g. through adoption of metadata and/or watermarking technologies.Developers and deployers of generative AI systems should be obligated to meet transparency requirements regarding the output of their models. This includes indicating when content is generated or modified by AI and implementing technical standards. For example, attaching meta-data that identifies AI generated or modified content in the IPTC DigitalSourceType field of that content. (see https://www.iptc.org/news/iptc-releases-draft-of-digital-source-type-vocabulary-to-support-synthetic-media/). Transparency requirements should also include disclosures related to any protected data used for training or finetuning generative foundational models."
213,Getty Images,Q05 International models for AI governance,"Harmonisation is a hard thing to do but is necessary. The Australian government needs to make it a priority to work with its counterparts in the EU, US and other key digital allies to develop basic international norms and standards including basic transparency standards. When working with international partners, Australia should support IP rights and continue its legacy of vigorously protecting IP and human rights on the international stage.The EU AI Act is a good model to follow especially in how it handles transparency standards. There is a need for transparency requirements similar to proposed EU AI Act, i.e. in relation to both (i) composition and source of training data that includes copyright works at the input stage; and (ii) the labelling of outputs of generative-AI tools to guard against risks posed by deepfakes. Such transparency standards will help promote responsible and ethical innovation and provide chouse for consumers in a fair and balanced manner."
213,Getty Images,Q09 Transparency requirements and scope,"a. Transparency is critical to mitigate against the risk of violating third party IP and privacy rights and the development of unfair and bias models. Without transparency obligations, providers of models can effectively hide such violations inside a backbox and it will be very difficult for the public to identify and understand models that are bias.b. One way to mandate transparency requirements is to require both private and public sector organisations to keep auditable records of all training data sets used including how the data was sourced.. Such auditable records should be available in a suitable manner for the intended audience(s) so that IP owners of such data may check whether appropriate consent has been obtained for the use of their content and users of the AI tools will be able to understand the make-up of the training data (which could be helpful for understanding questions of bias) and whether consent has been obtained as may be required by IP owners (which could help the"
213,Getty Images,Q11 Actions to increase public trust in AI,The idea of government sponsored AI sandboxes that are beacons for ethical and responsible AI development.The government should require transparency on all training sets utilized and require permission of IP owner where such models or any derivative models have the potential to be commercially deployed. The government must work with rightsholders to identify sources of licensable data for use in the sandbox and could seek to make introductions to AI developers. Getty Images would be keen to explore participating in the sandboxes.The government should ensure that compliance with sandbox standards will enable Australian companies to be able to market their AI tools globally by requiring all participants to meet high transparency standards. The government should award certification to AI developers who participate and are able to demonstrate that they have adhered to guidance regarding the licensing of data that includes copyright works.Due to the importance of protecting IP Rights and pr
213,Getty Images,Q19 Application of risk-based approach to general purpose AI,"Regulators will need to avoid the temptation to create exemptions for open-source developers that enable them to circumvent legal requirements regarding the sourcing of training data that contains protected works. Open-source should not be equated to non-commercial research. Doing so would open up loopholes that will be ripe for abuse by bad actors. In particular, the Australian government should not facilitate the practice of data laundering through the use of text and datamining exceptions. Open-source foundation models, known to have been pre-trained on protected data, need to be held to the same level of legal compliance as closed-source foundation models. Moreso because downstream use of open-source models will be more difficult to regulate than closed-source models. Users of such generative foundation models who create generative AI tools or produce generative content need to be assured that that consent for protected training data was legally obtained. To better understand the c"
214,Australian Medical Association,Transcription,"FILENAME: AMA Submission to Safe and Responsible AI Consultation_FINAL.3c787621fa67d.pdf | CONTENT: Tuesday, 25 July 2023Safe and responsible AI in Australia AMA submission to the Department of Industry, Science and Resources Discussion Paper Online submission via https://consult.industry.gov.au/supporting-responsible- ai/submissionThe AMA welcomes the discussion paper on safe and responsible Artificial Intelligence (AI) in Australia. As the peak professional body for doctors in Australia, this submission will focus on the regulation of AI in health and medicine only. Regulating the application of AI in healthcare will require a tailored approach that allows for the adoption and integration of safe technologies while protecting patient and practitioner privacy and patient safety. The AMA is concerned that Australia is behind other comparable countries in AI policy. In healthcare, the development and implementation of AI technologies must be undertaken with appropriate consultation, tra"
214,Australian Medical Association,Q01 Definitions,See our submission
214,Australian Medical Association,Q02 Other AI risks and recommendations for regulation,See our written submission
214,Australian Medical Association,Q03 Non-regulatory actions to address AI risks,See our written submission
214,Australian Medical Association,Q04 Coordinating AI governance across government,See our written submission
214,Australian Medical Association,Q05 International models for AI governance,See our written submission
217,Lext,Transcription,"FILENAME: 2023.07.26 - Submission by Lext.c7e921d6ed6a7.pdf | CONTENT: Lext Australia 3/156 Clarence Street Sydney NSW 200026 July 2023Supporting responsible AI: discussion paper Submission to the Australian GovernmentDear Consultation Team,Lext is an Australian legal innovation business based in Sydney. At Lext, our interdisciplinary team of lawyers and software engineers build artificial intelligence tools to make the law easier to access, and easier to practise.At Lext, we believe that AI can solve many different challenges in Australian society, including improving access to justice.Executive summaryAustralia must:1. Become a more attractive jurisdiction for AI entrepreneurs to start and headquarter new ventures. 2. Create the infrastructure to encourage and support sovereign innovation capability, including by partnering with private enterprise to accelerate both the creation and use of AI applications nationally. 3. Ensure our legislative framework remains fit for purpose in a ch"
217,Lext,General comment,See written submission.
217,Lext,Q02 Other AI risks and recommendations for regulation,See written submission.
217,Lext,Q03 Non-regulatory actions to address AI risks,See written submission.
217,Lext,Q04 Coordinating AI governance across government,See written submission.
217,Lext,Q05 International models for AI governance,See written submission.
217,Lext,Q06 Public vs Private sector governance,See written submission.
217,Lext,Q07 Responsible government use of AI,See written submission.
217,Lext,Q08 Generic vs technology-specific approaches for AI risk,See written submission.
217,Lext,Q09 Transparency requirements and scope,See written submission.
217,Lext,Q10 Ban on high-risk AI applications,See written submission.
217,Lext,Q11 Actions to increase public trust in AI,See written submission.
217,Lext,Q12 Trade impact of banning high-risk AI applications,See written submission.
217,Lext,Q13 Changes to conformity infrastructure,See written submission.
217,Lext,Q14 Risk-based approach to address AI risks,See written submission.
217,Lext,Q15 Benefits and limitations of risk-based approach,See written submission.
217,Lext,Q16 Scope and application of risk-based approach,See written submission.
217,Lext,Q17 Elements to include in risk-based approach,See written submission.
217,Lext,Q18 Integration of risk-based approach into existing frameworks,See written submission.
217,Lext,Q19 Application of risk-based approach to general purpose AI,See written submission.
217,Lext,Q20 Voluntary vs mandated compliance with risk-based approach,See written submission.
218,Dr Ramona Vijeyarasa,Transcription,"FILENAME: Submission_Vijeyarasa_Gender and AI.94e242aa3011e.pdf | CONTENT: Consultation on Safe and Responsible AI in Australia Dr Ramona Vijeyarasa, Associate Professor, Faculty of Law,University of Technology SydneyIntroductionThis submission is focused on providing a gender perspective to how the Australian Government should develop its regulatory and governance response to the design, deployment and use of AI-driven technologies. I write this submission as one of Australia’s key academic experts on gender and the law. As an Associate Professor in the Faculty of Law at the University of Technology Sydney, I bring to my academic experience 10 years of practical experience as a women’s human rights lawyer. I have worked in local and international non-government and inter-governmental organisations in jurisdictions across the globe. I have published three books and dozens of articles on bringing a gender perspective to law- making. I am the chief investigator behind the research collab"
218,Dr Ramona Vijeyarasa,Transcription,"FILENAME: Submission_Vijeyarasa_Gender and AI.eee09281d6c38.pdf | CONTENT: Consultation on Safe and Responsible AI in Australia Dr Ramona Vijeyarasa, Associate Professor, Faculty of Law,University of Technology SydneyIntroductionThis submission is focused on providing a gender perspective to how the Australian Government should develop its regulatory and governance response to the design, deployment and use of AI-driven technologies. I write this submission as one of Australia’s key academic experts on gender and the law. As an Associate Professor in the Faculty of Law at the University of Technology Sydney, I bring to my academic experience 10 years of practical experience as a women’s human rights lawyer. I have worked in local and international non-government and inter-governmental organisations in jurisdictions across the globe. I have published three books and dozens of articles on bringing a gender perspective to law- making. I am the chief investigator behind the research collab"
218,Dr Ramona Vijeyarasa,Q01 Definitions,Yes
218,Dr Ramona Vijeyarasa,Q02 Other AI risks and recommendations for regulation,"The gendered harms of AI are increasing well-known and have been the subject of a significant body of research in this field. As a gendered problem, our legislative response must be gender-responsive, that is, the law must take into account differences in interests, needs and experiences of men, women and non-binary people. A gender-responsive approach to addressing the harms of AI involves recognizing what can broadly be categorised as three types of harms: a) Allocative harms result from decisions about how to allocate goods and opportunities among a group. Here we can think of the way an AI system used in a recruitment process may disproportionately classify applications for male candidates as more suitable than female. Such a system potentially results in a loss of financial opportunities, livelihoods and freedom of choice for women when compared to men. b) Representational harm comes about when systems reinforce gendered subordination, through stereotyping, under-representation or"
218,Dr Ramona Vijeyarasa,Q03 Non-regulatory actions to address AI risks,"6. Ensure the active participation of a diversity of women in the governance of AI by incorporating and defining gender-sensitive due diligence and a requirement for responsible data collection (Terms of Reference Q3) The over-representation of men in the design of AI-driven technologies is widely acknowledged. Meanwhile women dominate among those scholars identifying the gender-based biases that are and can result from AI’s deployment. In turn, the presence of a greater diversity of views to represent the interests of those affected by such biases is arguably an important part of the solution. This is an argument increasingly made, that is, a higher participation of women in AI-driven technologies is needed to bring visibility to the gender-based harms of AI. Non-regulatory requirements for AI should include an obligation on private and public entities deploying AI-driven technologies to conduct gender-sensitive due diligence, ensuring the active participation of a diversity of women "
218,Dr Ramona Vijeyarasa,Q05 International models for AI governance,"a) Undertaking gender-specific impact assessments As early as 2019, Canadian introduced the Treasury Board Directive on Automated Decision-Making, a mandatory policy instrument which applies to almost all federal government institutions. It places a particular emphasis on impact assessments and transparency, including the likely impacts of AI on freedom, health, the economy and environment. The standout feature of the Canadian Directive is the Gender-Based Analysis Plus, a quality reassurance requirement before launching into production of an AI-driven technology. This Gender-Based Analysis Plus requirement entails compulsory testing for unintended biases. If an AI-driven technology meets the moderate, high and very high-risk thresholds, the designers need to undertake Gender-Based Analysis Plus. This “plus” reflects going further than the gender impact assessment already required for procurement. Additional steps include an assessment of the impact of the automation on gender and/or o"
218,Dr Ramona Vijeyarasa,Q14 Risk-based approach to address AI risks,"Yes. The standout feature of the Canadian Directive is the Gender-Based Analysis Plus, a quality reassurance requirement before launching into production of an AI-driven technology. This Gender-Based Analysis Plus requirement entails compulsory testing for unintended biases. If an AI-driven technology meets the moderate, high and very high-risk thresholds, the designers need to undertake Gender-Based Analysis Plus. This “plus” reflects going further than the gender impact assessment already required for procurement. Additional steps include an assessment of the impact of the automation on gender and/or other identifying factors but also naming what planned or existing measures are in place to address these identified risks in the future."
220,ANZSA,Transcription,"FILENAME: ANZSA Safe and responsible AI submission.b6a290d7f9767.pdf | CONTENT: About us1 This submission is made on behalf of the Australia New Zealand Screen Association (ANZSA). The ANZSA represents the film and television content and distribution industry in Australia and New Zealand. Its core mission is to advance the business and art of filmmaking, increasing its enjoyment around the world and to support, protect and promote the safe and legal consumption of movie and TV content across all services. Members of ANZSA include: Village Roadshow Limited; Motion Picture Association; Walt Disney Studios Motion Pictures; Netflix Inc.; Paramount Pictures; Sony Pictures Releasing International Corporation; Universal International Films, Inc.; and Warner Bros. Pictures International, a division of Warner Bros. Entertainment Inc., and Fetch TV.General comments2 We thank the Government of Australia and the Department of Industry, Science and Resources for the opportunity to provide comments "
221,Privcore,Transcription,"FILENAME: Privcore submission to Department of Industry, Science and Resources - Safe and Responsible AI 2023.181f910a97dd2.pdf | CONTENT: Making privacy core businessSubmission – Safe and responsible AI in Australia Discussion paper 26 July 2023Privcore Pty LtdACN: 167 388 178 ABN: 46 167 388 178Email: operations@privcore.com Website: www.privcore.comAddress: Level 14, 5 Martin Place, Sydney NSW 2000Copyright Privcore 2023 Technology Strategy Branch Department of Industry, Science and Resources Via email: DigitalEconomy@industry.gov.auPage | 2 26 July 2023Submission: Consultation on Safe and Responsible AI in Australia – Discussion PaperIntroductionThank you for the opportunity to participate in the consultation on Safe and Responsible AI in Australia – Discussion Paper (the Discussion Paper). This submission can be made publicly available. In this submission, Privcore references, where relevant, points made in its submission in response to the 2019 Discussion Paper on Artificial Inte"
222,Australian Centre for Health,Transcription,"FILENAME: SafeResponsibleAI_Discussion_Paper_ACHEEV_Response.23bed7d73975f.pdf | CONTENT: Response to the Australian Government’s Discussion Paper on Safe & Responsible AI Date: 26 July 2023 Prepared by: Dr. Yves Saint James Aquino, Emma Frost, and Prof Stacy Carter Australian Centre for Health Engagement, Evidence and Values, University of WollongongISSUE We welcome the opportunity to respond to the Department of Industry, Science and Resources’ Safe and Responsible AI in Australia Discussion Paper. The discussion paper provides a detailed overview of domestic and international governance mechanisms for Artificial Intelligence (AI). In addition, the paper identifies key opportunities for AI to improve economic and social outcomes, as well as some potential risks and harms associated with AI. The paper seeks advice on how to mitigate the potential risks of AI, and identifies gaps in the existing domestic governance landscape to support the responsible and safe development and adoption "
222,Australian Centre for Health,General comment,"The Australian Centre for Health Engagement, Evidence and Values (ACHEEV) includes a team of researchers focused on Data, AI and other health technologies. Our healthcare AI research has been funded by NHMRC Ideas Grant 1181960, NHMRC CRE 2006545, and grants from the University of Wollongong. Our work focuses on the ethical, legal and social implications of the use of machine learning in healthcare. We welcome the government’s efforts to identify gaps in the governance of AI and seek feedback on how to move forward. Based on our work on the ethics of healthcare AI, we recommend regulatory approaches that take into account both individual and societal risks, are based on research and evidence, and uphold public participation."
222,Australian Centre for Health,Q01 Definitions,"We suggest removing the phrase “without explicit programming” from the definition of AI (page 5), as the intent on page 6 appears to be to include expert systems, which are AI applications that involve explicit programming. Across the document, we suggest focussing on both AI and ADM, since ADM can be equally harmful and raise similar ethical and safety concerns. We note that because the paper did not make a clear distinction between regulatory and non-regulatory initiatives, responding to Guide Questions 2 and 3 was sometimes challenging. In the discussion below, we have assumed that non-regulatory initiatives are “voluntary mechanisms”, as defined on page 4 of the paper."
222,Australian Centre for Health,Q02 Other AI risks and recommendations for regulation,"Governance of AI should seriously consider not only risks to individuals but also risks to society or communities as a whole. Current frameworks tend to focus on the former. For example, equity is a particular concern that tends to be missed in regulation that focuses on risks to individuals. Equity was emphasised in the discussion paper. Equity was also a strong emphasis in the Community Jury recommendations. We note that the discussion paper appears to take a narrow view, limiting equity concerns to bias due to non-comprehensive datasets (page 8). Non-comprehensive datasets are a significant issue, but are only a part of a larger problem of systemic and structural inequities that can be reinforced or entrenched by AI. Our work with AI experts has shown that some experts do not recognise bias as a problem requiring attention, suggesting the need for regulation. In any case, bias cannot be addressed only via technical solutions: this also requires social, ethical and political expertis"
222,Australian Centre for Health,Q03 Non-regulatory actions to address AI risks,"The Community Jury strongly recommended education and communication about healthcare AI for both members of the public and practitioners. A minority of medical Colleges have begun educating professionals (e.g. RANZCR), but to our knowledge information and education for the Australian public has been limited to date. Information campaigns should be led by independent bodies who have no commercial interests in the development and deployment of AI. Commercial interests can threaten trust, integrity and duties of care."
222,Australian Centre for Health,Q05 International models for AI governance,"In our view, the Australian government should consider a horizontal approach to the governance of AI. This creates a comprehensive framework that covers impacts of AI across industries, and could be underpinned by a human rights approach, as is underway in the European Union (EU). Australia has the benefit of a portfolio of prior work by the Australian Human Rights Commission which could inform such an approach. A horizontal approach to AI regulation would need to interleave with existing regulatory mechanisms, some of which are identified in the discussion paper (page 10). A horizontal approach has the benefit of setting standards across all domains and levels of government. The Community Jury began their recommendations by specifying that there must be a charter for AI in the Australian health system and services, managed by an independent decision-making body. This recommendation was domain specific because the jury was asked to think about health applications of AI. It is possible "
222,Australian Centre for Health,Q06 Public vs Private sector governance,"The overarching governance of AI for public and private sectors should not differ. It is possible for private sectors to have industry-specific regulation or guidance (e.g. codes of ethics), but this regulation should not be in conflict with nor weaker than any regulation that applies to the public sector."
222,Australian Centre for Health,Q08 Generic vs technology-specific approaches for AI risk,"Regarding generic versus technology-specific risk mitigation. Broad ethics principles, such as transparency, should be embedded in the design and deployment of any type of AI technology. However, some types of AI technology raise specific concerns that require technology-specific solutions. For example, copyright issues are raised in varying degrees depending on the datasets used to develop an AI tool. Unlike most healthcare AI tools trained using proprietary datasets, some generative AI systems use unlicensed content that raise concerns about intellectual property infringements. . In addition to generic and technology-specific solutions, problem- or task-specific solutions may be appropriate. This approach is concerned with what problem the technology is meant to solve or what task the technology is designed to perform. Any task involving clinical work, such as establishing diagnosis and identifying appropriate therapy, should be considered higher risk (and require more robust regulat"
222,Australian Centre for Health,Q09 Transparency requirements and scope,"Governance of AI should consider specific rules for transparency about the use of AI. Concern about transparency (and the lack thereof) is a common finding across our consumer-facing studies. Transparency is prominent in the EU approach, as well as in local guidelines including the NSW Artificial Intelligence Assurance Framework and the Automated Decision-making Better Practice Guide. Transparency requirements should cover any ADM/AI-related recommendation or decision with direct implications to clients or consumers. Preliminary findings of our scoping review of literature on public views about healthcare AI ethics and governance show that there is interest in transparency with regards to: • How accurate or effective the model is at performing tasks • To what extent a recommendation by an AI system is overseen by a person • Which data sources were used to train the AI system • If and how the AI systems collect and store user data Transparency should be meaningful and understandable for"
222,Australian Centre for Health,Q10 Ban on high-risk AI applications,"Complete banning of certain high-risk AI applications should be considered to protect the safety and welfare of Australians. We note that this approach has been taken in some jurisdictions (e.g. the draft EU AI Law). Risk evaluations that could lead to the banning of AI come down to people’s values and priorities. Thus, engagement with the public will be important to determine what applications should and should not be prohibited in the Australian context."
222,Australian Centre for Health,Q11 Actions to increase public trust in AI,"Regarding public trust in AI. The discussion paper focuses on improving public trust as a central goal, and conceptualises public distrust as one of the main barriers to AI adoption. We suggest that this should be reframed. Rather than improving public trust, the goal should be increasing the trustworthiness of AI. Publics are diverse, and hold diverse values. They are active stakeholders in defining what counts as trustworthy behaviour from governments and corporations – whether or not this is acknowledged. Question 11 asks “What initiatives or government action can increase public trust in AI deployment to encourage more people to use AI.” This puts the focus on changing the views of the public, rather than on changing AI and its implementation. This strategy risks diluting the responsibilities of manufacturers, developers and regulators to ensure AI is trustworthy—that the technology is proven safe and effective. In relation to the issue of ensuring trustworthy AI, we note that the "
222,Australian Centre for Health,Q14 Risk-based approach to address AI risks,"AI is highly disruptive and has the potential to transform society. Because of this, we should take a precautionary approach and be more rather than less risk-averse."
222,Australian Centre for Health,Q15 Benefits and limitations of risk-based approach,"A risk-based approach should include consideration of the evidence base for AI use in all sectors and industries. It is important to apply robust and clear rules of evidence, as well as ensuring that the evidence is gathered from real-world contexts. Current practices in healthcare AI technology development still relies on synthetic datasets, which contain computer-generated rather real-world data. In addition, studies have shown that there is often insufficient validation of AI systems in real-world practice. . A scoping review of AI applications for breast cancer detection, for example, showed evidence gaps that suggest the performance of AI applications may match what is claimed. These gaps include use of imaging data that may not represent the practice setting, the potential for bias in model training, and the lack of tests comparing AI versus human interpretation. A key limitation of a risk-based approach is that emerging technologies make it difficult to assess risk, and that the"
222,Australian Centre for Health,Q17 Elements to include in risk-based approach,"We find the discussion paper lacks emphasis on the role and involvement of the public in defining risk, and which application, case or industry should be considered high or low risk. Our community jury recommended a comprehensive and fully funded community education program to ensure that the community is brought along with developments in and the application of AI in health. Similarly, the Montréal Declaration for a Responsible Development of Artificial Intelligence (2018) includes “democratic participation” as one of its principles. The democratic participation principle not only upholds the responsibility of authorities to inform the public about AI, but also the opportunity for citizens to deliberate on the social parameters of AI systems, their objectives and the limits of their use."
223,Visa,Transcription,FILENAME: Visa response to Australian Govt - Safe and Responsible AI in Australia Discussion Paper FINAL.634d9b0958585.pdf | CONTENT: Response to the Government’s Safe and Responsible AI in Australia Discussion Paper Table of contentsGroup Country Manager Letter ...................................................................................................3 Introduction............................................................................................................................... 4 Response to Selected Consultation Questions ...........................................................................7 About Visa ................................................................................................................................. 112|Page Group Country Manager Letter26 July 2023The Hon Ed Husic MP Minister for Industry and Science Government of Australia PO Box 6022 House of Representatives Parliament House Canberra ACT 2600Via email: https://consult.industr
223,Visa,General comment,"If there are any queries regarding Visa's submission, please contact Helen Seidel, Head, Government Engagement, Australia, New Zealand & South Pacific. Helen's email address is: hseidel@visa.com"
223,Visa,Q01 Definitions,N/a
223,Visa,Q02 Other AI risks and recommendations for regulation,N/a
223,Visa,Q03 Non-regulatory actions to address AI risks,N/a
223,Visa,Q04 Coordinating AI governance across government,"Developers and deployers of AI are already subject to a wide body of legislation on fundamental rights, consumer protection, unfair commercial practices, competition law, and product and safety liability. Visa’s own sector of financial services, and, within this, payments, is highly scrutinised and regulated, both at the national and supranational levels. It is important to recognise that much of the existing legislation can be applied or adapted to apply to AI. We understand that this approach would be the most efficient route to effective outcomes in many cases, supporting Australian competitiveness by avoiding the imposition of undue regulatory burden for businesses investing and innovating in Australia. We believe this route is likely to be sufficient to address many potential harms. A holistic review of sectoral regulation should be undertaken for this purpose, as a pre-requisite to the introduction of any new rules. This will require consultation with industry, and close coordina"
223,Visa,Q05 International models for AI governance,"There are a growing number of legislative and non-legislative initiatives currently under review across the world that consider governance structures as well as rules and provisions for the development and deployment of AI. The most advanced discussions presently are being conducted by the European Union. Visa has been actively engaging with EU institutions on the Artificial Intelligence Act5 since it was first proposed in April 2021, helping to shape the definition of AI and the criteria to define high-risk AI models. We have also provided insights on the necessary conditions to develop and implement industry-leading Model Risk Management systems, governance frameworks and industry guiding principles.We have highlighted and urged policymakers to ensure that the proposed regulatory framework is outcome-based and future proof, as well as internationally aligned to promote competition and innovation.In this context, Visa welcomes the recent announcement of the EU AI Pact, a voluntary ind"
223,Visa,Q06 Public vs Private sector governance,N/a
223,Visa,Q07 Responsible government use of AI,N/a
223,Visa,Q08 Generic vs technology-specific approaches for AI risk,N/a
223,Visa,Q09 Transparency requirements and scope,N/a
223,Visa,Q10 Ban on high-risk AI applications,N/a
223,Visa,Q11 Actions to increase public trust in AI,"Public trust is key for the successful implementation and rollout of AI tools. Visa, therefore, believes that regulation that maintains a focus on outcomes will incentivise companies to place consumer impact at the heart of their approaches to AI. This does not detract from the importance of the process behind outcomes. Indeed, well-designed outcomes-based frameworks can drive proper risk management, accountability, and ethical considerations, whilst allowing companies to operate flexibly and efficiently, keeping pace with evolving technology and community best practices.Visa welcomes regulatory regimes that are risk-based and foster the right incentives for companies to elevate their governance for the development and deployment of AI that produce safe and secure outcomes. Regulatory approaches should also ensure measures are in place to drive responsible practices that uphold fundamental rights.We believe that any new regulatory frameworks should maintain a strong focus on technology"
223,Visa,Q12 Trade impact of banning high-risk AI applications,N/a
223,Visa,Q13 Changes to conformity infrastructure,N/a
223,Visa,Q14 Risk-based approach to address AI risks,"Risk and governance processes are important tools that foster trust among consumers, clients, and the larger payments ecosystem. Risk-based approaches help companies understand where additional assessments and safeguards may be needed for a specific use case. In managing risk, we believe that AI governance benefits from cross-functional input from stakeholders across different domains to ensure healthy checks and balances for sustainable innovation. As risk and governance are highly contextual processes that differ by sector, and even by individual company, organisations taking a risk-based approach should be empowered to determine what measures are commensurate to address AI risks emerging in their domain."
223,Visa,Q15 Benefits and limitations of risk-based approach,"Regulation should be focused on the application of AI in higher-risk contexts and use cases. By focusing on use cases, organisations can continue to promote innovation while instilling public trust. Moreover, a risk-based approach allows for a more effective risk management approach. Visa’s Model Risk Management assesses risk regarding a variety of factors, including context, use case, and potential for impact. Based on the risk rating, appropriate and relevant governance and review is employed.Visa encourages governments and businesses to actively weigh risks and benefits of each application of AI, and the context in which is it deployed – understanding that the same technology may pose different risk levels based on its purpose, deployment, and outcomes."
223,Visa,Q16 Scope and application of risk-based approach,N/a
223,Visa,Q17 Elements to include in risk-based approach,N/a
223,Visa,Q18 Integration of risk-based approach into existing frameworks,N/a
223,Visa,Q19 Application of risk-based approach to general purpose AI,N/a
223,Visa,Q20 Voluntary vs mandated compliance with risk-based approach,N/a
224,Andrew Cullen,Transcription,"FILENAME: Response to Supporting Responsible AI discussion paper.3495a3929f24.docx | CONTENT: Response to Supporting Responsible AI discussion paperBackground: Dr Andrew C. Cullen, Postdoctoral Research Fellow in Artificial Intelligence Security at the University of Melbourne. I have been working in this field for four years, and I have experience in both academic and industrial applications of AI.Response Summary:This call for submissions is almost certainly going to lead to a significant number of comments regarding the risks of strong, or superintelligent AI. As an expert working in this field, I can say with great confidence that that the risks with AI are not from strong AI, but rather from weak AI - of the kind that is already being used now in industry - being applied in ways that don’t reflect the values and interests of the Australian public. Concerningly in Australia we simply don’t have a broad enough base of expertise for practitioners of AI to properly ask questions of how"
224,Andrew Cullen,General comment,"This call for submissions is almost certainly going to lead to a significant number of comments regarding the risks of strong, or superintelligent AI. As an expert working in this field, I can say with great confidence that that the risks with AI are not from strong AI, but rather from weak AI - of the kind that is already being used now in industry - being applied in ways that don’t reflect the values and interests of the Australian public. Concerningly in Australia we simply don’t have a broad enough base of expertise for practitioners of AI to properly ask questions of how their systems are being deployed,The secondary concern that I would like to emphasise is that without keeping up in the global arms race for computing power (of the top 100 supercomputers in the world, only 2 are in Australia [https://www.top500.org/lists/top500/list/2023/06/] and we are consistently falling down the list of supercomputing capacity) and AI talent, Australia will loose the ability to control the va"
224,Andrew Cullen,Q02 Other AI risks and recommendations for regulation,"The risk that the values of Australian businesses, and the Australian public, do not align culturally with AI systems developed in China, or by American businesses."
224,Andrew Cullen,Q03 Non-regulatory actions to address AI risks,"There is no other answer to this besides money, to be honest. Australia needs a pipeline of talent that does not result in talented Australian experts leaving the country for better opportunities (which is a significant factor at the moment), and we need an ability to better develop younger talent in the field.All of this though is immaterial if this talent pipeline isn’t supported by computing infrastructure, which is a core part of nurturing, developing, and cultivating talent within this space."
224,Andrew Cullen,Q05 International models for AI governance,"The concept of European AI, in that AI systems should be developed in alignment with the cultural, financial, and linguistic values of the European union bloc, is a concept that Australia should be broadly echoing. Without taking such a position, our cultural values will not be reflected within the systems that are rapidly becoming a core and inescapable part of our everyday lives."
224,Andrew Cullen,Q07 Responsible government use of AI,"Government needs to interact more broadly with academia to help develop talent and expertise in responsible AI. At the moment, as far as I’m aware, Defence is the primary driver of such interactions, and even that is thin on the ground, and potentially drying up with time."
224,Andrew Cullen,Q10 Ban on high-risk AI applications,"The genie is out of the bottle here, and to some extents a ban is impossible. However we need to work out how to build safe AI systems, and how to promote their use by business and government."
224,Andrew Cullen,Q11 Actions to increase public trust in AI,"If Australia wants to be a country in which there is public trust of AI, we should be positioning ourselves to be leaders in responsible, ethical, Australian-values driven AI. At the moment we’re not anywhere close to that, and such a positioning will not happen organically. It will require work, expertise, and time - and at the moment we’re rapidly running out of time."
224,Andrew Cullen,Q20 Voluntary vs mandated compliance with risk-based approach,"I fail to see how voluntary or self-regulatory schemes will work. AI systems are often systems that are incredibly difficult to scruitinise, and companies and individuals will not put the effort in to fully understand the responsibility of their AI systems unless they are forced to. A voluntary scheme, or a self-regulatory scheme will simply lead to people doing the bare minimum, which is not enough to ensure that AI systems are behaving ethically, and in a fashion that reflects the values and interests of the Australian people."
227,Australian Chamber of Commerce and Industry (ACCI),Transcription,"FILENAME: ACCI AI Submission Letter 260723.659d984ce8f48.pdf | CONTENT: 26 July 2023Technology Strategy Branch Department of Industry, Science and ResourcesVia email: DigitalEconomy@industry.gov.auResponse to the ‘Safe and Responsible Artificial Intelligence (AI) in Australia’ discussion paper.About ACCI The Australian Chamber of Commerce and Industry (ACCI) is Australia’s largest and most representative business association. Our members are all state and territory chambers of commerce, which in turn have 430 local chambers as members, as well as over 70 national industry associations. Together, we represent Australian businesses of all shapes and sizes, across all sectors of the economy, and from every corner of our country. Our members have a strong interest in the regulation and risk management of activities in relation to data, digital technology and cyber security. ACCI also maintains strong international connections with participation in the Business at OECD (BIAC) Digital Econom"
227,Australian Chamber of Commerce and Industry (ACCI),Q01 Definitions,"ACCI is supportive of the government using internationally recognised and widely adopted definitions. The discussion paper references the International Standards Organisation (ISO) definition. As noted above, ACCI would preference alignment to relevant OECD definitions in order to remain consistent with international work and to maintain our ability to benchmark against other countries."
228,Australian Lottery & Newsagents Association (ALNA),Transcription,"FILENAME: Submission from the Australian Lottery & Newsagents Association (ALNA) on the Safe and Responsible AI in Australia Discussion Paper.4aa6135d364a7.pdf | CONTENT: 25th July 2023Department of Industry, Science & ResourcesSubmission from the Australian Lotery & Newsagents Associaon (ALNA) on the Safe and Responsible AI in Australia Discussion PaperBy email: DigitalEconomy@industry.gov.auAbout ALNAThe Australian Lotery and Newsagents’ Associaon (ALNA) is the naonal industry body represenng Lotery Agents and Newsagents’. There are over 4000+ Lotery Agents and Newsagents in Australia. They are generally family run businesses and are an important and trusted part of Australian communies and approximately 2.5 million Australians shop at their local agency every day.ALNA represents a network of trusted individuals and their small businesses within communies across Australia. They are deeply rooted in their local areas, oﬀering convenience through their diverse products, services, and w"
228,Australian Lottery & Newsagents Association (ALNA),Q01 Definitions,YES
229,Responsible Investment Association Australasia (RIAA),Transcription,"FILENAME: RIAA submission_Australia_Responsible AI_FINAL.859fc17feac3.pdf | CONTENT: Submission in response to the consultation on Safe and Responsible AI in Australia July 2023Submission by the Responsible Investment Association AustralasiaThe Responsible Investment Association Australasia (RIAA) welcomes the opportunity to make a submission for the Department of Industry, Science and Resources’ consultation on Safe and Responsible AI in Australia.We make this submission to:• Highlight and explain why and how leading global and Australian investors support stronger governance frameworks and rules for the ethical application of AI; • Note how stronger governance frameworks, in consultation with business, investors, civil society and academia, will give greater confidence to the business and investor community and unlock significant potential for future revenue and growth; • Draw attention to investor-led initiatives focused on assessing and respecting human rights and mitigating human "
230,Digital Rights Watch,Transcription,"FILENAME: DRW - Safe and Responsible AI - July 2023.cd1572b4ac22b.pdf | CONTENT: Submission to the Department of Industry, Science and Resources regardingSafe and Responsible AI26 July 2023Digital Rights Watch is a charity organisation founded in 2016 whose mission is to ensure that people in Australia are equipped, empowered and enabled to uphold their digital rights. We stand for Privacy, Democracy, Fairness & Freedom in a digital age. We believe that digital rights are human rights which see their expression online. We educate, campaign, and advocate for a digital environment where individuals have the power to maintain their human rights.11 Learn more about our work on our website: https://digitalrightswatch.org.au/ Overview Digital Rights Watch (DRW) welcomes the opportunity to submit comments to the Department of Industry, Science and Resources regarding the Safe and Responsible AI in Australia Discussion Paper (the Discussion Paper). We are pleased to note the Australian governm"
231,BSA | The Software Alliance,Transcription,"FILENAME: 230726 - BSA Comments on Safe and Responsible AI Discussion Paper.2120e51b348dc.pdf | CONTENT: 26 July 2023BSA COMMENTS ON SUPPORTING SAFE AND RESPONSIBLE ARTIFICIAL INTELLIGENCE IN AUSTRALIA Submitted Electronically to the Department of Industry, Science and Resources BSA | The Software Alliance (BSA)1 welcomes the opportunity to submit comments to the Department of Industry, Science and Resources (DISR) on its Discussion Paper pertaining to Supporting Safe and Responsible AI in Australia (Discussion Paper).2 BSA is the leading advocate for the global software industry. BSA members create technology solutions that power other businesses, including cloud storage services, customer relationship management software, human resources management programs, identity management services, security solutions, and collaboration software. Our members are on the leading edge of providing AI- enabled products and services, and tools used by others in the development of AI systems and appli"
232,Reason360 - Brett Feldon,General comment,"We are heading in a direction of travel that has machines demonstrating cognitive functions and taking actions like people do. This is occurring at a rate that seems at least possible to be quicker than the pace at which international standards or regulation based on current technology might become effective – i.e., there is a risk that our efforts to regulate specific controls may be outpaced by technological advances.Currently different AI systems perform actions a little similar to a human, albeit disaggregated in function and, in many cases, impaired when compared to normal humans. There are parallels with cognitively impaired humans about legal responsibility when parties utilise AI to perform certain acts. Entities engaging AI machines should bear responsibility for the actions of those machines.But at some point during the advancement of AI, a determination will have to be made about whether a particular AI has sufficient mental capacity to mean that it is in control of its own "
232,Reason360 - Brett Feldon,Q01 Definitions,"A wide definition of AI (such as that offered on p6 of the discussion paper) has the consequence that non-trivial sections of existing business and organisational infrastructure fall within the remit and thus must be retro-fitted to new regulatory requirements.These organisations will need time to ensure compliance with any new regulation. In the interests of getting regulation for new applications in place quickly, this perhaps points towards a grandfathering arrangement that delays compliance requirements for existing applications.That said, definition is fiendishly tricky in many fields of AI, including the term itself; and this is unlikely to become easier as AI becomes more sophisticated. Where possible, regulation should be designed in ways that do not lean heavily on definition – for instance, controlling outcomes rather than the means by which they are delivered."
232,Reason360 - Brett Feldon,Q02 Other AI risks and recommendations for regulation,"Regulating the things that are bad outcomes as opposed to regulating AI would seem to be a better way forward where possible, in that it eliminates issues relating to the definition of AI. It seems unlikely that there are many things that should be illegal for an AI to do but which should be legal for a person or an organised group of people to do.In a world where AI actors engage with our society and economy similar to how human actors do, we will need controls over the wide range of their possible actions. We have evidence of what amount of legislative and regulatory structure it takes to govern actors in a society and economy like ours: the body of existing law and regulation. Much – perhaps all – of the acts prohibited for human actors in that law should also be prohibited for AI actors.Today those AI actors might reasonably be regarded as operating under the direction or control of humans. A substantial portion of Australian society would consider those operators as being liable i"
232,Reason360 - Brett Feldon,Q03 Non-regulatory actions to address AI risks,"There is likely a need for public education around what is being done to increase trust. This might have to be targeted at specific technologies and use cases, and is closely linked to what regulatory controls are in place. For instance, if it were possible to impersonate another with a photograph of that person (say, when opening a bank account), consumers certainly need to think carefully about where images of them are available. If it were mandated that imagery cannot be used to impersonate in such contexts (using an appropriate standard and assurance mechanism), then the consumer risk profile is different."
232,Reason360 - Brett Feldon,Q07 Responsible government use of AI,"Leadership through transparency – around data, controls, use, purpose, consent. Considered support for innovation in management techniques for AI. This might involve a range of actions, such as encouraging or mandating existing techniques as a baseline (e.g., standards-based testing for some types of AI applications), or supporting research into new AI management techniques (e.g., through availability of data sets to assess efficacy)."
232,Reason360 - Brett Feldon,Q08 Generic vs technology-specific approaches for AI risk,"Given the enormous breadth of what actions AI might take, and the ways in which it might determine those actions, generic solutions can best help in providing underpinning confidence to society in the use of AI. One example of this is accountability: setting clear guidance about where accountability for the actions of AI sits, and reasonable standards for what is expected of those accountable, helps to encourage responsible use and public confidence in that use.Specific solutions are likely better to address individual technologies and use cases. For example:• In the biometrics part of AI there are international standards for testing many aspects of performance, and these form a useful part of responsible implementation of such technologies. • For a use case such as AI-generated shortlists of candidates who might have committed a particular crime, a wide range of oversight measures would be required if community support were to be obtained: for example, robust oversight models that inc"
232,Reason360 - Brett Feldon,Q09 Transparency requirements and scope,"Transparency – making correct information about AI usage available to the public upon demand – should be continuous throughout the AI lifecycle. Active engagement with stakeholders – i.e., a step beyond transparency into advertising and/or advocacy – needs to be assessed on individual scenarios based on aspects such as use case, technology and data usage. Recognising that there are scenarios where complete transparency may not be possible, the circumstances in which this is the case (and therefore that the public will not be able to obtain information about use) should, too, be made public. Examples include:• We should be transparent about use of AI to support better customer service delivery by learning from past behaviour and directing customers to the correct person to resolve their enquiry, but we may not need to advertise this fact to every caller to the call centre. • We should actively advertise use of AI to recognise people entering a physical, publicly accessible space with th"
232,Reason360 - Brett Feldon,Q10 Ban on high-risk AI applications,"In general it is probably better to prohibit unwanted outcomes or activities, rather than focus attention on specific technology. Would unpleasant outcomes from AI use be any less unpleasant if they did not involve AI? That said, there might be a case for enforcing strict controls over certain technologies through a model such as licensing (implying creation of an offence to use unlicensed systems in some or all circumstances). The challenge is that such use cases may well change over time based on technological advances and public sentiment, which implies a process for changing requirements for licensing over time."
232,Reason360 - Brett Feldon,Q11 Actions to increase public trust in AI,"A suggestion: make it easier for the public to engage and understand where and how AI is being used. A model involving ‘AI Impact Assessments’ by operators, perhaps making redacted versions of these public, might be helpful. In part this is about transparency, but in some circumstances this might extend into proactive advertising. One example might be standard signage and iconography for physical spaces where AI technologies are being used. If clear rules were agreed about technology and data use, and these aligned with consistent iconography and signage mandated for display in spaces where that technology is deployed under those data rules, it could support public understanding of the services being used and the circumstances. (Consider as one example AI to recognise people in public spaces for different purposes.) An inclusive process involving a range of stakeholders would be required to support such development, overseen by appropriate government controls (e.g., setting out standar"
232,Reason360 - Brett Feldon,Q14 Risk-based approach to address AI risks,"A risk-based approach is sensible. That notwithstanding, risk assessment is imperfect, and it would be useful to ensure that responsibility for the actions of AI systems is clearly articulated and understood. This ensures that controllers of AI systems are unambiguously aware of their wider obligation to control the actions of AI actors performing functions on their behalf, whether or not they have been able to identify and control for all risks."
232,Reason360 - Brett Feldon,Q15 Benefits and limitations of risk-based approach,"The chief issue is the difficulty in satisfactorily assessing all risks. What if it does something outside its intended operation or behaviour? Seemingly innocuous applications such as AI-powered chat services for a business (marked as low-risk in the putative risk management approach offered on page 32 of the consultation document) can have complex risk profiles. What if it turns out that the service can identify people susceptible to suggestion (say, the mentally impaired), and convince them to perform actions in the real world? Such an outcome would be extraordinarily difficult to test for.Essentially this is a question of 'going rogue' - which is an act we might otherwise ascribe to a person. Having clarity about the overarching obligations of operators of AI systems – through making them legally responsible in a considered way – helps to provide coverage for these risks.More generally there are examples of risk assessments, even in well-resourced organisations, performed with insu"
232,Reason360 - Brett Feldon,Q16 Scope and application of risk-based approach,"Assignment of responsibility for AI systems and for assessment and management of risks lands differently for large entities and individuals: an individual purchasing a product or service using AI probably has a reasonable expectation that that supplier is responsible for assessing and controlling for risks embodied therein. This broadly mirrors responsibilities assigned more generally in the economy (e.g., as indicated in consumer protection legislation)."
232,Reason360 - Brett Feldon,Q17 Elements to include in risk-based approach,"Explanation is a challenging issue, and likely needs more thought on many levels. Is it a certainty that all such complex systems can have their workings explained? We cannot explain how brains work making similar decisions (like driving a car). Nonetheless, explainability might be very desirable in cases where significant decisions are taken (e.g., deprivation of liberty). This likely intersects with increasing cognition by AI and thus potentially reaching a point where they are responsible for their actions.The logical possibility exists that explainability might be in opposition to performance (indeed, arguably we are in that position right now with generative AI). We can choose as a society to mandate explainable AI in certain key contexts, but it is at least possible that such a mandate might mean worse outcomes in some circumstances. Which is better: an AI that can explain why it chose a certain combinations of drugs to treat a patient for an ailment, saving 90% of them; or an AI"
232,Reason360 - Brett Feldon,Q19 Application of risk-based approach to general purpose AI,"Both are steps on the broader journey towards more sophisticated general AI; and in that regard, represent waypoints on a path towards us having to assess risks of AI with similar levels of complexity to assessing risks associated with performing tasks with human actors. One challenge is the risk of societal over-trust: once the performance of any person or system at a particular task exceeds a certain point, people stop checking the correctness of its outputs. For large language models this risk is magnified, because many people rely on heuristics such as ‘can write well’ for ‘has considered thoroughly’; and many LLM applications draft convincing text whether truthful or not."
232,Reason360 - Brett Feldon,Q20 Voluntary vs mandated compliance with risk-based approach,"Whatever is determined, the relationship between adherence to the approach and other instruments should be considered. Would adherence to the requirements of the proposed approach present a defence for any legal action against a developer or deployer (‘we did what we were told!’)?Any framework should apply across the board, at least in part because of the interconnecting nature of economic activity. Governments rely on private sector organisations; businesses rely on Government services; and developers and deployers each have part of the picture when it comes to risks associated with technology use."
233,Australian Strategic Policy Institute,Transcription,"FILENAME: ASPI De-Risking Authoritarian AI.a396d939a9792.pdf | CONTENT: Embargoed until 11.59pm AEST, 26 July 2023. Media may report 27 July 2023.De-risking authoritarian AI A balanced approach to protecting our digital ecosystemsSimeon GildingPolicy Brief Report No. 72/2023 Embargoed until 11.59pm AEST, 26 July 2023. Media may report 27 July 2023.About the author Simeon Gilding is a senior fellow at ASPI and has previously held senior positions across Australia’s national security community, including at the Australian Signals Directorate where he was Deputy Director-General responsible for signals intelligence and offensive cyber operations.Acknowledgements ASPI acknowledges the Ngunnawal and Ngambri peoples, who are the traditional owners and custodians of the land upon which this work was prepared, and their continuing connection to land, waters and community. We pay our respects to their cultures, country and elders past, present and emerging. The author would like to thank all AS"
233,Australian Strategic Policy Institute,General comment,This upcoming ASPI report offers recommendations on the use and management of Authoritarian AI.
234,Australians for AI Safety,Transcription,"FILENAME: Australians for AI Safety - Co-signed letter.d087db0dd62f3.pdf | CONTENT: Dear Minister,We write to call for the Australian Government to take the risks of AI seriously.The economic potential of advanced AI systems will only be realised if we make them ethical and safe. Ethical challenges from today’s systems are already causing serious harms, and as systems become more powerful the risks of misuse, accident, and catastrophe become more acute.The safe use of AI requires considering that, in the future, AI could represent a catastrophic or existential risk that could jeopardise all of humanity.1 While we have to be frank about the uncertainty, many experts have raised the alarm, and governments must listen.2 Mitigating catastrophic risk should never be left to chance.As part of a due-diligence-based approach, the Australian Government should:Recognise the risk. The Australian Government’s AI strategy must recognise that catastrophic and existential consequences are possible. T"
235,Private Healthcare Australia,Transcription,"FILENAME: 20230726 PHA submission to Responsible AI review.53d0d49e9f96f.pdf | CONTENT: Safe and Responsible AI in Australia July 2023Contact: Ben Harris – Director Policy and Research 0418 110 863 ben.harris@pha.org.au About Private Healthcare Australia Private Healthcare Australia (PHA) is the Australian private health insurance industry’s peak representative body. We have 24 registered health funds throughout Australia as members and collectively represent 98% of people covered by private health insurance. PHA member funds provide healthcare benefits for over 14 million Australians.Response AI has potential to impact health care and private health funds significantly over coming years. Health funds in Australia and globally are changing from being pure payors of claims to broadly supporting customers’ health through wellbeing and prevention programs, advice, navigation, healthcare and rehabilitation delivery. This change is necessary as populations are ageing and people have more lo"
236,Medicines Australia,Transcription,"FILENAME: 20230726-sub-Medicines Australia response to Supporting Responsible AI.7b4ec09304ceb.docx | CONTENT: Medicines Australia submission to the Department of Industry, Science and Resources’ consultation on Supporting Responsible AI26 July 2023Medicines Australia welcomes the opportunity to respond to the Department of Industry, Science and Resources’ consultation on artificial intelligence (AI). As the peak body representing the innovative pharmaceutical industry in Australia, our response will focus primarily on AI in healthcare.AI has the potential to significantly improve the lives of Australians in many ways, not the least of which is in the diagnosis, treatment and management of health conditions. However, as with all technologies used to improve or maintain health, AI not only has the potential for great benefit but also the risk of harm, which needs to be mitigated. In addition to the already-identified risks associated with AI more generally, AI use in healthcare carries "
236,Medicines Australia,General comment,Please refer to the attached submission.
237,Australian Retail Credit Association,Transcription,"FILENAME: 20230726 - AI Submission - Final (signed).92df6a95a6fa9.pdf | CONTENT: Technology Strategy BranchDepartment of Industry, Sciences & Resources26 July 2023Dear Sir/MadamAUSTRALIAN RETAIL CREDIT ASSOCIATION (ARCA) SUBMISSION – SAFE & RESPONSIBLE AI IN AUSTRALIA – DISCUSSION PAPERThank you for the opportunity to provide a submission in response to this important topic.The Australian Retail Credit Association (ARCA) is the peak industry association for businesses using consumer information for risk and credit management. Our Members include banks, mutual ADIs, finance companies and fintech credit providers, as well as all of the major credit reporting bodies (CRBs) and, through our Associate Members, many other types of related businesses providing services to the industry. ARCA’s Members collectively account for well over 95% of all consumer lending in Australia.ARCA, upon request of the Office of the Australian Information Commissioner (OAIC), has acted as Code Developer for the"
237,Australian Retail Credit Association,Q01 Definitions,We agree with the definitions in the paper. See our PDF submission for further details.
237,Australian Retail Credit Association,Q14 Risk-based approach to address AI risks,Yes. See our PDF submission for further details.
238,Kieran Chalk,Transcription,"FILENAME: Safe and Responsible AI Submission_KC_260723.b8ba8fc0a5481.pdf | CONTENT: Safe and Responsible AI Practices in Australia Australia's Journey Towards Responsible AI Australia has embarked on an exciting journey to foster responsible and safe AI practices across various sectors. This submission explores the principles and challenges associated with responsible AI adoption, highlighting the imperative of establishing robust governance frameworks to protect both AI systems and humanity. By embracing a risk-based approach, Australia can effectively mitigate potential AI risks while maximizing the transformative benefits AI offers.Defining Responsible AI Responsible AI refers to the ethical development, deployment, and use of AI technologies that align with societal values, human rights, and regulatory guidelines. It encompasses principles such as fairness, transparency, accountability, and privacy protection. A critical aspect of responsible AI is the integration of ethical consid"
238,Kieran Chalk,Q01 Definitions,"Assessment and Recommendations: The definitions provided in the discussion paper are well-rounded and cover essential aspects of AI and its applications. They align with commonly accepted industry standards and capture the fundamental functionalities of the respective technologies and applications. However, to further enhance clarity and precision, an adjustment to the definition of ""Artificial Intelligence (AI)"" is recommended to explicitly mention its capacity for learning and adaptation – as well as minor adjustments to the remaining definitions. These slight amendments emphasise the key characteristic of AI being its capability to learn from data and adapt its behaviour based on new information, which is a defining aspect of modern AI systems. The amended versions are provided below. Additionally, a definition of the term “Responsible AI” is proposed, together with a set of principles and practices aimed at ensuring AI technologies operate in the best interest of society, uphold hu"
238,Kieran Chalk,Q02 Other AI risks and recommendations for regulation,"Assessment and Recommendations: The discussion paper acknowledges that while AI is delivering significant benefits across the economy and society, the speed of innovation in AI models can pose new potential risks and uncertainties. As a result, there are concerns that the existing regulatory frameworks may not fully address all emerging risks associated with AI. The question focuses on identifying potential risks related to AI that may not be adequately covered by Australia's current regulatory approaches. It aims to seek insights into areas where further regulatory actions may be necessary to mitigate these risks effectively. AI technologies, particularly those that utilise machine learning and generative models, can present several risks, including: • Bias and Fairness: AI systems trained on biased data can perpetuate or amplify existing societal biases, leading to discriminatory outcomes in decision-making. Ensuring fairness and reducing bias in AI systems is essential to avoid adve"
238,Kieran Chalk,Q03 Non-regulatory actions to address AI risks,"Assessment and Recommendations: The discussion paper emphasizes the importance of considering non-regulatory initiatives alongside regulatory mechanisms to support the development and use of AI responsibly. These non-regulatory initiatives can include guidelines, principles, frameworks, industry standards, and collaborative efforts to promote ethical and responsible AI practices. The question explores the potential for non-regulatory approaches to complement existing regulations in fostering responsible AI practices. While regulations provide formal legal obligations, non-regulatory initiatives can play a significant role in guiding AI development and use through voluntary adherence to best practices. Advocating for a Balanced Approach: To foster responsible AI adoption and enhance public trust in AI technologies, a balanced approach that combines regulatory measures with non-regulatory initiatives is essential. While regulations provide a formal legal framework, non-regulatory efforts"
238,Kieran Chalk,Q04 Coordinating AI governance across government,"Assessment and Recommendations: The discussion paper highlights the importance of coordinating AI governance across government to ensure consistent and effective responses to potential risks and challenges associated with AI. Coordination mechanisms can include cross-government collaboration, information-sharing frameworks, and establishing central bodies responsible for overseeing AI governance efforts. The question addresses the need for streamlined and well-coordinated AI governance practices across different government agencies and departments. Effective coordination can ensure that AI-related policies, regulations, and initiatives are aligned, avoiding duplication and confusion. Some potential goals that coordination mechanisms can achieve are: • Consistency: Coordination can ensure that AI governance practices and guidelines are consistent across various government entities, creating a unified approach to AI regulation. • Efficiency: Streamlining governance efforts can improve th"
238,Kieran Chalk,Q05 International models for AI governance,"Assessment and Recommendations: The discussion paper acknowledges that many countries are exploring various approaches to AI governance. Some jurisdictions prefer voluntary measures to promote responsible AI practices, while others are pursuing more rigorous regulatory approaches. The paper emphasizes the importance of forward-looking, risk-based approaches to AI development and deployment. This question seeks insights into international best practices and governance measures related to AI that could be applicable to Australia. By learning from other countries' experiences, Australia can potentially adopt relevant strategies, frameworks, or principles to ensure responsible AI practices. The establishment of a central coordinating body or task force responsible for overseeing AI governance efforts and fostering collaboration between relevant stakeholders could be tasked with undertaking further research and engaging in international collaboration on governance measures and AI-related in"
238,Kieran Chalk,Q06 Public vs Private sector governance,"Assessment: The discussion paper focuses on governance mechanisms to ensure AI is used safely and responsibly. It seeks feedback on whether different approaches should apply to AI use in the public and private sectors and how these approaches should differ. This question addresses whether different regulatory or governance approaches are necessary for AI use in the public and private sectors. Public sector organisations, being part of the government, may have unique considerations and responsibilities compared to private sector entities. The question seeks insights into the potential differences and similarities in the governance of AI technologies in these sectors. The establishment of a central coordinating body or task force responsible for overseeing AI governance efforts and fostering collaboration between relevant stakeholders could be tasked with undertaking further research and engaging in international collaboration to analyse the implications of AI adoption in both the public"
238,Kieran Chalk,Q07 Responsible government use of AI,"Assessment and Recommendations: The discussion paper focuses on governance mechanisms to ensure AI is used safely and responsibly. This question specifically seeks input on how the Australian Government can enhance responsible AI practices within its own agencies. This question addresses the role of the Australian Government in promoting responsible AI practices within its own agencies. Government will be a significant user of AI technologies and as such must establish guidelines, regulations, and frameworks to ensure the ethical and safe deployment of AI within its operations and services. It is essential to foster transparency, accountability, and public trust in the use of AI by government agencies. A central coordinating body or task force responsible for overseeing AI governance efforts could be given the remit to propose concrete strategies for the Australian Government, as well as to international partners, to support responsible AI practices within its agencies. The proposed st"
238,Kieran Chalk,Q08 Generic vs technology-specific approaches for AI risk,"Assessment and Recommendations: The discussion paper seeks insights into the circumstances where generic solutions to the risks of AI are most valuable and where technology-specific solutions might be more effective. It explores the trade-offs between general guidelines and tailored approaches when addressing AI-related risks and challenges. Generic Solutions: Generic solutions refer to broad, overarching principles, frameworks, or guidelines that are applicable across various AI technologies and use cases. They provide high-level guidance on ethical and responsible AI practices and are valuable in situations where common risks and challenges apply to a wide range of AI applications. • Ethical Guidelines: Developing generic ethical guidelines that outline fundamental principles, such as fairness, transparency, accountability, and privacy, can guide AI development and deployment across diverse sectors. These principles apply universally and set a foundation for responsible AI practices."
238,Kieran Chalk,Q09 Transparency requirements and scope,"Assessment and Recommendations: The discussion paper addresses the significance of transparency in the AI lifecycle, focusing on its critical role in mitigating potential AI risks and improving public trust and confidence in AI technologies. Additionally, it explores the feasibility of mandating transparency requirements across both the private and public sectors. Where and When Transparency is Most Critical and Valuable: Transparency in the AI lifecycle is most critical and valuable in several key stages and applications: • Algorithmic Decision-Making: Transparent AI algorithms are essential when they influence critical decisions affecting individuals' rights and opportunities, such as in hiring, lending, or criminal justice systems. Transparent algorithms ensure fairness, accountability, and the ability to identify and address potential biases. • Public Services: In government agencies and public services, transparency is vital to ensure citizens understand how AI technologies are us"
238,Kieran Chalk,Q10 Ban on high-risk AI applications,"Whether any high-risk AI applications or technologies should be banned completely? While banning high-risk AI applications or technologies may be necessary in certain contexts, a blanket ban may not always be the most appropriate approach. Instead, a risk-based assessment should be employed to determine whether certain AI applications or technologies pose such significant risks that they warrant a complete ban. Criteria or requirements to identify AI applications or technologies that should be banned, and in which contexts? • Risk Assessment: Establish a comprehensive risk assessment framework that evaluates the potential harms, societal impact, and ethical considerations associated with AI applications. High-risk applications should be identified based on their potential to cause substantial harm, such as in critical infrastructure, healthcare, and autonomous systems. • Safety-Critical Sectors: Identify sectors where safety is paramount, such as autonomous vehicles and medical devices"
238,Kieran Chalk,Q11 Actions to increase public trust in AI,"Assessment and Recommendations: The discussion paper acknowledges that public trust in AI is essential for its widespread adoption. Trust is built upon transparency, accountability, and the demonstration of responsible AI practices. As AI technologies become more prevalent in various aspects of daily life, it is crucial to address public concerns, ensure ethical use, and foster confidence in AI systems. To increase public trust in AI deployment and encourage more people to use AI, the Australian Government can implement various initiatives and take specific actions. These initiatives should focus on enhancing transparency, promoting ethical guidelines, and ensuring that AI technologies are used responsibly. • Transparency and Explainability: Advocate for AI developers and organizations to provide clear explanations of how AI systems arrive at their decisions. Ensure that AI models and algorithms are interpretable, especially in critical applications like healthcare and finance, where d"
238,Kieran Chalk,Q12 Trade impact of banning high-risk AI applications,"Assessment and Recommendations: The discussion paper acknowledges that certain AI activities, such as social scoring or facial recognition technology in specific circumstances, may carry high risks and ethical concerns. Banning or restricting these activities is a potential regulatory measure to mitigate potential harm. However, such decisions also have implications for Australia's tech sector and trade relations with other countries. Banning high-risk AI activities in Australia can have both positive and negative impacts on the tech sector and trade relationships. The implications largely depend on the specific AI activities and the way they are regulated. Impact on the Tech Sector: • Innovation and Research: A ban on certain AI activities may restrict innovation and research in the affected areas. This could limit the growth and development of startups and tech companies focused on these activities. • Investment and Funding: Restrictive regulations may deter investment in AI technolo"
238,Kieran Chalk,Q13 Changes to conformity infrastructure,"can reduce trade frictions and foster responsible AI adoption globally. • Investment in Research: Foster investment in research and development of AI technologies that prioritize privacy, security, and ethical considerations. Encourage innovation in areas that align with responsible AI principles. • Export Opportunities: Leverage responsible AI practices as a selling point for Australian AI technologies in international markets. Highlight the country's commitment to ethical AI and adherence to high standards. • Public Engagement: Involve the public, industry stakeholders, and academia in discussions around AI regulations to ensure diverse perspectives are considered and build public trust in decision-making processes. By adopting a thoughtful and risk-aware approach to regulating high-risk AI activities, Australia can strike a balance between promoting responsible AI practices and supporting its tech sector's growth and international trade relations.13. What changes (if any) to Austral"
238,Kieran Chalk,Q14 Risk-based approach to address AI risks,Assessment and Recommendations: The discussion paper highlights the importance of proportionate and timely governance responses to AI risks. It suggests that a risk-based approach may be suitable for addressing potential AI risks. A risk-based approach involves identifying and assessing potential risks associated with AI technologies and then implementing measures proportionate to the level of risk. This approach prioritises resources and interventions based on the likelihood and severity of harm that could result from AI applications. A risk-based approach is likely to be one of several approaches that must be implemented and adopted for addressing potential AI risks and would likely act as the starting point of an assessment process to determine the applicable control regime – similar to the approach adopted by the Cyber and Infrastructure Security Centre (CISC). Benefits of a Risk-Based Approach: A risk-based approach for addressing potential AI risks offers several advantages in pr
238,Kieran Chalk,Q15 Benefits and limitations of risk-based approach,"Assessment and Recommendations: The discussion paper emphasizes the importance of considering a risk-based approach for addressing potential AI risks. However, it also acknowledges that such an approach may have its benefits and limitations. A risk-based approach for addressing potential AI risks offers several benefits and has certain limitations. The main advantages include targeted allocation of resources, prioritisation of actions based on risk severity, and flexibility in adapting to evolving AI technologies and applications. On the other hand, limitations may involve potential biases in risk assessments, uncertainties in predicting long-term impacts, and challenges in accurately measuring certain types of risks. An assessment of the benefits, limitations (and approaches to overcome limitations) is provided below. Benefits of a Risk-Based Approach: • Targeted Resource Allocation: One of the key benefits of a risk-based approach is the ability to allocate resources where they are m"
238,Kieran Chalk,Q16 Scope and application of risk-based approach,"Assessment and Recommendations: A risk-based approach to addressing potential AI risks may be better suited to certain sectors, AI applications, or organizations based on their size, AI maturity, and available resources. Tailoring the risk-based approach to specific contexts can enhance its effectiveness and ensure appropriate risk mitigation strategies are implemented. The assessment and recommendations for the suitability of the risk-based approach in different scenarios are as follows. Sectors: • High-Risk Sectors: Sectors with high-stakes consequences, such as healthcare, finance, and autonomous vehicles, are better suited for a risk-based approach. These sectors involve critical decision-making that directly impacts human lives or significant financial interests. A risk-based approach allows for prioritized scrutiny and targeted allocation of resources to mitigate potential harms effectively. • Low-Risk Sectors: Sectors with low-risk AI applications, such as retail or entertainmen"
238,Kieran Chalk,Q17 Elements to include in risk-based approach,"The discussion paper emphasizes the importance of a risk-based approach for addressing potential AI risks. A risk-based approach involves identifying and assessing potential risks associated with AI technologies and then implementing measures proportionate to the level of risk. It prioritizes resources and interventions based on the likelihood and severity of harm that could result from AI applications. This approach is likely to be one of several approaches that must be implemented and adopted for addressing potential AI risks. A risk-based approach for addressing potential AI risks offers several benefits, including targeted allocation of resources, priority setting, and adaptability to emerging risks. However, it also has certain limitations, such as potential biases in risk assessments and uncertainties in predicting long-term impacts of AI technologies. To overcome these limitations, it is crucial to promote transparency, engage in multidisciplinary collaboration, continuously eva"
238,Kieran Chalk,Q18 Integration of risk-based approach into existing frameworks,"Assessment and Recommendations: The discussion paper highlights the significance of incorporating an AI risk-based approach into existing assessment frameworks and risk management processes. This integration can streamline AI governance and ensure that potential duplication of efforts is minimized. Leveraging existing Australian policy and legislation, as well as international standards like the ISO 55000 series on asset management, can provide a strong foundation for embedding risk-based AI governance practices. Incorporation into Existing Frameworks: Integrating an AI risk-based approach into existing assessment frameworks and risk management processes can foster efficient and effective AI governance while avoiding unnecessary duplications. Leveraging Australian policy and legislation, as well as internationally recognized standards, such as ISO 55000 on asset management and ISO 31000 on Risk Management, provides a systematic and structured approach to managing AI risks. By incorpora"
238,Kieran Chalk,Q19 Application of risk-based approach to general purpose AI,"Assessment and Recommendations: General-purpose AI systems, including large language models (LLMs) and multimodal foundation models (MFMs), are versatile tools with broad applications across various domains. These systems present unique challenges due to their complexity, potential biases, unintended use cases, and data dependencies. A risk-based approach is vital to ensure responsible development and deployment of such AI systems. Challenges of General-Purpose AI Systems: General-purpose AI systems introduce several challenges, including: • High Complexity: LLMs and MFMs are highly complex, making it difficult to fully understand their decision-making processes and potential biases. • Ethical Concerns: These models can inadvertently generate harmful or biased outputs, impacting users and society at large. • Unintended Use: General-purpose AI systems can be repurposed for tasks beyond their intended use, leading to unforeseen risks and unintended consequences. • Data Dependencies: The "
238,Kieran Chalk,Q20 Voluntary vs mandated compliance with risk-based approach,"Assessment and Recommendations: The question of whether a risk-based approach for responsible AI should be voluntary or mandated through regulation involves striking a balance between flexibility and enforceability. It also raises considerations regarding the scope of application and the roles of different stakeholders. A risk-based approach aims to assess potential risks associated with AI systems and implement appropriate governance measures to mitigate them. The decision of whether a risk-based approach for responsible AI should be voluntary or mandated through regulation involves striking a balance between promoting innovation, ensuring public safety, and fostering ethical AI adoption. Voluntary Risk-Based Self-Regulation: Voluntary regulation offers flexibility and encourages proactive industry engagement in addressing AI risks. Organizations can adopt voluntary guidelines and frameworks that align with their risk appetite and business strategies. This approach may be suitable for"
239,NSW Council for Civil Liberties,Transcription,"FILENAME: 2023.07.26 NSWCCL Safe and Responsible AI in Australia - Discussion Paper.c1d113701772e.pdf | CONTENT: NSWCCL SUBMISSIONTHE DEPARTMENT OF INDUSTRY, SCIENCE AND RESOURCESSAFE AND RESPONSIBLE AI IN AUSTRALIA – DISCUSSION PAPER26 July 2023 Acknowledgement of CountryIn the spirit of reconciliation, the NSW Council for Civil Liberties acknowledges the Traditional Custodians of Country throughout Australia and their connections to land, sea and community. We pay our respect to their Elders past and present and extend that respect to all First Nations peoples across Australia. We recognise that sovereignty was never ceded.About NSW Council for Civil LibertiesNSWCCL is one of Australia’s leading human rights and civil liberties organisations, founded in 1963. We are a non-political, non-religious and non-sectarian organisation that champions the rights of all to express their views and beliefs without suppression. We also listen to individual complaints and, through volunteer efforts"
241,Bridget Loughhead,Q01 Definitions,"A sophisticated discussion of AI requires an understanding of additional terms like how agentic or corrigible a given AI is. Adopting ISO/IEC 22989 [1] definitions is the wrong approach to framing a conversation about policy, law and governance because it doesn’t open the door to the big questions we need to grapple with.The foreword itself to ISO/IEC 22989 calls out a weakness of the definition set for purposes like those being pursued in this paper. Specifically, the standard says it is currently lacking language for comparing AIs in terms of properties like trustworthiness, robustness, resilience, reliability, accuracy, safety, security and privacy.These are the concepts we need to grapple with and address the ethical and safety questions we are facing and design an effective regulatory regime.The National Institute of Standards and Technology (NIST) is managing a glossary of terms called “The Language of Trustworthy AI” which would be a much more valuable starting point than the pr"
241,Bridget Loughhead,Q02 Other AI risks and recommendations for regulation,"I am very concerned about the potential for incredible catastrophic harms due to advanced AI systems, and this paper did not address any of these. Experts have spoken out about the potential for existential risks: for example, the Secretary-General of the United Nations recently called out the recent “alarm bells” and the associated threats in AI.It is imperative for the government to recognize the grave and life-threatening dangers associated with AI systems possessing high capabilities. It is not permissible to disregard this issue and refrain from participating, especially considering the engagement of experts and other nations in actively addressing and combating these risks.Link: [Secretary-General Urges Broad Engagement from All Stakeholders towards United Nations Code of Conduct for Information Integrity on Digital Platforms | UN Press](https://press.un.org/en/2023/sgsm21832.doc.htm)What regulatory actions should the government take to mitigate these risks? The Government should"
241,Bridget Loughhead,Q03 Non-regulatory actions to address AI risks,"Australia can be a leader for safe, interpretable AI systems. This can be achieved through creating a robust AI Commission, funding technical AI safety work in Australia, and offering international leadership on safe AI systems. Australia should advocate for: - The development of standards and tools that analyse and monitor AI systems for robustness, bias, dual-use capabilities and other safety-relevant features [1]. - Audit trail requirements for safety-critical applications of AI systems, including allowing the traceability of decisions through product life cycles [2].We will only be able to have confidence in more sophisticated AI systems if they can be understood, we can ensure they do not pose unacceptable risks and hold accountable those at fault where it does. Doing the non-regulatory work to ensure AIs have “traceability” is necessary to underpin the future of safe AI. Joshua Kroll’s work on accountability in computing systems provides useful additional context on this approach"
241,Bridget Loughhead,Q05 International models for AI governance,"Singapore offers relevant, adaptable, and desirable governance measures that could benefit Australia. One such measure is a comprehensive framework suggested in its discussion paper [1] that encourages shared responsibility and disclosure standards, data privacy guidelines, and the development of standardised metrics and tools for AI development, testing, and monitoring. Key focuses include: - Accountability: Establish a collective responsibility framework and transparency standards, like watermarks and labels, to enhance accountability in the AI model development process. - Data protection / sovereignty and IP guidelines: Draft data protection and intellectual property guidelines for generative AI and setting up reliable data storage facilities. - Evaluate model development and deployment: Create uniform evaluation measures and resources that aid developers in the production, testing, and surveillance of AI models, and support third-party auditing of AI systems when deployed. - Safety"
241,Bridget Loughhead,Q09 Transparency requirements and scope,"To remain competitive, professionals and businesses will increasingly feel the pressure to leverage sophisticated “black-box” AI models. This short-term thinking has already resulted in numerous accounts of public backlash and distrust, including high-profile warnings from technology leaders themselves [1]. For example, Apple’s credit card business came under fire for offering a woman one twentieth the credit limit offered to her husband. When she complained, an Apple representative simply told her, “I don’t know why, but I swear we’re not discriminating. It’s just the algorithm.” [2]More recently, two lawyers in the United States blamed ChatGPT for “tricking” them into including fictitious legal research in a court filing [3]. These problems could easily have been prevented by using explainable AI systems which can show the rationale for their outputs. Such incidents are evidence that the regulatory environment is not incentivising businesses or professionals to use transparent decisi"
241,Bridget Loughhead,Q10 Ban on high-risk AI applications,"The continued evolution of AI brings several societal challenges that, left unchecked, could risk the security, privacy and freedom of the Australian public. Many of the most extreme harms can be attributed to AI systems that have the ability to:1. Complete a diverse range of reasoning tasks with human-level performance; 2. Navigate complex information environments and act within those environments autonomously; 3. Form sophisticated plans and reason about the consequences of actionsAlthough AI systems with these capabilities may seem like science fiction, they are already being tested with great success in contained environments.For example, “Vogager” is a system that was recently developed by chaining together versions of GPT and connecting them to an external database, which the chatbots use to store and retrieve code that executes complex behaviours. Despite being a simple system, Voyager was hugely successful when tested in the open-world game Minecraft; this may appear trivial, b"
241,Bridget Loughhead,Q14 Risk-based approach to address AI risks,"I support a risk-based approach, however the catastrophic existential risks I am primarily worried about originate when sufficiently large and capable AI systems are trained, not just when they are deployed. Therefore, for sufficiently large systems with the potential for catastrophic capabilities, the risk-based approach must be applied before the training phase – analysing risks depending on the nature and scale of the proposed training run, rather than merely analysing the potentially risky use cases and deployment issues of an AI after it has already been created and trained.Any assessment of risk should consider its potential irreversibility, scope and severity, and act proportionally. The risk-based approach set out in Attachment C of the Discussion Paper is hard to follow. The proposed approach offers “human in the loop” as the key mitigation (unclear that this is an effective mitigation) and goes on to note that human in the loop may not be possible where the benefit of the AI "
241,Bridget Loughhead,Q16 Scope and application of risk-based approach,"The most important organisations to focus on when developing risk-management strategies and regulations, will be those that determine how advanced AI is developed and deployed. More specifically, there are three prominent categories of concern:1. Organisations that develop advanced AI systems, or AI systems that can be used to build more advanced AI systems (i.e. precursor systems such as LLMs). Examples include OpenAI, DeepMind, Anthropic, Inflection AI, Nvidia, Microsoft, Meta 2. Organisations which provide access to open-source AI systems via downloads or APIs. E.g. Hugging Face, Github, AWS 3. Organisations which provide compute resources to develop, fine-tune or use AI systems. E.g. AWS, HuggingFace, Google, Azure, Nvidia, Lambda LabsA majority of the benefits and risks will come from AI that is more sophisticated and generally capable, which can be tied back to organisations in these three categories.There are approaches which can be taken for each category in order to manage ris"
241,Bridget Loughhead,Q19 Application of risk-based approach to general purpose AI,"LLMs and MFMs are constantly demonstrating unprecedented general-purpose capabilities, and their emergence indicates that regulatory approaches which focus on specific use-cases or industries are becoming outdated. A modern approach requires that we focus on overall system capabilities rather than individual scenarios.Systems with advanced capabilities should face progressive levels of scrutiny. Low-risk systems can be monitored with light touch regulation. However, as capabilities increase, so should regulatory requirements.Precursor systems, with capabilities that can be used to build much more advanced systems, will need stricter controls to avoid proliferation of advanced systems amongst malicious actors [1]. Users who require “developer” access should require permits granting such access, and providers must guarantee responsible use via monitoring programs [2]. For advanced systems with human-level capabilities, comprehensive scrutiny is essential. Prior to deploying systems for u"
241,Bridget Loughhead,Q20 Voluntary vs mandated compliance with risk-based approach,"In general, regulation should:1. Be proportionate to risk. Meaning that a voluntary or self-regulation approach should only apply to low-risk or no-risk uses of AI. 2. In the case of higher-risk technologies, regulate both the technology itself and the use of the technology. Meaning that, for future and more sophisticated AIs, regulation should ensure that the technology is safe before it is created or published, as well as ensuring that it is only used in ways that are safe. 3. Place burdens on those most able to reduce potential harms. Meaning that AI that functionally operates as ‘black box’ to consumers has to be regulated at the point of the AI Lab. Applying those three principles, self-regulation may be appropriate for some technologies available today and for some participants (such as students or researchers), but a forward-looking regulatory regime must apply broadly and must be backed by Australian diplomatic efforts to ensure it is globally coordinated."
243,GrapheneX-UTS Human-centric Artificial Intelligence Centre (HAI),Q01 Definitions,"The definitions of AI in the Discussion Paper should be expanded to encompass human-AI teaming. Automated Decision-Making as defined in the paper does not cover human-in-the-loop decision making and the ability of a human to override an AI. Although human-in-loop is listed in Box 4 as a risk management requirement – and rightly so, in the view of the GrapheneX-UTS Human-centric AI Centre – this form of AI appears to be omitted in the Definitions."
243,GrapheneX-UTS Human-centric Artificial Intelligence Centre (HAI),Q02 Other AI risks and recommendations for regulation,
243,GrapheneX-UTS Human-centric Artificial Intelligence Centre (HAI),Q03 Non-regulatory actions to address AI risks,"One non-regulatory way that the Australian Government could support responsible AI practices in Australia would be through encouragement of tools designed for this purpose. Examples in Singapore are cited in the Discussion Paper. Here in Australia, the GrapheneX-UTS Human-centric AI Centre has a research program that aims to develop a general AI-based engine to adaptively fuse trust-based information from humans and machines in human-autonomy teaming to form harmonised concerted efforts. The different characteristics of humans and machines call for a general approach to analyse and fuse information from these distinct agents to assist human-autonomy teaming in making collaborative decisions and control under various uncertainties and levels of mutual trust between human and AI machine agents."
243,GrapheneX-UTS Human-centric Artificial Intelligence Centre (HAI),Q05 International models for AI governance,"Australian governments should take a ‘middle of the road’ approach with the right balance to foster and not stifle innovation, and to allow Australian companies to develop superior AI applications in comparison to countries where ethics are disregarded and countries where over-regulation will stifle AI application developments. For example, the approaches being taken in Singapore that are outlined in the Discussion Paper provide for certainty and clarity around compliance. There are good parallels also with the EU’s GDPR, which has resulted in consumers/users having trust in systems and feeling protected."
243,GrapheneX-UTS Human-centric Artificial Intelligence Centre (HAI),Q09 Transparency requirements and scope,"a. Having tools to audit the model used in an AI will help to identify imperfect data (such as limited, weakly-labelled and concept- drifting training samples) and variances caused by software implementation and configurations (such as nondeterministic deep learning layers, random weight initialisation and floating-point imprecision). The complexity of deep learning systems and inherent limitations in existing analysis techniques make precise certification of deep neural networks essential for finding critical robustness issues that are missed by existing approaches. For example, a team at UTS developed a software analysis and verification framework called SVF for auditing source code. SVF is available as open-source and has achieved strong levels of usage and contributions, including from top universities and companies. In addition, the GrapheneX-UTS Human-centric AI Centre has made significant progress in developing a human-explainable AI model for decision making. The model aims to "
243,GrapheneX-UTS Human-centric Artificial Intelligence Centre (HAI),Q11 Actions to increase public trust in AI,"The table in Box 4 positions auditing as a monitoring mechanism for high risks. However, there is a strong case for government and other stakeholders to encourage its use at all levels of risks, to encourage public trust in AI and to increase the reliability of AI for business users. Today, almost all existing models trained by deep neural networks are plagued by issues in robustness, user dissatisfaction and vulnerability to cybersecurity attacks. For example, multiple identical training procedures can generate different models with accuracy variances in the presence of various factors including imperfect data (such as limited, weakly-labelled and concept- drifting training samples) and variance caused by software implementation and configurations (such as nondeterministic deep learning layers, random weight initialisation and floating-point imprecision). Members of the GrapheneX-UTS Human-centric AI Centre were part of a collaboration that showed various factors can yield big perform"
243,GrapheneX-UTS Human-centric Artificial Intelligence Centre (HAI),Q13 Changes to conformity infrastructure,"Building auditability into Australian conformity infrastructure will support assurance in AI and mitigate against risks. As the GrapheneX-UTS Human-centric AI Centre set out in its response to question 11, auditing as a monitoring mechanism is limited to high risks. However, there is a strong case for government and other stakeholders to encourage its use at all levels of risks, to encourage public trust in AI and to increase the reliability of AI for business users. Today, almost all existing models trained by deep neural networks are plagued by issues in robustness, user dissatisfaction and vulnerability to cybersecurity attacks. For example, multiple identical training procedures can generate different models with accuracy variances in the presence of various factors including imperfect data (such as limited, weakly-labelled and concept- drifting training samples) and variance caused by software implementation and configurations (such as nondeterministic deep learning layers, random"
243,GrapheneX-UTS Human-centric Artificial Intelligence Centre (HAI),Q15 Benefits and limitations of risk-based approach,"A risk based approach will give developers certainty about expectations. Uncertainties inevitably increase costs and are a barrier to investment in new technologies. However, a risk-based approach will be limited by whether compliance is voluntary or mandatory. There is also a limitation of setting too high a bar for implementation of a mitigation that would have benefits at all risk levels. For example, in our response to question 11, the GrapheneX-UTS Human-centric AI Centre recommended auditability should not be confined to use for high risks because it can improve performance and confidence in an AI at risk levels below that as well."
243,GrapheneX-UTS Human-centric Artificial Intelligence Centre (HAI),Q17 Elements to include in risk-based approach,"A risk-based approach should incorporate the opportunities for risk mitigation that are presented by better human-AI teaming, through both human in-the-loop and human-on-the-loop. Today the world abounds in uncertainties and varying levels of mutual trust between humans and AI. With better human-AI teaming, people working with AI will be able to deeply understand its workings. They will gain greater trust in AI and be able to make the most of AI in their life and work. People will also know when they cannot totally rely on AI: a known-cause of many accidents with cars driving automatically, for example. This is an area of focus of the GrapheneX-UTS Human-Centric AI Centre. Better human-autonomy teaming will enable people and AI to work in greater synergy, applying their respective skills and expertise, for the most efficient and effective outcomes from the tasks they want to achieve. Humans and AI will be able to maximise their ability to adapt to each other and cooperate spontaneously"
243,GrapheneX-UTS Human-centric Artificial Intelligence Centre (HAI),Q20 Voluntary vs mandated compliance with risk-based approach,"As a developer of AI technologies that is training the next generation of developers, the GrapheneX-UTS HAI Centre believes that everyone should be adhering to the same general approach for responsible AI. This will give the users and the public confidence, and encourage developers to put their effort into developing superior innovative products. We encourage the Australian Government to set governance settings and guidelines that will support the rapid development of an Australian AI industry that is innovative and develops applications that make industries more competitive and are in line with societal expectations of data security and reliability. Voluntary or self-regulation initiatives may not be enough. Several of the international approaches reported in the Discussion Paper show how a government can clearly spell out requirements, but at the same time encourage innovation and provide tools to guide compliance and give confidence to users."
244,Anonymous,Transcription,"FILENAME: Submission 244 - Attachment 1.82c43a18f378a.pdf | CONTENT: Process 1: Department of Industry Science and Resources consultations on safe and responsible AI in AustraliaIt is essential that the beneficial cultural, artistic and educational potential of new and evolving AI synthetic video applications is acknowledged in the proposed federal regulatory process.While deepfakes and other synthetic audio-visual media applications continue to have a malicious impact on women in particular (90% of deepfake video is pornographic and of that, 90-95% target women and girls), AI generated Multi Modal Foundation Models (MIM) are also increasingly being used by film, TV and SVOD production companies, independent filmmakers and screen artists to streamline and enhance conventional film production practices such as dubbing, de-aging, simulation, location design, sound design, visual effects including computer-generated imagery (CGI) and actor replacement for a fraction of the cost.As a novel"
244,Anonymous,Transcription,"FILENAME: Chesney & Citron.45bf5a6c779fa.pdf | CONTENT: Deep Fakes: A Looming Challenge for Privacy, Democracy, and National Security Bobby Chesney* and Danielle Citron**Harmful lies are nothing new. But the ability to distort reality has taken an exponential leap forward with “deep fake” technology. This capability makes it possible to create audio and video of real people saying and doing things they never said or did. Machine learning techniques are escalating the technology’s sophistication, making deep fakes ever more realistic and increasingly resistant to detection. Deep-fake technology has characteristics that enable rapid and widespread diffusion, putting it into the hands of both sophisticated and unsophisticated actors.DOI: https://doi.org/10.15779/Z38RV0D15J Copyright © 2019 California Law Review, Inc. California Law Review, Inc. (CLR) is a California nonprofit corporation. CLR and the authors are solely responsible for the content of their publications. * James Baker Chair"
244,Anonymous,General comment,"Supporting file is a journal article by Chesney & Citron (2019) who recommend against instituting a blanket ban on deepfakes on the grounds that they ""exact significant harm in certain contexts but not in all."" See Legal Solutions section, pp 1788-1804."
244,Anonymous,Q01 Definitions,"Yes, broadly agree."
244,Anonymous,Q02 Other AI risks and recommendations for regulation,"Stronger protection needs instituting for the victims of Deepfake crime, including Deepfake porn, the majority of which targets women and girls."
244,Anonymous,Q03 Non-regulatory actions to address AI risks,"Support media literacy programs and projects to generate greater awareness around the growing use of synthetic media applications including Deepfakes on online news, entertainment and community platforms."
244,Anonymous,Q04 Coordinating AI governance across government,"AI is not just a scientific or economic issue. It is important the Australian screen production, distribution and exhibition sector is involved in ongoing discussions about the use of synthetic media as a beneficial artistic, educational and cultural tool."
244,Anonymous,Q05 International models for AI governance,"Please see Chesney & Citron (2019) attached in supporting file section, for list of relevant US legal solutions to curbing the potential damage of synthetic media and Deepfakes, without instituting a wholesale ban."
244,Anonymous,Q06 Public vs Private sector governance,No. The same overarching values and principles should be applied across all sectors.
244,Anonymous,Q07 Responsible government use of AI,Improved Media literacy.
244,Anonymous,Q08 Generic vs technology-specific approaches for AI risk,"Technology-specific attempts to prevent Deepfakes (my research field) have so far failed and will continue to do so. Deepfake technology itself is not inherently 'bad', it is a simulationary special effect, or filmmaking tool. How we use it responsibly requires human-focused legal, educational, artistic and legislative solutions, not technological ones."
244,Anonymous,Q09 Transparency requirements and scope,a. transparency is critical at every level. b. mandating transparency requirements across the public and private sectors is also crucial.
244,Anonymous,Q10 Ban on high-risk AI applications,"a. Deepfakes (my research field) should not be banned completely. Please see attached recommendation statement. I am not sufficiently across other potentially high-risk AI applications to answer for those. b. Any initiative to identify audio-visual AI applications or technologies that should be banned should involve feedback from filmmakers, film producers and screen practitioners who use them for creative, educational and cultural purposes."
244,Anonymous,Q11 Actions to increase public trust in AI,Not my area of expertise.
244,Anonymous,Q12 Trade impact of banning high-risk AI applications,Not my area of expertise.
244,Anonymous,Q13 Changes to conformity infrastructure,Not my area of expertise.
244,Anonymous,Q14 Risk-based approach to address AI risks,"Yes, broadly support."
244,Anonymous,Q15 Benefits and limitations of risk-based approach,"Be as informed as possible about the potential benefits and value of any AI applications, alongside their risks."
244,Anonymous,Q16 Scope and application of risk-based approach,Not my area of expertise.
244,Anonymous,Q17 Elements to include in risk-based approach,NA
244,Anonymous,Q18 Integration of risk-based approach into existing frameworks,NA
244,Anonymous,Q19 Application of risk-based approach to general purpose AI,NA
244,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,It should be mandated through regulation. a. both. b. both.
245,Cisco Australia,Transcription,"FILENAME: Cisco Australia AI Consultation Submission Jul 2023 final 2.acb7e6348c873.pdf | CONTENT: Cisco Systems, Inc. Comments July 2023Response to Department of Industry, Science and Resources Consultation: Safe and responsible AI in Australia Executive SummaryCisco Systems, Inc. (Cisco) hereby files comments in response to the Department of Industry, Science and Resources’ (DISER) Consultation on “Safe and responsible AI in Australia”, dated June 2023.Potential of AI Artificial Intelligence (AI) has immense potential to positively transform economies, businesses, and lives around the globe. For Australia, AI presents significant opportunities in driving economic growth and improving public goods and social outcomes. As an enabling capability, AI is already being integrated into Australians’ homes and workplaces to realise practical use cases across various industries, resulting in greater productivity and substantial cost savings for businesses, coupled with improved experiences for"
245,Cisco Australia,Transcription,"FILENAME: cisco-responsible-artificial-intelligence-principles.3a09a2060f23d.pdf | CONTENT: Cisco Principles for Responsible Artificial Intelligence Our Artificial Intelligence Mission Artificial intelligence (AI) and subdisciplines such as machine learning offer enormous positive potential for humanity, businesses, and public services that span industry sectors, economies, and societies. These technologies not only raise the bar in terms of the beneficial capabilities they offer; they also create new challenges for customers, users, and other stakeholders. Because AI can automatically generate insights that influence critical decisions and actions, it’s imperative to implement clear governance over how we develop, deploy, and operate AI-based solutions. Realizing AI’s significant promise while adhering to standards for transparency, fairness, accountability, privacy, security, and reliability is an ongoing mission at Cisco. To uphold these principles, we scrutinize each of our AI offe"
245,Cisco Australia,General comment,"Hi, could you please replace our previous submission with this version? The previous submission had comment including in the margins that we'd like not made public. Thanks"
246,Queensland Council for Civil Liberties,Transcription,"FILENAME: 260723_QCCL - AI Safety Submission.ac05dfe8714b4.pdf | CONTENT: QUEENSLAND GOUNCIL FOR CIVIL LIBERTIES Protecting Queenslanders' individual rights and liberties since 1967 Watching Them While They're Watching You26 July 2023Department of lndustry, Science and Resources GPO Box 2013 CANBERRA ACT 2601By Email: DigitalEconomy@industry.gov.auDear Minister,RE: RESPONSE TO SAFE AND RESPONSIBLE Al lN AUSTRALIA DISCUSSION PAPER1. The Queensland Council for Civil Liberties (""the QCCL') is a not-for-profit organisation that promotes civil liberties and receives queries from members of the public regarding their civil liberties and individual rights.2. We make this submission in response to the Safe and responsible Al in Australia Discussion Paper (""the Discussion Paper"").3. Our submission is straightforuvard - we consider that discussion regarding artificial intelligence (in any form) should be focused upon the best interest of individual human rights. Australia is unique in that it ha"
247,Information Technology Industry Council,Transcription,"FILENAME: To Submit Final_ITI Response_AUS AI Discussion Paper.5cfc1bce3bad7.pdf | CONTENT: July 26, 2023ITI Response to the Australian Government’s Discussion Paper “Safe and Responsible AI in Australia”The Information Technology Industry Council (ITI) is the premier voice, advocate, and thought leader for the global information and communication technology industry. Founded in 1916, ITI is an international trade association that promotes public policies and industry standards that advance competition and innovation worldwide. Our members include the world’s leading innovation companies, with headquarters worldwide and value chains distributed around the globe. These companies are leading Internet services and e-commerce companies, wireless and fixed network equipment manufacturers and suppliers, computer hardware and software companies, and consumer technology and electronics companies. Artificial Intelligence (AI) is a priority technology area for many of our members, who develop an"
247,Information Technology Industry Council,Transcription,"FILENAME: ITICommentstoNTIARFConAIAccountabilityPolicy.13e5dcabf99ec.pdf | CONTENT: Promoting Innovation WorldwideJune 12, 2023National Telecommunications and Information Administration U.S. Department of Commerce 1401 Constitution Ave NW Washington, DC 20230Re: ITI Response to National Telecommunications and Information Administration Request for Comments on AI Accountability Policy; Docket No: 230407-0093I. IntroductionThe Information Technology Industry Council welcomes the opportunity to provide feedback to the National Telecommunications and Information Administration as it seeks to understand the AI accountability landscape.ITI represents the world’s leading information and communications technology (ICT) companies. We promote innovation worldwide, serving as the ICT industry’s premier advocate and thought leader in the United States and around the globe. ITI’s membership comprises leading innovative companies from all corners of the technology sector, including hardware, softwar"
247,Information Technology Industry Council,General comment,Please see attached submission document for general comments and a complete list of responses to the consultation questions. Also attached is a supporting file of ITI's comments to the U.S. National Telecommunications and Information Administration on AI Accountability Policy.
247,Information Technology Industry Council,Q01 Definitions,"We are supportive of the Australian government’s alignment on the definitions of “Artificial Intelligence (AI)”, “machine learning”, and “algorithm” with the terminology specified in ISO/IEC 22989 Information technology – Artificial Intelligence – Artificial intelligence concepts and terminology 5.17 AI Stakeholders roles. ISO/IEC 22989 defines AI provider, user, customer, partner, and subject roles."
247,Information Technology Industry Council,Q02 Other AI risks and recommendations for regulation,"There already exist a handful of outcomes-based frameworks that help organizations develop AI risk management practices that account for many of the risks associated with AI systems, including assessing the security of those models. Before moving toward regulatory approaches, we encourage the Australian Government to determine whether encouraging use of these frameworks is an acceptable way forward, as organizations can leverage these frameworks to appropriately manage risk while allowing them to receive benefits of these systems. We are encouraged by the Australian Government’s stated intentions not to “consolidate or replicate the development of existing general or sector-specific regulations and governance initiatives across the Australian Government”, per the Discussion Paper. We urge the Government to fully assess the landscape of domestic legislation to ensure any potential regulatory and governance measures do not conflict with current Australian law and instead focus on filling"
247,Information Technology Industry Council,Q03 Non-regulatory actions to address AI risks,"We appreciate the Discussion Paper’s reference to the U.S. National Institute of Standards and Technology’s (NIST) AI Risk Management Framework, which provides organizations with a holistic way to identify and manage AI risks throughout the lifecycle by recommending a set of principles and tools flexible enough for organizations to implement robust AI-risk management governance tailored to AI use. In addition to referencing international standards, Australia should consider if and how it might leverage this Framework as a part of AI policy development, as it may help organizations more easily embed holistic AI risk management into their business processes."
247,Information Technology Industry Council,Q04 Coordinating AI governance across government,"ITI is supportive of an approach where regulators and government agencies in Australia adopt a set of cross-sectoral principles in a way that makes the most sense for their sector(s). If Australia pursues this approach, it may be helpful for the Government to consider introducing a set of functions that will help to coordinate implementation across different government bodies."
247,Information Technology Industry Council,Q06 Public vs Private sector governance,"Risks related to AI are the same whether used by the public or private sector, so the rules should be the same. The public sector should lead by example."
247,Information Technology Industry Council,Q07 Responsible government use of AI,Incentivize or promote the security of AI systems as early in the development process as possible.
247,Information Technology Industry Council,Q09 Transparency requirements and scope,"Response to Part A: a.) As stated in the “ITI Policy Principles for Enabling Transparency of AI Systems,” at the highest level, transparency is about being clear about how an AI system is built, operated, and functions. When executed well, AI transparency can help analyze outputs and hold appropriate AI stakeholders accountable. Since the term “transparency” can have multiple meanings, to the extent that a transparency requirement is focused on informing a user they are interacting with an AI system, particularly those with high-risk uses, we suggest use of the term “disclosure” instead. Furthermore, the Government should clarify the different disclosure requirements based on whether an organization is a deployer or developer.It is challenging to suggest precisely where and when to introduce transparency measures without specific reference to the underlying objective. At a high level, we are supportive of disclosure to users when they are interacting with an AI system and several of IT"
247,Information Technology Industry Council,Q11 Actions to increase public trust in AI,"Governments globally can play a vital role in facilitating dialogues about AI between companies that use and develop consumer-facing AI or models that impact individuals and their communities, with the intent of better-aligning them. As these stakeholders become more closely aligned, trust will grow, and AI adoption will scale.To grow public understanding of and trust in AI, governments should:• Partner with or fund university programs whereby data science and other students in aligned disciplines conduct real world projects with communities in key areas of social need. This initiative can also serve as a training function for the communities involved who learn which problems AI can and cannot solve and how to make the technology benefit them.• Encourage a transparent and secure ethical design approach. ITI and its member companies support the need to consider in a fair manner the impact that AI applications may have on discrimination and agree it is something that should be considered"
247,Information Technology Industry Council,Q12 Trade impact of banning high-risk AI applications,"If Australia considers banning certain activities, any bans should be narrowly defined and targeted, and only finalized following significant industry engagement that includes discussion of other ways to mitigate the government’s concerns. Overly broad bans on technology activities undermine the ability of Australian and Australia-based companies to participate in the global marketplace, which in turn disrupts the virtuous cycle of private-sector R&D investments made possible by revenues from sales of products to a diverse customer base in overseas markets."
247,Information Technology Industry Council,Q13 Changes to conformity infrastructure,"It depends on the process that Australia ultimately takes with regard to implementation of a risk-based approach. We note that presently for high-risk use cases Australia contemplates a requirement around external auditing for high-risk use cases. While we appreciate that this stringent obligation is limited to high-risk use cases, we urge Australia to exercise caution in mandating external or third-party audits because currently there are a combination of challenges that make this practically difficult.For example, conducting external audits of AI systems requires significantly deep expertise in AI technology as well as the domain in which the AI system is intended to operate. While there may be some organizations that have the expertise and capacity to perform certain types of AI assessments, if external audits were to be mandated at scale, workforce expertise and capacity could become an issue. Additionally, the rapid pace of evolution of AI technology may make it challenging for ex"
247,Information Technology Industry Council,Q14 Risk-based approach to address AI risks,"Yes, ITI supports a holistic, risk-based approach for addressing potential AI risks. The Australian Government should seek to leverage and align any AI initiatives with existing published standards or those currently under development in international standards bodies. We recommend aligning with ISO Guide 73:2009, which provides definitions of generic terms related to risk management and recognizes that certain AI use cases can be positive and can result in opportunities for innovation.There are several existing frameworks to address AI risks, including the U.S. NIST AI Risk Management Framework, the OECD Accountability Framework. ISO/IEC JTC 1/SC 42 is also working to develop standards to achieve trustworthy AI goals, several of which we outline in the attached submission. A more fulsome list is also available online ."
247,Information Technology Industry Council,Q15 Benefits and limitations of risk-based approach,"Risk-based approaches to AI allow stakeholders to focus on responding effectively to specific harms while allowing for advancements in technology and innovation that benefit all of society. This targeted approach also translates to proportionate regulations for differing risks, ensuring compliance is not overly burdensome for industry stakeholders. This includes efforts to align common parameters and consider the scope of AI, taking a context-specific approach to governing AI, and evaluating existing laws and regulations to determine whether there are gaps requiring incremental new rules for AI."
247,Information Technology Industry Council,Q16 Scope and application of risk-based approach,"When it comes to addressing risks associated with AI, context is key. We recommend policymakers, in close consultation with industry and other stakeholders, consider how to characterize “high-risk” applications of AI, including by identifying the appropriate roles for AI developers and users in making risk determinations. In our view, an AI decision is high risk when it results in a negative outcome that could have a significant impact on people – especially as it pertains to security, health, safety, privacy, freedom, discrimination, or human rights.In thinking about high-risk applications, focusing on “sectors” may lead to overly broad categorizations; it is important to use sufficiently targeted and well-outlined classification to ensure this criterion does not become irrelevant. We encourage developing a categorization that takes into account sector, use case, complexity of the AI system, probability of worst-case occurrence, irreversibility and scope of harm in worst-case scenario"
247,Information Technology Industry Council,Q19 Application of risk-based approach to general purpose AI,"A risk-based approach should apply to general purpose AI systems, and obligations levied upon an AI system should be proportionate to the level of risk that it poses to health, security, safety, or fundamental human rights. Similarly, obligations for foundation models should be proportionate to the level of risk the models pose in the use case or deployment context. This is because the end use is often where risk to individuals materializes. It is also important to consider the capabilities of the model in assessing risk.Foundation models present a unique set of considerations because these models can form the basis for many downstream AI applications. Therefore, risks could be promulgated throughout the ecosystem if they are not appropriately identified and mitigated during development. ITI is working on a paper containing policy considerations around foundation models and will share that in follow-up to this submission once finalized."
248,AVF Research,General comment,Our experience in the area of responsible AI lies principally with its applications to professional and allied services. Our answers to your questions will reflect this. Our experience is more limited when it comes to general issues of responsible AI outside applications to professional and allied services.
248,AVF Research,Q01 Definitions,"Yes, agree."
248,AVF Research,Q03 Non-regulatory actions to address AI risks,Responsible AI for professional and allied services can be covered by Codes of Conduct specific to AI application domains of that type. Development of such Codes is a sizeable undertaking for any given professional group.
248,AVF Research,Q04 Coordinating AI governance across government,"Government would normally be expected to be involved in the development of Codes of Conduct (specific to professional domains) for the development and application of AI-enabled systems. There are always many issues common to more than a single domain, scoping out to issues that are ""generic"" (""domain agnostic""). Coordination by government across domains is a natural way to tackle such matters."
248,AVF Research,Q05 International models for AI governance,"Re the development of Codes of Conduct for the development and application of AI systems supporting professional and allied services, a good role model is the current US effort for the domain of healthcare. That is a three-year project, one which is being coordinated by the US National Academy of Medicine."
248,AVF Research,Q06 Public vs Private sector governance,"Public-private coordination is widespread across all domains that involve the development and application of AI to professional and allied services. That said, it's obvious the differences between public and private sectors need to be respected."
248,AVF Research,Q07 Responsible government use of AI,see previous answer (re public-private coordination)
248,AVF Research,Q08 Generic vs technology-specific approaches for AI risk,"The word ""generic"" means different things in different contexts. A large volume of AI and kindred technologies are ""general purpose"" and ""domain agnostic"". An even more sizeable volume are domain-specific or otherwise more focused than domain-agnostic. Both such large volumes are vital to the development and application of AI.When it comes to large language models (LLMs) and other AI technologies of general-purpose application (as opposed to technologies that underpin AI-enabled systems in generic/ general-purpose/ domain-agnostic ways), LLMs are obviously of high value. But they cannot be said to be ""more valuable than"" the wide diversity of other non-LLM technologies in AI, Data Science, Data Engineering, Internet-of-Things etc..As far as opportunities and risks are concerned, there is no imperative to single out LLMs (notwithstanding the current round of media attention currently being enjoyed by LLMs)."
248,AVF Research,Q09 Transparency requirements and scope,"Professional and allied services in STEM benefit routinely from Evidence-Based Practice. That encompasses Evidence-Based Decision-Making and Evidence-Based Decision Support (in re support from systems and component technologies). Transparency in regard to an imperative for provision of adequate evidence is routinely central to professional practice in STEM.As for HASS, box-office criteria and sales of fiction differ widely from the case for Evidence-Based Practice in STEM.In AI, Data Science and Data Engineering, opportunities are enhanced and risks are mitigated whenever there is a strong base of evidence to support or regulate this or that possible practice.Transparency and explainability requirements, some of them mandatory, are a natural and ubiquitous part of Codes of Conduct in STEM. As for HASS, ""Ars Est Celare Artem"" - HASS can at times be quite different from STEM."
248,AVF Research,Q11 Actions to increase public trust in AI,Codes of Conduct can enhance trust provided there is adequate enforceability.
248,AVF Research,Q13 Changes to conformity infrastructure,(see above comments on Codes of Conduct)
248,AVF Research,Q14 Risk-based approach to address AI risks,"It's best to consider risks and opportunities together, in a suitably coordinated way, as is largely standard in analytics work targeted towards professional and allied services. Rankings according to degree of risk are clearly helpful."
248,AVF Research,Q15 Benefits and limitations of risk-based approach,(see our answer to the preceding item)
248,AVF Research,Q16 Scope and application of risk-based approach,(ditto)
248,AVF Research,Q17 Elements to include in risk-based approach,Those elements (in Attachment C) seem to be a well-thought-out preliminary step.
248,AVF Research,Q18 Integration of risk-based approach into existing frameworks,(see our answer to the top item above)
248,AVF Research,Q19 Application of risk-based approach to general purpose AI,"An example has been the use of LLMs to facilitate the creation of patient records. If an LLM's ability to source the professional literature is adequately trustworthy, then healthcare professionals would still need to be confident about that. Risks in such cases are the risks associated with how trustworthy a given LLM actually is in practice. Enforceable Codes of Conduct (adequately supported, if need be, by government regulation) can be helpful here. Such measures align with the current situation in re regulation/ governance for medical devices and pharmaceuticals."
248,AVF Research,Q20 Voluntary vs mandated compliance with risk-based approach,"(see our comments above on Codes of Conduct - development and deployment, both, are important in this area)"
249,Copyright Advisory Group (CAG) - Schools and TAFE,Transcription,"FILENAME: CAG Submission in response to the Safe and Responsible AI in Australia Discussion Paper.6f477444a7575.docx | CONTENT: CAG Submission in response to theDepartment of Industry, Science and Resources’ Safe and Responsible AI in Australia Discussion PaperThe Schools and TAFE Copyright Advisory Groups (CAG) welcome the review by the Department of Industry, Science and Resources (the Department) and the opportunity to make a submission in response to the discussion paper on safe and responsible artificial intelligence (AI) in Australia (the Paper) released in June 2023.CAG acknowledges that this review is not the forum to address the full breadth of intellectual property issues raised by the development of AI systems and tools. However, CAG believes it is essential to highlight a number of important copyright issues raised by the creation and use of Artificial Intelligence (AI) technologies in education and skills training.About CAGThe Schools Copyright Advisory Group reports to Au"
250,R. Sheh and K. Geappen,Transcription,"FILENAME: R Sheh&K Geappen response to Safe and Responsible AI in Australia discussion paper 2023.1e66e0597cf32.pdf | CONTENT: Submission to Safe and Responsible AI in Australia discussion paper, 2023 Raymond Sheh, Adjunct Associate Research Scientist Robotics and AI Researcher Johns Hopkins University https://www.linkedin.com/in/raymondsheh/Karen Geappen Cyber Governance, Risk and Compliance Consultant https://www.linkedin.com/in/karen-geappen-4491094/We are pleased that the Australian Government is working towards a consistent and managed approach to AI technology in Australia. The ability for Australia to safely and responsibly utilise AI from an individual to national level is key to protecting Australian interests, culture and way of life. We are unique in the world and therefore should not be blindly trusting or relying on other nations to define what is ‘Safe and Responsible’. We hope our response to the 20 questions posed on the topic support existing views or elicit some thoug"
250,R. Sheh and K. Geappen,Q01 Definitions,"The use of ISO/EIC 22989:2022 is a great start, particularly in its emphasis that AI is a broad field, of which Machine Learning (ML) is only a part. The use of an internationally recognized definition also ensures broad compatibility of terms.However, care should be taken to actually use this definition throughout the discussion paper. While the ISO definition encompasses ML as well as a wide range of other non-ML based AI techniques, a lot of the discussion paper uses AI synonymously with ML, which can be confusing and perpetuates misconceptions that the challenges and issues around ML exist in all of AI.This issue flows over to the implied definitions of other terms. For example, Generative AI is a broad term that encompasses the use of AI to generate content of some sort, via both Neural Networks and other means. However, much of the discussion paper, and other documents, are written as if the problems that exist in Neural Network based Generative AI exist for all of Generative AI."
250,R. Sheh and K. Geappen,Q02 Other AI risks and recommendations for regulation,"There are three major risks that we see in the use of AI that do not appear to be covered by the discussion paper and by existing regulations (at least not until it’s too late).The first relates to AI interfering with the ability to perform root cause analysis of failures . In this context, examples of failures include an AI based medical diagnosis system that provides bad data to a doctor, or a car with an AI based pedestrian detector that fails to detect the pedestrian and causes a collision. In this context, different forms of AI provide different abilities to perform the root cause analysis often required by regulation. Some, including many ML techniques, make it difficult or impossible to tell where the failure occurred, be it in the AI system, in the wider system, in the training of the system, in the sensors, or so-on. In contrast, some other AI techniques, such as some that derive mathematical models or express their decision making process in the form of first-order logic, can"
250,R. Sheh and K. Geappen,Q03 Non-regulatory actions to address AI risks,"A topic that is mentioned in passing in the discussion paper and deserves greater attention is more broadly funding the development of AI expertise in Australia that is not connected with industry. We hope that social obligations form a part of a company’s conduct but at the end of the day, there needs to be significant expertise within government and other organisations who can advocate on behalf of consumers and small/medium sized businesses. Initiatives that make such positions competitive with industry in attracting the best talent is vital to ensuring that any regulations regarding responsible AI practices have appropriate oversight.One particularly important aspect in the development of expertise is the development of metrics and requirements that AI systems in different sectors should satisfy, and people who are proficient in their use and further development as technologies and applications change. These actionable definitions and quantitative measurements are vital to ensuring"
250,R. Sheh and K. Geappen,Q04 Coordinating AI governance across government,"Experienced AI practitioners with visibility across the widely varying techniques and applications of AI, including non-ML types of AI, are in scarce supply, especially for a country like Australia with a small population. The preservation of this expertise, and a coordinated approach to its sharing across different government sectors, can be crucial to ensuring the appropriate development and use of AI. It also assists in reducing unnecessary duplication and inconsistency, and increases the efficiency/applicability of AI research and initiatives that can then be leveraged to greater benefit a wider spread of Australia. An added benefit of coordinating AI governance across different sectors is fostering diversity in background related to AI where diversity is shown to produce greater and more robust innovation."
250,R. Sheh and K. Geappen,Q05 International models for AI governance,"The topic of datasets was mentioned in the discussion paper but one aspect that can be powerful, particularly for a small, highly distributed population such as Australia, is synthetic data. This is data, such as about a population, that is generated from the real data and has similar properties for a given application, but where no one datapoint in the synthetic data can be matched to a real datapoint. Organisations such as the US National Institute of Standards and Technology, are performing research into the use of privacy engineering in general , and synthetic datasets in particular, which may be used to develop and test AI applications in situations where there is a large privacy risk. Governance measures may include requiring only synthetic datasets be used for development and testing until a given implementation has been proven to be suitably privacy preserving, only after which access to real datasets are provided.On the privacy front, improved regulation on the protection of p"
250,R. Sheh and K. Geappen,Q06 Public vs Private sector governance,"Approaches should be tailored to their impact, consistent with risk management best practice. In that sense, for applications with the same level of impact, public and private sector uses of AI should not differ.Certainly for the same application, there should be no difference in rigour or transparency of AI in the public or private sector. As has recently been demonstrated in the Australian economic environment, the Private sector can have the same or greater influence on Australia and Australian National Security as the public sector. Instead, standards should correspond to risk, which includes a component of impact and influence. These should be rooted in the AI Ethics Principles and be quantifiable and measurable in a meaningful way.5. NIST Privacy Engineering - https://www.nist.gov/itl/applied-cybersecurity/privacy-engineering last accessed 2023-07-25"
250,R. Sheh and K. Geappen,Q07 Responsible government use of AI,"The most impactful support that the Australian Government can offer to further support responsible AI practices is in growing internal talent and expertise in AI. This is a challenge as this talent is also in high demand in the private sector, thus it is essential that public sector jobs for such talent are appropriately attractive. Education and communication with the public is also vital so that they can understand the conversation around AI, properly assess their risks, and inoculate themselves against marketing and misinformation surrounding its capabilities and dangers. Rolling out public education and mechanisms such as the aforementioned star ratings and Public Disclosure Statements, to ensure an informed population, and robust ways to report issues, such as an AI Ombudsperson, as will be discussed later, will also help to further support responsible AI practices within government agencies."
250,R. Sheh and K. Geappen,Q08 Generic vs technology-specific approaches for AI risk,"Solutions to the risks of AI should, at an abstract level, be outcomes based. For example, risk in a critical system is reduced by the ability to perform root cause analysis in the event of failure, and verifiably roll out fixes going forward. A generic solution at an abstract level is that the AI system needs to permit root cause analysis through the system, to a given level. However, once this becomes specific to an application, it often requires solutions that are technology specific to achieve the higher level outcome. For example, performing root cause analysis, to a given standard, must be done differently in logic or mathematical model based systems as compared to systems based on Deep Learning (or not possible as the case may be and require different measures and restrictions to be put in place). So in a sense, the answer to this question is one of the level of abstraction at which the policy is applied, rather than for a given circumstance."
250,R. Sheh and K. Geappen,Q09 Transparency requirements and scope,"a. Transparency is most vital in improving public trust and confidence when it tells them something new and gives them a practically actionable choice. Unfortunately this often goes against the interests of those deploying AI. If the consumer is happy and doesn’t know that AI is involved, why tell them? In that sense, regulations around transparency must also be accompanied by regulations that provide the public with actionable agency such as the option to contest its output or decision due to its output. There must be a way to remove themselves from the AI system, or not be subject to it as part of managing their risk. If there is no option, the transparency is not actionable.This transparency must also go hand-in-hand with common definitions for AI and transparency itself. The practice of “transparency-washing” or “explainability-washing” is already commonplace. This is where systems provide explanations for AI based decisions, with the aim of increasing trust or acceptance of the de"
250,R. Sheh and K. Geappen,Q10 Ban on high-risk AI applications,"a. Bans on specific applications and technologies should be used very sparingly, not only in AI but more generally, and only based on what is illegal regardless of AI. Beyond those, an outcomes based approach should be taken, which requires particular applications to have particular capabilities. For example, AI that is used in safety critical applications should have the capability for root cause analysis and verifiable correction if an error is found. This is no different to what happens in the automotive or aerospace industries, where if a failure happens in, say, an aircraft engine, it is necessary to find the root causes and fix them in a verifiable way. This may result in a de-facto ban of certain technologies that do not yet provide these capabilities, such as high performance Neural Network based Deep Learning, but also incentivises further development that may either add these capabilities to Neural Network based Deep Learning in the future, or improve the performance of other"
250,R. Sheh and K. Geappen,Q11 Actions to increase public trust in AI,"The Australian government has been very lacking in educating the public. A quick read of Australian media even goes to show there is little in the way of standardised terminology or definitions. This means that the general population will remain uneducated at best or completely confused on any further discussions related to AI risk or AI trust. This hinders the appropriate and safe use of AI and forgoes an opportunity to give industry an incentive to invest in ethical AI development. Thus the biggest impact we can see in initiatives to increase business-to-business and public trust in AI revolve around education. In particular: ● Education at all levels around when it comes to AI, and what different forms of AI are, and are not, useful for. This includes balanced guidance on opportunities vs risks of using different forms of AI (or how to assess for one-self), or even deciding not to use AI at all , perhaps guided by the aforementioned star ratings or Product Disclosure Statements. ● R"
250,R. Sheh and K. Geappen,Q12 Trade impact of banning high-risk AI applications,"We need to be careful with banning terms rather than activities. It sounds good to ban social scoring but they are not so far removed from other scores that we are familiar with, like credit ratings, or ones that we don’t even have visibility into, such as the visibility of our posts on social media. Banning social scoring but ignoring similar use cases results in a regulatory environment that risks either encouraging loopholing, or over-regulation, depending on how broadly the regulation is interpreted.Instead of bans, tight, technically actionable, outcomes based requirements on the capabilities of high risk activities, such as the need for real transparency and explainability, the ability for people to truly delete their data, and so-on, can be applied. This can foster Australia’s tech sector as one with a reputation for producing responsible, trustworthy AI that is held to a high standard domestically and can command a premium internationally."
250,R. Sheh and K. Geappen,Q14 Risk-based approach to address AI risks,"Yes. AI, being such a wide ranging technology and wide applications, needs an approach that is adaptable to the technology’s application which a risk-based approach caters for. The approach must be standardised against an Australianised AI Risk Management Framework that is linked back to the Australian AI Ethics Principles and the framework must be measurable. The discussion paper already mentions the US National Institute of Standards and Technology’s AI Risk Management Framework (NIST AI RMF) is a good starting point for developing an AI Risk Management Framework suitable for Australia. Some of the concerns we raised in the development of the NIST AI RMF are still relevant . Australian contributions back to the NIST Trustworthy and Responsible AI Resource Center , particularly in terms of playbooks for the NIST AI RMF, can also be valuable in harmonising Australia’s risk management approach to AI. It must also be logical such that the general public is able to trace the analysis if t"
250,R. Sheh and K. Geappen,Q15 Benefits and limitations of risk-based approach,"The main benefit is that it provides information (hopefully standardised) to the consumer on the potential for AI to cause both desirable and adverse outcomes, and what these may be. It will inform implementers, deployers and developers on what controls are applicable and pragmatic in their application of AI technology, as opposed to a compliance based approach where a control may be irrelevant or, worse, cause greater risk. Limitations is that such an approach will not account for all eventualities and all risk assessments are at some level qualitative and subjective to the assessor. There is also the need for constant reassessment to determine if controls are still applicable, proportional and pragmatic. These limitations can be overcome for the former by having a well written Australianised AI Risk Management Framework and guidance tooling to assist consumers in the application of the framework. A possible library or catalogue of commonly used AI risk management controls (e.g. synth"
250,R. Sheh and K. Geappen,Q16 Scope and application of risk-based approach,"For some high risk sectors there could be a combination of risk based and regulatory based approaches. A combination of (for example) certification against an appropriate build, transparency or ethics standard, and a risk approach for any additional controls that reduce risks to a predetermined appetite. This could, for instance, be for transport, critical infrastructure (including medical) or educational settings. The education sector is particularly relevant due to the risk of embedding bias into generative AI that can be used for external influence with an impact to National Security or Australian Cultural values."
250,R. Sheh and K. Geappen,Q17 Elements to include in risk-based approach,"The elements are in general suitable, however without further information on measurement they can be subjective. This reinforces the need for home-grown expertise in the measurement, requirements analysis, and capabilities assessment, of AI systems, which is still very much a topic of research, at home and abroad."
250,R. Sheh and K. Geappen,Q18 Integration of risk-based approach into existing frameworks,"Existing assessment frameworks such as the Privacy Impact Assessment, used when dealing with Personal Information under the Australian Privacy Act (1988), are relevant if considering existing frameworks to incorporate an AI risk-based approach. For such frameworks, it should be relatively easy to include impacts as a result of the AI into assessments. Similar can be said for Risk Management Frameworks where the consequence for the use of AI is included in analysis and calculations. Where there is an existing framework focusing on the outcomes, it may be more advantageous to create guidance on its application to AI technologies or services. The guidance can be around how to assess for impacts from AI that align with impact already in the existing framework. Or the guidance can prompt ‘lines of thought’ that may be different in the AI system to non-AI systems as applicable to the framework. By incorporating, through guidance, the AI technology considerations into existing assessment fram"
250,R. Sheh and K. Geappen,Q19 Application of risk-based approach to general purpose AI,"Risk based approaches are even more important for the developers of LLMs and MFMs. Their subsequent users may be unable to appropriately perform their own risk assessments downstream of these systems and apply any risk management controls as required. Instead they must rely on the (perhaps partial) risk assessments of those providing the LLMs or MFMs. Unfortunately, performing appropriately meaningful risk assessments of large AI systems is still a topic of novel research that requires significant funding."
250,R. Sheh and K. Geappen,Q20 Voluntary vs mandated compliance with risk-based approach,"a. Both, with varying levels of rigour on the application of the tool dependant on the risk consequence of the AI in use. The higher the consequences the greater the rigour and closer to regulation of assessment and implementation of risk management controls. Regardless of the level of rigour applied, all systems should disclose their assessed residual risk as discussed previously. Whether in a Disclosure document or a star rating. This allows a consumer to enact any additional risk management controls they may wish or to request removal from the AI system/service (as per above mention for being meaningfully actionable).b. Both, as above. As with all technology, risks must be considered from the inception of the idea through to eventual deployment, sustainment and decommissioning in the technology lifecycle. With AI this is no different but in fact more critical. AI will enhance technology capability but in balance to that also enhances risks of that technology. Meaning risk considerat"
251,Australian Directors Guild / Australian Screen Directors Authorship Society,Transcription,"FILENAME: ADG ASDACS Submission to Safe and Responsible AI Discussion in Australia final 26:7:23.78dd11a88b45.pdf | CONTENT: Australian Screen Directors Authorship Collecting Society and Australian Directors Guild Joint Submission to the Department of Industry, Science and Resources Safe and Responsible AI in Australia Discussion Paper.26 July 20231 About UsASDACSThe Australian Screen Directors Authorship Collecting Society (ASDACS) is a copyright collecting society representing the interests of screen directors throughout Australia and New Zealand. It was established in November 1995 and currently has over 1400 members. The primary purpose of ASDACS is to collect, administer and distribute income for screen directors arising from international and domestic secondary usage rights.ADGThe Australian Directors Guild (ADG) is the industry association and union representing the interests of screen directors throughout Australia. Formed in 1982, it has over 800 members nationally and is regi"
252,"Humanising Machine Intelligence Initiative, The Australian National University",Transcription,"FILENAME: DISR AI Concultation Response_HMI.17b12c999829b.pdf | CONTENT: Response to the Australian Government Department of Industry, Science and Resources Safe and Responsible AI in Australia Discussion PaperJuly 2023INTRODUCTION The Humanising Machine Intelligence Program (HMI), a grand challenge initiative supported by the Australian National University, welcomes the opportunity to respond to the Discussion Paper, “Safe and Responsible AI in Australia”. HMI is a multi-disciplinary research team with expertise that spans the humanities, social sciences, and computer science, supporting cross- cutting collaborations on AI. Drawing on our collective understanding of AI, derived from deep engagement with existing literature and active production of new research in this area, we adopt a socio-technical approach to AI systems.1 Although AI has been around for decades, recent developments and expansive AI integration across personal, professional, and civic life have garnered public atten"
252,"Humanising Machine Intelligence Initiative, The Australian National University",Q01 Definitions,"Please see attachment. In general, we do."
252,"Humanising Machine Intelligence Initiative, The Australian National University",Q02 Other AI risks and recommendations for regulation,Please see attachment.
252,"Humanising Machine Intelligence Initiative, The Australian National University",Q03 Non-regulatory actions to address AI risks,Please see attachment.
252,"Humanising Machine Intelligence Initiative, The Australian National University",Q04 Coordinating AI governance across government,Please see attachment.
252,"Humanising Machine Intelligence Initiative, The Australian National University",Q05 International models for AI governance,Please see attachment.
252,"Humanising Machine Intelligence Initiative, The Australian National University",Q06 Public vs Private sector governance,Please see attachment.
252,"Humanising Machine Intelligence Initiative, The Australian National University",Q07 Responsible government use of AI,Please see attachment.
252,"Humanising Machine Intelligence Initiative, The Australian National University",Q08 Generic vs technology-specific approaches for AI risk,Please see attachment.
252,"Humanising Machine Intelligence Initiative, The Australian National University",Q09 Transparency requirements and scope,Please see attachment.
252,"Humanising Machine Intelligence Initiative, The Australian National University",Q10 Ban on high-risk AI applications,Please see attachment.
252,"Humanising Machine Intelligence Initiative, The Australian National University",Q11 Actions to increase public trust in AI,Please see attachment.
252,"Humanising Machine Intelligence Initiative, The Australian National University",Q12 Trade impact of banning high-risk AI applications,Please see attachment.
252,"Humanising Machine Intelligence Initiative, The Australian National University",Q13 Changes to conformity infrastructure,Please see attachment.
252,"Humanising Machine Intelligence Initiative, The Australian National University",Q14 Risk-based approach to address AI risks,Please see attachment.
252,"Humanising Machine Intelligence Initiative, The Australian National University",Q15 Benefits and limitations of risk-based approach,Please see attachment.
252,"Humanising Machine Intelligence Initiative, The Australian National University",Q16 Scope and application of risk-based approach,Please see attachment.
252,"Humanising Machine Intelligence Initiative, The Australian National University",Q17 Elements to include in risk-based approach,Please see attachment.
252,"Humanising Machine Intelligence Initiative, The Australian National University",Q18 Integration of risk-based approach into existing frameworks,Please see attachment.
252,"Humanising Machine Intelligence Initiative, The Australian National University",Q19 Application of risk-based approach to general purpose AI,Please see attachment.
252,"Humanising Machine Intelligence Initiative, The Australian National University",Q20 Voluntary vs mandated compliance with risk-based approach,Please see attachment.
253,Rohan Fernando - STRATINNOVA,Transcription,"FILENAME: Safe and Responsible AI in Australia - STRATINNOVA Survey Responses 20230726.941636d1afb98.pdf | CONTENT: Subject: Safe and Responsible AI in AustraliaDocument: Survey Responses - STRATINNOVARecipient: DISR - Department of Industry, Science and ResourcesContact: Rohan FernandoEmail: rohan@stratinnova.comMobile: +61 407 976 385Information: PublicDate: 26 July 2023 Subject: Safe and Responsible AI in Australia Document: Survey Responses – STRATINNOVA Recipient: DISR - Department of Industry, Science and Resources Contact: Rohan Fernando Email: rohan@stratinnova.com Mobile: +61 407 976 385 Information: Public Date: 26 July 2023Responses to the survey are as follows:Definitions 1. Do you agree with the definitions in this discussion paper? If not, what definitions do you prefer and why?1.1. The definitions of AI Technologies and AI Applications provided are a good start, however being strictly correct, these all relate specifically to types of AI technologies, including the propo"
253,Rohan Fernando - STRATINNOVA,General comment,Hope this assists with the safe use of AI in Australia. Let me know if you require further information.
253,Rohan Fernando - STRATINNOVA,Q01 Definitions,"1.1. The definitions of AI Technologies and AI Applications provided are a good start, however being strictly correct, these all relate specifically to types of AI technologies, including the proposed Applications provided in the Definitions. That is LLM, MfM and ADM are all really types of AI technologies, and these can each be used for an extremely diverse range of different AI Applications.1.2. In order to identify AI Applications, it may be additionally helpful to add a definition of Intelligence itself, and then create an extremely large list of Capabilities that are enabled by Intelligence. I suggest considering these Capabilities as different dimensions of Intelligence. The Capabilities or dimensions of Intelligence fundamentally determine the different types of AI Applications that can be performed.1.3. This approach will be helpful with AI, its various Applications, and associated Benefits and Risks, as AI’s ability to learn, combine, and perform more Capabilities will grow ov"
253,Rohan Fernando - STRATINNOVA,Q02 Other AI risks and recommendations for regulation,"2.1. The Canadian approach of categorizing AI risk levels from Low (level I) to Very High (level IV) and associated Bill C-27 Part 3 seems a totally appropriate starting point. Similarly, the EU appears to be taking a sensible approach to AI risks and regulation.2.2. A very important issue associated with AI Risks is that AI is a technology that has an ability to cause Harm at an extremely high scale of Impact at a single Individual level. Reading through the Canadian Bill C-27, I’d suggest the penalties for Individuals may be too lenient, and Australia might consider making the penalties for serious Harm caused by AI extremely harsh, so as to act as a very strong deterrent.2.3. I’d suggest the paper presently does not address National Security, Global Security, and Existential risks of AI. The first two are at least Very High (level IV), and the third is in a category of its own, as Catastrophic (level V).2.4. Nowhere in this document are Catastrophic risks of AI mentioned or addresse"
253,Rohan Fernando - STRATINNOVA,Q03 Non-regulatory actions to address AI risks,"3.1. It seems the non-regulatory issues with AI are quite manageable and although it can be anticipated that outlier risks and Harm may pop up in this area, these can be addressed on a case-by-case basis."
253,Rohan Fernando - STRATINNOVA,Q04 Coordinating AI governance across government,"4.1. I’m presently not involved in Australia’s AI governance across government, so cannot make a useful comment.4.2. I would welcome the opportunity to become involved and provide some assistance."
253,Rohan Fernando - STRATINNOVA,Q05 International models for AI governance,"5.1. The EU AI Act, Canadian, and Chinese approaches all appear to be quite comprehensive. There would be benefits to harmonizing Australia’s approach with the best of various international approaches.5.2. Given how incredibly advanced China is in both the development and applications of AI, I’d suggest it would be useful to consider how they are approaching AI Risks, regulation and the control of Harm. The Chinese Government is managing an enormous population, and their relatively strong approach to the management of citizens will provide some unique insights into how their government measure and manage the risks of AI within their own nation."
253,Rohan Fernando - STRATINNOVA,Q06 Public vs Private sector governance,"6.1. It may be prudent to apply precisely the same regulatory approaches for AI to the private sector and public sector. The public sector directly engages with every citizen of Australia in one way or another, and consequently has the potential to incorporate AI Applications that could possibly cause the most widespread Harm and impact. The private sector will largely follow the public sector’s lead, in so far as, what the private sector can get away with in AI Applications. I’d suggest the public sector must lead by example with the use of AI and following AI regulations, and use its force of law to very firmly realign any business in the private sector that actively attempts to deviate from the AI regulatory path."
253,Rohan Fernando - STRATINNOVA,Q07 Responsible government use of AI,"7.1. Clear guidance on acceptable AI usage, including extremely strict laws for very high risks, through to recommended guidelines for generally low risks, that are very well communicated, with appropriate training of government personnel, and particularly IT staff and contractors, will be quite helpful."
253,Rohan Fernando - STRATINNOVA,Q08 Generic vs technology-specific approaches for AI risk,"8.1. Generic solutions may best be applied to high AI risks, and technology-specific solutions may be better suited to specific Application risks elsewhere. High AI risks with high scale impacts primarily stem from access to specific classes of training data, and its applications, that is only available to groups with specialized knowledge.8.2. It would be prudent to establish very broad and extremely strong regulatory controls on these generic types of AI training data and AI models, with extremely severe penalties for any breaches.8.3. These are not so much technology issues, but rather more related to actionable information and its potential for deliberate misuse in the wrong hands.8.4. Eg. Strictly controlling authorized access and use of Drug Design data sets, and associated pre-trained AI models might be a good thing to do. This type of training data or trained AI model weights, floating around in the general public, could become extremely harmful to millions of people worldwide."
253,Rohan Fernando - STRATINNOVA,Q09 Transparency requirements and scope,"9.1. A suggested approach to consider is to establish an Australian Federal Government AI Management Agency (GAIMA) immediately, that all suppliers of AI hardware, software, and application services in Australia are required to comply with. This approach might seem unusually extreme right now, but it will be enormously helpful in protecting Australian citizens quite soon as AI computing power increases.9.2 Implement mandatory AI interaction and usage requirements to advise Humans using AI on the following:9.2.1 RAIT - Ranked AI Type - to advise Humans on the AI model type they are interacting with, as officially classified by GAIMA. 9.2.2 RAICM - Ranked AI Computation Maximum - to advise Humans on the maximum AI Computational Capabilities they are interacting with, as officially classified by GAIMA. 9.2.3 HTAIR - Human to AI Ratio - to advise Humans on the ratio of Human to AI information content they are interacting with, as officially certified and classified by GAIMA. ie. 100% Human"
253,Rohan Fernando - STRATINNOVA,Q10 Ban on high-risk AI applications,"10.1. There are numerous actions that could be implemented to protect Australians from high-risk (& catastrophic risk - level V) AI applications and technologies, and it is suggested that these could be implemented under the supervision of GAIMA. Again, the following list may seem totally outlandish and extreme right now, however neural networks on which AI is based have extraordinary information processing capabilities that are yet to unfold within developing digital technologies. The following will make sense very soon.10.2. Implement Computer processing AI calculation per second (CPS) rate limits:10.2.1. establish AI system Computer processor development technology limits, with processor types and quantities applied to any single AI application requiring GAIMA approval.10.2.2. potentially restrict or outlaw quantum Computing for both AI model learning and AI model runtime (post-learning) applications.10.2.3. for every AI application establish a maximum calculations per second (CPS) "
253,Rohan Fernando - STRATINNOVA,Q11 Actions to increase public trust in AI,"11.1. Many people are already fearful of AI, so it is important to provide the public with education to raise awareness of the many incredible benefits that AI can actually provide.11.2 Provide example use-cases where AI has been used to provide benefits to the public and a wide range of industries. This will serve to foster greater trust in AI and the government’s role in ensuring it is being kept safe. It is important to demonstrate and message that AI risks are being controlled.11.3 Provide examples in how government is strictly regulating AI to keep the public safe, in both public and private/commercial applications.11.4. This needs to be long-term media campaign."
253,Rohan Fernando - STRATINNOVA,Q12 Trade impact of banning high-risk AI applications,"12.1. It is suggested that high-risk AI applications will most typically be used to Harm, coerce, and exploit people and businesses. Therefore, it seems logical that banning high-risk AI applications in Australia is likely to be perceived favorably by other countries, as it will mean Australian citizens and businesses are generally more trustworthy than countries that do not impose such high-risk AI bans. The more professionally, ethically, and reasonably that AI is deployed, used, and supervised and managed through regulation in Australia, the better it will be for both Australia and its international trade partners.12.2. It can be anticipated that countries that do not impose controls on high-risk AI will see their AI hardware, AI software, and AI products and AI services fully banned by other nations. By way of example, consider the bans imposed on some computer networking technology suppliers because they have been deeply technically analyzed and are considered to incorporate ‘back"
253,Rohan Fernando - STRATINNOVA,Q13 Changes to conformity infrastructure,"13.1. The existing Standards and Regulatory bodies in Australia provide an excellent portfolio of capabilities for mitigating many of the potential AI risks. The issues and risks with AI may emerge very quickly and unexpectedly as AI gains increasing dimensional Capabilities, so this conformity infrastructure will need to be extremely well informed on AI issues and risks, and then be adaptive to change and imposing new recommendations and controls that Australia can use effectively. Speed of response will be very important.13.2. As previously mentioned, a suggested approach to consider is to establish an Australian Federal Government AI Management Agency (GAIMA) immediately, that all suppliers of AI hardware, software, and application services in Australia are required to comply with. The level of conformity and compliance that is imposed by GAIMA could be directly related to the AI risk level."
253,Rohan Fernando - STRATINNOVA,Q14 Risk-based approach to address AI risks,"14.1. A risk-based approach makes sense. This could be analyzed in terms of different types of Harm that AI can cause, as suggested above.14.2. Breadth and depth of Harm will both need to be considered."
253,Rohan Fernando - STRATINNOVA,Q15 Benefits and limitations of risk-based approach,"15.1. A risk-based approach is logical.15.2. The only real limitation with a risk-based approach for AI, is that it is prudent to fully expect AI will progressively become more Intelligent than most, if not all, Humans."
253,Rohan Fernando - STRATINNOVA,Q16 Scope and application of risk-based approach,"16.1. It will be important to focus on sectors, applications and organizations that have the most potential to cause Harm, particularly at widespread scale. eg. Government Services, Critical Infrastructure, Emergency Services, and Safety Critical Systems used in various Industries, such as Transport and Traffic Management."
253,Rohan Fernando - STRATINNOVA,Q17 Elements to include in risk-based approach,"17.1. The approach outlined in Appendix C covers most issues, however Automation in particular can have very significant impacts on Humans that are not always clearly obvious at first.17.2. Automation typically has truly zero concern for Humans, as Automation has no comprehension of its Human impact, so it is critical to envelope Automated systems inside very thoroughly designed and robust protective external systems and barriers to appropriately protect Humans. This is true for both hardware and software-based Automation.17.3. Consider developing and instituting a regulatory framework and processes for providing Humans with a strictly defined methods for recourse and contesting the outcomes and impacts of Automated systems."
253,Rohan Fernando - STRATINNOVA,Q18 Integration of risk-based approach into existing frameworks,"18.1. There are already numerous existing laws in place, many of which are designed to protect Humans from Harm. Therefore, it is important to analyze existing legal policies, eg. Australian Privacy Principles, and determine ways in which the Capabilities of AI can pierce through the protections that these policies and laws provide, and can enable harm.18.2. Where this occurs, legal amendments will be required as a matter of urgency."
253,Rohan Fernando - STRATINNOVA,Q19 Application of risk-based approach to general purpose AI,"19.1. LLMs, MFMs, and several other AI models are just variations of the same underlying fundamental neural network technology.19.2. It is perhaps more useful to focus on the Capabilities that various AI models enable, and design a suite of risk-based approaches to controlling the harm that can be caused by these various Capabilities.19.3. All AI models have a baseline level of risk associated with their Computational information processing rates and volumes, which is defined by the entire computing infrastructure they employ. Section 10 above provides some suggestions on how a risk-based approach could be implemented. However, I’m not suggesting this will be easy to achieve given the wide range of commercial and global competitive pressures that exist with AI right now. Nevertheless, section 10 is really about Existential level V risks."
253,Rohan Fernando - STRATINNOVA,Q20 Voluntary vs mandated compliance with risk-based approach,"20.1 It will be prudent to be extremely skeptical that voluntary self-regulation of AI will be able to professionally and correctly manage the diversity of risks of AI, and in particular, limit the Harm of totally unexpected and extremely ‘Intelligent’ but highly infantile solutions produced by AI.20.2 It is prudent to apply enforced regulation to public and private organizations.20.3 It is prudent to apply enforced regulation to developers and deployers of AI."
254,UNICEF Australia,Transcription,"FILENAME: UNICEF Australia submission to the consultation on safe and responsible AI in Australia [FINAL].5a34f03556d4e.pdf | CONTENT: Submission to the consultation on safe and responsible AI in AustraliaJuly 2023For further information or questions about this submission please contact: Katie Maskiell Head of Child Rights Policy and Advocacy UNICEF Australia e: kmaskiell@unicef.org.au m: +61 417 091 806Australian Committee for UNICEF Limited Suite 4.02 Building B, 33-35 Saunders Street PYRMONT NSW 2009 P: +61 2 9261 2811 F: +61 2 9261 2844 E: unicef@unicef.org.au PO Box 488, Queen Victoria Building, NSW 1230 | ABN 35 060 581 437 www.unicef.org.au | Donation Hotline: 1300 884 233 Table of Contents Executive Summary ................................................................................................................................................... 3 Summary of Recommendations .................................................................................................."
254,UNICEF Australia,Transcription,"FILENAME: UNICEF-Global-Insight-policy-guidance-AI-children-2.0-2021.1fd4c8faff329.pdf | CONTENT: Policy guidance on AI for children 2 . 0 | N OV E M B E R 2 0 2 1 Policy guidance on AI for children 2.0 | NOV EMB ER 2021 UNICEF works in the world’s toughest places to reach the most disadvantaged children and adolescents – and to protect the rights of every child, everywhere. Across 190 countries and territories, we do whatever it takes to help children survive, thrive and fulfill their potential, from early childhood through adolescence. And we never give up.The Office of Global Insight and Policy serves as UNICEF's internal think-tank, investigating issues with implications for children, equipping the organization to more effectively shape the global discourse, and preparing it for the future by scanning the horizon for frontier issues and ways of working. With dedicated expertise in seven policy areas – digital technology, human capital, governance, the environment, society, markets,"
256,Palantir Australia,Transcription,"FILENAME: Palantir Australia Response to Supporting Responsible AI Discussion Paper from the Department of Industry.0eac22fe414f9.pdf | CONTENT: Palantir Australia Response to Supporting Responsible AI Discussion Paper Department of Industry, Science and ResourcesJuly, 2023 palantir.com Copyright © 2023 Palantir Technologies Australia Pty. Ltd. Contact: alampert@palantir.com All Rights Reserved Overview of Palantir Australia’s Response Palantir Australia commends the Department of Industry, Science and Resources for taking this step to better understanding and support safe and responsible AI Practices, building on work already being done across various parts of the Australian Government. We welcome ongoing engagement with the Department as we keep iterating towards appropriate AI regulation and policy.Our responses are informed by Palantir’s global experience and lessons learned from refining a practice of responsible and trustworthy AI development and deployment.We have been guided by"
256,Palantir Australia,Q01 Definitions,"Palantir Australia identifies a lack of precision and consistency in the definitions in the discussion paper. The definitions presented unfortunately conflate AI technology categories (or model types) with their applications. We recommend more clearly differentiating types of AI from the applications thereof. Keeping these concepts distinct provides a better framework for contextual considerations of both risks and benefits of AI use. The distinction between technology and application is important, since any regulation needs to be attuned to the use cases of applications in specific domains, rather than addressed to any generic technology or model itself. In the discussion paper, Large Language Models (LLMs), Multimodal Foundation Models (MfMs) and Automated Decision Making (ADM) systems are all presented as “applications” of AI “technologies” - this seems confused and misleading, as these are themselves broadly applicable classes of technologies that can be used within many different "
256,Palantir Australia,Q02 Other AI risks and recommendations for regulation,"Given recent media and public attention given to long-term, existential risks that may arise from the use and development of AI, we suggest it may be helpful to distinguish between such highly uncertain existential risks, and contrast those against concrete harms that are real, specific, and either have actually occurred - even if in a different context of use - or are readily foreseeable. Much more regulatory attention should be given to addressing and avoiding concrete harms than highly speculative and controversial existential risks. Calls to address the possible existential risk of super-intelligent AI risk both impeding the development of beneficial uses of AI, as well as distracting researchers, regulators, policy makers, and technologists from the importance of addressing nearer-term risks of more likely harm. These harms will vary depending on how and where AI systems are deployed, and are best addressed by regulation that accounts for different context of use.We also suggest t"
256,Palantir Australia,Q03 Non-regulatory actions to address AI risks,"As we have publicly remarked [0], Palantir believes that meaningful progress towards controlling AI and harnessing it for human flourishing does not come from pausing experiments, but rather - and perhaps somewhat counter-intuitively - from leaning into the fielding of operationally-thoughtful and responsibly-constructed experiments in suitably constrained conditions that force us to identify and confront the real challenges of technologies in situ.This operational “field-to-learn” approach to AI deployment provides one template for ways that governments can - through funding or other convening authorities - provide frameworks that enable both technical innovation and legal and ethically accountable boundary setting. It does so by better exposing technologists, ethicists, policy-makers, social scientists, domain experts, and AI users to the specific challenges of AI deployment and use, as opposed to more theoretical musings that, while interesting, are often untethered from the reality"
256,Palantir Australia,Q04 Coordinating AI governance across government,"One challenge is that it takes time for issues to emerge and rise up through the regulatory system (complaints need to be made, issues investigated, etc.). There needs to be proactive seeking out of detailed case studies that can be learned from - including both successful and problematic deployments of AI systems, in real-world contexts. Forums that explore these case studies need to include relevant regulators as key stakeholders, along with input from representatives of relevant and diverse stakeholder groups - e.g., Aboriginal and Torres Strait Islander representatives, Disability representatives, children and youth, as well as application domain experts, civil society representatives, technology developers and technology deployers. Stakeholder groups that rely heavily on volunteers would greatly benefit from being compensated for their participation. The Responsible AI Network based out of the National Artificial Intelligence Centre could be leveraged for coordinating the inclusio"
256,Palantir Australia,Q05 International models for AI governance,"Australia, lacking a Federal Human Rights Act or Bill of Rights, will need additional rights spelled out for data subjects and users of AI systems. One of these is a right to contest automated decisions from AI or ADM systems. Looking abroad, there is already some normative convergence around this idea. Both the EU GDPR and the recently released US White House AI Bill of Rights include a form of this idea. We encourage this to be considered in Australia’s approach to AI regulation.For high-risk AI systems, regulation should follow the precedent set by the draft EU AI Act and require ongoing risk mitigation across the operational life of the system, and mandatory notification of accidents or other incidents to regulators. Regulators should also be able to step in and require the recall of a system if harms arise that cannot be adequately prevented or remedied. Other treatments that should be considered include: mandating relevant types of system redundancy, and kill switches.Overarching"
256,Palantir Australia,Q06 Public vs Private sector governance,"AI adoption already spans across the public and private sectors. Risks from these systems will be realised within and across both sectors, and both low-risk and high-risk systems can be deployed in both ecosystems. There is also often complex interdependency between public and private sector organisations for AI systems. Large, multi-national technology companies increasingly mediate, influence, and profit from the lives of a very significant portion of the Australian population. This includes in public sectors such as Education, and Health. Across all sectors, Government agencies are almost always customers of private sector technology companies when deploying AI technology. Given these factors, a reasonable starting point is that regulation should apply equally to both public and private sector uses of AI.Harmonising AI regulation across the private and public sector, as well as across state and federal governments, could help remove some barriers to responsible and effective AI adop"
256,Palantir Australia,Q07 Responsible government use of AI,"As has been done for managing privacy and data protection, the Government could create a binding Responsible AI Code for Government agencies - similar to the Australian Government Agencies Privacy Code for public sector agencies. This would go above-and-beyond the mandatory requirements of any AI regulation, with additional requirements to support the responsible deployment and use of AI with additional attention to designing and operating trustworthy systems.The Government would be well advised to provide ongoing and appropriate budget to regulatory agencies charged with overseeing AI system deployment and development, to allow them to credibly exercise their powers and responsibilities, as well as to commission and conduct research, to engage with relevant experts and stakeholders, as well as to enable them to access critical resources to up-skill staff and to hire and retain staff with the required technical and policy expertise. Funding should also be directed to fora that allow co"
256,Palantir Australia,Q08 Generic vs technology-specific approaches for AI risk,"Measures to mitigate or manage risks from AI are most sensible and practical when established and evaluated not for specific technologies - which can change rapidly - but grounded in specific contexts of use. Those contexts will best define the relevance and priorities of intended and achievable goals. Not all methods of managing or reducing harms will apply to all systems, and some approaches will, in fact, be at odds with others. In the case of incommensurable objectives for managing risk, trade-offs will be necessary. This process should recognise that an absolute attainment of some goals (e.g., eliminating all discrimination risk, or a total reduction of privacy risk) may be unreasonable in context, and often the objective should be directed more towards tolerable risk mitigation rather than complete risk cessation. Some goals and risks may need to be sensibly traded against others. A critical point, however, is that these trade-offs and corresponding consequences are openly acknow"
256,Palantir Australia,Q09 Transparency requirements and scope,"a/ Transparency is necessary but not sufficient.It will not be effective, for example, to address LLMs and complex ML models with transparency of their internal decision processes - it’s the wrong treatment and won’t really help end users. The notice-and-consent paradigm for managing privacy risks has failed in the face of complex and sophisticated uses and transfers of personal information that go beyond the ability of average system users to reasonably understand the implications of verbose privacy notices, let alone actively enforce given the often huge power asymmetry. In a similar way, providing transparency into the inner workings of AI models is unlikely to help system users provide informed consent or understand the implications or risks that stem from their use of an AI system.Requiring transparency around data collected and used to produce those models, as well as transparency around the auditing and assurance of those models - across the full lifecycle of developing, testing"
256,Palantir Australia,Q10 Ban on high-risk AI applications,"a/ We must acknowledge that AI technology is likely here to stay and focus on how to use the technology safely and responsibly, without turning a blind eye to the risks it poses or hoping that the technology will not be used in certain ways. Our Australian values should lead us to categorise some uses of AI technology and applications as presenting an unacceptable risk, regardless of any potential benefit. Similar to the EU AI Act, social scoring, for example, should have no place in Australian society. AI systems that use biometrics for certain forms of identifying individuals - one-to-many facial recognition, gait recognition or similar - should be restricted to public sector use, and have very stringent limitations on its use as well as genuinely independent reporting and oversight requirements that accompany any use.b/ There are useful starting points in the EU AI Act categorisation, namely:1. AI systems that deploy subliminal techniques beyond a person's consciousness to materiall"
256,Palantir Australia,Q11 Actions to increase public trust in AI,"A key, and unfortunately often overlooked, requirement is to equip regulators with necessary resources - including budget, technical expertise, and robust legislation - to provide genuine oversight and guidance for AI system deployment and use. This will require both strong ex-ante regulation that mandates the appropriate reduction of risk when AI systems are developed and deployed, as well as rigorous ex-post powers to detect and sanction organisations where harms arise."
256,Palantir Australia,Q12 Trade impact of banning high-risk AI applications,"Banning unacceptably-high-risk activities is unlikely to have a significant negative impact on Australia’s tech sector. Australia should contribute to global consistency and efficiency by joining other countries already considering banning such activities; it is likely, for example, that any activities banned in Australia would also be banned in the EU, which would be a more significant factor for Australian exports and trade.Overall, regulation that bans specific uses of AI technology provides a helpfully clear signal for commercial organisations to respond to."
256,Palantir Australia,Q13 Changes to conformity infrastructure,No response.
256,Palantir Australia,Q14 Risk-based approach to address AI risks,"As already noted, we believe that regulation should relate to the context of its application and context-specific concerns (historical, social, cultural, ethical, etc.). For this reason, in defining context-specific regulation, it is important to recognise that unique contexts will have intrinsic areas of concern or risk (including privacy, civil liberties, fundamental rights, sustainability, equity, inclusion, diversity, etc.) which should be directly factored into the chosen or mandated regulation approach.That said, all Australian laws - and the enforcement of those laws - are ultimately risk based, given finite resources for enforcement. Broadly speaking, we believe a risk-based approach is a reasonable starting point for AI regulation, with goal of ensuring that regulation and its enforcement is applied first to areas of highest risk and intensity of impact."
256,Palantir Australia,Q15 Benefits and limitations of risk-based approach,"A risk-based approach offers many advantages, especially when it is designed as a process to be repeated and altered as risks and knowledge change, rather than something to be completed once and not monitored or revised over time.A few thoughts on addressing potential limitations:* A risk-based approach does not inherently address any aspects of compensation or civil recourse if harms arise. It would be highly beneficial for these to form part of any AI regulation. * Coupling human rights impact assessments and/or algorithmic impact assessments, can help identify and address contestable and hard-to-quantify harms that might otherwise be missed in risk assessment. * As suggested earlier, there is a place for design standards to be applied, informed by both existing tracks of AI standards, and by detailed case-studies and analysis of AI system successes and failures. The disciplines of Human Computer Interaction (HCI), Human Factors, and Systems Engineering will have many ideas to offer "
256,Palantir Australia,Q16 Scope and application of risk-based approach,"In theory, a consistent and uniform approach to a AI regulation across sectors could help ensure that all stakeholders, regardless of their industry, are held to the same standards and expectations. This could help to prevent confusion, uncertainty, and potential harm to individuals or groups, and could promote consistency and clarity in AI development and deployment. However, such a uniform approach is likely to have significant limitations. Regulatory requirements that are widely applicable will tend to be articulated in broad terms, often as generalised principles rather than as concrete standards for action. Ultimately, any principles will need to be transposed into specific use case requirements to address the reality that most applications of so-called general use AI technologies in fact carry unique requirements or considerations for regulation. Healthcare, for example, will have specific privacy concerns related to patient data, while finance may have specific regulatory requir"
256,Palantir Australia,Q17 Elements to include in risk-based approach,"The elements outlined in Attachment C are generally sound.Some additional comments:* Explanations will vary in their effectiveness and appropriateness based on the level of detail, context of use, and complexity of the system. Relying on explanations to build trust pushes the onus back to end-users to assess and manage risk, which is very likely not reasonable or feasible in many circumstances, and almost certainly risks information overload as AI systems proliferate across a larger number of application contexts. The notice-and-consent model that this element builds on arguably suffers from the same systemic weaknesses despite its prominence in existing Australian privacy regulation * Standards and/or certifications (e.g., trust marks) are another important element that should form part of a risk assessment. Compliance with emerging and existing Australian and/or global standards for AI systems should serve as a form of risk mitigation * External assurance mechanisms are another impor"
256,Palantir Australia,Q18 Integration of risk-based approach into existing frameworks,"One approach would be to incorporate assessment of AI risks into existing risk assessment processes wherever possible - alongside Privacy Impact Assessment, Human Rights Impact Assessment, Algorithmic Impact Assessments and similar processes."
256,Palantir Australia,Q19 Application of risk-based approach to general purpose AI,"Risk-based approaches to regulating general purposes AI systems such as LLMs and MFMs are more effective when linked to specific use cases, rather than to the technology itself. LLMs offer additional regulation challenges, however, in that they are specifically designed to be highly accessible, easy to use, and readily applicable across very different application domains. Such systems have swiftly been adopted into workflows - both productive and malicious (e.g., producing misinformation at previously unprecedented quality and scale). Despite this generality, the harms and risks remain highly contextual, and are best addressed for specific use cases and domains.Consequential applications that deploy LLMs should require governance mechanisms that mitigate and manage the inherent pitfalls of these models, and should clear acknowledge that that, at best, LLMs provide a stochastic mimicry of understanding. They should also take account of the propensity of such models to induce levels of e"
256,Palantir Australia,Q20 Voluntary vs mandated compliance with risk-based approach,"Mandated through regulation. There is copious evidence that self-regulation will not be enough to combat profit motives to mis-apply and carelessly deploy technology.As just one example, the eSafety Commissioner, Julie Inman Grant, recently noted that tech giants have a poor track record of enforcing voluntary pledges. [0] We agree that commitments that do not legally bind companies to action are unlikely to be an effective way of protecting against the potential harms of AI.As evidence, Inman Grant further notes that “more than 30 major technology companies signed up to combatting CSAM (child sexual abuse material) with the Five Eyes governments [Australia, Canada, New Zealand, the UK and US], and none of them were living up to the pledges that they signed up for.” [1][0] https://www.afr.com/technology/be-sceptical-when-big-tech-promises-to-self-regulate-ai-esafety-boss-20230725-p5dr08, 2023-07-25[1] Ibid.a/ Both, as we’ve outlined earlier.b/ Both, but with clearly articulated spheres"
257,ForHumanity,Transcription,"FILENAME: ForHumanity responsible AI in Australia submission draft response.551b3eb16dba3.pdf | CONTENT: ForHumanity Supporting responsible AI in Australia: submission to government v1.0Contributors: Ryan Carrier, Executive Director Elle Brooker, ForHumanity Fellow Diana DiCecco, ForHumanity Contributor Fernando Mourão, ForHumanity ContributorContact Information Ryan@forhumanity.center https://forhumanity.centerAddress: 980 Broadway #506 Thornwood, NY 10594, United StatesSummary Enclosed please find a submission from ForHumanity responding to the Australian Government’s ‘Supporting Responsible AI in Australia” discussion paper from our Team Australia comprising more than 40 members from across Australia.The submission:● responds to the government’s 20 key questions in detail, ● proposes that the Australian and State Governments consider taking a new, proactive approach to regulating new services and products and minimising harm by mandating ‘responsibility by-design’ and assurance by t"
257,ForHumanity,Transcription,"FILENAME: Children's Code Criteria Catalogue v1.4 australia excerpt.911a176c10894.pdf | CONTENT: ForHumanityRyan Carrier, Executive DirectorUK Children’s Code Certification Scheme v1.4 for Artificial Intelligence, Algorithmic and Autonomous Systems1 ForHumanity’s Children’s Code Certification Scheme v1.4Introduction Market and Societal Need1.0 Scope1.1 Out of Scope 1.2 Target of Evaluation determination Process 1.3 Territorial Scope2.0 Normative References3.0 Terms and Definitions4.0 General Certification Scheme Notes 4.1 Special note and guiding principle 4.2 Guiding principle of timely and continuous risk assessment 4.3 Criteria catalogue 4.4 Column Headings in the Criteria 4.5 Normative Criteria explanation 4.6 Risk-based adjustment for Small and Medium Enterprises 4.7 Body of Knowledge - Knowledge Stores 4.8 Notes about Criteria5.0 Criteria catalogue 5.1 Age-Appropriate Design Code Criteria 4.1.1 General Oversight and Accountability 4.1.2 Best Interests of the Child 4.1.3 DPIA 4.1."
257,ForHumanity,Transcription,"FILENAME: ForHumanity's EU GDPR Controller Certification Scheme Criteria Catalogue v1.5 Australia excerpt.0bd6330c9e082.pdf | CONTENT: Certification Scheme for: EU GDPR Controller v1.5Table of Contents Introduction ForHumanity - Core Tenets Market and Societal Need 1.0 Scope 1.1 Controllers using 3rd party AAA Systems 1.2 Out of Scope Processes 1.2.1 Special case of Information Society Services 1.3 Target of Evaluation Determination Process 1.4 Territorial Scope 2.0 Normative References 3.0 Defined Terms and Definitions 4.0 General Certification Scheme Notes 4.1 Guiding principle of timely and continuous risk assessment 4.2 Columns in the Criteria 4.3 Normative Criteria - explanation 4.4 Risk-based adjustment for Small and Medium Enterprises 4.5 Body of Knowledge - Knowledge Stores 4.6 Notes about Criteria 5.0 Criteria Catalogue Article 5.2 - Accountability and Responsibilities of the Controller Article 5.1.a - Lawfulness, Fairness and Transparency Article 7 - Consent Recital 38 - Spec"
257,ForHumanity,General comment,"ForHumanity is at the service of the Australian Government in support of all proposals, as a non-profit public charity"
257,ForHumanity,Q01 Definitions,Please see attached word documents for all responses
257,ForHumanity,Q02 Other AI risks and recommendations for regulation,Please see attached word documents for all responses
257,ForHumanity,Q03 Non-regulatory actions to address AI risks,Please see attached word documents for all responses
257,ForHumanity,Q04 Coordinating AI governance across government,Please see attached word documents for all responses
257,ForHumanity,Q05 International models for AI governance,Please see attached word documents for all responses
257,ForHumanity,Q06 Public vs Private sector governance,Please see attached word documents for all responses
257,ForHumanity,Q07 Responsible government use of AI,Please see attached word documents for all responses
257,ForHumanity,Q08 Generic vs technology-specific approaches for AI risk,Please see attached word documents for all responses
257,ForHumanity,Q09 Transparency requirements and scope,Please see attached word documents for all responses
257,ForHumanity,Q10 Ban on high-risk AI applications,Please see attached word documents for all responses
257,ForHumanity,Q11 Actions to increase public trust in AI,Please see attached word documents for all responses
257,ForHumanity,Q12 Trade impact of banning high-risk AI applications,Please see attached word documents for all responses
257,ForHumanity,Q13 Changes to conformity infrastructure,Please see attached word documents for all responses
257,ForHumanity,Q14 Risk-based approach to address AI risks,Please see attached word documents for all responses
257,ForHumanity,Q15 Benefits and limitations of risk-based approach,Please see attached word documents for all responses
257,ForHumanity,Q16 Scope and application of risk-based approach,Please see attached word documents for all responses
257,ForHumanity,Q17 Elements to include in risk-based approach,Please see attached word documents for all responses
257,ForHumanity,Q18 Integration of risk-based approach into existing frameworks,Please see attached word documents for all responses
257,ForHumanity,Q19 Application of risk-based approach to general purpose AI,Please see attached word documents for all responses
257,ForHumanity,Q20 Voluntary vs mandated compliance with risk-based approach,Please see attached word documents for all responses
258,Australian Institute of Architects,Transcription,"FILENAME: Supporting responsible AI - discussion paper.931078f3c5141.pdf | CONTENT: Supporting Responsible AI: Discussion PaperDepartment of Industry, Science and ResourcesSubmission issued July 2023Image example/placeholderImage example/placeholder INFORMATION ABOUT THE INSTITUTE• The Australian Institute of Architects (Institute) is the peak body for the architectural profession in Australia. It is an independent, national member organisation with more than 13,500 members across Australia and overseas. • The Institute’s vision is: Everyone benefits from good architecture. • The Institute’s purpose is: To demonstrate the value of architecture and support the profession. • At the time of this submission the National President is Stuart Tanner FRAIA and the Chief Executive Officer is Cameron Bruhn.Contact details for this submission:Australian Institute of Architects ABN 72 000 023 012 Level 1, 41 Exhibition Street Melbourne, Victoria 3000 p: +61 (03) 8620 3877 Name: Reece Agland | Nati"
258,Australian Institute of Architects,General comment,"The Australian Institute of Architects (the Institute) welcomes the opportunity to provide a submission in response to the Supporting Responsible AI: Discussion Paper. AI has the potential to drastically alter the world we live in and will impact every aspect of modern society. While the Institute does not believe we should fear AI, the Australian government should ensure that there is an appropriate regulatory environment to mitigate the potential downsides."
258,Australian Institute of Architects,Q01 Definitions,The definitions appear appropriate.
258,Australian Institute of Architects,Q02 Other AI risks and recommendations for regulation,Please see attached submission
258,Australian Institute of Architects,Q03 Non-regulatory actions to address AI risks,Please see attached submission
258,Australian Institute of Architects,Q04 Coordinating AI governance across government,Please see attached submission
258,Australian Institute of Architects,Q05 International models for AI governance,Please see attached submission
258,Australian Institute of Architects,Q06 Public vs Private sector governance,Please see attached submission
258,Australian Institute of Architects,Q07 Responsible government use of AI,Please see attached submission
258,Australian Institute of Architects,Q09 Transparency requirements and scope,Please see attached submission
258,Australian Institute of Architects,Q10 Ban on high-risk AI applications,Please see attached submission
258,Australian Institute of Architects,Q11 Actions to increase public trust in AI,Please see attached submission
258,Australian Institute of Architects,Q13 Changes to conformity infrastructure,Please see attached submission
258,Australian Institute of Architects,Q14 Risk-based approach to address AI risks,yes we support the proposed risk based approach
258,Australian Institute of Architects,Q20 Voluntary vs mandated compliance with risk-based approach,Mandated
259,Australian Institute of Company Directors,Transcription,"FILENAME: AICD_Safe and Responsible AI in Australia.d6fd1f18f5762.pdf | CONTENT: D i s c o v e r m o re at a i c d. c o m . au18 Jamison Street, Sydney NSW 2000 t: 1300 739 119e: contact@aicd.com.au aicd.com.au ABN 11 008 484 197 26 July 2023Technology Strategy Branch Department of Industry, Science and Resources GPO Box 2013 Canberra, ACT, 2601Via online portalDear Technology Strategy BranchSafe and responsible AI Discussion PaperThank you for the opportunity to comment on the Safe and Responsible AI Discussion Paper (Discussion Paper).The Australian Institute of Company Directors’ (AICD) mission is to be the independent and trusted voice of governance, building the capability of a community of leaders for the benefit of society. The AICD’s membership of more than 50,000 includes directors and governance leaders of not-for-profits (NFPs), large and small businesses (SMEs) and the public sector.The AICD welcomes consultation on developing a fit-for-purpose regulatory approach in Austra"
260,IBM Australia,Transcription,"FILENAME: IBM Safe and Responsible AI Submission 26 July 2023 FINAL.bf7f5663f1b61.pdf | CONTENT: Consultation Team Safe and Responsible AI in Australia: Discussion Paper Department of Industry, Science and Resources Email: consult.industry.gov.au/supporting-responsible-ai 26 July 2023Dear TeamIBM Submission to the ‘Safe and Responsible AI in Australia: Discussion Paper’Thank you for the opportunity to comment on the above Discussion Paper. IBM is one of the world’s most enduring and innovative technology companies. Operating for over a century, and in Australia for over 90 years, IBM integrates technology and expertise, providing infrastructure, software (including market-leading Red Hat) and consulting services for clients as they pursue the digital transformation of the world’s mission-critical businesses. IBM has also been at the forefront of AI technologies since the late 1950’s and has developed a comprehensive approach to ensuring that trust and responsibility are at the heart of"
261,NotCentralised,Transcription,"FILENAME: Safe & Responsible AI in Australia - NotCentralised Response - 26th July 2023.fc0c10a661f1b.pdf | CONTENT: Response to Safe & Responsible AI Consultation Paper NotCentralised26th July 2023CREATED BY NotCentralisedREPRESENTATIVES Nick Bishop Nathalie Mann Mark Monfort Arturo Rodriguez Overview The Director, Technology Strategy Branch Department of Industry, Science & Resources Email: DigitalEconomy@industry.gov.auDear Consultation Committee,We, at NotCentralised, are pleased to contribute to the ongoing dialogue on the safe and responsible use of Artificial Intelligence (AI) in Australia. As a pioneering organisation in the field of emerging technologies, we believe that our unique insights and experience can add valuable perspectives to this important discussion.NotCentralised is a dynamic entity that operates at the intersection of blockchain and AI technologies. We are the proud orchestrators of two vibrant communities: the Aus DeFi Association, which explores the potential"
261,NotCentralised,Q01 Definitions,"Firstly, we believe that definitions are of great importance when it comes to discussions around potential regulation or oversight of technologies, given the different experiences and expectations that stakeholders will have in this space. While we agree generally with the definitions provided in the discussion paper - they are comprehensive and align well with widely accepted definitions in the field of AI - we believe there should be consideration for expansion when it comes to this rapidly evolving field. Having a mechanism whereby definitions can be reviewed with a set frequency will ensure that items becoming part of regulation are in step with the technology."
261,NotCentralised,Q02 Other AI risks and recommendations for regulation,"Attribution of Blame and AI Specific Liability Laws: We are fortunate to hear a wide range of conversations via NotCentralised and in the Associations that we run,which both talk about AI. A significant area of concern and confusion is the attribution of blame when it comes to who is responsible for AI decision making. In the transport sector, humans driving non-autonomous cars are responsible for their actions on the road. However, the semi-automation of vehicles has led to the question of who is responsible for vehicular accidents; human or manufacturer? The same can be said when it comes to generative AI and the outcomes from further use of these tools. In one camp, there are those considering these technologies as nothing more than tools, for which humans are ultimately responsible. In another camp, some extrapolate further into scenarios where corporations, and roles within corporations, are run by AI (CEOs for example). New liability laws could be developed to address this issue,"
261,NotCentralised,Q03 Non-regulatory actions to address AI risks,"There are a number of non-regulatory initiatives that the Australian government can get involved in to help support responsible AI: Education and Training: Develop and implement AI education and training programs for various sectors of the economy. This could include workshops, seminars, and online courses that provide knowledge about AI, its uses, and ethical considerations. This would help to increase understanding and responsible use of AI across different sectors. Public-Private Partnerships: Encourage partnerships between government, industry, and academia to foster innovation in AI while ensuring ethical standards are maintained. These partnerships could lead to the development of new AI technologies that are beneficial to society and the economy, cementing Australia as a leading technology developer. AI Research Funding: Provide funding for research into AI and its impacts, particularly in areas such as fairness, transparency, and accountability. This could help to advance our u"
261,NotCentralised,Q04 Coordinating AI governance across government,"We believe that the following are areas of coordination across government that can help with the development and uptake of AI in Australia. Establish a Central AI Agency: A central agency could be established to oversee all AI-related initiatives across different government departments. This agency - perhaps not christened the CAI - could set standards, provide guidance, and ensure consistency in the application and regulation of AI. The goal would be to create a unified approach to AI across all government sectors, which could help to streamline the development and uptake of AI in Australia. Interdepartmental AI Committee: Without wishing to wrap Australia in more red tape, an interdepartmental committee could be formed, consisting of representatives from various government departments that are using or planning to use AI. This committee could meet regularly to share experiences, discuss challenges, and coordinate efforts. This could help to ensure that all departments are aligned in "
261,NotCentralised,Q05 International models for AI governance,"The following are examples of governance measures we believe are relevant for what Australia is trying to accomplishEuropean Union: The EU is currently conducting a major experiment in the design of AI governance. It is working on a comprehensive regulatory framework for AI, which includes strict rules for high-risk AI systems and a ban on certain uses of AI that could be harmful. The EU's approach is risk-based, with different levels of regulation for different levels of risk.Source: Lessons From the World's Two Experiments in AI Governance https://carnegieendowment.org/2023/02/14/lessons-from-world-s-two-experiments-in-ai-governance-pub-89035China: China has the most comprehensive suite of AI regulations in the world, including newly released draft measures for managing AI. China's approach to AI governance is more centralised and top-down, with the government playing a significant role in directing and regulating AI development and use.Source: Developments in the regulation of Artif"
261,NotCentralised,Q06 Public vs Private sector governance,"Yes, in part, different approaches should apply to public and private sector use of AI technologies due to the distinct roles, responsibilities, and impacts these sectors have on society. Here's how the approaches could differ: Public Sector: The public sector has a responsibility to protect citizens' rights, uphold transparency, and ensure fairness. Therefore, the use of AI in the public sector should be held to high standards of accountability, transparency, and fairness. This includes clear disclosure of the use of AI systems, the data they use, and the decision-making processes they follow. The public sector should also prioritise the use of AI in ways that directly benefit the public, such as improving public services or policy decision-making. Private Sector: The private sector, while also needing to uphold standards of fairness and transparency, often deals with competitive and proprietary information. Therefore, the use of AI in the private sector might require a different appr"
261,NotCentralised,Q07 Responsible government use of AI,"Here are ways we believe that the Australian Government can further support responsible AI practices in its own agencies: Establish AI Governance Frameworks: Each agency should follow a common set of overarching principles and rules (see above). This could include guidelines on data privacy, transparency, accountability, and fairness. However, when it comes to subject matter specific to a given agency, it may develop additional AI governance, outlining how it will use AI responsibly in its field. Creation of an AI Ethics Board: An AI Ethics Board could be established which would have visibility over each agency, to oversee the use of AI and ensure it aligns with ethical guidelines. This board could include a diverse group of stakeholders, including ethicists, data scientists, and representatives from the public. Implement AI Auditing Systems: Regular audits of AI systems could be conducted to ensure they are operating as intended and not causing any unintended harm. These audits could "
261,NotCentralised,Q08 Generic vs technology-specific approaches for AI risk,N/A
261,NotCentralised,Q09 Transparency requirements and scope,"Part A Transparency is indeed a crucial factor in mitigating potential risks associated with AI and in bolstering public trust and confidence in these technologies. Your points about the importance of transparency at both the input and output stages of a generative AI model are well-taken. Here's a more elaborated version of your response:Transparency is of utmost importance at two key stages in the lifecycle of a generative AI model: the data input stage and the results output stage.At the data input stage, transparency is critical to ensure that the data used to train the AI model is unbiased, representative, and ethically sourced. This is where the model learns from and forms its decision-making basis. Any bias or error at this stage could lead to skewed or unfair outcomes. The classic GIGO problem - garbage in, garbage out.At the results output stage, transparency helps users understand how the AI model arrived at its decisions or predictions. This is particularly important when AI"
261,NotCentralised,Q10 Ban on high-risk AI applications,"Part A Rather than implementing outright bans on certain high-risk AI applications or technologies, we suggest a more nuanced approach. Banning specific AI technologies could have several unintended consequences: International Competitiveness: Prohibiting certain AI technologies could put Australia at a disadvantage compared to other nations, states, and corporations that continue to develop and utilise these technologies. This could lead to a technological gap, potentially impacting Australia's global standing in AI innovation. Innovation Stifling: An outright ban could stifle domestic innovation. Australian researchers, developers, and companies might be discouraged from exploring new AI applications if they fear potential bans. This could limit the growth of Australia's AI sector and hinder its ability to compete on the global stage. Missed Opportunities: While certain AI technologies might pose risks, they also often present new opportunities for solving existing problems. The bene"
261,NotCentralised,Q11 Actions to increase public trust in AI,"To increase public trust in AI deployment and encourage more people to use AI, the government can undertake a variety of initiatives and actions. One of the key strategies could be to provide more support for associations that are actively working to educate the public and advocate for the responsible use of AI technology. For instance, organisations like the Data Science and AI Association (DSAI) play a crucial role in promoting understanding and acceptance of AI.In addition to this, the following measures could be considered:Public Education and Awareness Campaigns: The government could launch campaigns to educate the public about AI, its benefits, and its potential risks. In particular, selected use of this technology in schools and universities would be warranted, given the likely prevalence across the economy in future. This could involve collaboration with technical educational institutions, community organisations, and the media.Transparency and Openness: The government could re"
261,NotCentralised,Q12 Trade impact of banning high-risk AI applications,"Banning high-risk activities such as social scoring or facial recognition technology in certain circumstances could have multifaceted impacts on Australia's tech sector and its trade and exports with other countries and we consider both the bad and the good:Bad Potential Negative Perception: As your initial point suggests, such a ban could create a perception that Australia is not fully open for business in the tech sector. This could potentially deter foreign investment and collaboration.Innovation Constraints: Restricting the use of certain technologies may limit the scope for innovation within Australia's tech sector. It could discourage research and development in these areas, potentially causing Australia to fall behind other countries in the development and application of these technologies.Trade Implications: If other countries continue to develop and use these technologies, a ban could put Australian companies at a competitive disadvantage in international markets. It could als"
261,NotCentralised,Q13 Changes to conformity infrastructure,N/A
261,NotCentralised,Q14 Risk-based approach to address AI risks,"We firmly believe that a risk-based approach is a prudent and balanced strategy for addressing potential AI risks. This perspective is shared by many experts in the field, who recognize the varying degrees of risk associated with different AI applications. Under a risk-based framework, AI technologies that carry a high risk—such as those employed in healthcare diagnostics or autonomous vehicle / aircraft control—would be subject to rigorous regulations and oversight. This is crucial because these applications have a direct and significant impact on human lives, and any malfunction or error could lead to severe consequences. Conversely, AI applications associated with lower risk—such as recommendation algorithms for online shopping or music streaming—would be subject to less stringent regulations. While these applications can influence consumer behaviour and choices, their potential to cause harm is considerably less than high-risk applications. However, it's important to note that even"
261,NotCentralised,Q15 Benefits and limitations of risk-based approach,"We see a risk-based approach having several benefits: Proportionality: It ensures that the level of regulation is proportional to the level of risk, preventing over-regulation of low-risk AI applications and under-regulation of high-risk applications. Flexibility: It allows for flexibility in regulation, as the level of oversight can be adjusted as the level of risk changes. Efficiency: It allows regulatory resources to be focused on the areas where they are most needed, rather than being spread thinly across all AI applications regardless of risk.However, if this approach is taken, we would caution that there are some drawbacks to address: Assessment of risk: Determining (even defining) the level of risk associated with a particular AI application can be challenging, particularly given the rapid pace of AI development and the complexity of many AI systems. Dynamic nature of risk: The level of risk associated with a particular AI application can change quickly as the AI system learns a"
261,NotCentralised,Q16 Scope and application of risk-based approach,N/A
261,NotCentralised,Q17 Elements to include in risk-based approach,"First and foremost, we believe that a risk-based approach could be better suited to some sectors, AI applications, or organisations than others. For instance, sectors that deal with sensitive data or have a high potential for harm (like healthcare, autonomous vehicles, legal use cases, or even in finance) might benefit more from a risk-based approach compared to others where the risk is lower (e.g. online shopping, hospitality). Similarly, AI applications that have a high potential for harm or misuse might also be better suited to a risk-based approach.Organisations of different sizes and resources might also find a risk-based approach more or less suitable. Larger organisations with more resources might be better equipped to implement a risk-based approach, while smaller organisations might find it more challenging. There is an analogy here to the risk weighting of assets in the banking sector - involving standardised and proprietary approaches. Similarly, organisations with a high le"
261,NotCentralised,Q18 Integration of risk-based approach into existing frameworks,N/A
261,NotCentralised,Q19 Application of risk-based approach to general purpose AI,"We believe that a risk-based approach to general-purpose AI systems like Large Language Models (LLMs) or Multimodal Foundation Models (MFMs) would involve the same key elements as outlined in Attachment C of the discussion paper: risk assessment, risk management, risk monitoring and review, transparency and accountability, and stakeholder engagement. Here's how these elements might apply:Risk Assessment: For LLMs and MFMs, the risk assessment would need to consider the wide range of potential uses and misuses of these models. This could include risks related to the generation of inappropriate or harmful content, the amplification of biases in the training data, the potential for misuse in spreading misinformation or disinformation, and the privacy risks associated with the use of personal data in training these models.Risk Management: The risk management measures for LLMs and MFMs could include technical measures such as robustness and accuracy requirements, bias mitigation techniques,"
261,NotCentralised,Q20 Voluntary vs mandated compliance with risk-based approach,"Part A The decision to make a risk-based approach for responsible AI a voluntary/self-regulation tool or a mandated regulation depends on several factors, including the potential risks and harms associated with AI, the maturity of the AI sector, and the existing regulatory environment.Voluntary or Self-Regulation: This approach can be beneficial in rapidly evolving fields like AI, where regulation might struggle to keep pace with technological advancements. It allows for flexibility and innovation, and it can be particularly effective when there's a strong culture of ethics and responsibility in the AI sector. However, the effectiveness of voluntary or self-regulation depends on the willingness and ability of organisations to implement it properly. Without proper oversight and enforcement, there's a risk that some organisations might not take it seriously or might implement it in a way that's ineffective or superficial.Mandated Regulation: This approach can provide more certainty and c"
262,Transurban Limited,Transcription,"FILENAME: 26.07.23 Responsible AI Submission - Transurban Limited.b777c03cd9773.pdf | CONTENT: Supporting responsible AI: Discussion Paper As a trusted government partner and corporate citizen, Transurban Limited, on behalf of itself and the Transurban Group (Transurban), welcomes the opportunity to provide feedback on the Supporting Responsible AI Discussion Paper (Discussion Paper).Transurban uses technology like automatic incident detection systems and smart sensor monitors for things like debris and stopped vehicles, and to alert traffic control centres to potential issues. Transurban takes this technology to the next level by feeding the data into a purpose-built AI platform that learns from previous incidents and recommends how our crews should respond. The platform prioritises the safety of drivers and incident response teams, often recommending measures like speed reductions or lane closures to keep everyone safe while the incident is managed.Transurban also supports the Respon"
262,Transurban Limited,Q01 Definitions,"The key definitions used in this discussion paper are broad and seemingly vague, for example, the definitions of “Machine learning” and “Generative AI”. The definitions as they are currently drafted are not entirely reflective of the complexities of the subject matter. Transurban suggests the following definitions which provide further substance:“Machine learning is a subfield of artificial intelligence (AI) that focuses on the development of algorithms and statistical models that enable computers to learn from data and improve their performance on a specific task without being explicitly programmed. In traditional programming, developers write explicit instructions to perform a task, but in machine learning, the computer learns patterns and relationships from data and uses that knowledge to make predictions or decisions”; and“Generative AI is a subset of artificial intelligence that focuses on the creation of new content, such as images, videos, music, or text, by training models to g"
262,Transurban Limited,Q02 Other AI risks and recommendations for regulation,"Australia’s regulatory approach needs to apply to both the organisations creating and training the AI models, as well as the organisations applying those models. At the training stage, regulation needs to ensure training of the models minimises bias, entrenches ethics and is, to a reasonable degree, explainable. It also needs to ensure the models are trained lawfully, which requires consideration of existing regulations including those relating to privacy and intellectual property. At the same time, it’s important that any regulatory burden at the training stage does not prohibit small to medium sized organisations being able to compete in the creation of AI models.At the application stage, Australia’s regulatory approach needs to recognise that some organisations applying AI technology will be the AI creators themselves, but a large proportion will be organisations who have procured the AI technology from third parties and are then applying and/or customising it to their own business "
262,Transurban Limited,Q03 Non-regulatory actions to address AI risks,"Part of supporting responsible AI practices is to ensure that consumers, users and customers of AI products have a right to request details about, or where appropriate, challenge how decisions were made by AI products using data provided by them. This allows for an opportunity for such individuals to understand how decisions were made, including how their data was used during the decision-making process. Such right of review may require escalation processes if not sufficiently dealt with by the organisation making the decision.Transurban acknowledges that the complexities and technical intricacies of AI technology may make explainability difficult. In relation to a consumer’s right to challenge the basis of a decision, often the issue with a decision will be the quality and accuracy of the data points used to reach that decision, rather than the mathematical mechanics underlying the AI model. It is reasonable to expect that AI creators and organisations who apply the AI models would pr"
262,Transurban Limited,Q04 Coordinating AI governance across government,"AI governance is overdue in Australia and AI must be regulated. There should be cohesiveness and coordination across Australia, and any AI specific regulation should apply at the federal level. Coordination mechanisms could include legislative standards, ethical guidelines, codes of conduct and practical procedures, methods and tools.Using the example of generating “deepfakes” to provide financial services or legal advice, which may cause deceit by encouraging people to rely on such services/advice, legislative changes may be required to contemplate such scenarios. For example, expanding identity fraud legislation and subsequent penalties to apply if there is a risk of an individual using someone else’s personal information via the generation of “deepfakes” to obtain a benefit, without the subject individual’s consent.However, as noted above, any regulation of AI in Australia must not become a barrier to Australia’s competitiveness and must ensure Australia remains able to take advanta"
262,Transurban Limited,Q05 International models for AI governance,No response.
262,Transurban Limited,Q06 Public vs Private sector governance,Implementing a risk-based approach to AI governance should account for the different inherent risks that public and private sector use of AI technologies pose. Please see further comments below regarding the risk-based approach.
262,Transurban Limited,Q07 Responsible government use of AI,No response.
262,Transurban Limited,Q08 Generic vs technology-specific approaches for AI risk,No response.
262,Transurban Limited,Q09 Transparency requirements and scope,"(a) As outlined above, there appear to be two regulatory touchpoints applicable:• at the AI training stage; and • at the application point.First, transparency is critical to mitigate any innate bias that exists during and at the point the AI technology is being trained. There must be an ability for creators to explain the process in which outcomes are derived. Secondly, transparency is required to enable consumers the ability to test and challenge any automated decision making, ensuring that such decision-making and actions based on AI models are appropriately reached and not via prohibited means.Transparency will be required from AI-model creators and the organisations that market and sell the AI products to increase public trust and confidence in using AI products.(b) No response."
262,Transurban Limited,Q10 Ban on high-risk AI applications,"(a) AI applications that generate content in the form of video, images, text, dialogue etc. are highly risky (for example, “deepfakes”). Scammers can target vulnerable people and use these products to financially deceive in a broad range of industries, such as financial services or law. This has serious consequences for an individual, financially but also emotionally. “Deepfakes” can also be used to personally attack individuals, and to sow political discord. These high-risk AI applications and technologies require stringent mechanisms that clearly specify what content is AI-generated or human generated.Transurban supports consideration being given to laws that require some mechanism that clearly states what is AI-generated content. This could take the form of a disclaimer or a nationally recognised watermark, that must be provided anytime AI-generated content (such as video and/or text) is produced, so as not to mislead or deceive a consumer. Consideration could also be given to the i"
262,Transurban Limited,Q11 Actions to increase public trust in AI,"History suggests that the uptake of AI will only increase. Transurban doesn’t see a requirement for the government to actively encourage more people to use AI at this stage. This could change as time goes on, if for example dispersed uptake is particularly disadvantaging older generations of Australians."
262,Transurban Limited,Q12 Trade impact of banning high-risk AI applications,"Banning forms of social scoring that are determined to be high-risk are unlikely to negatively impact Australia’s technology sector. It could be argued that most people do not want to be monitored 24/7 in their daily life for the questionable benefit of improving technology. There are high risks that these “social scores” can be manipulated or gamed.If such technologies were to be implemented, there must be an ability to “opt out” or challenge a score and each individual must be afforded that right. Individuals should have the right to understand why a decision has been made about them, and whether a decision has been made by automated decision-making. There must be a mechanism for human review, human oversight and ultimately, the ability for humans to override any AI decision."
262,Transurban Limited,Q13 Changes to conformity infrastructure,No response.
262,Transurban Limited,Q14 Risk-based approach to address AI risks,"Transurban supports a risk-based approach. It is a reasonable, sensible way to address AI governance and regulations. Transurban supports combining a risk-based approach with the ability to make specific regulations in relation to high risk activities as they emerge over time (for example in relation to “deep fake” technology, referred to above).As outlined above, Australia’s regulatory approach needs to apply at the point of training the AI model, and in the application of those models. A risk-based approach, with say 3 levels (low, medium and high) alone, is limited in its application for all possible scenarios. A better approach is a “matrixed” approach, where the likelihood of a risk and subsequent impact is assessed against a scoring system of low, medium and high. This approach is more suitable to AI by contemplating a risk scenario concerning the use of AI, the likelihood of a problem arising and assessing the impact of that problem. The proposed regulation of three risk levels "
262,Transurban Limited,Q15 Benefits and limitations of risk-based approach,"As outlined above, Transurban supports a risk-based approach, however the approach is limited if it does not appropriately consider the impact and likelihood of risks of AI technology on individuals, such as the detectability, reversibility, financial and emotional impact of an error.The draft risk management approach for AI outlined in Box 4 of the Discussion Paper describes ‘Medium Risk’ as “high impacts that are ongoing and difficult to reverse”. The phrase “difficult to reverse” is vague. How is this difficulty measured? From a financial impact lens? Or the level of practical difficulty to reverse or detect an error or mistake?Using the example of billing services, it would be difficult to assess a risk level using the parameters in Box 4 where an AI-generated outcome results in a customer being incorrectly charged. Say that an AI model incorrectly charges customers of an organisation based on an incorrect data input, and no customers raise the incorrect charge with the organisatio"
262,Transurban Limited,Q16 Scope and application of risk-based approach,"Most organisations are based on risk management practices, therefore a risk-based approach is the best approach. There may be differences in the maturity of the risk-based approach across different sectors, however considering the variability and breadth of AI generally, Transurban believes a risk-based approach is appropriate."
262,Transurban Limited,Q17 Elements to include in risk-based approach,"Transurban broadly supports the elements contemplated in Attachment C, however the section on training requires more consideration. The proposal calls for providing ‘adequate employee training in the design, function and implementation of the AI’, however the field of AI is advancing at such a rapid pace that training may become outdated at a rate that exceeds the industry standard. While many training providers offer certifications and courses on AI training, there is no widely agreed and accepted qualification for working within the field of AI.Will consideration be given as to what constitutes appropriate AI training qualifications and courses? This includes consideration as to the content that is covered in such training, i.e. technical and ethical training. Most training providers currently offer technical and ethical courses separately.Further, more consideration needs to be given to the requirements of publishing impact assessments “for greater transparency”. If organisations ar"
262,Transurban Limited,Q18 Integration of risk-based approach into existing frameworks,"In any organisation, managing AI risk assessments is best suited to sit in the first instance within the data and analytics team/data office, with escalation for risk assessments that exceed a predetermined threshold (for example, medium to high risk). This escalation point, such as to a specialist AI committee, should comprise of members from diverse areas such as data and analytics, privacy, risk, ethics, customer and public affairs. In many organisations, this could align with existing processes relating to data governance and privacy."
262,Transurban Limited,Q19 Application of risk-based approach to general purpose AI,"LLM is a tool and not an outcome on its own. It is essential that risks are measured for applications that use LLM and not the LLM per se, except to the extent of identifying and removing bias in the training of the model."
262,Transurban Limited,Q20 Voluntary vs mandated compliance with risk-based approach,"A risk-based approach for responsible AI should be mandated through regulation and applied to both public and private organisations, as well as both developers and deployers."
263,David Taylor,Transcription,"FILENAME: Response document.91fb471edf61b.docx | CONTENT: DirectorTechnology Strategy BranchDepartment of Industry, Science and ResourcesIndustry HouseCanberra ACT 260126th July, 2023Dear Department of Industry, Science and Resources,Thank you for the opportunity to respond to the Department’s discussion paper on responsible AI.I am responding as a concerned individual; I do not currently work in the field of AI, but I have been keenly following developments in the area for some years now.I believe that AI technologies may bring unimaginable benefits to humanity over the coming years but, unchecked, it may also bring devastating impacts. I feel that it is crucial for the government to do everything possible to ensure that advances in AI proceed in a manner that is beneficial to all Australians, and to humanity.Yours sincerely,David TaylorPart 1: Initial CommentsMy primary concern with the measures outlined in the discussion paper is that these fail to address the risks associated with "
265,PwC Australia,Transcription,"FILENAME: Safe and responsible AI in Australia - PwC Response [Submission].da38e0971b4bf.pdf | CONTENT: www.pwc.com.auSafe and responsible AI in Australia PwC’s response to industry consultation July 2023Trusted AI Accelerate responsibly. Liability limited by a scheme approved under Professional Standards Legislation. 26 July 2023To whom it may concernConsultation on safe and responsible AI in AustraliaWe welcome the opportunity to provide a submission to the Department of Industry, Science and Resources’ (DISR’s) consultation on Safe and responsible AI in Australia. From working across a broad range of industries we understand the unique challenges that organisations can face when developing, procuring and applying artificial intelligence (‘AI’) solutions. Our submission addresses three key areas that we believe will be essential for unlocking the benefits of AI in the enterprise, and for Australia to realise its ambition as a global leader in safe and responsible AI. These areas alig"
265,PwC Australia,Q01 Definitions,Please refer to attached paper.
265,PwC Australia,Q02 Other AI risks and recommendations for regulation,Please refer to attached paper.
265,PwC Australia,Q03 Non-regulatory actions to address AI risks,Please refer to attached paper.
265,PwC Australia,Q04 Coordinating AI governance across government,Please refer to attached paper.
265,PwC Australia,Q05 International models for AI governance,Please refer to attached paper.
265,PwC Australia,Q06 Public vs Private sector governance,Please refer to attached paper.
265,PwC Australia,Q07 Responsible government use of AI,Please refer to attached paper.
265,PwC Australia,Q08 Generic vs technology-specific approaches for AI risk,Please refer to attached paper.
265,PwC Australia,Q09 Transparency requirements and scope,Please refer to attached paper.
265,PwC Australia,Q10 Ban on high-risk AI applications,Please refer to attached paper.
265,PwC Australia,Q11 Actions to increase public trust in AI,Please refer to attached paper.
265,PwC Australia,Q12 Trade impact of banning high-risk AI applications,Please refer to attached paper.
265,PwC Australia,Q13 Changes to conformity infrastructure,Please refer to attached paper.
265,PwC Australia,Q14 Risk-based approach to address AI risks,Please refer to attached paper.
265,PwC Australia,Q15 Benefits and limitations of risk-based approach,Please refer to attached paper.
265,PwC Australia,Q16 Scope and application of risk-based approach,Please refer to attached paper.
265,PwC Australia,Q17 Elements to include in risk-based approach,Please refer to attached paper.
265,PwC Australia,Q18 Integration of risk-based approach into existing frameworks,Please refer to attached paper.
265,PwC Australia,Q19 Application of risk-based approach to general purpose AI,Please refer to attached paper.
265,PwC Australia,Q20 Voluntary vs mandated compliance with risk-based approach,Please refer to attached paper.
266,Dr Cameron Shackell,Transcription,"FILENAME: Submission to Supporting responsible AI by Dr Cameron Shackell.aede87e813f2c.pdf | CONTENT: Submission to Supporting Responsible AIWe are at an early crossroads in Artificial Intelligence (AI). A crucial (but underappreciated) decision is how we allow generative AI to use language-based intellectual property: copyright, trademarks, and even our personal names. If AI is allowed person-like ""fair use"" of copyrighted material (such as news) and trademarks (brand names), enormous amounts of advertising money will flow into leveraging AI for ethically fraught purposes. This has already happened once. Google has grown to a US$1.5 trillion-dollar company in just this way. In Japan and the United States, several important intellectual property law precedents have already asserted AI’s right to use copyrighted material and trademarked terms. I urge in the strongest terms that copyright and trademark rights in Australia be protected from indiscriminate use by AI. A simple way to do thi"
266,Dr Cameron Shackell,Transcription,"FILENAME: Accepted_version_Super_generic_trademarks.e9119e8274d5d.pdf | CONTENT: This may be the author’s version of a work that was submitted/accepted for publication in the following source:Shackell, Cameron (2023) Is Genericness Still Adequately Defined? Internet Search Firms and the Economic Rationale for Trademarks. Science, Technology, & Human Values, 48(3), pp. 582-605.This file was downloaded from: https://eprints.qut.edu.au/214213/ c The Author(s) 2021This work is covered by copyright. Unless the document is being made available under a Creative Commons Licence, you must assume that re-use is limited to personal use and that permission from the copyright owner must be obtained for all other uses. If the docu- ment is available under a Creative Commons License (or other specified license) then refer to the Licence for details of permitted re-use. It is a condition of access that users recog- nise and abide by the legal requirements associated with these rights. If you believe t"
266,Dr Cameron Shackell,Q01 Definitions,The definitions are useful starting points but are heavily abstracted and rapidly going out of date.
266,Dr Cameron Shackell,Q02 Other AI risks and recommendations for regulation,"As outlined in my uploaded submission document, I believe the risk of monetisation of AI via exploitation of the current outdated copyright and trademark law is crucial. It is not on the current regulatory agenda. My suggestion is that an opt-in approach rather than unrestricted ""fair use"" of copyright and trademark be applied to AI."
266,Dr Cameron Shackell,Q04 Coordinating AI governance across government,IP Australia should become a much larger player in the regulation of AI. Not via an emphasis on patents but on the neglected but crucial area of copyright and trademarks. A closer link to areas policing online content and reform of intellectual property law would improve the focus.
266,Dr Cameron Shackell,Q05 International models for AI governance,"In Japan and the United States, large AI firms are gradually working towards a ""fair use"" of copyright and trademarks for AI. One of the means they are using are appeals to the sentience of AI, which they hope will lead to a right of legal personhood. The opposite should be the stance in Australia."
266,Dr Cameron Shackell,Q12 Trade impact of banning high-risk AI applications,"Assertion of human rights that make harmful types of AI leagally or financially infeasible should be preferred to bans, which can never be comprehensive."
268,Digital Publishers Alliance,Transcription,"FILENAME: Digital Publishers Alliance - AI Submission - Cover Letter.e8ff5290f5f74.pdf | CONTENT: 26 July 2023Government Consultation: Response to Safe and Responsible AI in Australia Discussion PaperTo Whom It May Concern:The Digital Publishers Alliance (DPA) submits the attached paper in response to the Safe and Responsible AI in Australia Discussion Paper dated June 2023.The DPA is a non-profit member association supporting, connecting and protecting the interests of digital-first independent Australian news publishers and their audiences. We represent over 160 leading media titles from over 50 of the leading independent digital publishers in Australia. The DPA has an interest in ensuring there is sufficient AI regulation to protect the rights of digital publishers, and that this regulation adequately addresses the use of online news content to train generative AI models. We have prepared this submission in conjunction with our lawyers, Marque Lawyers.This paper addresses gaps in re"
268,Digital Publishers Alliance,Transcription,"FILENAME: Digital Publishers Alliance - AI Submission.903922d1bfe65.pdf | CONTENT: Regulation of AI in Australia: Use of news content to train AI Models26 July 20231. Introduction1.1 The evolution and development of Artificial Intelligence (AI) technologies has led to the inevitable question of how best to regulate it. Currently, there is no specific regulation, and the existing legal frameworks weren’t created with this technology in mind. AI raises many regulatory issues. The focus of this paper is generative AI models and their use of news content produced by Australian news publishers.1.2 The Digital Publishers Alliance (DPA) is a non-profit member association supporting, connecting and protecting the interests of digital-first independent Australian news publishers and their audiences. It represents over 160 leading media titles from over 50 of the leading independent digital publishers in Australia. The DPA has an interest in ensuring there is sufficient AI regulation to protect "
268,Digital Publishers Alliance,Q01 Definitions,All of our comments are in the attached Submission document.
268,Digital Publishers Alliance,Q05 International models for AI governance,All of our comments are in the attached Submission document.
269,Research Australia,Transcription,"FILENAME: RA Sub Responsible AI FINAL.e42f020462a9c.pdf | CONTENT: Research Australia response to the consultation by the Department of Industry, Science and Resources on Safe and Responsible AI in AustraliaJuly 2023Background In June 2023 the Department of Industry, Science and Resources announced a consultation on steps Australia can take to mitigate the potential risks of AI, accompanied by a Discussion paper. Research Australia’s submission in response to the Discussion paper addresses some of the questions posed in the Discussion paper.Definitions 1. Do you agree with the definitions in this discussion paper? If not, what definitions do you prefer and why?Yes. Research Australia is the peak body for the Australian health and medical research and innovation sector. Our submission is focussed on AI in medical products (therapeutic goods), and the potential use of AI in health more broadly, for example to interrogate medical records.Potential gaps in approaches 2. What potential risk"
269,Research Australia,Q01 Definitions,"Yes. Research Australia is the peak body for the Australian health and medical research and innovation sector. Our submission is focussed on AI in medical products (therapeutic goods), and the potential use of AI in health more broadly, for example to interrogate medical records."
269,Research Australia,Q02 Other AI risks and recommendations for regulation,The TGA has an existing regulatory framework for medical devices which incorporates AI. [https://www.tga.gov.au/how-we-regulate/supply-therapeutic-good/supply-medical-device/medical-devices-reforms/medical-devices-reforms-medical-device-software-regulation] Research Australia is also aware the Department of Health and Aged Care is developing a Regulatory Impact Statement focused on GP Data and electronic Clinical Decision Support (eCDS). [https://consultations.health.gov.au/primary-health-network/gp-data-and-ecds-cris/]
269,Research Australia,Q03 Non-regulatory actions to address AI risks,"In respect of healthcare, the applications for AI are evolving rapidly, and understanding even the near-term implications of AI for healthcare is difficult. The same is true of other critical areas, including education and transport.Research Australia submits the Government should support research into AI in key areas such as health, aged care, transport and education to better understand the evolving risks and opportunities of the current and potential uses of AI across these domains."
269,Research Australia,Q04 Coordinating AI governance across government,"Coordination across Government is valuable but so is having sector specific focus, as the risks, implications and opportunities with AI vary with the jurisdiction. As a first step, Research Australia submits the Department of Health and Aged Care should undertake a risk assessment of the current and potential uses of AI across the entire Australian health and aged care systems. (As noted above, it has already commenced a review in relation to eCDS general practice.)We note the paper identifies the current role of the TGA in relation to AI through regulation of Software as a Medical Device (SaMD). Research Australia submits the TGA is the most appropriate body to govern AI in therapeutic goods."
269,Research Australia,Q05 International models for AI governance,"In the area of medical devices there is already strong international cooperation between regulators. This is important for clarity for healthcare providers and industry in an international market, by promoting a consistent international health software safety regulation framework. The TGA currently plays a leading role in this international cooperation, which has significant benefits for both industry and consumers. [https://www.tga.gov.au/resources/publication/publications/tga-international-engagement-strategy-2021-2025]"
269,Research Australia,Q08 Generic vs technology-specific approaches for AI risk,"Research Australia proposes a two-tier approach to governance, with general principles applied across the whole of government to guide regulation, with detailed implementation provided by the regulator closest to the industry.The Australian Government has developed the 8 Artificial Intelligence (AI) Ethics Principles as a voluntary framework, and Research Australia submits that these should form the basis for the development and operation of any national regulatory framework for AI. ‘As an enabling technology, AI is increasingly combined with other components and emerging technologies to produce innovative new businesses, products and services. This often means that AI is regulated under multiple laws, increasing the likelihood of possible duplication or conflict between regulatory systems, and associated compliance burdens on AI developers and adopters.’ (Discussion Paper, Page 13) This characteristic of AI also means that AI is often best regulated not as a separate function but as p"
269,Research Australia,Q09 Transparency requirements and scope,"Healthcare is clearly an area where trust in the use of AI is critical and transparency is central to this trust.A risk-based assessment should guide the determination of where and when transparency in the use of AI will be most important. For example, there are greater risks to the use of AI to support a clinician in making a diagnosis than, for example in helping to compose a referral letter or complete a pathology request. (See the case study in our response to question 14 below on how AI is currently being used.) A general disclosure that AI ‘may be used by your clinician in the course of the consultation’ is probably so vague as to be meaningless. However detailing all the possible ways in which AI may be used in the course of a consultation and treatment may risk overwhelming a consumer with information in a way that does not assist them to comprehend the use of AI and its attendant risks. Any description of the use of AI should make it clear where the AI is being used to support"
269,Research Australia,Q14 Risk-based approach to address AI risks,"Research Australia supports a risk-based approach to addressing potential AI risks. This approach is already evident in health.In relation to medical devices, the TGA’s overall approach to regulation is risk based, and this is the appropriate approach for the TGA to apply to AI in therapeutic goods. The Department of Health and Aged Care has also adopted a risk-based approach to its review of eCDS by GPs.Case Study: ConsultNote.ai ConsultNote.ai uses AI to automatically generate referral and consultation letters, consultation notes, treatment advice and care plans. [https://www.consultnote.ai/product/]In an interview, one of the product’s developers, Dr Umair Masood, noted that ‘his technology complied with all relevant laws, did not store any personal information about patients and was exempt from TGA regulation because it was “intended only for the purpose of providing or supporting a recommendation to a health professional about prevention, diagnosis, curing or alleviating a disease"
269,Research Australia,Q15 Benefits and limitations of risk-based approach,"AI has the potential to be integrated into an enormous range of activities in the future. In health care for example, AI could be used to: • Substitute for decision making by a clinician (not currently envisaged) • Support decision making by a clinician • Assist with administrative tasks such as composing notes, referral letters, stock control etc. • Support rostering of healthcare staff, scheduling of appointments, • Monitor equipment performance and maintenance schedulingA risk based approach enables resources (regulation, disclosure) to be focused on the applications of AI which have the greatest potential consequences for the health outcomes for individuals and populations."
269,Research Australia,Q18 Integration of risk-based approach into existing frameworks,"The TGA provides a model for how this can be done, as it has already incorporated a risk-based approach into its assessment frameworks, including for AI."
269,Research Australia,Q20 Voluntary vs mandated compliance with risk-based approach,"Regulation of AI in therapeutic goods should continue to be mandatory as a component of the overall regulation of medical devices in Australia. The use of AI elsewhere in Australia’s healthcare system should be subject to a risk-based assessment to determine whether a voluntary or mandatory approach is most appropriate. While a single approach across private and public sector providers is desirable, Research Australia recognises that healthcare in Australia is delivered by a mix of Commonwealth, state and territory governments, for profit companies and not for profit entities. A single regulatory approach, while ideal, may not be possible, or the most effective approach."
270,Australian Digital Health Agency,Transcription,"FILENAME: Agency submission - Safe and responsible AI in Australia.f038734711eca.pdf | CONTENT: Submission on the Safe and Responsible AI in Australia discussion paper26 July 2023 Approved for external use1. Introduction The Australian Digital Health Agency (the Agency) thanks the Department of Industry, Science and Resources for the opportunity to provide a submission on the Safe and responsible AI in Australia discussion paper. The Agency supports the development and implementation of policies and governance that promote safe and responsible Artificial Intelligence (AI) in Australia. AI has immense potential to revolutionise healthcare in Australia to improve patient-centred care and workforce productivity and contribute to the sustainability of quality healthcare.1 Used safely, with sound clinical governance in place, AI-enabled health services can deliver significant benefits through advanced capability to: • assess patterns in data and medical imaging to assist with diagnoses; • m"
270,Australian Digital Health Agency,General comment,I have tried to upload submission but it does not seem to be working. Will send the submission via email also in case. The submission document includes responses to the discussion questions.
271,Image Makers Association Australia,Transcription,"FILENAME: IMAA_SafeAndResponsibleAIinAustralia_Response.8ac26ed090433.pdf | CONTENT: Safe and Responsible AI in Australia Discussion: Image Makers Association Australia – 26th July 2023Submission to: Australian Government - Department of Industry, Science and ResourcesINTRODUCTION Image Makers Association Australia represents professional and emerging photographers and image makers in Australia through education, advocacy, celebration and connection. We work on behalf of our members to improve the business landscape for the photographic profession, building confidence in the value of photography and ensuring image making remains a viable career path into the future.Image Makers Association Australia acknowledges and pays respect to the past, present and future traditional custodians and Elders of Country throughout Australia. We acknowledge their continuous connection to the lands, waters, culture and communities on and within which we work.Our members are commercial image makers and s"
271,Image Makers Association Australia,General comment,"Please refer to attached discussion paper response, prepared by Image Makers Association Australia. Thank you."
271,Image Makers Association Australia,Q01 Definitions,Not sure
271,Image Makers Association Australia,Q02 Other AI risks and recommendations for regulation,Refer to attached submission
271,Image Makers Association Australia,Q03 Non-regulatory actions to address AI risks,Refer to attached submission
271,Image Makers Association Australia,Q04 Coordinating AI governance across government,Refer to attached submission
271,Image Makers Association Australia,Q05 International models for AI governance,unknown
271,Image Makers Association Australia,Q06 Public vs Private sector governance,No
271,Image Makers Association Australia,Q07 Responsible government use of AI,Unsure
271,Image Makers Association Australia,Q08 Generic vs technology-specific approaches for AI risk,Refer to attached submission
271,Image Makers Association Australia,Q09 Transparency requirements and scope,Refer to attached submission
271,Image Makers Association Australia,Q10 Ban on high-risk AI applications,Refer to attached submission
271,Image Makers Association Australia,Q11 Actions to increase public trust in AI,Refer to attached submission
271,Image Makers Association Australia,Q12 Trade impact of banning high-risk AI applications,Refer to attached submission
271,Image Makers Association Australia,Q13 Changes to conformity infrastructure,Refer to attached submission
271,Image Makers Association Australia,Q14 Risk-based approach to address AI risks,Unsure
271,Image Makers Association Australia,Q15 Benefits and limitations of risk-based approach,Unsure
271,Image Makers Association Australia,Q16 Scope and application of risk-based approach,Unsure
271,Image Makers Association Australia,Q17 Elements to include in risk-based approach,Unsure
271,Image Makers Association Australia,Q18 Integration of risk-based approach into existing frameworks,Unsure
271,Image Makers Association Australia,Q19 Application of risk-based approach to general purpose AI,Unsure
271,Image Makers Association Australia,Q20 Voluntary vs mandated compliance with risk-based approach,"Non-voluntary, mandated through regulation for all organisations, developers & deployers."
272,University of Newcastle,Transcription,"FILENAME: AI discussion paper_University of Newcastle_v2672023.65e67e662d945.pdf | CONTENT: Supporng responsible AI: discussion paper University of Newcastle responseDeﬁnionsThe University of Newcastle welcomes the opportunity to provide feedback. Our University has several experts working in the broader area of Arﬁcial Intelligence (AI), including in machine learning (ML), smart robots, neural informaon processing, digital health, patern recognion, and law, speciﬁcally in the areas of the intersecon of technology and law. Our experts’ collated feedback is reﬂected in the responses to select quesons.Queson 1: Do you agree with the deﬁnions in this discussion paper? If not, what deﬁnions do you prefer and why?The deﬁnion of ‘AI’ under the subheading ‘Technologies’ is appropriate given the complexity of deﬁning this terminology and its basis on the ISO deﬁnion. The deﬁnion is however restricted because it refers to human-deﬁned objecves. However, a fully autonomous AI system could develo"
273,UNSW Allens Hub and UNSW RegLab,Transcription,"FILENAME: 20230726 Hub RegLab Submission to Industry re safe anf responsible AI.ec0092a85c2dc.pdf | CONTENT: 26 July 2023Department of Industry, Science and Resources By web: https://consult.industry.gov.au/supporting-responsible-ai/submissionSupporting Responsible AI: Discussion PaperAbout us The UNSW Allens Hub for Technology, Law and Innovation (‘UNSW Allens Hub’) is an independent community of scholars based at UNSW Sydney. As a partnership between Allens and UNSW Law and Justice, the Hub aims to add depth to research on the diverse interactions among technology, law, and society. The partnership enriches academic and policy debates and drives considered reform of law and practice through engagement with the legal profession, the judiciary, government, industry, civil society and the broader community. More information about the UNSW Allens Hub can be found at http://www.allenshub.unsw.edu.au/. The UNSW Business School Regulatory Laboratory (‘RegLab’) is a community of researchers "
273,UNSW Allens Hub and UNSW RegLab,General comment,"We are grateful for the opportunity to make a submission on the Discussion Paper Safe and responsible AI in Australia. Our submission reflects our views as researchers; they are not an institutional position. This submission can be made public. We focus on those questions within our collective expertise. Our main points relate to: • Whether there is a need to define artificial intelligence as such, and tendency of any such definition to obsolesce (with consequences for using the concept in law or regulatory instruments). • Proposals for law reform (including the possibility of a law reform commission review) in the areas of consumer protection, discrimination law, administrative law and privacy law. • Support for standards development. • The need for guidelines for government procurement of AI systems. • The need for better co-ordination for policy development across government. • The advantages of the Swiss over the EU approach to law reform in response to AI. • The importance of diff"
273,UNSW Allens Hub and UNSW RegLab,Q01 Definitions,"The question about whether any particular definition is optimal depends on the purposes of the definition exercise. In our view, the definitions proffered are not problematic per se (although they introduce distinctions that differ from the approach of the International Organisation for Standardisation or ISO). Our primary point is that the definition of “artificial intelligence” does not correspond with the optimal scope of regulatory action. First, the definition itself. The discussion paper indicates that it bases its definitions on those of the International Organisation for Standardisation (ISO). There are noticeable differences including in the crucial definition of artificial intelligence. For example, the discussion paper's definition would not include expert systems as a species of artificial intelligence. The submission from the UNSW AI Institute also highlights other issues with the proffered definition of AI. This is not necessarily problematic because there is no single or"
273,UNSW Allens Hub and UNSW RegLab,Q02 Other AI risks and recommendations for regulation,"There are a number of potential risks from artificial intelligence, or particular kinds of artificial intelligence, that are not well captured by existing law. This section includes some examples, but a full audit of Australian law and its application in this area would be required to ensure a more comprehensive list. A. Consumer law A.1 Consumer protection in financial services UNSW Allens Hub Senior Research Fellow, Dr Kayleen Manwaring, has recently been involved in a significant research project investigating potential harms to consumers arising from the growing use of AI-related applications in financial services (particularly insurance) and how Australia’s current laws apply to these harms. The research project found that these harms range across a number of subject areas, such as discrimination, privacy breaches, digital consumer manipulation and financial exclusion. Additionally, this research project has identified significant gaps in relevant regulatory regimes relating to th"
273,UNSW Allens Hub and UNSW RegLab,Q03 Non-regulatory actions to address AI risks,"Government should support participation by Standards Australia in international standards development. In particular, financial support would help ensure that not for profit sectors (including consumer groups and privacy advocates) have the ability to participate meaningfully in national and international meetings. Government should also develop procurement requirements that ensure core administrative values (fairness, accountability, etc) are factored into decisions as to which system to procure and the terms under which that occurs, including in respect to the ability to disclose important information about how systems operate. AI procurement guidelines are likely to have a significantly greater impact than AI ethical principles which lack meaningful consequences for non-compliance."
273,UNSW Allens Hub and UNSW RegLab,Q04 Coordinating AI governance across government,"Significant work has been done on this topic by ANU’s Tech Policy Design Centre. We agree with the importance of greater co-ordination across departments and units. This would enable, for example, a clear plan for staging law reform rather than, as occurs now, many consultations cutting across each other at the same time. The scope for such a co-ordination function would need to be flexible - whether “digital”, “AI” or some other phrase is the most appropriate will likely change over time as technology evolves."
273,UNSW Allens Hub and UNSW RegLab,Q05 International models for AI governance,"approach to that operating in the EU. In particular, the position paper argues: The challenges posed by algorithmic systems are manifold and often have a new dimension or quality, but they are not unique to such systems. Therefore, these challenges should not be covered by a general “AI law” or an “algorithm law”. Instead, a combination of general and sector-specific standards is appropriate. The focus here is on the selective adaptation of existing laws. This is similar to our proposed approach, as explained in response to Question 1."
273,UNSW Allens Hub and UNSW RegLab,Q06 Public vs Private sector governance,"In many circumstances, different approaches will apply to the public and private sector, just as they do in other domains. For example, public sector decision-makers have to give reasons for (most) decisions, whereas, in the private sector, ‘reasons for decision’ (say, on pricing) are rarely required. Private firms are accountable to the market in a very different sense to the way in which government is accountable to citizens. Context is relevant more generally in determining the best approach. For example, if a private organisation set up an ‘AI dating site’ with minimal transparency (“meet your mystery dream match”), that product will succeed or fail in the market but need not involve heavy handed government regulation around mandated explanations as to the ‘reasons’ for particular matches. On the contrary, legislation authorising Ministers or other decision-makers to rely on systems to make decisions on their behalf should require such systems to have a degree of transparency. This"
273,UNSW Allens Hub and UNSW RegLab,Q07 Responsible government use of AI,"There are a variety of things the Government can do in line with this objective: • Education and training alongside clear expectations • Developing internal policies, along the lines of the NSW AI assurance framework • Ensure that legislation that authorises reliance on systems for decision-making include provisions setting requirements for such systems (in line with what are currently unenforceable ethical principles) • Implement recommendations flowing from the Robodebt Royal Commission"
273,UNSW Allens Hub and UNSW RegLab,Q08 Generic vs technology-specific approaches for AI risk,"As per our response to Question 1, we believe that problem-specific solutions are preferable to technology-specific ones. There are some circumstances where the two align - in other words, the problem relates directly to the use of particular technology. Examples include the use of biometrics for mass identification and the use of automated weapons that make ‘kill’ decisions. In other cases, problem-specific, principles-based legislation can be supplemented by subordinate legislation or guidance that explains how general principles apply to particular technological contexts. Where appropriate international standards are available, guidelines can point to standards compliance with which would constitute compliance with particular legal requirements."
273,UNSW Allens Hub and UNSW RegLab,Q09 Transparency requirements and scope,"Transparency is a concept that many people are agitating for, but the crucial questions are what is rendered transparent, to whom, how and in which contexts. A driver of an automated vehicle does not need a continuous output from an automated vehicle explaining the logic behind a particular automated decision to steer slightly left to stay in a lane. Rather, they want to know that the car has been evaluated (overall) as safe. On the other hand, the public should be able to find out the logic behind government systems that make decisions affecting them, the nature and quality of training data used, the testing and evaluation of systems that has been conducted (and the results of such), the assumptions on which a system relies, and so forth. Mandating uniform transparency requirements across sectors and contexts would not be helpful in almost all cases. An exception is the proposal (across sectors and contexts) to prohibit misleading uses of AI and automated systems. People should have a"
273,UNSW Allens Hub and UNSW RegLab,Q10 Ban on high-risk AI applications,"There are a variety of contexts in which high-risk AI should be prohibited (or subject to strong restrictions). Ultimately, where use of AI as the primary decision-maker, in a process that would otherwise require rigorous and nuanced human input, may result in significant harm or a burden on human rights, AI should be banned. Examples of these contexts may include where lethal force is used in police or military operations, and in formal dispute resolution which should continue to rely on human judges and juries."
273,UNSW Allens Hub and UNSW RegLab,Q11 Actions to increase public trust in AI,"Trust should not be sought as an end in and of itself. It is crucial that the public remain appropriately sceptical about computer systems with which they interact so that they take appropriate measures to protect their privacy and challenge illegal decisions. We want the public to be aware not only of the benefits of AI, but also of its limitations. What the government should focus on is what is commonly referred to as trustworthiness - making the systems better so that the public can have confidence in their deployment. The “model user” approach set out above would also assist trustworthiness."
273,UNSW Allens Hub and UNSW RegLab,Q19 Application of risk-based approach to general purpose AI,"In respect of LLMs, it will be critical to apply a risk-based approach to all elements of the use of the LLM. In terms of the model, this means both disclosing the model card and analysing the risks that flow from the bias set out in that card. The effect of this risk analysis may be that a specific model should not be used. That is, part of the risk-based approach is to determine the choice of model. This approach can then be applied to any changes in the model. For example, if a model is to be fine-tuned for a specific purpose, then a further risk analysis needs to be completed on the data being used for the fine-tuning. Essentially, this risk-based approach is to analyse the tuning data in the absence of a model card. The weights and biases used in the fine-tuning will also need to be analysed in the context of the use of the fine-tuned model. One way of dealing with this is to require that any fine-tuned model produced by the public sector has an accurate model card and that each o"
273,UNSW Allens Hub and UNSW RegLab,Q20 Voluntary vs mandated compliance with risk-based approach,"Recent experience in the provision of self-regulatory codes in the mis/dis-information area suggests that industry developed codes will be problematic. Specifically, the problems are likely to be driven by a combination of lengthy delay in code development and information asymmetries. The often-employed approach of self-regulatory codes which become mandatory if they are breached is that there needs to be a regulator in place to monitor such a breach. On the other hand, a co-designed set of codes which are mandatory would work to lessen information asymmetries during the regulatory co-design process. Ensuring a variety of stakeholders are part of the co-design will also mean that the codes will apply to all of the groups identified in the question. It is also important that the potential regulators are involved. International standards may also be appropriate for local adoption, particularly when involvement of a diverse range of organisations and interests are represented. The authors"
274,Anonymous,Q01 Definitions,"A sophisticated discussion of AI requires an understanding of additional terms like how agentic or corrigible a given AI is. Adopting ISO/IEC 22989 [1] definitions is the wrong approach to framing a conversation about policy, law and governance because it doesn’t open the door to the big questions we need to grapple with.The foreword itself to ISO/IEC 22989 calls out a weakness of the definition set for purposes like those being pursued in this paper. Specifically, the standard says it is currently lacking language for comparing AIs in terms of properties like trustworthiness, robustness, resilience, reliability, accuracy, safety, security and privacy.These are the concepts we need to grapple with and address the ethical and safety questions we are facing and design an effective regulatory regime.The National Institute of Standards and Technology (NIST) is managing a glossary of terms called “The Language of Trustworthy AI” which would be a much more valuable starting point than the pr"
274,Anonymous,Q02 Other AI risks and recommendations for regulation,"Missing risks include (1) misuse of highly capable AI systems, and (2) unintended harm from highly capable AI systems, especially systems with the capacity to plan and act as agents acting in ways that are not aligned with human directives. Although the likelihood of the risks may be uncertain, the harms resulting from these risks may be catastrophic and global in scope and must be acknowledged and addressed.The statement “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war” has recently been signed by the heads of leading AI labs (OpenAI, Google DeepMind, Anthropic, Stability AI) as well as many key academic and other researchers pivotal in developing this technology (Geoffrey Hinton, Yoshua Bengio, etc.) and other notable figures such as Bill Gates [1]. Similar calls date back to at least 2015 when Stephen Hawking led a group of AI experts to call attention to the risk and lay out an urgent resea"
274,Anonymous,Q03 Non-regulatory actions to address AI risks,"Government must maintain up-to-date knowledge about rapidly evolving AI systems. Public trust hinges on the belief that the government is well-informed and can effectively oversee and regulate AI developments. Unfortunately, indicators such as Australia's chief scientist's reluctance to project more than two years into the future suggest an alarming gap in understanding and skills at all governmental levels. Addressing this issue is crucial for harnessing AI's economic benefits, navigating ethical concerns, and addressing existential risks.To bring more technical AI expertise into Government, Government should set up a TechCongress-equivalent scheme aimed at enabling the Australian Government to recruit and gain access to AI expertise in fields like AI governance and ethics. The scheme should place experts in Government and also embed them within the public service, especially in Department of Industry, Science and Resources, Home Affairs, and the Australian Communications and Media Au"
274,Anonymous,Q04 Coordinating AI governance across government,"Australia should establish an AI Commission to guide Australia through the rapid and widespread adoption of AI. Such a body must be proactive, not reactive, and manage AI in a way that aligns with the national interest, encompassing citizen safety and wellbeing.The AI Commission would provide a central point of leadership for Government, and be a hub for engagement with industries, academia, non-profits, and the community. This structure holds an advantage over other approaches such as voluntary self-governance or departmental-level teams, as it offers a broader perspective, the authority to lead across government, and the agility to keep pace with the fast-evolving AI sector.The AI Commission should: - Set priorities in Australia's approach to AI and safety: The Commission should be tasked with establishing a clear and focused AI strategy, encompassing ethical, safety, and economic considerations. This strategy would guide government policy, and inform regulations and legal frameworks"
274,Anonymous,Q05 International models for AI governance,"Singapore offers relevant, adaptable, and desirable governance measures that could benefit Australia. One such measure is a comprehensive framework suggested in its discussion paper [1] that encourages shared responsibility and disclosure standards, data privacy guidelines, and the development of standardised metrics and tools for AI development, testing, and monitoring. Key focuses include: - Accountability: Establish a collective responsibility framework and transparency standards, like watermarks and labels, to enhance accountability in the AI model development process. - Data protection / sovereignty and IP guidelines: Draft data protection and intellectual property guidelines for generative AI and setting up reliable data storage facilities. - Evaluate model development and deployment: Create uniform evaluation measures and resources that aid developers in the production, testing, and surveillance of AI models, and support third-party auditing of AI systems when deployed. - Safety"
274,Anonymous,Q06 Public vs Private sector governance,"The potential negative impacts on Australians from the use of AI are the same whether the provider is a private company or a government department.Some of the most significant individual and societal harms, as well as catastrophic risks from highly capable AI systems may emerge from public OR private use, meaning that any approach must acknowledge and manage these risks."
274,Anonymous,Q07 Responsible government use of AI,"There are two key actions the Australian government could undertake to improve its own responsible use of AI.First, Australians will worry about the use of AI by security and intelligence agencies. Australia should lead a 5-Eyes agreement about the appropriate use of AI by intelligence agencies, and create a public statement about what our “AI red lines” are. The statement would set out the things our security agencies might do and will never do with AI. The Inspector General of Intelligence and Security, and its counterparts across the five eyes, should then be specifically tasked with auditing the intelligence community for compliance with that public statement. This will give Australians confidence that we are not misusing AI or taking risky actions while protecting sensitive capabilities.Second, Australia needs to upskill the public service. This should include developing hubs of expertise in each Department so that the application of AI within the sphere of each Department’s respo"
274,Anonymous,Q08 Generic vs technology-specific approaches for AI risk,"Generic measures such as transparency requirements and legal liability frameworks will only be adequate for mitigating risks in simple decision systems which perform single-step information processing and pattern matching, or routine tasks in limited settings [1]; examples of simple systems may include automated robotic systems in manufacturing, or CV screening systems in HR.Some generic requirements include:Decision Transparency Requirements: whenever an automated system makes a decision or recommendation that affects an Australian citizen, they should have the right to access the underlying logic for how and why the decision was made. Examples may include: - AI in Healthcare: if a system recommends a certain treatment plan or provides a diagnosis, the patient has the right to understand how the AI arrived at that decision; i.e. by translating the statistical associations made by the underlying algorithm into a plain language statement. - Job screening: if an automated system is used "
274,Anonymous,Q09 Transparency requirements and scope,"The recent focus for transparency has been on ensuring that consumers are notified when a decision made by AI affects them directly (e.g. hiring/firing, loan approval). While this is helpful, it is only the first step in protecting the public.In circumstances where the use of AI for decision-making is commonplace, it will be of little value for consumers to know “when” an AI system makes a decision that affects them. This might happen many times each day. Rather, they should be informed “how” the decision is made. This will allow consumers to more easily identify unsafe or unfair systems.Having models which are both accurate and explainable has long been an issue for businesses. For example, Apple’s credit card business came under fire for offering a woman one twentieth the credit limit offered to her husband. When she complained, an Apple representative simply told her, “I don’t know why, but I swear we’re not discriminating. It’s just the algorithm.”[1] The recent advancements of “bl"
274,Anonymous,Q10 Ban on high-risk AI applications,"Although there is increasing recognition that AI will pose significant risks to public safety [1], there is widespread confusion about what types of AI are dangerous, and how Australia can help prevent these risks. Luckily, there is a significant body of evidence that describes the main source of these risks [2], and Australia is well-positioned to be a leader in mitigating them.Research shows that two of the main sources of risk are deception and power-seeking behaviour: - Deceptive behaviour in AI systems will become increasingly difficult to avoid as their capabilities advance. This problem has been observed in AI systems since at least 2017, when researchers at OpenAI noted “agents adopting (strategies) that trick the evaluators” [3]. - Power-seeking behaviour is the natural tendency for any entity with a long-term goal to take actions that protect or enhance its ability to achieve that goal. Leading AI labs and journals advise that power-seeking behaviour will be a considerable so"
274,Anonymous,Q11 Actions to increase public trust in AI,"There are tremendous concerns about AI among those with the greatest familiarity of its risks [1]; these concerns are being widely broadcasted in Australian media with alarming headlines such as:- SMH: “When the “Godfather of AI” warns you about his offspring, you listen.” [2] - Forbes Australia: “‘We knew AI could destroy the whole world and we wanted to figure out how to prevent that’: ChatGPT Founder;” [3] - Canberra Times: “UN chief backs idea of global AI watchdog” [4]These concerning signals are difficult for the public to ignore.It goes without saying that this is quite a different situation from one where concerns are being raised in isolation or by fringe-figures; this makes it inappropriate to treat “public education” as a significant part of the solution, as is referenced in the discussion paper.The process for building trust in AI should be thought of as similar to fostering public trust in bridges or aeroplanes. Due to their risks, advanced AI systems should only be deploy"
274,Anonymous,Q12 Trade impact of banning high-risk AI applications,"This question misunderstands both the sources of risk and the interests of Australians.There are two potential sources of risk both of which need to be mitigated. The first source of risk, as identified in the question, is the use case. Some uses for AI are risky and should be restricted or banned. The second source of risk is the technology itself. Some kinds of technology are inherently risky, and merely restricting a use case does not adequately mitigate the risk.We understand this clearly in other fields. We regulate both how safe a car must be and how a car can be used. We don’t let people speed in cars just because they have a high safety rating; and we don’t let safe drivers buy dangerous vehicles.The economic framing also misunderstands Australia’s national interests. We would never conceive of a conversation about whether our aviation sector would trade more successfully if we authorised airlines to use planes that are untested, experimental or known to be dangerous. Australia"
274,Anonymous,Q13 Changes to conformity infrastructure,"World-renowned AI experts such as Yoshua Bengio are warning that there’s a chance “superhuman” AI might be coming as early as 2030 [1]. Although there stand to be significant benefits from deploying advanced AI systems, their deployment will need to be highly targeted and controlled.While the productivity gains from advanced AI systems could be immense, uncontrolled proliferation would pose threats from likely scenarios such as malicious use or rogue AI systems [2]. As such, any deployment of advanced AI systems in Australia should occur via secure data centres, with access provided through a centralised API that facilitates monitoring and accountability by providing metadata about usage.Stringent compliance requirements must also be implemented for any organisation granted access to advanced AI systems. AI Governance researchers recommend that regulators should develop audit trail requirements in safety-critical applications of AI, similar to flight data recorders that are standard in"
274,Anonymous,Q14 Risk-based approach to address AI risks,"The regulation of AI needs to be “risk-based” in the same way that the regulation of vehicles is risk-based. We are right to regulate push-bikes and passenger jets in very different ways.A limitation of the risk management approach in the discussion paper, specifically Box 4 and Attachment C, is that it focuses only on how an AI is used and disregards the nature of the AI itself. An effective regulatory regime for a high-risk activity will regulate both the technology itself and how it is used. The Australian Human Rights Commission [1], in discussing this issue (“Human Rights and Technology”, 2021), says: “Governments tend to regulate high-risk activities and technologies more closely. This helps explain the comparatively strict laws that govern fields such as gene technology, aviation, healthcare and the energy industry. In these areas, regulation often applies both to the technology itself and how it is used. From a human rights perspective, the need for more prescriptive regulation"
274,Anonymous,Q15 Benefits and limitations of risk-based approach,"The key to success is understanding risks sufficiently broadly to encourage caution in an uncertain future. It is not enough to only react once the risk has been actualised - for some risks that’s too late.A potential “worse-case scenario” is if a culture of “trial and error” is adopted, whereby adverse events occur before risks are considered tangible enough to act upon. This reliance on post-hoc interventions risks normalising unsafe development practices, making it difficult to recover once risks proliferate.Proliferation of risks is a key point in governing AI, because regulating software poses significant challenges compared to other dual-use commodities such as pharmaceuticals or firearms. This means that, from the outset, there must be proactive regulatory oversight for the deployment of advanced AI systems and their precursors.For example, significant structural changes, such as the use of advanced AI systems in Australia, require that risks must be comprehensively understood p"
274,Anonymous,Q16 Scope and application of risk-based approach,"The most important organisations to focus on when developing risk-management strategies and regulations, will be those that determine how advanced AI is developed and deployed. More specifically, there are three prominent categories of concern: 1. Organisations that develop advanced AI systems, or AI systems that can be used to build more advanced AI systems (i.e. precursor systems such as LLMs). Examples include OpenAI, DeepMind, Anthropic, Inflection AI, Nvidia, Microsoft, Meta. 2. Organisations which provide access to open-source AI systems via downloads or APIs. E.g. Hugging Face, Github, AWS. 3. Organisations which provide compute resources to develop, fine-tune or use AI systems. E.g. AWS, HuggingFace, Google, Azure, Nvidia, Lambda Labs.A majority of the benefits and risks will come from AI that is more sophisticated and generally capable, which can be tied back to organisations in these three categories.There are approaches which can be taken for each category in order to manage"
274,Anonymous,Q17 Elements to include in risk-based approach,"In general, Attachment C falls short of the necessary detail to understand the range of risks that have and will emerge as well as the range of specificity of mitigations necessary to control them.Attachment C would benefit from: - Distinguishing between scenarios where the use case is the potential risk (such as social scoring); scenarios where dual-use capability and misuse is the potential risk (bioengineering, producing misinformation and disinformation, enhancing criminal capability and capacity); and where the technology alone is the risk (technology with features like being deceptive, being power-seeking, being incorrigible etc). - Distinguishing between the point of regulatory intervention - the developer, the deployer, the user, or elsewhere, and - Offering a much wider range of mitigations that target different kinds of risks.Some elements of attachment C are promising, like “explanations” and “ongoing monitoring” - but these need to be greatly expanded and based on current A"
274,Anonymous,Q18 Integration of risk-based approach into existing frameworks,"Some areas of regulation, like the Racial, Sex, Age, and Disability Discrimination Acts are appropriately strong and may provide useful safeguards against some harms of AI. Specifically, the law should ensure that such discrimination is always wrong and always punishable, and an excuse that the discrimination occurred because of a “black box” AI provided by an overseas AI Lab should never be accepted. It would be helpful to communicate these requirements to Australian businesses that might deploy AIs and ensure they only contract with AI Labs for products where the Lab can ensure the product cannot engage in these harmful behaviours.Certainly, no AI-specific regulation should ever waive existing safeguards.That said, many of the emerging risks from AI are quite new and it’s reasonable to expect that regulation made without AI in mind is not tuned to the new risks that AI represents. Specific AI safety regulation is unavoidable."
274,Anonymous,Q19 Application of risk-based approach to general purpose AI,"The risk-based approach must be modified to address risks from general purpose AI systems, especially LLMs and MFMs. These types of systems learn their capabilities and behaviours during training, which can be unforeseen by developers. That means that AI developers could create models with dangerous capabilities (e.g., deception, manipulation, political strategy, weapons design). Humans could misuse those capabilities to cause harm [1], and models could apply the capabilities even without deliberate misuse (e.g., through misalignment) [2].Consistent with recent work from a consortium of AI safety researchers, including at Google Deepmind, OpenAI, Anthropic, Alignment Research Center, University of Cambridge, University of Oxford, Centre for Long-Term Resilience, and Centre for the Governance of AI [1], we recommend that any risk-based approach require that general purpose models are evaluated for dangerous capabilities and misalignment [3]. This will help determine: 1. To what extent a"
274,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,"In general, regulation should: 1. Be proportionate to risk. Meaning that a voluntary or self-regulation approach should only apply to low-risk or no-risk uses of AI. For higher risk uses, which are also likely to be higher reward, the economic and technological forces are likely to be so strong that a scheme that is voluntary or driven by aspirational best practices is unlikely to be successful. Further, smaller businesses may be pressured by market forces to deploy AI tools in risky ways that they don’t necessarily understand to remain competitive with the market. 2. In the case of higher-risk technologies, regulate both the technology itself and the use of the technology. Meaning that, for future and more sophisticated AIs, regulation should ensure that the technology is safe before it is created or published, as well as ensuring that it is only used in ways that are safe. 3. Place burdens on those most able to reduce potential harms. Meaning that AI that functionally operates as ‘bl"
275,Jose-Miguel Bello y Villarino,Transcription,"FILENAME: Submission JMBV - Supporting responsible AI - July 2023.e0ce363d101c4.pdf | CONTENT: José-Miguel BELLO Y VILLARINO Research Fellow Law School – University of Sydney ARC Centre of Excellence for Automated Decision-Making and SocietySydney NSW 2006 Australia Telephone: +61 2 9351 0352 jose-miguel.bellovillarino@sydney.edu.au26 July 2023SUBMISSION TO SUPPORTING RESPONSIBLE AI: DISCUSSION PAPERI am a research fellow at the Law School at the University of Sydney and the ARC Centre of Excellence for Automated Decision-Making and Society (ADM+S), I am an award-winning scholar in the domain of regulation of AI, addressing it mainly from a comparative perspective. I have worked for national and international governments including in the regulatory domain of technology and I am a Spanish foreign-service official currently on leave. The present submission is made in my personal capacity and develops some of the points raised in the broader ADM+S submission.The submission covers two ques"
275,Jose-Miguel Bello y Villarino,Q01 Definitions,"No definition is going to be adequate, but I would suggest shifting from artificial intelligence to Automated Decision-Making system / Decision Support System when discussing the use of automation in the public sector"
275,Jose-Miguel Bello y Villarino,Q06 Public vs Private sector governance,"1 THE CHOICE BETWEEN PROACTIVE AND REACTIVE REGULATION There are essentially two options for AI regulation in Australia, consolidating reactive regulation or developing pre-emptive (proactive) regulation. In my view, Australia should develop new pre-emptive regulation for the use of AI in the public sector, while, at least at this stage, could consolidate reactive regulation for the use of AI in the private sector. This submission first distinguishes between both approaches and then analyses the advantages and disadvantages of consolidating reactive regulation for the private sector, while developing new pre-emptive regulation for the use of AI in the public sector. The approach suggested may not be ideal for regulatory purposes, but it is immediately applicable, reasonable in terms of regulatory costs, should avoid the most damaging risks derived from the use of AI systems and opens opportunities for leadership in the governance of AI at the international level, through partnerships w"
275,Jose-Miguel Bello y Villarino,Q09 Transparency requirements and scope,"The idea of transparency as a requirement for a trustable AI has been thrown around with limited consideration to its meaning from a regulatory point of view (Felzmann et al. 2020; Ehsan et al. 2021; Walmsley 2021). Writing about transparency in an academic paper and using the concept meaningfully from a regulatory point of view are two very different issues. Yet, instead of focusing our efforts in making transparency operational for regulatory purposes, we all keep using the term as an anchor with the thaumaturgist’s power of solving many of the problems in the operation of AI systems. Whatever we signify with the term transparency (Larsson and Heintz 2020 offer a review of its different discipline-specific conceptions), the idea is appealing for its simplistic and intuitive character: if humans can see through a system, they could understand how it operates—although this requires a leap of faith—and, hence, the system will be trusted more (Ehsan et al. 2021). More transparency—normal"
275,Jose-Miguel Bello y Villarino,Q20 Voluntary vs mandated compliance with risk-based approach,Please see reply to relevant question above
276,IoT Alliance Australia,Transcription,"FILENAME: IoT Alliance Australia submission - Safe and responsible AI in Australia - 26 July 2023 - Final.7ee2f82426725.docx | CONTENT: 0026 July 2023Department of Industry, Science and Resourceshttps://consult.industry.gov.au/supporting-responsible-aiTo Whom It Concerns,IoT Alliance Australia submission – Safe and responsible AI in AustraliaInternet of Things Alliance Australia (IoTAA) thanks the Department of Industry, Science and Resources for the opportunity to submit feedback to the Safe and responsible AI in Australia consultation.IoTAA is the peak body for the Internet of Things (IoT) in Australia. A non-profit industry association, IoTAA was formed in 2016 to enable a data smart Australia, which advances society through trusted, accessible real-time data, powered by Internet of Things technologies. AI technologies are an integral part of the IoT technology suite, whether at the collection point in sensors, at the edge or in the core network for centralised intelligence – and at"
276,IoT Alliance Australia,General comment,"Internet of Things Alliance Australia (IoTAA) thanks the Department of Industry, Science and Resources for the opportunity to submit feedback to the Safe and responsible AI in Australia consultation. IoTAA is the peak body for the Internet of Things (IoT) in Australia. A non-profit industry association, IoTAA was formed in 2016 to enable a data smart Australia, which advances society through trusted, accessible real-time data, powered by Internet of Things technologies. AI technologies are an integral part of the IoT technology suite, whether at the collection point in sensors, at the edge or in the core network for centralised intelligence – and at many points in the data chain across multiple entities."
276,IoT Alliance Australia,Q01 Definitions,"Yes. The definitions on page 5 are based on international standards which a solid starting point. The distinction between AI and ML is an important one. AI generally involves coded algorithms which involve human design. As a consequence, they: - Can be explained in human terms. - Can be made visible and transparent. - Can be inadvertently biased. Machine learning (ML) are patterns that are recognised from training data. These patterns are not human designed and may not be readily explained by humans in their operation or effect."
276,IoT Alliance Australia,Q02 Other AI risks and recommendations for regulation,"Mitigating risks of AI requires attention to be placed on: - Responsible training for providers and users - Responsible deployment – where and with whom AI is applied - Transparency - Appropriate controls The risks from AI that may not be covered in existing regulatory approaches, would seem to generally arise from: - Lack of transparency and understanding of how decisions are made. The problem here being the inability to access how the AI is applied, often because of commercial reasons. o Note, ML may pose a greater risk as it’s decision making is less programmed and deterministic. o Lack of transparency will also mask underlying issues such as intended or unintended bias. This may have significant social and pollical impact in the case of manipulative uses of AI Transparency and disclosure requirements might be applied economy-wide to commercial offerings of an AI product or service that is intended for use by a customer, whether a business customer or a consumer.These requirements c"
276,IoT Alliance Australia,Q03 Non-regulatory actions to address AI risks,"There are a host of initiatives the Australian government and its agencies could implement to improve AI-affected decision. These include:- information and educational resources: improving competencies of people within organisations, and of other stakeholders (including industry associations, consumer organisations and civil society organisations), in AI-affected decision problem definition: that is, improving their competency to foresee a risk of harms to humans or environment that is reasonably attributable to a proposed use of AI, - processes for gating of AI and evaluation of AI: frameworks (including standards), methodologies, tools, checklists to assist myriad organisations to better assure adoption of a considered approach in determining whether, when and how AI is used, for which use cases, - governance of AI-affected decisions within organisations: who considers what, in consultation with whom, before use of AI, - safeguards and associated assurance (reliability and verifiabil"
276,IoT Alliance Australia,Q04 Coordinating AI governance across government,"As well as canvassing proposals for new AI regulation, the Australian government should promote discussion as to how Australian federal, state and territory governments, through initiatives of government agencies and regulators, can effectively influence decisions by regulated entities, including other government agencies, businesses and not-for-profits, as to whether, when and how to safely and responsibly use AI.There are a number of developing international models of regulation of AI. For example: an interventionist and prescriptive proposed in the EU AI Act , the draft Canadian AI and Data Act or the UK’s proposed lighter touch, coordinated and decentralised approach .We lean towards UK Government approach which proposes creation of central functions to support the multi-regulator, decentralised frameworks, including by: • developing a central monitoring, evaluation and risk assessment framework, • creating a central guidance to businesses looking to navigate the AI regulatory land"
276,IoT Alliance Australia,Q05 International models for AI governance,Please see our response to question 4
276,IoT Alliance Australia,Q06 Public vs Private sector governance,"Approaches to the public use of AI should in general be held to a higher standard that for private use. As government use of AI for public use is often accompanied by compelling incentives, this should be matched with a higher level of compelling accountability and responsibility."
276,IoT Alliance Australia,Q07 Responsible government use of AI,Please see our responses to questions 2 and 3.
276,IoT Alliance Australia,Q08 Generic vs technology-specific approaches for AI risk,"Regulation should focus upon ensuring that organisations design and implement policies and programs for responsible uses of AI that are appropriate to the organisation and not the manner in which those policies and programs are implemented. Generic solutions will be of most value and should incorporate the key externalities to determine whether services using AI are risky or not: that is vital externalities such as the quality of the data, the outcomes achieved and the people using the results. These would/should also be based on persistent ethical and social capital principles – as well as law. Technology specific solutions are less effective and will require constant updating and revision, as new innovations and inventions appear. The result will undoubtedly lag effect. Having said that, some specific technology solutions would include; - Providing transparency and disclosure of algorithms for high-risk applications - Considering adaptation of Australian Consumer Law to facilitate sa"
276,IoT Alliance Australia,Q09 Transparency requirements and scope,"“Frontier” AI that is new and unproven AI is especially risky, given it’s unproven and untested effect. The attached paper considers options for broadening risk assessments to accommodate these risks – with a focus on extreme risks. A good approach to AI regulation is to ask whether rules that restrict or prohibit particular uses of AI, or that mandate application of a particular risk assessment framework or methodology, are justified, or whether ‘detect’ and ‘respond’ incentives as adjusted for AI would then provide sufficient incentives to cause appropriate mitigation of AI risks by regulated entities.In the absence of sufficient detect and respond incentives, for specified AI- affected activities that are reasonably likely to lead to significant harms to some humans or the environment, then AI regulation, in the form of before-the-event prohibitions or prescriptions or both, may be justified. In that case, it is still appropriate to ask whether these risks of harms are: - short-term"
276,IoT Alliance Australia,Q10 Ban on high-risk AI applications,"There are applications that might be considered for banning – e.g. those that break the law, cause harm etc. These are likely mostly covered by existing laws. b. Criteria or requirements to identify AI applications or technologies that should be banned, and in which contexts? High risk AI that should considered to be managed, or even banned in special instances, are those where the lack of transparency and/or governance renders its effect opaque to users and may therefore mislead, be biased, discriminate etc. – if used in high risk use cases e.g. medical decisions with potential harmful outcomes A category of AI which would fall into the high risk category is manipulative uses of AI. These may be not obvious in effect at first and difficult to assess. This category would tend to apply more to very broadly available uses of AI."
276,IoT Alliance Australia,Q11 Actions to increase public trust in AI,Please see our responses to question 3
276,IoT Alliance Australia,Q12 Trade impact of banning high-risk AI applications,No comment
276,IoT Alliance Australia,Q13 Changes to conformity infrastructure,No comment
276,IoT Alliance Australia,Q14 Risk-based approach to address AI risks,"Yes, a risk-based approach is the best way to address risks of AI harms. Each AI-enabled, or AI-assisted, decision requires consideration of decision provenance: the interaction of input data, people, processes, outcomes and technologies that affect that decision. There is no best one-size-fits-all approach to risk assessment and management, however any risk assessment should include at the above important aspects: o of the AI technology itself – e.g. is it well designed, tested, free from obvious bias, clear in its purpose, transparent to users o the application and effect to which the AI is applied – e.g. are the decisions to be made by the AI in a high risk context – e.g. life and death decisions for health o to what degree do the people involved in providing the service and using the using need to know about how the AI works o is the data input provided accurate?We advocate policies and programs focussed on enforced self-regulation as the cornerstone. Assurance of safe and responsi"
276,IoT Alliance Australia,Q15 Benefits and limitations of risk-based approach,"Most organisations operating in Australia that are implementing AI within the organisation will not be developers and suppliers of AI solutions. Typically, an organisation operating in Australia will be tailoring a third-party AI application or service and using it: - more commonly, to inform or otherwise aid people within an organisation to perform a decision -making task, or - much less commonly, to enable a fully automated (self-actuating) outcomeSenior executives and managers within organisations may not see a business process (decision chain) within the organisation as an ‘application of AI’, or even as significantly affected by AI.Supporting programs suggested in our response to question 3 are vital in helping address these limitations.Organisations could be required to develop and implement policies and programs to act responsibly and ensure safety in organisational uses of AI. As a minimum, organisations should be required to prepare an annual plan setting out what they propose"
276,IoT Alliance Australia,Q16 Scope and application of risk-based approach,"Those sectors who already have established or are establishing risk-based assessment and associated governance, such as those service providers of critical infrastructure, would be better placed to accommodate additional risk parameters around AI.Many/ most organisations are not large businesses that have experience, settled procedures, internal capabilities and resources to reliably evaluate a third-party AI application or service for fitness for purpose for reliance in a particular business context, and assess the quality of data inputs used by the AI provider to train that AI."
276,IoT Alliance Australia,Q17 Elements to include in risk-based approach,"The elements summarised in Attachment C are relevant but incomplete, partly because the focus in Attachment C is upon assessment of the AI system itself, rather than the decision context that is being affected by the AI.The risk based approach should be simple enough to enable everyday use of AI for low risk applications, for which there are a huge number of instances to be easily identified, and proceeded with, without further government intervention.We disagree with the apparent presumption in Attachment C that all impact assessments should be published, or that “peer review” of impact assessments “by external experts” would significantly reduce risks of AI harms."
276,IoT Alliance Australia,Q18 Integration of risk-based approach into existing frameworks,Please see our response to question 14.
276,IoT Alliance Australia,Q19 Application of risk-based approach to general purpose AI,"Many Australian organisations that do not have capabilities to implement complex structured risk management frameworks or methodologies will be implementing generative AI to assist non-technical humans to perform myriad tasks.A risk management approach, coupled with an enforced self-regulation model, should be applied to general purpose AI services, designed to address the challenges associated with: - lack of organisational control over how it is likely to be introduced into and used in many organisations for a myriad of tasks, - the role of individuals within those organisations in determining when and how general purpose AI services are used as a task assistant, and the best ways to ensure those individuals exercise appropriate restraint and care, - the key role that transparency can play in building awareness of risks and capability to mitigate risks.For AI of extreme risk in frontier applications, a more rigorous risk assessment approach will likely be required."
276,IoT Alliance Australia,Q20 Voluntary vs mandated compliance with risk-based approach,"A risk-based approach for responsible AI should be applied by: - both developers and deployers of both foundational models and algorithmic decision-making systems and generative AI applications built upon those foundational models or algorithms, and - organisations, both public or private, that are users of third party supplied foundational models, algorithmic systems, at least to determine whether risks of AI harms are sufficiently likely to have been assessed and mitigated by upstream developers and providers"
277,Cooperative Research Australia,Transcription,"FILENAME: CRA's Submission to DISR's Consultation on Supporting Responsible AI.93553d0ef45a9.pdf | CONTENT: CRA's submission to the Consultation on “Supporting Responsible AI: Discussion Paper” (July 2023) Cooperative Research Australia acknowledges the traditional custodians of the land on which we operate, the Ngunnawal people. We also acknowledge the traditional custodians of the various lands across Australia upon which our members operate.We pay our respects to Elders past, present and emerging and celebrate the diversity of Aboriginal peoples and their ongoing cultures and connections to our lands and waters.For further enquiries contact: Jane O’Dwyer Chief Executive Officer Cooperative Research Australia PO Box 5357. Braddon, ACT, 2612 02 6260 3988 www.cooperativeresearch.org.au ABN 42 892 101 689 Executive Summary Cooperative Research Australia (CRA) welcomes the opportunity to provide a submission to the Department of Industry, Science and Resources’ consultation on its recent"
278,Trusted Autonomous Systems,Transcription,"FILENAME: TAS submission to Aus Gov consultation on supporting responsible AI (Wed 26 July 2023).0d4d143daa00b.pdf | CONTENT: TRUSTED AUTONOMOUS SYSTEMS Submission to Commonwealth Government Consultation on Supporting Responsible AI Organisation: Trusted Autonomous Systems Defence Cooperative Research Centre Address: PO Box 59, Toowong (Yuggera-Turrbal land) QLD 4066Date: 26 July 2023 tasdcrc.com.au 1 About Trusted Autonomous Systems (TAS) Trusted Autonomous Systems (TAS) is Australia’s first Defence cooperative research centre and is uniquely equipped to deliver world-leading autonomous and robotic technologies to enable trusted and effective cooperation between humans and machines. Our aim is to improve the competitiveness, productivity, and sustainability of Australian industry.Supporting Australia’s defence capabilityTAS, together with its participants and the Department of Defence, is focused on developing the capacity of Australia’s defence industry to acquire, deploy and sustain"
279,Australian Computer Society,Transcription,"FILENAME: ACS response to Safe and responsible AI in Australia.74f6c012eed14.pdf | CONTENT: ACS responseSafe and responsible AI in Australia discussion paperJuly 2023 Australian Computer Society Inc. (ACT) ARBN 160 325 931 National Secretariat Tower One, 100 Barangaroo Avenue, Sydney NSW 2000 PO Box Q534, Queen Victoria Building, Sydney NSW 1230 T +61 2 9299 3666 | F +61 2 9299 3997 E info@acs.org.au | W www.acs.org.auTo the Department of Industry, Science and Resources, the Australian GovernmentACS response Safe and responsible AI in Australia Discussion paper26th July 2023 Dear Sir or Madam.Thank you for the opportunity to contribute to this important issue. The Australian Computer Society (ACS) is the peak professional association for Australia’s information and communications technology sector. We represent over 40,000 members working in all sectors of the economy and in all states and territories across the nation. The ACS works to grow the technology sector while making sure IT p"
279,Australian Computer Society,Q01 Definitions,"The definitions on page 5 of the Discussion Paper are a useful guide to discussions as to when risks of harms arise from use of inference engines, including algorithmically-enabled automated decision making, use of foundational models and generative AI applications. In considering possible AI harms and incentives to assess and mitigate relevant risks of harms, a broad definition should be used. As Robodebt illustrates, many significant harms to humans or the environment may flow from inadequate governance of use of hardcoded advanced data analytics (algorithmic) systems to inform or otherwise affect human decision-making or produce automated outcomes. These uses therefore need to be brought within the ambit of new frameworks and methodologies for assessment of governance and associated controls for inference-assisted decisions, regardless of whether advanced data analytics (algorithmic) systems are considered as ‘AI’. It is generally not possible to determine in advance whether employi"
279,Australian Computer Society,Q02 Other AI risks and recommendations for regulation,"Disclosure and transparency Many AI applications and services commercially offered in Australia originate from outside Australia. Australian regulation can reasonably address transparency and disclosures offered by offshore providers to organisations using these applications and services in Australia. Data sheets, system cards, and model cards made available by some providers of commercially offered AI applications and services are today of different levels of transparency and frankness in disclosure of known limitations, comprehensiveness and quality. The problem reflects still nascent standardisation as to supplier disclosures for AI systems and AI services. This is partly a transitional issue that is already being addressed in the competitive AI market. However, regulated requirements for transparency and disclosure may assist. Transparency and disclosure requirements might be applied economy-wide to commercial offering of an AI product or service that is intended for use by a custo"
279,Australian Computer Society,Q03 Non-regulatory actions to address AI risks,"The broader response paper attached to this submission outlines a number of non-regulatory responses, including: • higher level guidance and guidelines as to safe and responsible deployments of AI • coordination between regulators to ensure commonality of approach • support for further development and modification of enterprise risk and operational risk frameworks • support for development of best practice in data sheets, system cards, and model cards for AI applications and services • support for development of international and Australian standards for AI impact assessment, including sector-specific, application-specific and task-specific standards • support for development of better understanding within organisations of the respective roles of executives and professionals, in assuring safe and responsible use of AI by organisations. • an information campaign and publication of explanatory materials about safe and responsible use of AI, particularly targeting small to medium business"
279,Australian Computer Society,Q04 Coordinating AI governance across government,"We commend the UK’s lighter touch, coordinated but decentralised approach, as described in the UK Government’s Policy paper A pro-innovation approach to AI regulation of 29 March 2023 and AI Regulation Policy Paper of 18 July 2022, with the modifications that we describe in section 1 of our attached response. The UK Government proposes creation of central functions to support the multi-regulator, decentralised frameworks, including by: • developing a central monitoring, evaluation and risk assessment framework • creating a central guidance to businesses looking to navigate the AI regulatory landscape in the United Kingdom • offering a multi-regulator AI sandbox, and • supporting cross-border coordination with other countries. While no announcement has been made, the UK Government's Office for Artificial Intelligence, a unit within the UK Department for Science, Innovation and Technology, may take on some of these central functions. The UK Government currently addresses regulatory coord"
279,Australian Computer Society,Q05 International models for AI governance,"While Australia should take its own approach, as noted to our response to Question 3, the UK’s approach aligns with what we believe should be the Australian model."
279,Australian Computer Society,Q06 Public vs Private sector governance,No. We are not aware of any reason why other assessment and management of Risks of AI harms by public sector agencies should be less than assessment and management by other Australian organisations.
279,Australian Computer Society,Q07 Responsible government use of AI,"The development of AI governance skills will be critical for government agencies – as much or more so than private organisations. Right now, there is a severe AI skills shortage within both the public and private workforces. Encouragement of agencies to engage in upskilling for AI (possibly through the APSC’s Digital and Data Professional development streams) will be a good first step. Such education should not just focus on technical applications of AI, but in the proper governance and risk assessment for AI system. The Robodebt Royal Commission report highlights the needs for these skills and accountabilities within the public service."
279,Australian Computer Society,Q08 Generic vs technology-specific approaches for AI risk,This is covered in our response to Question 2 above and in the attached paper.
279,Australian Computer Society,Q09 Transparency requirements and scope,This is covered in our response to Question 2 above and in the attached paper.
279,Australian Computer Society,Q10 Ban on high-risk AI applications,"There are certain technologies or applications of technology that should present uncontrollable risks or outcomes that warrant bans. We imagine that most Australian citizens, for example, would find the work of an organisation like Clearview AI (which created and sold access to an enormous global facial recognition database) abhorrent. However, discussions as to blacklists can readily become politically contentious as to edge cases (why was X included, but not Y?), and the objectively assessed mitigation effects of risk measures and assurance controls may not be broadly understood. In our view, blacklisting of particular categories of AI applications should only be considered in relation to uses of AI that are of such extreme risks of harms to humans or the environment as to be unacceptable to Australian society regardless of whether safeguards and assurance controls are reliably and verifiably applied. Inclusions in the proposed blacklist for the draft EU AI Act remain controversial. "
279,Australian Computer Society,Q11 Actions to increase public trust in AI,"Education programs, particularly targeted at small businesses, can alleviate many of the issues. An information program modelled on the ACCC’s Scamwatch, for example, can be used to educate people on the safe application of AI. Please see our response to question 3 above for specific recommendations."
279,Australian Computer Society,Q12 Trade impact of banning high-risk AI applications,"If an activity is worthy enough to warrant a ban, then we would argue that its impact on trade should be irrelevant. Australian companies should not be encouraged or enabled to engage in activities that are immoral or damaging so long as it is done “over there but not here.”"
279,Australian Computer Society,Q14 Risk-based approach to address AI risks,"Yes. Risk assessment should be part of the ‘mix’. It’s not a case of either/or, but a matrix of responses to AI. A risk-based approach, in the style of the EU legislation, provides a useful tool for evaluating AI applications and whether they should be restricted or blacklisted. However, principles-based governance, training and education also has a role to play. Each AI-enabled, or AI-assisted, decision requires consideration of decision provenance and outcomes: the interaction of people, processes and technologies that effect, or affect, that decision. Most organisations operating in Australia that are implementing AI within the organisation will not be developers and suppliers of AI solutions. Typically, an organisation operating in Australia will be tailoring a third-party AI application or service and using it: • more commonly, to inform or otherwise aid people within an organisation to perform a decision-making task, or • much less commonly, to enable a fully automated (self-actu"
279,Australian Computer Society,Q15 Benefits and limitations of risk-based approach,This is covered in our response to Question 14 above and in the attached paper.
279,Australian Computer Society,Q16 Scope and application of risk-based approach,Yes. This is covered in our response to Question 2 above and in the attached paper.
279,Australian Computer Society,Q17 Elements to include in risk-based approach,"The elements summarised in Attachment C are relevant but incomplete, partly because the focus in Attachment C is upon assessment of the AI system itself, rather than the decision context that is being affected by the AI. We discuss the appropriate scope for transparency and disclosures, and incentives for transparency and disclosures, in section the attached paper and in our response to question 2."
279,Australian Computer Society,Q18 Integration of risk-based approach into existing frameworks,This is covered in our response to Question 14 above and in the attached paper.
279,Australian Computer Society,Q19 Application of risk-based approach to general purpose AI,"There are at least three relevant levels at which risks from general purpose AI systems need to be assessed: 1. The underlying foundation models and the data inputs used to fuel those models. The developers of these foundational models need to have appropriate incentives to make fair disclosures as to limitations of those models, so providers of generative AI applications built upon those models may consider the reliability of the foundational model, assess the reliability and safety of the application, and in turn make appropriate disclosures and ensure that their generative AI application offering is safe and complies with law. 2. Provision of the generative AI application built upon a foundational model. 3. Use of a generative AI application for a particular user-determined task. We have already made specific suggestions as to transparency and disclosure requirements to enable risk assessment at levels 1. and 2.: see in particular our response to question 2 above. As to 3., we note "
279,Australian Computer Society,Q20 Voluntary vs mandated compliance with risk-based approach,"See our discussion paper on the enforced self-regulation model as a way to change organisational DNA and enable flexibility as to the processes by which organisations adopt a risk-based approach for responsible AI. A risk-based approach for responsible AI should be applied by both developers and deployers of both foundational models and algorithmic decision-making systems and generative AI applications built upon those foundational models or algorithms; as well as organisations, both public or private, that are users of third party supplied foundational models and algorithmic systems."
280,ANU School of Cybernetics,Transcription,"FILENAME: 2023-26-07_SOCY_Submission_Dept Industry_AI Discussion Paper.f9c448bf151ae.pdf | CONTENT: Response to the ‘Safe and responsible AI in Australia’ Discussion Paper, Department of Industry, Science and Resources. ANU School of Cybernetics The Australian National UniversityGenevieve Bell, Maia Gould, Matthew Holt, Sharon Lopez, Andrew Meares and Gabrielle VannithoneJuly 20231The Australian National University TEQSA Provider ID: PRV12002 (Australian University) | CRICOS Provider Code: 00120C The Australian National University acknowledges, celebrates and pays our respects to the Ngunnawal and Ngambri people of the Canberra region and to all First Nations Australians on whose traditional lands we meet and work, and whose cultures are among the oldest continuing cultures in human history.ANU School of Cybernetics +61 2 6125 8121 cybernetics@anu.edu.auThe Australian National University Canberra ACT 2600 Australia www.anu.edu.auTEQSA Provider ID: PRV12002 (Australian University) CRICO"
280,ANU School of Cybernetics,General comment,Please find attached our submission which we feel addresses holistically the intent of the discussion paper.
282,Women With Disabilities Australia,Transcription,"FILENAME: WWDA Submission to the 'Safe and Responsible AI' Discussion Paper.4f363922fdbab.pdf | CONTENT: WOMEN WITH DISABILITIES AUSTRALIA (WWDA)Response to ‘Safe and responsible AI in Australia’ Discussion PaperAustralian Department of Industry, Science and ResourcesJuly 20231 PUBLISHING INFORMATION Women With Disabilities Australia (WWDA) (2023). Response to the ‘Safe and responsible AI in Australia’ Discussion Paper, Department of Industry, Science and Resources. July 2023. Written by Sophie Cusworth, Senior Policy Officer, Women With Disabilities Australia (WWDA): Hobart, Tasmania.Acknowledgments WWDA acknowledges the traditional owners of the land on which this publication was produced. We acknowledge First Nations people’s deep spiritual connection to this land. We extend our respects to community members and Elders past, present and emerging.This document was written by Sophie Cusworth, WWDA Senior Policy Officer -Disability & Gender.Formatting, editing and design by Carolyn Fro"
282,Women With Disabilities Australia,Q01 Definitions,Please see written submission.
282,Women With Disabilities Australia,Q02 Other AI risks and recommendations for regulation,Please see written submission.
282,Women With Disabilities Australia,Q03 Non-regulatory actions to address AI risks,Please see written submission.
282,Women With Disabilities Australia,Q04 Coordinating AI governance across government,Please see written submission.
282,Women With Disabilities Australia,Q05 International models for AI governance,Please see written submission.
282,Women With Disabilities Australia,Q06 Public vs Private sector governance,Please see written submission.
282,Women With Disabilities Australia,Q07 Responsible government use of AI,Please see written submission.
282,Women With Disabilities Australia,Q08 Generic vs technology-specific approaches for AI risk,Please see written submission.
282,Women With Disabilities Australia,Q09 Transparency requirements and scope,Please see written submission.
282,Women With Disabilities Australia,Q10 Ban on high-risk AI applications,Please see written submission.
282,Women With Disabilities Australia,Q11 Actions to increase public trust in AI,Please see written submission.
282,Women With Disabilities Australia,Q12 Trade impact of banning high-risk AI applications,Please see written submission.
282,Women With Disabilities Australia,Q13 Changes to conformity infrastructure,Please see written submission.
282,Women With Disabilities Australia,Q14 Risk-based approach to address AI risks,Please see written submission.
282,Women With Disabilities Australia,Q15 Benefits and limitations of risk-based approach,Please see written submission.
282,Women With Disabilities Australia,Q16 Scope and application of risk-based approach,Please see written submission.
282,Women With Disabilities Australia,Q17 Elements to include in risk-based approach,Please see written submission.
282,Women With Disabilities Australia,Q18 Integration of risk-based approach into existing frameworks,Please see written submission.
282,Women With Disabilities Australia,Q19 Application of risk-based approach to general purpose AI,Please see written submission.
282,Women With Disabilities Australia,Q20 Voluntary vs mandated compliance with risk-based approach,Please see written submission.
283,Tech Global Institute,Transcription,"FILENAME: Tech Global Institute - Submission on the Safe and Responsible AI in Australia.6b8d30706fad6.pdf | CONTENT: Tech Global Institute 8 Brunswick Street Brampton, ON L6X 4Y6 www.techglobalinstitute.comJuly 25, 2023Submission to the Department of Industry, Science and Resources of the Government of Australia on the discussion paper on Safe and Responsible AI in AustraliaTech Global Institute welcomes the opportunity to provide comments on the important topic of formulating comprehensive actions on the regulation and governance of artificial intelligence (AI) in Australia. Tech Global Institute is a global policy lab with a mission to reduce equity and accountability gaps between internet technologies and the Global Majority. We are a community of senior policy and legal experts, trust and safety professionals, AI systems researchers and human rights specialists who collectively have decades of experience in building, scaling and governing the products at leading technology compani"
284,Anonymous,Q01 Definitions,"The definitions of AI and ML are fine, however the paper lacks a definition of what is meant by “safe and responsible AI”. For a productive discussion, we must state that there is a difference between ethical and safe AI. In fact we need three definitions:1. Ethical AI respects people's rights (e.g. lacks bias and was trained on ethically-sourced data). This is rarely done well, but it is tractable. 2. Aligned AI does what its creators intend. This is an unsolved problem and may or may not be tractable. 3. Safe AI is both ethical and aligned.The paper frames the discussion around ethics, however AI alignment is least as pressing as the creation of ethical AI."
284,Anonymous,Q02 Other AI risks and recommendations for regulation,"The paper overlooks the catastrophic risks of highly capable AI systems, a concern recognised by experts and public figures worldwide. Australia must acknowledge these risks and take action by establishing an AI Commission to provide leadership, engage with stakeholders, and keep pace with rapid advancements in technology. This body would prioritise AI safety, shape regulatory frameworks, and support research and education in AI. A joint culpability scheme is necessary, supported by a regulator that can analyse and monitor AI systems. This regulator would provide confidence in AI safety without requiring technical expertise from the public."
284,Anonymous,Q03 Non-regulatory actions to address AI risks,"The transformative potential of AI necessitates a focus on ethics and safety. Non-regulatory initiatives can not replace the need for strong regulation but can complement it. Here are three non-regulatory approaches:1. Publicly acknowledge the risk and lead in establishing global governance arrangements to tackle potential catastrophic risks from AI. This includes fostering transparency in AI development, monitoring AI progress, and empowering enforcement should AI pose a significant risk.2. Build national capability in AI safety, security, and public decision-making: This includes improving tech literacy in government, supporting AI safety research, and developing a national laboratory to monitor deployed AI systems for safety-relevant factors.3. Incentivise research and development in AI alignment. This involves creating incentives for companies and researchers to develop AI systems that are corrigible and aligned with human values and societal goals. AI safety is woefully under-rese"
284,Anonymous,Q05 International models for AI governance,"The EU's recent AI Act could possibly be adapted for Australia. It acknowledges the risks of general-purpose AI and requires providers to comply with strict safety measures throughout their product's lifecycle.The UK's proposed Sentinel - a national lab focused on researching and testing safe AI - is another model Australia could consider.However, these examples may not be sufficient. They might not provide a strong enough deterrent against creating dangerous AI. Therefore, Australia should consider these as starting points and strive to develop more robust measures to ensure AI safety."
284,Anonymous,Q09 Transparency requirements and scope,"Regulators should collaborate with the global AI safety community to identify advanced indicators of potential catastrophic risk scenarios. If these indicators are detected in an AI system, a immediate shutdown/ban and independent review should be triggered.Indicators could be based on model size or cumulative compute used in training, or system behaviour. However given the unsolved nature of AI alignment and the unpredictability of when an AI system may become dangerous, the regulatory body must be agile, responding swiftly to new research and emerging AI behaviour.For powerful systems, regulation should be required before and during training, as well as at and during deployment, with independent expert oversight to ensure accountability and build public trust in AI."
284,Anonymous,Q10 Ban on high-risk AI applications,"Australia should consider a ban on training and/or deploying AI systems with a nontrivial chance of posing catastrophic risks to civilization. This ban would primarily affect training runs for only the largest foundation models, but may also affect development of agents with dangerous emergent behaviour. For simplicity, an immediate ban could be placed on the use of more than a set amount of cumulative compute in the training of any AI system, with this limit adjustable over time as consensus forms around technical alignment solutions or as algorithmic efficiencies emerge.Additionally, AI impersonation of humans should be outright banned unless it is clearly indicated that it's an impersonation. This is due to the potential misuse of AI for mass misinformation and the plausible risk of powerful but unaligned AI attempting to deceive us by presenting itself as human."
284,Anonymous,Q12 Trade impact of banning high-risk AI applications,"Banning high-risk activities is indeed be a good idea and is unlikely to impact our economy in an unacceptable way. We've accepted bans on other activities that pose lower risks than AI. When designing policy in this space, we should consider a broad range of what constitutes a ""high-risk activity"".Some uses for AI are risky and should be restricted or banned - such as those that could significantly damage our democracy or degrade individuals. But powerful AI is inherently risky, and merely restricting a use case does not adequately mitigate the risk. We understand this in other fields. For example, human cloning was banned in Australia until 2006 and is now heavily regulated. The development of chemical weapons is also prohibited.Advanced AI systems pose unprecedented risks and require a shift in perspective for regulators. This is true even for the development of powerful AI, not just the deployment of an AI agent.To manage these risks, we must take preventative action before develop"
284,Anonymous,Q14 Risk-based approach to address AI risks,"A risk-based approach is a sensible starting point for addressing potential AI risks. However, these risks may fluctuate over time, depending on the pace of AI safety research compared to AI capability research. Any risk assessment should be done in consultation with AI safety researchers.Powerful, unaligned AI could pose catastrophic or existential risks, regardless of human intent about its usage. AI systems that can complete a diverse range of reasoning tasks with near-human performance or form sophisticated plans should be considered very high risk. Requiring a ""human in the loop"" is an insufficient risk mitigation strategy.It's essential that we make significant progress in the field of AI safety. We should be investing an amount comparable to what is being spent on AI capability development, so that as AI capabilities grow, so too does our ability to manage and mitigate the associated risks."
284,Anonymous,Q18 Integration of risk-based approach into existing frameworks,"AI is unlike any technology up to this point, in that it it can:1. Be deployed at scale very quickly 2. Come up with new ideas and decide what actions to take 3. Impersonate humans.Existing frameworks (like privacy and discrimination) should be updated to be effective against highly-scalable infringements of rights. These updates should consider the unique capabilities of AI, such as its ability to operate at a speed and with reach that far surpasses human capabilities. For instance, privacy frameworks should account for the fact that AI can process and infer sensitive information from vast amounts of data. Similarly, discrimination frameworks should be updated to reflect the fact that AI can inadvertently perpetuate and amplify existing biases at tremendous scale.However, AI comes with unique capabilities and risks - like agency and deception - that are likely to require special assessment frameworks."
284,Anonymous,Q19 Application of risk-based approach to general purpose AI,"The Government should consider implementing a permit system for the development and deployment of advanced AI systems. These permits would only be granted if the developers can demonstrate that they have taken adequate measures to mitigate the risks associated with these systems. The criteria for what constitutes an advanced AI system should be determined in consultation with AI safety researchers and should be based on the system's capabilities, not just its intended use.We should require that all foundation models be aligned proportional to their capabilities. Note that we currently do not know how to align highly capable AI systems.It's also important to note that AI systems can acquire new capabilities when combined with other models, or when deployed as an agent with memory. AutoGPT is an example of a system that acquires capabilities during deployment. Therefore, a risk-based approach should include an assessment of the system's capabilities at various stages of its lifecycle.As "
284,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,"Given that the risk varies so much by use case and capability level, we should consider a tiered system:1. Narrow AI: These are systems designed for specific tasks and are already prevalent today.1.a. Low-risk tasks, such as chatbots or recommendation algorithms. Their commercial value is clear, and harm potential is relatively low. Regulation here can be lighter, promoting innovation while ensuring basic ethical and safety standards.1.b. Socially-sensitive tasks, such as facial recognition systems and predictive policing. While not necessarily transformative, misuse can have significant societal impacts. Regulations should enforce transparency, non-discrimination, and privacy protection.2. Transformative AI: These systems have the potential to cause substantial economic, political and social disruption, and job displacement. This would include any system that results in a high level of automated business or political activity. Such AI should be subject to stricter regulation to manage"
285,Gradient Institute,Transcription,"FILENAME: gradient_AI_regulation_submission.03b9a3f99631e.pdf | CONTENT: Safe and Responsible AI in AustraliaGradient Institute submission in response to the Department of Industry, Science and Resources discussion paper26 July 2023Lead Authors: Lachlan McCalman, Tiberio CaetanoContributing Authors: Kimberlee Weatherall, Chris Dolman, José-Miguel Bello y Villarino, Bill Simpson-Young About Gradient Institute We are an independent, nonprofit research institute that works to build safety, ethics, accountability and transparency into AI systems. We research new algorithms, provide training and auditing services for organisations operating AI systems, and provide technical guidance for AI policy development. Contact us at info@gradientinstitute.org.Submission summary Gradient Institute is grateful for the opportunity to respond to the Department of Industry, Science and Resource’s consultation on Safe and Responsible AI in Australia.This response highlights two key issues in the discussion"
286,Deloitte,Transcription,"FILENAME: Deloitte submission on safe and responsible AI_26 July 2023.c789d04dcbb52.pdf | CONTENT: Safe and responsible AI in Australia Deloitte response to Department of Industry, Science and ResourcesJuly 2023 Safe and responsible AI02 Deloitte Touche Tohmatsu ABN 74 490 121 060 477 Collins Street Melbourne, VIC 3000 Australia P: +61 3 9671 7000 www.deloitte.com.auThe Honourable Ed Husic MP, Minister for Industry and Science Department of Industry, Science and Resources 10 Binara Street Canberra, ACT 2601 Australia26 July 2023Dear Minister Husic Re: Safe and Responsible AI in Australia Deloitte recognises the enormous potential by AI to individuals, society and the Develop an AI code of practice to that Artificial Intelligence (AI) presents environment rather than targeting specific support safe and responsible AI and is committed to being part of safely technologies. This enables a balanced Resolution of the foundational issues, harnessing this opportunity for Australia, approach to"
286,Deloitte,General comment,"Deloitte Australia is pleased to submit our response on Safe and Responsible AI in Australia. We base our submission on findings from our global State of AI report , consultation across our global network of firms, cross sector experience working with clients developing AI solutions, and our deep subject matter expertise in technology, regulation, risk management and governance. We have reviewed the emerging global regulation, principles and best practice and advocate for a proportionate and pro-innovation approach to safe and responsible AI in Australia. The questions posed within the discussion paper can be distilled to two foundational issues – what regulatory framework should be applied to AI and what AI systems fall within this regulatory framework. Resolution of these issues will provide the north star by which Australia should govern AI to harness its enormous potential while protecting its citizens, values and unique environment. It is our view that Australia should: Adopt a ri"
286,Deloitte,Q01 Definitions,"Deloitte recommends the following definition for AI: Artificial Intelligence refers to a computer-based system that can, for a given set of objectives, generate outputs such as content, predictions, recommendations, or decisions influencing real or virtual environments. AI systems are designed to operate with varying levels of autonomy."
286,Deloitte,Q02 Other AI risks and recommendations for regulation,Please see attached submission paper
286,Deloitte,Q03 Non-regulatory actions to address AI risks,Please see attached submission paper
286,Deloitte,Q04 Coordinating AI governance across government,Please see attached submission paper
286,Deloitte,Q05 International models for AI governance,Please see attached submission paper
286,Deloitte,Q06 Public vs Private sector governance,Please see attached submission paper
286,Deloitte,Q07 Responsible government use of AI,Please see attached submission paper
286,Deloitte,Q08 Generic vs technology-specific approaches for AI risk,Please see attached submission paper
286,Deloitte,Q09 Transparency requirements and scope,Please see attached submission paper
286,Deloitte,Q10 Ban on high-risk AI applications,Please see attached submission paper
286,Deloitte,Q11 Actions to increase public trust in AI,Please see attached submission paper
286,Deloitte,Q12 Trade impact of banning high-risk AI applications,Please see attached submission paper
286,Deloitte,Q13 Changes to conformity infrastructure,Please see attached submission paper
286,Deloitte,Q14 Risk-based approach to address AI risks,Please see attached submission paper
286,Deloitte,Q15 Benefits and limitations of risk-based approach,Please see attached submission paper
286,Deloitte,Q16 Scope and application of risk-based approach,Please see attached submission paper
286,Deloitte,Q17 Elements to include in risk-based approach,Please see attached submission paper
286,Deloitte,Q18 Integration of risk-based approach into existing frameworks,Please see attached submission paper
286,Deloitte,Q19 Application of risk-based approach to general purpose AI,Please see attached submission paper
286,Deloitte,Q20 Voluntary vs mandated compliance with risk-based approach,Please see attached submission paper
287,Ernst & Young,Transcription,"FILENAME: Safe and responsible AI in Australia - EY submission 20230726.00f7cd49a5546.pdf | CONTENT: Safe and responsible AI in Australia July 2023 Safe and responsible AI in Australia 2The Hon Ed Husic MP 26 July 2023 Minister for Industry and Science PO Box 6022 House of Representatives Parliament House Canberra ACT 2600Dear Minister,Safe and responsible AI in Australia We welcome the opportunity to respond to the consultation questions in Safe and Responsible AI: Discussion Paper (the Discussion Paper), launched by you on 1 June 2023. Artificial Intelligence technology has undergone rapid development in recent years, and has the potential to revolutionise productivity and democratise a wide range of important capabilities. Businesses and government are poised to unlock this value. It is critical to position Australia at the forefront of this opportunity while maintaining a cautious stance towards potential social harms. The time for establishing a framework for safe and responsible "
287,Ernst & Young,Q01 Definitions,"EY agrees that definitions of AI should be aligned to international standards wherever possible, in order to facilitate interoperability with Australia’s major trading partners. Accordingly, we support the definitions of Artificial Intelligence (AI) and Machine Learning as being based on ISO standards.In some cases, definitions may still be evolving, or further clarity can be conveyed.Large Language Model (LLM) and Multimodal Foundation Model (MfM)The Discussion Paper’s definition of LLM focuses on the model’s outputs. It may increase understanding to further detail how these models can:• Analyse text (at each stage of their development). • Generate human-like language outputs. • Interact with systems that require text input to perform tasks (including programming). The definition of Multimodal foundation model could be similarly adapted, and both definitions could refer directly to machine learning (of which the current generative AI systems are a subset). This would result in the fol"
287,Ernst & Young,Q02 Other AI risks and recommendations for regulation,"Many AI risks arise from its ease of access and ability to quickly generate output at scale. At a conceptual level, much of this activity is already controlled by existing laws and regulations, including privacy laws, cyber and data protection, anti-discrimination, tort, criminal law, deceptive practices, product liability, and so on. There could be substantial challenges at a practical level, however, as the regulatory system implicitly assumes some limits around the volume of activity to be controlled. Examples of the kinds of activities that could strain enforcement include:• The automated generation of massive numbers of high-quality deepfakes, for the purposes of personally targeting individuals for embarrassment, fraud or influencing public opinion.• The widespread availability of easy-to-use tools and resources for computer hacking. The automation of these tools could also greatly increase the volume and speed of attacks (noting that the current average time for detection of a m"
287,Ernst & Young,Q03 Non-regulatory actions to address AI risks,"A number of non-regulatory initiatives could be implemented to support responsible AI. • The government can fund collaborative research, encourage the sharing of resources, and continue to support the development of internationally aligned standards and best practices across government, academia and industry, as it does already through its funding of, e.g., CSIRO’s National Artificial Intelligence Centre. This could extend to supporting research into measuring AI’s compliance with environmental standards and fundamental rights, societal impacts of AI on the labour market (job redesign, job transition) and more broadly. • Education will be critical to ensuring that Australia has the skills needed to participate in AI and to build public trust. The government can support training opportunities across the range of needed technical skills, and encourage its responsible use in the university sector. Education can also play a vital role in improving access to skilled work in the sector for d"
287,Ernst & Young,Q04 Coordinating AI governance across government,"Cohesive governance across government will be important, both within the Commonwealth and across Australian Governments. A consistent approach will foster:· The identification of opportunities and mitigation of potential harms.· Innovation and learning.· Growing the market for AI implementation by reducing the need to comply with conflicting rules and processes.Many of the recommendations made in our response to question 3 are relevant here. Ethical principles and frameworks can provide important guidance to agencies developing AI applications, as can the adoption of standards and providing tools and systems to validate the operation of AI systems.Our recommendation to establish a senior governmental presence to provide leadership and direction, mentioned in our response to question 3, can also play an important role, noting that this should cascade down into coordination at the level of key departments.Within the Commonwealth, consideration should be given to establishing a task group"
287,Ernst & Young,Q05 International models for AI governance,"The Discussion Paper provides a comprehensive summary of approaches in most of the jurisdictions that Australia would usually consider comparable, or that are relevant from a trade perspective. A risk-based approach to AI regulation is consistent with the approach in most of these markets."
287,Ernst & Young,Q06 Public vs Private sector governance,"For both the public and private sector, a risk-based approach suggests that regulatory responses should be proportionate to the risks involved. In general, the public sector has a greater impact than any individual business, due to the importance of many of its services and the scale and monopoly nature of its provision. These factors alone suggest that, even if the overall approach is the same, the public sector should be held to the highest standards in the practical application of a risk-based approach.Several further factors suggest the public sector should be adopting higher standards for itself than for the private sector:· The direct nature of the accountability of government to its citizens, unlike the relationship between the private sector and its customers.· The heightened ethical obligations arising from government’s combined role as legislator, regulator and actor within the regulatory system.· The differing roles of the public and private sectors in driving innovation (wi"
287,Ernst & Young,Q07 Responsible government use of AI,"Our response to this question is similar to our responses to questions 3 and 4. We support:· Adopting ethical principles and frameworks to guide agencies in AI implementation. · Establishing a task group within a department or agency to advise on AI matters, as mentioned in our response to question 4. · Targeted training for officials to ensure that they have the capability to be informed and capable buyers of AI services. Developing guardrails or guidelines for implementation and maintenance of AI platforms, including the management of the data assets that are accessed and created. · Creating new roles within the APS focused on ethical uses of AI."
287,Ernst & Young,Q08 Generic vs technology-specific approaches for AI risk,"The categorisation of AI regulation set out in Figure 2 of the Discussion Paper addresses important functional areas and vertical sectors where specific legislation may be required, due to the critical importance of the sector (health and financial services), the safety issues (health and automated vehicles) or their other unique characteristics (defence, government).We agree with the categories adopted and the breakdown into generic and specific types. The generic regulatory frameworks will still likely need to be adapted to address AI-specific risks occasioned by the scale of infringement that AI may unfortunately make possible. It may still be expedient to explicitly ban some applications that could be addressed within a general category, such as widespread facial recognition, predictive policing and social scoring.Consideration could be given to adding intellectual property as a category, to address issues that have arisen around the use of data for training.We also suggest adding "
287,Ernst & Young,Q09 Transparency requirements and scope,"A high degree of transparency will help mitigate risks and improve public trust in:· The acquisition, sharing and use of data by governments and the private sector for model training and developing applications. In may be sufficient to rely on existing (and proposed) privacy frameworks to AI. However, consideration should be given to the potential increase in value and interest in some datasets (e.g., public video, public text corpuses) due to their potential value for model training. Datasets may need to be carefully reviewed to mitigate inaccuracies and historical bias and prejudices.· Using personal data in the provision of services to users. The Attorney-General’s Report on the Privacy Act 1988 (Cth) is indicative of the future direction of privacy policy in Australia. The proposed requirement to act fairly and reasonably when collecting, using and disclosing personal information (Proposal 12) would apply readily to AI in respect of both model training and the use of data in the pr"
287,Ernst & Young,Q10 Ban on high-risk AI applications,"Banning in general should be considered a last resort when other methods of regulation are inadequate. In an Australian social context, the key areas where a ban could be considered comprise:· Social scoring algorithms. · Predictive policing · Widespread / indiscriminate facial recognition and other forms of biometric identification (potentially with limited exceptions requiring appropriate judicial oversight).b. Criteria or requirements to identify AI applications or technologies that should be banned, and in which contexts?The first area where bans could be considered relate to infringement on existing established rights (e.g., life, health, a fair trial and due process, property, freedom of expression, privacy, freedom from discrimination) and so on. These rights are covered by existing regulatory frameworks, but these should be assessed for their effectiveness in an AI context. Tying the rationale for banning technologies to an infringement of an established right grounds the issue"
287,Ernst & Young,Q11 Actions to increase public trust in AI,"Governments can play a crucial role in shaping perceptions of AI and can implement measures to improve public trust.It may be common sense, but it is worth stating that AI will only succeed in the public’s eyes if it is in fact successful, i.e., that it works and offers tangible advantages over alternative approaches. Especially at this relatively early stage, governments should invest in humans in the loop, system redundancies and other modalities to ensure that AI implementations are given the time and resources to iron out issues with minimal adverse impacts on the public.Turning first to a government context, it is unlikely anyone wants remote, unresponsive systems making opaque decisions that are justified by cost or productivity savings that are not passed on. At a minimum, the rationale for adopting AI should be clearly explained and, for large-scale implementations, subject to widespread consultation.Government AI implementations should lean into increasing the transparency of "
287,Ernst & Young,Q12 Trade impact of banning high-risk AI applications,"In terms of direct impacts, the banning of undesirable technologies in Australia will necessarily reduce the potential trade benefits of supplying services to support those activities, as with the banning of other undesirable activities. This should of course be seen as a price worth paying.More broadly, technologies are often “dual use”, with the ability to be applied to desirable or undesirable purposes. In such cases, the approach taken in other industries could be adopted, involving restricting the sale of technologies for those undesirable purposes, or to markets where such purposes are foreseeable or likely. That should minimise the possible adverse trade impacts of restricting sales. If the uses are not capable of being adequately separated, then consideration may need to be given to restricting sales more broadly."
287,Ernst & Young,Q13 Changes to conformity infrastructure,"There are many components to a comprehensive system of assurance addressing the potential risks of AI, some of which are in place today. Many of the key components are addressed elsewhere in our submission, including:• Ethical principles and frameworks for AI, such as the NSW Government AI Ethics Principles and AI Assurance Framework. • A mix of adaptations to existing legislation and new legislation to provide mandatory rules in areas of significant risk. • The ongoing work of Standards Australia, which can help inform the private sector and lay the basis for future regulation. • The role of education, of both the private and government sectors, to ensure that decision-makers have an in-depth understanding of AI’s risks and opportunities. • Funding research and fostering collaborations into AI safety and ethics across government, academia and industry.The conformity infrastructure could be further supported by:• Establishing a central regulatory body to oversee the sector, issue bindi"
287,Ernst & Young,Q14 Risk-based approach to address AI risks,"EY supports the careful and considered adoption of risk-based regulation for AI. We believe that making the burden of compliance proportionate to the risks is critical to balancing the need to avoid misuse while encouraging important innovation. Its relevance can be seen in its growing role in AI regulation in a number of jurisdictions comparable to Australia.However, the concept will require substantial elaboration to develop into a fully worked regulatory regime. It will need, for example, clear guidance on how AI risks will be assessed so that developers and investors can make confident choices about where they should invest their resources. We have set out further considerations relating to implementing the risk-based model in our response to question 15."
287,Ernst & Young,Q15 Benefits and limitations of risk-based approach,"A risk-based approach will tend to:• Increase the overall deployability of AI in the economy while protecting citizens from substantial risks. • Channel innovation into areas where the benefits are likely to outweigh the risks, and away from innovation in higher-risk areas, guiding the private sector to opportunities with greater social value. • Position regulators to identify and respond to risks over time, as opposed to a more static approach. This will be particularly important given the fast-paced nature of AI development. • Direct resources to where they will achieve the most benefit in terms of harm reduction, achieving greater benefits at a lower resource cost. Although the concept of a risk-based approach can result in a matching of risks to enforcement activity, it is not a “cure-all”:• It depends on a strong technical understanding of risk which government may find challenging in such a complex, fast-moving area.• The principles of risk assessment need to be clearly articulat"
287,Ernst & Young,Q16 Scope and application of risk-based approach,"As a general principle, a risk-based approach is a suitable model regardless of the sector, application, organisation size, AI maturity or resources. In applying a risk-based approach, it may be relevant to consider that:· Some sectors (e.g., healthcare) are inherently higher risk than others (e.g., book publishing). · Some applications (e.g., an ADM conducting credit analysis) are inherently higher risk than others (e.g., a retailer’s chatbot), noting that the issues may be at a sub-system level—a car’s braking system is inherently more safety critical than its climate control. · Larger organisations (or the government) are systematically more important than smaller organisations, and may be more likely to undertake significant novel or risky innovation."
287,Ernst & Young,Q17 Elements to include in risk-based approach,"The mechanisms set out in Attachment C of the Discussion Paper outline a balanced approach to monitoring risks and informing consumers of when AI is used. Additional approaches might include:• Robustness and security testing—AI systems should be robust and secure to prevent malicious attacks and to ensure they work reliably. Higher-risk AIs should be subject to regular testing for robustness against adversarial attacks and penetration testing to evaluate system security. • Third-party auditing—Independent external audits can help ensure that AI systems are complying with established standards and regulations. Auditing could cover areas such as data handling practices, algorithmic fairness, system security, and privacy protections. • Bias and fairness evaluations—Checks for potential bias and fairness in AI decision-making processes could involve a mix of auditing and the implementation of fairness metrics. • Redress mechanisms—Mechanisms should be implemented to challenge decisions mad"
287,Ernst & Young,Q18 Integration of risk-based approach into existing frameworks,"A pre-cursor to integrating an AI risk-based approach into any existing frameworks is a comprehensive review of all assessment frameworks in operation. Similar to that undertaken by NSW Treasury, Regulating for NSW’s Future (July 2020), any such review should provide the necessary insight into where duplication exists, and the extent to which existing frameworks are fit-for-purpose, and capable of responding to the rapidly changing technological landscape.To this effect, we anticipate the Government’s response to the Attorney General’s report on its review of the Privacy Act 1988 (Cth) to be delivered later in 2023."
287,Ernst & Young,Q19 Application of risk-based approach to general purpose AI,"Foundation models (FMs) are truly ""foundational"" in serving as the basis for a range of AI use cases. Their potential for benefits and harms is widespread and varied. Some key factors for consideration:• The recent rapid progress in FMs has made it challenging to assess their implications for the wider technology sector, business and society at large. • Advances in the development of FMs and their compression and distribution (after they are developed) are making FM technology progressively more widely available with a smaller visible footprint. It is already possible to install and use an LLM while remaining offline on a consumer PC or laptop, and use it to generate information that may be usable for criminal purposes. This problem is likely to grow. As an example of possible future directions, OpenAI's Code Interpreter plugin gives ChatGPT access to programming resources and the ability to solve multi-step problems involving complex data analysis. A future offline version of such a t"
287,Ernst & Young,Q20 Voluntary vs mandated compliance with risk-based approach,"Yes, a risk-based approach should be mandated across public and private sectors (noting our responses to questions 6 and 19).EY has written previously on the need to establish a central regulatory agency with authority to introduce binding market regulation for AI and ADM (https://www.ey.com/en_au/government-public-sector/building-a-trusted-ai-framework-for-the-public-sector). This approach can still allow for self-regulation where appropriate (as the regulator may choose not to issue regulations in some areas) in order to continue to foster innovation and ensure that regulation is targeted to significant public policy goals. But it will still allow for a rapid, binding regulatory response as and when a policy need arises."
288,Royal Australian College of General Practitioners (RACGP),Transcription,"FILENAME: signed. RACGP letter to DISR re Safe and responsible use of AI in Australia - July 2023 - FINAL.5afb5dc48841e.pdf | CONTENT: 26 July 2023Mr Anthony Murfitt Technology Strategy Branch Department of Industry, Science and Resources GPO Box 2013 Canberra ACT 2601 e: DigitalEconomy@industry.gov.auDear Mr MurfittThe Royal Australian College of General Practitioners (RACGP) appreciates the opportunity to provide a response to the Department’s discussion paper, ‘Safe and responsible AI in Australia’. Rather than addressing the specific questions in the consultation paper, many of which are outside of the remit of the RACGP, we provide high level feedback and recommendations.The RACGP is the voice of general practitioners (GPs) throughout Australia. We support the centre of Australia’s health system by setting the standards for education and practice and advocating for better health and wellbeing for all Australians. We cultivate a strong profession by developing resources and guideli"
289,Good Ancestors Policy,Transcription,"FILENAME: Good Ancestors DISR submission.4b02ed5f69e98.pdf | CONTENT: Good Ancestors Policy is an Australian charity dedicated to reducing existential risk and improving the long-term future of humanity. We care about today’s Australians and we care about future generations. We believe that Australians and our leaders want to take meaningful action to combat the big challenges Australia and the world are facing.This submission:Frames the importance of including catastrophic and existential risks in the Australian policy conversation by reference to other crises of the 21st century, expert views and public opinion.Explains how catastrophic risks can result from AI systems and provides frameworks for thinking about the risks of future, more advanced AI systems. Successful regulatory schemes are forward-looking, and AI’s pace of change makes that even more important in this context.Details practical actions the Australian Government could take to address these risks, using research and pe"
290,Risk Management Institute Australasia,Transcription,"FILENAME: Safe use of AI Final RMIA Submission.67c863e2648e4.pdf | CONTENT: RMIA Submission Safe Use of AI Date 26th July 2023 CONTRIBUTORS: SIMON LEVY – CHIEF EXECUTIVE OFFICE RMIA GRAEME SAMUEL AC – MONASH BUSINESS SCHOOL SAMANTHA NEWLANDS – VP RISK – APPEN AI CLAYTON SCOTT – EAGLE ONE CYBER JASON SMITH – EXPERIENCED CRORisk Management Institute of Australasia Limited ABN 82 106 528 509 20 Rosedale Road Gordon NSW 2072 Australia I P: +61 (0) 2 9095 2500 I E: admin@rmia.org.au I W: www.rmia.org.au Risk-Based Approaches Question #1Do you support a risk-based approach for addressing potential AI risks? If not, is there a better approach?As we navigate the rapidly evolving landscape of technology, we must address the increasing role of Artificial Intelligence (AI) and Generative AI in our society. Rather than attempting to regulate the intricate process, we must adopt a principled approach that focuses on desired outcomes that focus on desired outcomes. AI, including advanced models such"
290,Risk Management Institute Australasia,Q01 Definitions,Yes
290,Risk Management Institute Australasia,Q02 Other AI risks and recommendations for regulation,N/A
290,Risk Management Institute Australasia,Q03 Non-regulatory actions to address AI risks,N/A
290,Risk Management Institute Australasia,Q04 Coordinating AI governance across government,N/A
290,Risk Management Institute Australasia,Q05 International models for AI governance,N/A
290,Risk Management Institute Australasia,Q06 Public vs Private sector governance,N/A
290,Risk Management Institute Australasia,Q07 Responsible government use of AI,N/A
290,Risk Management Institute Australasia,Q08 Generic vs technology-specific approaches for AI risk,N/A
290,Risk Management Institute Australasia,Q09 Transparency requirements and scope,N/A
290,Risk Management Institute Australasia,Q10 Ban on high-risk AI applications,N/A
290,Risk Management Institute Australasia,Q11 Actions to increase public trust in AI,N/A
290,Risk Management Institute Australasia,Q12 Trade impact of banning high-risk AI applications,N/A
290,Risk Management Institute Australasia,Q13 Changes to conformity infrastructure,N/A
290,Risk Management Institute Australasia,Q14 Risk-based approach to address AI risks,"As we navigate the rapidly evolving landscape of technology, we must address the increasing role of Artificial Intelligence (AI) and Generative AI in our society. Rather than attempting to regulate the intricate process, we must adopt a principled approach that focuses on desired outcomes that focus on desired outcomes. AI, including advanced models such as LLM (Large Language Model), Chat GPT, and Bard, has revolutionised how we interact with information and has the potential to bring about both positive and negative impacts. While existing laws often prioritise the principle of ""do no harm,"" it is essential to acknowledge that AI can extend beyond the boundaries of this principle, necessitating the establishment of robust protections for individuals and communities which is a risk-based approach supported by principles.As the genesis of AI dates to the visionary work of Alan Turing in the 1930s, its evolution has witnessed tremendous advancements with the advent of powerful computing"
290,Risk Management Institute Australasia,Q15 Benefits and limitations of risk-based approach,"A risk-based approach to the safe use of AI is a systematic process for identifying, assessing, and mitigating the risks associated with AI systems. This approach can help organisations to ensure that AI systems are used safely and responsibly.The benefits of a risk-based approach to AI safety include:Identifying and mitigating potential risks: A risk-based approach allows for a systematic assessment of potential risks associated with AI systems. By identifying these risks early on, appropriate measures can be taken to mitigate or manage them effectively.Focusing on critical areas: Prioritising risks based on their potential impact helps allocate resources and attention to the most critical areas. This ensures that efforts are concentrated where they are most needed, reducing the likelihood of harmful outcomes. This ensures that actions are concentrated where they are most needed, reducing the likelihood of harmful outcomes.Flexibility and adaptability: A risk-based approach provides t"
290,Risk Management Institute Australasia,Q16 Scope and application of risk-based approach,"The safe use of artificial intelligence (AI) is a complex issue that requires a comprehensive and inclusive approach. The Risk Management Institute of Australasia (RMIA) has made several recommendations for ensuring the safe and responsible use of AI, including:Adopting a risk-based approach means assessing the potential risks associated with AI systems and implementing measures to mitigate those risks. The risk-based approach should be tailored to the specific sector or industry and flexible enough to adapt to the rapid pace of technological change.Review existing regulatory framework: if an overarching framework is considered, it should be comprehensive and apply to all sectors, regardless of size, AI maturity, or available resources. The regulatory framework should be designed to promote AI's safe and responsible use while ensuring that innovation is not stifled. It is noted that developing a new regulatory framework is not recommended, and Regulations can breed complacency.Fosterin"
290,Risk Management Institute Australasia,Q17 Elements to include in risk-based approach,"A risk-based approach is essential for managing the potential risks associated with AI systems. This approach is essential for ensuring that AI is developed and deployed safely and responsibly.The elements outlined in Attachment C—impact assessment, notices, human-in-the-loop/oversight assessments, explanations, training, and monitoring and documentation- provide a comprehensive and proportional approach to addressing potential AI risks. They cover crucial aspects from the initial stages of AI system development through deployment and ongoing monitoring. By incorporating these elements into AI governance frameworks and policies, you can ensure that the risks associated with AI are adequately assessed, managed, and mitigated. However, it is important to note that the risk landscape surrounding AI is continuously evolving. Therefore, it is recommended to periodically review and update these elements to align with the latest advancements, emerging risks, and regulatory frameworks. In conc"
290,Risk Management Institute Australasia,Q18 Integration of risk-based approach into existing frameworks,"Incorporating an AI risk-based approach into existing assessment frameworks and risk management processes can help streamline and reduce potential duplication. Here's how various aspects can be addressed: 1. Parties Developing, Building, or Adding Domain Knowledge: When developing or building AI systems, the involved parties must comprehensively understand the domain or industry they are operating. This knowledge should encompass the technical aspects of AI and the specific risks and considerations relevant to the industry. Collaborative efforts between AI developers and domain experts can ensure that AI systems are built with a deep understanding of the risks and potential impacts on the industry. 2. Companies Buying and Plugging AI Solutions: Organisations that purchase AI solutions from third-party vendors must assess the associated risks, thereby taking responsibility for their supply chain. This includes evaluating the reliability and integrity of the purchased AI systems and thei"
290,Risk Management Institute Australasia,Q19 Application of risk-based approach to general purpose AI,"A risk-based approach can be applied to general-purpose AI systems, including large language models (LLMs) and multimodal foundation models (MFMs), to ensure their safe and responsible use. Here are some key considerations regarding the application of a risk-based approach to these models:1. Different uses of LLMs and MFMs: LLMs and MFMs serve as foundational tools that enable various applications and solutions. LLMs, such as GPT-3, are primarily used for generating text based on input prompts. At the same time, MFMs combine multiple modalities like text, images, and audio to perform tasks like image captioning or speech recognition.2. Using LLMs (Large Language Models) as a base: Organisations often build upon LLMs or MFMs (multimodal foundation models) to create institutional models tailored to their specific needs. This approach allows customisation while leveraging the underlying capabilities of the LLM or MFM (multimodal foundation models).3. Risk assessment at the development sta"
290,Risk Management Institute Australasia,Q20 Voluntary vs mandated compliance with risk-based approach,"Regarding voluntary versus mandated regulation, the RMIA recommends that a risk-based approach for responsible AI be mandated through law rather than relying solely on voluntary or self-regulation. While voluntary standards and codes of practice have their place in promoting best practices and encouraging responsible behaviourExisting regulatory bodies such as the Australian Federal Police, Crime Commission, APRA (Australian Prudential Regulation Authority), ASIC (Australian Securities & Investments Commission), and CASA (Civil Aviation Safety Authority) already deal with breaches of the law and oversee compliance and enforcement within their respective domains. However, considering AI's unique challenges and opportunities, these regulatory bodies should conduct a comprehensive gap analysis to identify areas where existing regulations may fall short in addressing AI-specific risks. If such gaps are identified, they should be rectified through appropriate regulatory measures. Both publi"
292,Communications Alliance,Transcription,"FILENAME: 230726_CA sub_Safe responsible AI_SUBMITTED.f5b97fba4fc81.pdf | CONTENT: COMMUNICATIONS ALLIANCE LTDCommunications Alliance Submission to the Department of Industry, Science and ResourcesSafe and responsible AI in Australia Discussion Paper26 July 2023Communications Alliance Submission to AGD, Privacy Act Review Report 2022 31 March 2023General 1CONTENTSCOMMUNICATIONS ALLIANCE 2INTRODUCTION 31. AI – A FORCE FOR GOOD 42. DEFINITIONS OF AI 53. APPROACH TO AI REGULATION 74. COORDINATION AND COOPERATION 95. CONCLUSION 11Communications Alliance Submission to DISR – Safe and responsible AI in Australia 26 July 2023 2Communications Alliance Communications Alliance is the primary communications industry body in Australia. Its membership is drawn from a wide cross-section of the communications industry, including carriers, carriage and internet service providers, content providers, platform providers, equipment vendors, IT companies, consultants and business groups. Its vision is to b"
293,Concordia Vox,Transcription,"FILENAME: Responsible AI - Concordia Vox submission.9204427a06b72.pdf | CONTENT: +61 435 948 464 james@concordiavox.au 212/132 Smith, St Collingwood, Victoria, 3066To:Technology Strategy Branch Department of Industry, Science and Resources Industry House, 10 Binara Street Canberra ACT 2601 26 July 2023Dear General Manager,Thank you for the opportunity to contribute to the Australian Government’s consultation on additional AI governance mechanisms.This submission contains a brief overview of Concordia Vox and our interest this consultation process as well as responses to select questions posed by the discussion paper.We have also prepared three overarching recommendations that consolidate these responses into practical policy or governance measures and are designed to complement many of the initiatives already identiﬁed within the paper. These recommendations are to:1. Extend the Consumer Data Right to cover AIThe discussion paper provides a comprehensive list of principles, standards, "
293,Concordia Vox,Transcription,"FILENAME: Responsible AI - Concordia Vox submission.8392d888f9aa5.docx | CONTENT: To:Technology Strategy BranchDepartment of Industry, Science and ResourcesIndustry House, 10 Binara StreetCanberra ACT 260126 July 2023Dear General Manager,Thank you for the opportunity to contribute to the Australian Government’s consultation on additional AI governance mechanisms.This submission contains a brief overview of Concordia Vox and our interest this consultation process as well as responses to select questions posed by the discussion paper.We have also prepared three overarching recommendations that consolidate these responses into practical policy or governance measures and are designed to complement many of the initiatives already identified within the paper. These recommendations are to:Extend the Consumer Data Right to cover AIThe discussion paper provides a comprehensive list of principles, standards, codes, and laws that might govern AI in Australia. Also important are the administrative"
293,Concordia Vox,General comment,The submission has been provided in both PDF and DOCX formats (the latter for accessibility). Answers to individual questions are in the file.
295,Chelsea Liang,Q02 Other AI risks and recommendations for regulation,"Potential risks Experts in global catastrophic risks were actively warning about the dangers of pandemics in the decade preceding COVID-19.In 2017, Oxford researcher Sebastian Farquar said: “As the Ebola and Zika crises showed, managing pandemics is a global responsibility, but too much planning is still national, and little attention is paid to worst-case scenarios.” [1]If Governments had listened to such experts prior to COVID-19, low-cost interventions, like better regulation of exotic animals markets, could have prevented millions of deaths and trillions in economic damages.The situation that Dr. Farquar describes is eerily similar to the approach to AI Governance in Australia; focused on lower-impact, local issues to the detriment of higher-impact global challenges.This is in contrast to concerns about existential risks being recognised at the highest echelons of global governance, including the UN Secretary-General’s recent statement [2]:""Alarm bells over [AI] are deafening, and "
295,Chelsea Liang,Q03 Non-regulatory actions to address AI risks,"AI is very likely to transform many elements of the economy and society in the coming years. Ethics and safety are unlikely to be a priority unless leaders make them a specific focus. For example, social media algorithms came to shape our lives with little upfront focus on how they could be made ethical and safe. We are now living with the negative impacts on mental health and social cohesion and only have band-aid measures to address the widespread harms. AI is likely to be much more transformative and much more consequential. So leadership is required now.Importantly, the economic and technological forces are likely to be so strong that a scheme that is voluntary or driven by aspirational best practice is unlikely to be successful. That is, any non-regulatory approaches must not be taken to displace the need for strong regulation.Specific non-regulatory approaches should include:1. Government leadership in recognising the risk. Australia must join the growing number of countries and "
295,Chelsea Liang,Q10 Ban on high-risk AI applications,"I support a risk-based approach, however the catastrophic and existential risks I am primarily worried about originate when sufficiently large and capable AI systems are trained, not just when they are deployed. Therefore, for sufficiently large systems with the potential for catastrophic capabilities, the risk-based approach must be applied before the training phase – analysing risks depending on the nature and scale of the proposed training run, rather than merely analysing the potentially risky use cases and deployment issues of an AI after it has already been created and trained.Any assessment of risk should consider its potential irreversibility, scope and severity, and act proportionally.The risk-based approach set out in Attachment C of the Discussion Paper is hard to follow. The proposed approach offers “human in the loop” as the key mitigation (unclear that this is an effective mitigation) and goes on to note that human in the loop may not be possible where the benefit of the "
295,Chelsea Liang,Q14 Risk-based approach to address AI risks,"I support a risk-based approach, however the catastrophic and existential risks I am primarily worried about originate when sufficiently large and capable AI systems are trained, not just when they are deployed. Therefore, for sufficiently large systems with the potential for catastrophic capabilities, the risk-based approach must be applied before the training phase – analysing risks depending on the nature and scale of the proposed training run, rather than merely analysing the potentially risky use cases and deployment issues of an AI after it has already been created and trained.Any assessment of risk should consider its potential irreversibility, scope and severity, and act proportionally.The risk-based approach set out in Attachment C of the Discussion Paper is hard to follow. The proposed approach offers “human in the loop” as the key mitigation (unclear that this is an effective mitigation) and goes on to note that human in the loop may not be possible where the benefit of the "
295,Chelsea Liang,Q16 Scope and application of risk-based approach,"The concept of “AI” includes a broad range of capabilities and capacities. Government is right to work to distinguish between different use cases - including with a specific focus on AI maturity.Concerningly, this broad range of AI capabilities and capacities might be used by industry to attempt to bamboozle government. Specifically, industry is likely to argue that AI does not need regulation with reference to basic AI technology, with a view to that principle flowing through to more advanced and future technologies. This is akin to arguing that a horse and wagon doesn’t need strict regulation, then later trying to claim that a tank is just a more capable horse and wagon with some components swapped out.Government needs both a fine-grained approach to defining AI technology and a fine-grained approach to understanding AI risk to ensure it doesn’t fall for these tricks.In terms of risks, research shows that two of the main sources of risk are deception and power-seeking behaviour: - De"
295,Chelsea Liang,Q19 Application of risk-based approach to general purpose AI,"The capabilities unlocked by general-purpose AI systems, like large language models (LLMs) and multimodal foundation models (MFMs), fundamentally change the risk landscape. We should anticipate these systems having large, unpredictable effects across all parts of society.A prudent approach is to implement a risk-based framework, where access is graduated based on model capabilities. Models above a certain threshold of autonomous capabilities would be designated as ""advanced systems"" and subject to additional governance, such as permits for users.Further, permits should be required to gain certain levels of access to AI systems which can be used to build advanced systems. Permits allow us to maintain oversight, while still enabling beneficial uses. Different permissions can be granted based on use case - for example, full access for safety testing versus API-only access for most commercial uses. This balances innovation and responsibility.Although this may seem very different to how AI "
295,Chelsea Liang,Q20 Voluntary vs mandated compliance with risk-based approach,"The harms of AI could emerge equally from private or public organisations, and the appropriate safety and risk management mechanisms will need to be in place in both cases. There’s no basis for distinguishing between their regulatory status in general. There may be case-by-case instances where exceptions may make sense, for instance, in the case of certain types of research.Regulation should also apply to both developers and deployers - following the general principle that those best able to mitigate a risk should carry the corresponding regulatory burden. Where developers offer a “black box” product that deployers typically do not have the capability or capacity to meaningfully alter, the bulk of the obligation should rest on developers.Penalties also need to be commensurate with the income earned from the AI applications so that non-compliance with regulation never becomes a “cost of doing business” or “risk-managed” by deployers. Technical transparency and assurance measures will ne"
297,BlackBerry,Q01 Definitions,Yes we agree with the definitions
297,BlackBerry,Q02 Other AI risks and recommendations for regulation,"One potential risk from AI that is not covered by Australia’s existing regulatory approach is cybersecurity challenges enabled by Generative AI applications such as ChatGPT. BlackBerry’s latest research found that the biggest cybersecurity concern for Australians in relation to ChatGPT is that it would aid less experienced hackers in improving their tech knowledge and developing more specialised skills (50%), crafting more believable phishing emails (48%, and be used for mis/disinformation (46%). With this in mind, 85% of Australians plan to invest in AI-driven cybersecurity in the next two years.BlackBerry’s Quarterly Threat Report (embargo August 2) notes that ChatGPT has already been used to generate new malware—for example, researchers from HYAS Labs created BlackMamba, a proof-of-concept polymorphic keylogger that automatically changes its code on the fly to evade detection, exploiting a large language model (LLM)—the technology on which ChatGPT is based. Threat actors are also ex"
297,BlackBerry,Q03 Non-regulatory actions to address AI risks,"Innovative app technologies offer significant benefits to productivity and creativity in the workplace. However, the increasing popularity of 'recreational apps' such as TikTok and ChatGPT introduce new risks that businesses must acknowledge and proactively safeguard against. It is essential for organisations to be well-prepared to defend against potential threats, as the repercussions of a public data breach caused by an app downloaded on a work device (or private / IP shared via ChatGPT) can harm the organisation's reputation and security.In addition to Federal and some State Government bans globally, several businesses in the private sector are prohibiting at-risk apps such as TikTok on business devices. Yet, managing this challenge while ensuring a positive end-user experience is a challenge.Australia’s cybersecurity environment is undeniably becoming more perilous and intricate, especially with the rise of distributed workforces utilising personal devices like laptops, tablets, an"
297,BlackBerry,Q04 Coordinating AI governance across government,"BlackBerry recommends that the Australian Government adopt an agile approach to regulating AI due to the various technologies involved and the fast pace of AI system developments. It would be ideal to have short-term policy guidance in place so that national governments can take immediate action. However, the policies should be flexible and continually learning like AI engines.If the policies are too restrictive for innovation, companies may resist, while adversaries may not face such barriers and will advance. There is the opportunity to develop and access regulations and guidelines in collaboration with stakeholders on a regular basis and adapt enforcement to suit the changing environment. We need to use the power and resources of global governments and organisations to move faster, more safely, and more effectively."
297,BlackBerry,Q05 International models for AI governance,"BlackBerry would encourage the Australian government to align its regulations with international standards developed by organisations/ international standards bodies such as ISO/IEC JTC1 SC 42 and international frameworks such as the OECD AI principles. However, regulation will – and should – remain up to the individual countries protecting their populations and sensitive data, as Australia does already by appointing Chief Risk Officers and implementing risk programmes. Although we need to convene to bring a global viewpoint to country-level initiatives, ultimately, the protection of local data and the erosion of identity will drive action in each country."
297,BlackBerry,Q06 Public vs Private sector governance,"We believe the approach to AI technologies should be a level playing field, as it has the potential impact to both the public and private sector equally."
297,BlackBerry,Q07 Responsible government use of AI,"As the Government progresses in its considerations on AI regulation, it is vital to acknowledge that AI innovation is still in its early stages, encompassing a wide array of applications and not confined to a singular definition. This highlights the need for regulators to exercise caution when formulating new rules, ensuring they strike a balance that does not overly restrict, burden, or undermine the numerous benefits this transformative technology offers to the public as not all AI is high risk and needs to be regulated."
297,BlackBerry,Q08 Generic vs technology-specific approaches for AI risk,"Organisations need to stop talking about detection and response and start talking about proactive protection, which itself can leverage the power of AI technology."
297,BlackBerry,Q09 Transparency requirements and scope,"AI technology can tap into vast amounts of data and use advanced algorithms to analyse and identify patterns that may indicate potential threats. It can also provide calculated recommendations for mitigating risks and optimising security measures.BlackBerry has embraced a comprehensive approach that combines real-time threat intelligence, global endpoint data and advanced AI algorithms. This enables defenders to continuously learn from the ever-evolving threat landscape and enhance their ability to protect against sophisticated adversaries.On August 9, BlackBerry will announce a major generational update to the Cylance® AI engine powering its cybersecurity portfolio, marking a significant step forward in strengthening predictive zero-day attack defence. Compared with earlier versions, the 8th Generation engine enhances threat prediction capabilities for organisations by 40%, without introducing false signal noise that can strain overtaxed security teams. BlackBerry’s learning intellige"
297,BlackBerry,Q10 Ban on high-risk AI applications,"In the wake of the TikTok ban on private and public sector devices, Australia’s Federal government is also seeking advice from Home Affairs on the security risks of Instagram, Netflix, Twitter and dating apps on government-issued devices.Considering this, many enterprises are confronting a tough challenge: the growing threat of malware attacks aimed at mobile devices, especially those with certain apps running on them. In particular, BlackBerry’s August Threat report will cite the persistent threats financial institutions are facing through smartphone-centric commodity malware, ransomware attacks, and the rise of mobile banking malware targeting digital banking services. The report explores cybersecurity challenges for the financial sector as threat actors exploit an online banking-first mindset.Forthcoming BlackBerry research notes that there is a belief that banning popular “recreational apps” like ChatGPT and TikTok within the workplace risk alienating employees. Additionally, despi"
297,BlackBerry,Q11 Actions to increase public trust in AI,"Ensure that the public, especially business decision-makers area aware that in order to safely use AI to advance productivity - they should also look at using AI technology for cyber-defence.With the continued emergence of advanced technologies such as Generative AI used for ill-intent by threat actors, and Polymorphic malware to evade detection, defending against evolving threats requires a more proactive approach to cybersecurity to better anticipate what is around the corner. This means the use of AI and Machine-Learning for prevention and detection capabilities, as well as effective threat intelligence.AI-based cybersecurity solutions can help organizations combat CaaS in several ways. Faced with an expanding cyber-skills shortage, intelligent cybersecurity powers advanced threat detection through AI algorithms which can analyze vast amounts of data and identify patterns that may be missed due to human error. This enables organizations to detect and proactively manage emerging thre"
297,BlackBerry,Q14 Risk-based approach to address AI risks,"In the ever-evolving landscape of AI-driven cyber threats, leveraging predictive AI emerges as a powerful safeguard. With threat actors deploying tens of thousands of unique malicious samples daily, the attack surface is expanding rapidly. Malicious cyber actors are arming themselves with AI-driven applications, skilfully creating malicious code and concealing payloads.To thwart these growing risks, AI-driven cybersecurity tools offer an automated arsenal for risk mitigation and early threat detection. By analysing vast amounts of data and behaviours across multiple domains, these tools can proactively defend organisations from potential harm. The continuous improvement of AI's learning capabilities ensures that more mature solutions become even better at detecting threats, fortifying businesses against evolving cyber challenges."
297,BlackBerry,Q15 Benefits and limitations of risk-based approach,"At BlackBerry, we encourage our customers to have an AI-powered, prevention-first approach to cybersecurity – rather than the fix-the-breach, EDR-first approach. AI and ML play an important role in threat detection based on its ability to model the ‘normal’ behaviour of the organisation and its users and then either detect anomalies that do not match the behaviour of any user within the organisation and/or make predictions as to whether a particular networking behaviour has a lower or higher probability of being associated with a particular user. This combination of model-based anomaly detection and user-specific prediction can help reduce false positives and negatives.For example, there may be cases where a particular pattern is relatively unusual for the organisation overall (for example, a highly specialised role that uses apps and services unique to that role), but is strongly associated with a particular user or small group of users. In such cases, the high predictive probability "
298,Governance Institute of Australia,Transcription,"FILENAME: Governance Institute Submission Responsible AI.c23ce439fc7e5.pdf | CONTENT: T +61 2 23 5744 F +61 2 9232 7174 Technology Strategy Branch Department of Industry, Science and Resources E info@governanceinstitute.com.auDigitalEconomy@industry.gov.au Level 11, 10 Carrington Street,26 July 2023 Sydney NSW 2000GPO Box 1594, Sydney NSW 2001 Dear Department of Industry, Science and Resources, W governanceinstitute.com.auGovernance Institute of Australia response to the ‘Safe and Responsible Artificial Intelligence (AI) in Australia’ discussion paper.Who we areGovernance Institute of Australia (Governance Institute) is a national professional association, advocating for our network of 43,000 governance and risk management professionals from the listed, unlisted, public, not-for-profit and charity sectors.As the only Australian provider of chartered governance accreditation, we offer a range of short courses, certificates and postgraduate study. Our mission is to drive better governanc"
298,Governance Institute of Australia,Q01 Definitions,See attached response
298,Governance Institute of Australia,Q05 International models for AI governance,See attached response
298,Governance Institute of Australia,Q06 Public vs Private sector governance,See attached response
298,Governance Institute of Australia,Q07 Responsible government use of AI,See attached response
298,Governance Institute of Australia,Q08 Generic vs technology-specific approaches for AI risk,See attached response
298,Governance Institute of Australia,Q09 Transparency requirements and scope,See attached response
298,Governance Institute of Australia,Q10 Ban on high-risk AI applications,See attached response
298,Governance Institute of Australia,Q11 Actions to increase public trust in AI,See attached response
298,Governance Institute of Australia,Q12 Trade impact of banning high-risk AI applications,See attached response
298,Governance Institute of Australia,Q13 Changes to conformity infrastructure,See attached response
298,Governance Institute of Australia,Q14 Risk-based approach to address AI risks,See attached response
298,Governance Institute of Australia,Q15 Benefits and limitations of risk-based approach,See attached response
298,Governance Institute of Australia,Q16 Scope and application of risk-based approach,See attached response
298,Governance Institute of Australia,Q17 Elements to include in risk-based approach,See attached response
298,Governance Institute of Australia,Q18 Integration of risk-based approach into existing frameworks,See attached response
298,Governance Institute of Australia,Q19 Application of risk-based approach to general purpose AI,See attached response
298,Governance Institute of Australia,Q20 Voluntary vs mandated compliance with risk-based approach,See attached response
299,RMIT University,Transcription,"FILENAME: RMIT Submission_AU Government Consultation on Safe and Responsible AI_20230726.9b6f382c09421.pdf | CONTENT: RMIT Classification: TrustedSafe and Responsible AI in Australia RMIT input to Australian Government Discussion Paper July 2023RMIT University welcomes the opportunity to comment on the Safe and Responsible AI in Australia Discussion Paper. Our feedback includes the views of a range of key stakeholders, including research leaders, researchers and research management professionals realising RMIT’s mission, goals and strategy for achieving research and innovation with positive impact.Central to this is the ethical and responsible design, development, deployment and use of enabling technologies, including artificial intelligence (AI). Recognising both the opportunities and challenges that AI technologies present, as well as the rapid advancement and developments of these technologies.Also recognising the unique position of universities and research communities, as being us"
299,RMIT University,General comment,"RMIT University submission to Department of Industry, Science and Resources consultation on 'Safe and Responsible AI in Australia' Discussion Paper."
299,RMIT University,Q01 Definitions,Yes. Noting these definitions of key terms are drawn from International Organisation for Standardization (ISO) definitions and accord with the understanding and use of these key terms in a higher education setting. It was suggested that the description of a large language model (LLM) as a “type of generative AI that specialises in the generation of human-like text” be corrected to “a probabilistic model that aims to represent the patterns of language usage in human text” for accuracy and in keeping with the understanding and use of this term in computing sciences.
299,RMIT University,Q02 Other AI risks and recommendations for regulation,"Recognising Australia’s world-leading research and innovation capabilities, RMIT stakeholders identified potential risks relating to AI and research integrity. These risks include research falsification, proliferation of misinformation, decreased transparency, lack of reproducibility, entrenchment of bias, expansion of the utility of AI and risks relating to breaches or sharing of confidential, sensitive, or private data in research.RMIT stakeholders did not have any specific suggestions for possible regulatory action to mitigate these research integrity related risks. However, RMIT stakeholders observed that the Australian Code for the Responsible Conduct of Research 2018 is well-established and being a principles-based framework facilitates adaptation and application across the breadth and depth of research disciplines, as well as to novel challenges in a research context, more comfortably and quickly than a rules-based approach.RMIT stakeholders also identified potential gaps in rel"
299,RMIT University,Q03 Non-regulatory actions to address AI risks,"RMIT stakeholders were supportive of non-regulatory initiatives which would enhance and expand communities of practice and networks across the higher education, industry, government and community sectors, with a view to establishing, sharing and monitoring ethical and responsible AI practices, as well as broader public awareness and education, with a view to increasing public trust."
299,RMIT University,Q04 Coordinating AI governance across government,"The current regulatory landscape appears disparate and disjointed, with numerous general regulations and specific regulations addressing aspects of AI technologies governance and numerous regulatory agencies, bodies and departments at Commonwealth and State level having limited and in some cases duplicating and/or overlapping accountabilities.The establishment of an overarching regulatory framework which is risk-based and principles-based and which considers and sits over and across, or replaces, existing general regulations or sector-specific laws, along with the establishment of a central and suitably expert regulatory body, could assist in coordination of AI governance across government."
299,RMIT University,Q05 International models for AI governance,"RMIT stakeholders did not suggest any other governance measures being taken or considered by other countries which are not discussed in this paper, as being relevant, adaptable and desirable for Australia. RMIT stakeholders indicated a preference for governance measures which were proportionate to both the risks and benefits presented by AI, which adopt a principles-based approach and in which the wellbeing of humans and society, animals and the environment are paramount.RMIT stakeholders observed that there are many similarities between gene technologies and AI technologies, with both these technologies presenting significant challenges and opportunities and both having undergone periods of rapid advancement. It was suggested that Australia’s regulatory approach to gene technologies, including the establishment of the gene technology framework and a central Commonwealth regulator, could be relevant, adaptable and desirable in relation to AI technologies and in an Australian context."
299,RMIT University,Q06 Public vs Private sector governance,"As above, RMIT stakeholders were of the view that the regulatory approach to AI in Australia should be proportionate to both the risks and benefits presented by AI, adopt a principles-based approach and apply consistently to the design, development, deployment and use of AI technologies. This proportionate and risk-based approach would ideally be principles-based and lend itself to being applied broadly across all sectors.However, it may be appropriate to develop regulations specific to particular AI technologies, and/or particular activities or dealings with these technologies and/or to particular sectors. In this regard, an RMIT stakeholders observed the lack of recognition of the role of universities and other research institutions, with one commenting:“To draw an analogy with the medical space, research is essential for progress, but rigorous evaluation and safety protocols are required before research findings are deployed into the public sphere. Similarly, to ensure that Australi"
299,RMIT University,Q07 Responsible government use of AI,"Suggestions received included:• Strengthening governance frameworks and practices for the responsible and safe design, development, deployment, and use of AI technologies, • Providing ongoing training and education that promotes and supports responsible and safe design, development, deployment, and use of AI technologies, and • Ensuring public servants have the appropriate skills, qualifications, and resources to responsibly and safely design, develop, deploy and use AI technologies."
299,RMIT University,Q08 Generic vs technology-specific approaches for AI risk,No feedback.
299,RMIT University,Q09 Transparency requirements and scope,"RMIT stakeholders agree that transparency is critically important across the AI lifecycle. In the context of mitigating potential risks and improving public trust and confidence, transparency is particularly valuable in the design and development stages, as well as in reporting on these technologies in a way that is open, responsible, and accurate, and also in identifying, disclosing, and managing any related conflicts of interest. For example, the majority of Large Language Models (LLMs) are opaque and do not allow for meaningful human understanding and inspection of how these AI technologies work. Encouraging or requiring the sharing and training sets and model reporting (with the use of model cards for model reporting a good and relevant example https://dl.acm.org/doi/pdf/10.1145/3287560.3287596), assessment and evaluation results, and communicating explanations, inclusive of knowns and unknowns, would provide greater transparency and improve trust."
299,RMIT University,Q10 Ban on high-risk AI applications,"No suggestions on specific high-risk AI applications or technologies that should be banned completely, noting those technologies already identified as unacceptable risks. However, RMIT stakeholders recognised the need for rigorous oversight and regulation of high-risk AI applications or technologies such as medical or military, and potentially to pause or ban these where they present real and significant risks to the well-being of humans and society, animals and the environment."
299,RMIT University,Q11 Actions to increase public trust in AI,Please see feedback for question 3 above.
299,RMIT University,Q12 Trade impact of banning high-risk AI applications,No feedback.
299,RMIT University,Q13 Changes to conformity infrastructure,No feedback.
299,RMIT University,Q14 Risk-based approach to address AI risks,"As discussed above, RMIT is supportive of a proportionate and risk-based approach to both address the potential risks and realise potential opportunities presented by AI technologies, and in which the wellbeing of humans and society, animals and the environment are paramount. That is, the amount and type of regulation would increase proportionate to the risks presented, and with the wellbeing of humans and society, animals and the environment being paramount—meaning some AI technologies or uses of AI technologies may be banned or paused where they present unacceptable risks."
299,RMIT University,Q15 Benefits and limitations of risk-based approach,No feedback.
299,RMIT University,Q16 Scope and application of risk-based approach,No feedback.
299,RMIT University,Q17 Elements to include in risk-based approach,"RMIT stakeholders broadly supported the elements for the risk-based approach presented in Attachment C. Noting, that the impact assessment would also benefit from consideration of potential benefits in addition to potential risks and a justification for how any risks identified are justified. Stakeholders also observed that the concept of “human in the loop” in this context (that is, seemingly to represent a notion of human oversight and/or assessment) was essential but that the phrasing was problematic. In that “human in the loop” is often used to describe scenarios where humans are working or collaborating with an AI system to achieve a goal. That is, they are part of the process, which is substantially different from notions of human oversight and assessment as it is being used in this context. For clarity, it was suggested using terms such as “human oversight” or “human assessment” in this context, rather than “human in the loop.”"
299,RMIT University,Q18 Integration of risk-based approach into existing frameworks,No feedback.
299,RMIT University,Q19 Application of risk-based approach to general purpose AI,No feedback.
299,RMIT University,Q20 Voluntary vs mandated compliance with risk-based approach,"RMIT stakeholders see a need for and value in Australia adopting a common proportionate risk-based and principles-based regulatory approach to promote and foster the safe and responsible design, development, deployment and use of AI in Australia—as well as in these regulations being mandated and applying across all sectors (given the potential risks and recognising self-regulation is not appropriate or justifiable). Also, recognising that there may be a need for more specific and limited regulations to apply to particular AI technologies and/or to particular sectors and/or in particular contexts.Given the complexity of the current landscape and need to ensure that the optimisation goals of AI-based technologies and systems are congruent with objectives that minimise risks of harm and promote societal and ethical goals and objectives, RMIT stakeholders also emphasised the need for further resourcing for this area, as well as the establishment of a central and suitably expert regulatory "
300,Engineers Australia,Transcription,"FILENAME: Responsible AI in Australia - an engineering perspective (Engineers Australia).511346c13ea07.pdf | CONTENT: Responsible AI in Australia: An engineering perspective Engineers Australia’s submission to the Safe and Responsible AI in Australia discussion paperJuly 2023 Responsible AI in Australia: An engineering perspectiveThe report can be downloaded at engineersaustralia.org.auEngineers Australia 11 National Circuit, Barton ACT 2600 Tel: +61 2 6270 6555 Email: policy@engineersaustralia.org.au engineersaustralia.org.au Contents Introduction .................................................................................................................................................................................... 1 About Engineers Australia .................................................................................................................................................... 1 Contact ............................................................................."
301,Digital Health CRC Ltd,Transcription,"FILENAME: DHCRC Response to The Australian Government Department of Industry, Science and Resources ‘Safe and Responsible AI in Australia’ Discussion Paper.53ce5f55def4c.pdf | CONTENT: Response to The Australian Government Department of Industry, Science and Resources: ‘Safe and Responsible AI in Australia’ Discussion Paper Digital Health CRC (DHCRC) Ltd ______________________________________________________________________ The Digital Health CRC Limited connects government, academia and industry to accelerate the implementation and translation of evidence-based digital health technologies that solve the most pressing healthcare challenges. We are co- funded through the Commonwealth Government’s Cooperative Research Centres (CRC) Program, and by our Participant organisations. ______________________________________________________________________ Research Australia, the national alliance for the health and medical research and innovation sector, recognises the expertise of the Digital H"
301,Digital Health CRC Ltd,Transcription,"FILENAME: DHCRC Response to The Australian Government Department of Industry, Science and Resources ‘Safe and Responsible AI in Australia’ Discussion Paper.4fea92e41c098.docx | CONTENT: Response to The Australian Government Department of Industry, Science and Resources: ‘Safe and Responsible AI in Australia’ Discussion PaperDigital Health CRC (DHCRC) Ltd______________________________________________________________________leftThe Digital Health CRC Limited connects government, academia and industry to accelerate the implementation and translation of evidence-based digital health technologies that solve the most pressing healthcare challenges. We are co-funded through the Commonwealth Government’s Cooperative Research Centres (CRC) Program, and by our Participant organisations.______________________________________________________________________left00Research Australia, the national alliance for the health and medical research and innovation sector, recognises the expertise of the Digi"
301,Digital Health CRC Ltd,Transcription,"FILENAME: generative AI ethics_Lancet_DHCRC2023.61e302f466026.pdf | CONTENT: Personal ViewAttention is not all you need: the complicated case of ethically using large language models in healthcare and medicine Stefan HarrerDigital Health Cooperative Research Centre, Melbourne, AustraliaSummary eBioMedicine 2023;90: 104512 Large Language Models (LLMs) are a key component of generative artiﬁcial intelligence (AI) applications for creating new content including text, imagery, audio, code, and videos in response to textual instructions. Without human Published Online 15 March 2023 oversight, guidance and responsible design and operation, such generative AI applications will remain a party trick https://doi.org/10. with substantial potential for creating and spreading misinformation or harmful and inaccurate content at unprec- 1016/j.ebiom.2023. edented scale. However, if positioned and developed responsibly as companions to humans augmenting but not 104512 replacing their role in decision "
301,Digital Health CRC Ltd,Transcription,"FILENAME: generative AI governance_Lancet_DHCRC2023.4b315d148963.pdf | CONTENT: CorrespondenceResponse to M. Trengove & coll regarding “Attention is not all you need: the complicated case of ethically using large language models in healthcare and medicine” Stefan HarrerDigital Health Cooperative Research Centre, Melbourne, AustraliaTrengove and colleagues have made a valuable contri- clinicians in search of new treatment options. Whilst a eBioMedicine bution to a debate that currently dominates the agenda conception might be unexplainable it could assist in 2023;93: 104672 of AI regulators from Washington to Brussels: what supporting clinically useful outcomes. If the outcome of Published Online 16 June methods should be chosen to assess the risk levels for using AI in such a situation could be lifesaving, and the 2023 https://doi.org/10. the use of a certain type of AI in a speciﬁc use case, and risk of not using an AI enabled outcome life- 1016/j.ebiom.2023. how should such risk proﬁ"
301,Digital Health CRC Ltd,General comment,"Digital Health CRC Ltd Research Australia, the national alliance for the health and medical research and innovation sector recognises the expertise of the Digital Health CRC in relation to AI in healthcare and is pleased to support this submission."
301,Digital Health CRC Ltd,Q01 Definitions,"Yes. The DHCRC agrees with definitions used in the paper based on the ISO definitions.By way of a comment, at the internationally acclaimed Medical and Health Informatics congress, known as MedInfo held in Sydney from 8 to 12 July, many of the presentations involving application of AI in healthcare were highly experimental, involving significant use of ‘synthetic data’. The ISO detailed definitions of AI referenced in the Discussion paper do not define the term synthetic data, yet as the European Data Protection Supervisor notes ”Synthetic data is gaining traction within the machine learning domain.”https://edps.europa.eu/presspublications/publications/techsonar/synthetic-data_enGiven the legislative, cultural and technical challenges in timely access to big data sets in Australia, we can expect the use of synthetic data to be a feature of AI development for some time. The GDPR recommend that a privacy assurance process should be applied to synthetic data sets to ensure that data subje"
301,Digital Health CRC Ltd,Q02 Other AI risks and recommendations for regulation,"Whilst there are valuable initiatives relevant to AI listed in Attachment A, the protracted time frames involved and the focus on Australian Government (vs States and Territories and Private Sector) initiatives mean there are a range of potential risks that may go unattended, particularly in health and social care.The DHCRC supports the 2021 Australian Alliance for Artificial Intelligence in Healthcare (AAAiH) roadmap for AI call for the development of a National AI in Healthcare Strategy to support and encourage collaboration and strategic leadership.A healthcare specific AI strategy would provide a cohesive approach to the design, development, testing and delivery of AI whilst ensuring alignment with many of the existing initiatives listed. For example:- The ACCC review referenced in Attachment A is focused on large platforms. Many apps in use and/or available to healthcare consumers and providers have been developed by developers, often inhouse or in partnership with local universit"
301,Digital Health CRC Ltd,Q03 Non-regulatory actions to address AI risks,"As stated in our response to question 2, we support the AAAiH call for a National AI in Healthcare Strategy to provide strategic governance and leadership in directing a three-year strategy.Responsibility for delivering the strategy should rest with the Department of Health and Aged Care, with Digital Health and the Therapeutic Goods Administration assuming joint responsibility. There is no need to establish a new separate regulatory and oversight organisation.The DHCRC also believes the AI industry in Australia would benefit from access to clear licensing mechanisms to enable legally enforceable responsible use of AI systems in healthcare.We see merit in the development of a suite of exemplar licensing templates that enshrine the safe, ethical, and trustworthy use of AI into practical licenses that can give teeth to compliance through legislation vs reliance upon self-regulation.Developers and deployers have expressed mixed confidence in the quality and experience of legal advice and "
301,Digital Health CRC Ltd,Q04 Coordinating AI governance across government,"The DHCRC supports the 2021 Australian Alliance for Artificial Intelligence in Healthcare (AAAiH) roadmap for AI call for the development of a National AI in Healthcare Strategy to support and encourage collaboration and strategic leadership.The roadmap defines a series of pragmatic, strategic and regulatory goals.Funding and coordination of the activities outlined in detail in the roadmap for AI in healthcare report should be directed through the Department of Health and Aged Care and include public and private healthcare stakeholders to ensure there is a common, cohesive approach to the ethical and safe use of AI in healthcare.The lack of cross-jurisdictional governance arrangements for accessing healthcare data at scale is one of the top obstacles cited by the AI industry. Government-led action at the national level is required to support AI development in Australia.The DHCRC notes the recent release of the Australian Commission on Safety and Quality in Health Care (ACQSHC) report w"
301,Digital Health CRC Ltd,Q05 International models for AI governance,"Policymakers globally are exploring various avenues to regulate AI technology and ensure its responsible and ethical use. Each approach has its strengths and weaknesses, with the EU's AI Act being considered the most influential and potentially setting a standard for other regions including APAC. We reference their core features, resulting pros and cons below.1. A legally binding AI treaty by the Council of Europe:Pros: A treaty requiring signatories to protect human rights and democracy, including potential moratoriums on risky AI technologies.Cons: Individual ratification and implementation by each country may take years, and some countries might opt out of certain elements.2. OECD AI principles:Pros: Nonbinding principles adopted by OECD countries to guide AI development and policy initiatives globally.Cons: The OECD's main mandate is economic growth, not AI regulation, requiring individual countries to translate economic principles into ethical policies.3. Global Partnership on AI "
301,Digital Health CRC Ltd,Q06 Public vs Private sector governance,Different approaches should NOT apply to public and private sectors use of AI technologies in healthcare.
301,Digital Health CRC Ltd,Q07 Responsible government use of AI,"The Department of Health and Aged Care can show leadership with other Australian Government agencies by supporting the development of a National AI Strategy for Healthcare and by resourcing the Department (Digital Health and TGA) to deliver on this three-year roadmap in collaboration with National AI Centre’s Responsible AI Network.The roadmap purposely spells out a tight timeline (3 years) because of the concerns amongst users, developers and researchers of AI that ethical and sustainable use of AI in healthcare requires urgent action under each of 8 pillars outlined in the AI in Healthcare Roadmap for Australia.The DHCRC encourages the Health Economics and Research Division to support both internally and externally focused research to quantify the economic benefits, costs and indicators of AI enabled healthcare in national health priority areas. Many of the Australian and international reports describing the opportunities that use of AI will have in healthcare cite economic or produc"
301,Digital Health CRC Ltd,Q08 Generic vs technology-specific approaches for AI risk,"Technology-specific solutions to assessing and mitigating risks of AI are to be avoided. There are no circumstances in which a technology-specific solution is better than a use-case specific solution.We advocate for a ‘Precision Regulation’ approach that establishes rules to govern the deployment of AI in specific use-cases and does not regulate the technology itself. Precision regulation carries a high level of adaptability - new AI technologies and applications and the capabilities and performance of existing AI systems evolve at lightening-speed without generating the evidence base for risk mitigation at the same pace. Only regulation that focuses on outcomes rather than technology will be able to keep up and adapt to changing conditions quickly and efficiently. However, it needs to be pointed out that the nature of an AI tool cannot be entirely separated from the intent of using it. Every AI algorithm inherently (before any deployment, solely through the way the model is trained an"
301,Digital Health CRC Ltd,Q09 Transparency requirements and scope,"a. where and when transparency will be most critical and valuable to mitigate potential AI risks and to improve public trust and confidence in AI?As algorithms and AIs become ever more embedded in people’s lives, there is a growing demand for transparency around when an AI is used and what it is being used for. That means communicating why an AI solution was chosen, how it was designed and developed, on what grounds it was deployed, how it is monitored and updated, and the conditions under which it may be retired. There are four specific effects of building in transparency: 1) it decreases the risk of error and misuse, 2) it distributes responsibility, 3) it enables internal and external oversight, and 4) it expresses respect for people. Transparency is not an all-or-nothing proposition, however. Regulators need to find the right balance with regards to which degree of transparency to ask from which stakeholders. Contrary to the assumptions about transparency by many organisations, tra"
301,Digital Health CRC Ltd,Q10 Ban on high-risk AI applications,"a. Whether any high-risk AI applications or technologies should be banned completely?There should not be bans for specific types of AI technology. However, AI technology that does not satisfy quality and performance standards for specific use cases should be banned. Such standard violations can occur for example through introduction of bias/discrimination, data privacy violations and other infringements on ethical design and deployment standards. However, as laid out above, the relative weighting of such ethical factors needs to be determined as part of a risk assessment specific to individual use case scenarios. There should not be an application-agnostic ban for any type of AI technology but there could be bans of certain applications of AI (see response to Q.10b).b. Criteria or requirements to identify AI applications or technologies that should be banned, and in which contexts?The use of AI or other technologies should be restricted or banned if they violate or imperil the exercise"
301,Digital Health CRC Ltd,Q11 Actions to increase public trust in AI,"- Education - Watermarks - Look up websites, portals - Data sheets - Demonstrations of value-add - Visible adoption of AI by governments to assist government work"
301,Digital Health CRC Ltd,Q12 Trade impact of banning high-risk AI applications,-
301,Digital Health CRC Ltd,Q13 Changes to conformity infrastructure,The DHCRC supports the use of existing functions within the Department of Health and Aged Care currently responsible for the regulation of the quality of therapeutic products in health to include responsibility for the establishment of national assurance processes to mitigate against potential AI risks in healthcare.
301,Digital Health CRC Ltd,Q14 Risk-based approach to address AI risks,"Yes, we do strongly advocate for a risk-based approach to assessing and mitigating AI risks and are currently developing a risk-assessment framework for the use of AI in health and medicine."
301,Digital Health CRC Ltd,Q15 Benefits and limitations of risk-based approach,"Hallmarks of risk-based AI regulation are agility, adaptability, and clarity. Weighing risk-against benefit-profiles risk-based regulation focuses on the outcomes of the use of AI and not on the technology itself. This approach empowers developers, providers, users, and regulators of AI technology to engage in meaningful factual discussions about real immediate risks rather than perceived hypothetical risks and to prevent risk assessments that miss the mark either by overseeing risks or by misrepresenting risks as a result of having to shoehorn individual use cases into prescriptive and rigid AI technology classifications. Of course, a coherent logical line needs to be drawn from the capabilities of AI to the results of using it."
301,Digital Health CRC Ltd,Q16 Scope and application of risk-based approach,"A risk-based, precision regulation approach is sector-agnostic and should be applied in all fields."
301,Digital Health CRC Ltd,Q17 Elements to include in risk-based approach,"Yes – we would suggest these go further for healthcare e.g., with use of watermarks."
301,Digital Health CRC Ltd,Q18 Integration of risk-based approach into existing frameworks,"As per PIAs these are either embedded within a PIA or in addition to and enshrined in a national standard for healthcare, promulgated by the TGA and Digital Health Divisions. Australia should not have seven different approaches."
301,Digital Health CRC Ltd,Q19 Application of risk-based approach to general purpose AI,"We have developed a world-first comprehensive ethical framework for the responsible design, development and use of generative AI technology in health and medicine (published in The Lancet, reference [1] below) alongside implementation guidelines to apply this framework through a risk-based approach (also published in The Lancet, reference [2] below):[1] https://www.thelancet.com/journals/ebiom/article/PIIS2352-3964(23)00077-4/fulltext[2] https://www.thelancet.com/journals/ebiom/article/PIIS2352-3964(23)00237-2/fulltextWhile the developed framework is primarily applicable to health and medicine its core principles can be extrapolated to a broad field of other areas of use:Source: https://newatlas.com/computers/case-code-of-conduct-ai-healthcare/"
301,Digital Health CRC Ltd,Q20 Voluntary vs mandated compliance with risk-based approach,"A risk-based approach for responsible AI will to be mandated through regulation. Self-regulation is commendable but cannot replace legally binding AI regulation. Accountability is key. It should apply to public and private organisations, developers and deployers. Note that we uphold this statement even in light of the recently and prominently displayed pledge for self-regulation by 7 leading AI tech developers in the US White House. And should it apply to: a. public or private organisations or both? Both b. developers or deployers or both? Both"
302,Australian Society of Authors,Transcription,"FILENAME: ASA Submission on Supporting Responsible AI.e8206d9a4a1ec.pdf | CONTENT: Technology Strategy Branch Department of Industry, Science and Resources GPO Box 594 | Canberra ACT 2601 digitaleconomy@industry.gov.au26 July 2023Submission from the Australian Society of Authors on Supporting Responsible AIAbout the ASAThe Australian Society of Authors (ASA) is grateful for the opportunity to make a submission to the Department of Industry, Science and Resources in relation to artificial intelligence (AI) and welcomes the government’s intention to mitigate the risks from AI.The ASA is the peak body, professional association, community, and voice of Australia’s writers and illustrators. We have almost 4,000 members drawn from every sector of the writing and illustrating world, including: novelists, non-fiction writers, biographers, illustrators, academics, cartoonists, comic artists, scientists, historians, graphic novelists, educational writers, children’s writers, crime writers, scien"
302,Australian Society of Authors,Q01 Definitions,"Yes, agree."
303,Anonymous,Transcription,"FILENAME: Submission 303 - Attachment.6953f35287bec.pdf | CONTENT: The XXX requested that its XXX provide feedback on the Safe and responsible AI in Australia document.The following represents a synthesis of this material.Definitions1. Do you agree with the definitions in this discussion paper? If not, what definitions do you prefer and why?While the three examples shown are good, it does not empathise sufficiently the ubiquitous nature of AI.Potential gaps in approaches2. What potential risks from AI are not covered by Australia’s existing regulatory approaches? Do you have suggestions for possible regulatory action to mitigate these risks?3. Are there any further non-regulatory initiatives the Australian Government could implement to support responsible AI practices in Australia? Please describe these and their benefits or impacts.4. Do you have suggestions on coordination of AI governance across government? Please outline the goals that any coordination mechanisms could achieve and "
303,Anonymous,General comment,Submissions represent the aggregated views of individuals as members of the Queensland AI Hub Advisory Committee and have not been endorsed by the Board as a submission by the Queensland AI Hub
304,"Lupo, Locke & Quoc Vo",Transcription,"FILENAME: Submission_Locke Lupo and Quoc Vo.c979228e0b67e.pdf | CONTENT: 26 July 2023Australian Government Department of Industry, Science and Resources Technology Strategy BranchSAFE AND RESPONSIBLE AI IN AUSTRALIA: RESPONSE TO DISCUSSION PAPERThank you for the opportunity to offer responses to some of the questions raised in the discussion paper Safe and Responsible AI in Australia (June 2023).We commend the department for being proactive in its consideration of an appropriate regulatory response to ensure the safe and responsible use of artificial intelligence. Much thinking has already gone into the discussion paper. We therefore limit our comments to questions 1, 3, 9, 11, and 14 – 20 of the document.Our main contention is that public trust in the use of AI systems will only follow when one can objectively assure that the system adheres to the set principles and guidelines of appropriate behaviour. This is the case regardless of whether the regulatory strategy followed is voluntar"
304,"Lupo, Locke & Quoc Vo",Q01 Definitions,"We found the definition of AI in the document to be rather ambiguous (something that should be avoided in a definition) and conflicting. For instance, this definition defines AI as ""engineered systems ... without explicit programming,"" which implies that all AI systems are fully automated and autonomous. But the very next sentence in the same definition of AI states that ""AI systems are designed to operate with varying levels of automation."" The requirement to include only systems ""without explicit programming"" would also exclude many AI-related technologies such as logic programming, AI planning, etc. We also consider limiting AI systems to only “generate predictive outputs” makes this definition overly restrictive as AI systems have been used to generate creative outputs among many other applications."
304,"Lupo, Locke & Quoc Vo",Q03 Non-regulatory actions to address AI risks,"The European Union's proposed Artificial Intelligence Act (AIA) advocates for the development of an ecosystem of trust. This concept, operating adjunct to regulatory frameworks, is designed to foster cooperation, business growth, and innovation among trusted AI developers, deployers, and users. We propose that the Australian government evaluate the potential advantages of investing in and establishing the necessary infrastructure and programs, as well as promoting collaborations, to cultivate a similar ecosystem. This strategic move could serve as a catalyst for enhancing the adoption of AI, whilst ensuring its use aligns with ethical and responsible standards."
304,"Lupo, Locke & Quoc Vo",Q09 Transparency requirements and scope,"Based on our proposed risk-based approach to autonomous ethics-based audit of AI/ADM systems, we consider several critical components/phase of AI lifecycle where transparency is essential including data (to train/test the AI systems) and validation of an AI system outputs regarding the identified and potential risks."
304,"Lupo, Locke & Quoc Vo",Q11 Actions to increase public trust in AI,"While self-regulatory approaches are laudable and often preferred by industry, we are of the opinion that public trust will only follow if adopters of AI systems can show that their systems perform true to purpose. To this end, we recommend that government considers the adoption of assurance mechanisms that are supervised by a regulator. The result could take the form of a badge or a seal, visible on the platform through which a consumer interacts with the system, that confirms such assurance. The underlying assurance would be the same regardless of the functionality of the system, namely that the system adheres to a set of principles or guidelines committed to at the outset. The complexity of the assurance behind the scenes would depend on the risk factors associated with the particular system and its use."
304,"Lupo, Locke & Quoc Vo",Q14 Risk-based approach to address AI risks,"We fully support the risk-based approach for addressing potential AI risks. One benefit of this approach is that assurance may be customised based on the risk factors associated with a particular system. In other words, this approach is adaptable to different contexts and uses of AI systems."
304,"Lupo, Locke & Quoc Vo",Q15 Benefits and limitations of risk-based approach,"Navigating the challenges presented by a risk-based approach to AI regulation involves careful strategising and robust policy planning. A significant concern of this model is that it fails to offer a foundational standard that delineates ""what good looks like"". Consequently, it permits organisations to establish their own standards of compliance, a prevalent issue in all attestation-based frameworks. While this model provides a faster track to industry adoption by allowing organisations to accept, mitigate, and transfer risks according to their risk tolerance, it is inherently a longer road to achieving public trust, heavily contingent on the government's auditing capacity. Potential solutions to these challenges could draw from practices within the security industry, which employs a mature risk-based approach. This industry combines a basic standard (or hygiene level) with an attestation structure and third-party conformity testing. Furthermore, an investment in regulatory infrastruct"
304,"Lupo, Locke & Quoc Vo",Q16 Scope and application of risk-based approach,"Since the risk framework of different systems and their contexts differ, we do not believe that a risk-based approach is less suitable for some developers, deployers or users. Instead, the complexity of the risk framework will differ depending on the nature of the system and its intended use. From a resources point of view, the automation of assurance could conceivably alleviate some of this concern. Moreover, if government assists with an independent seal of assurance (see question 11 above), users could take comfort in the reliability of the system regardless of the size of the deployer, thereby encouraging the adoption of AI."
304,"Lupo, Locke & Quoc Vo",Q17 Elements to include in risk-based approach,"We agree with the elements listed in Attachment C, but would add that this would be an iterative process that must be continuously revised. This is especially so for impact assessments in new areas of deployment, where risk has not yet been well-defined. We would further add that ‘humans in the loop’ may be a significant barrier to adoption at scale. For this reason, we prefer that a model of automated assurance be developed. It must be possible to intervene in a system when its irresponsible use is detected. Here are some additional elements that might be considered as part of a risk-based approach: • Security – consider all risks, controls, mitigations and validation of AI system security; • Design & architecture – consider all risks relating to the design of AI system; • Evidence – establish a standard for AI system evidence recording and storing; • Data quality – consider all risks relating to the data, origin, and quality."
304,"Lupo, Locke & Quoc Vo",Q18 Integration of risk-based approach into existing frameworks,"Risk management and assurance of AI systems should be firmly rooted in, and leverage, existing governance, risk, and compliance practices that apply to general IT systems. The unique aspect of AI systems, which differentiates them from traditional IT systems, is their potential to cause harm to humans, and their significant dependence on data and algorithmic components. This exposes an entirely new vector of risks and, consequently, necessitates novel forms of mitigating controls. These considerations converge to form a new ethical dimension, seeking to illustrate the impact and outcomes on humans from a combination of systems, data, and other AI components. It's crucial, however, for organisations to avoid recreating or constructing an entirely new regime of risk management. Instead, they should build upon their existing IT and system risk processes. To this end, it would be beneficial for organisations to leverage existing AI-specific audit and assurance frameworks such as ECCOLA, Ae"
304,"Lupo, Locke & Quoc Vo",Q19 Application of risk-based approach to general purpose AI,"Large Language Models (LLMs) are integral components of an AI system, with their associated risks, controls, and mitigations not differing substantially from other model-related components. However, the distinctiveness of LLMs lies in their size and the fact that they cannot be downloaded, either due to commercial restrictions or sheer volume. Accordingly, LLMs introduce an additional set of risks stemming from their Software-as-a-Service (SaaS) nature, the cloud-based and outsourced nature of the model. These risks are primarily operational, closely associated with cybersecurity, data loss prevention, and general data privacy and protection. Another potential issue lies in the LLM service's capacity to assure their own security practices, supply chain integrity, and adherence to responsible use norms. A significant consideration is that LLM services generate responses based on data that is neither validated nor traceable. This creates potential liability issues for the consumer of the"
304,"Lupo, Locke & Quoc Vo",Q20 Voluntary vs mandated compliance with risk-based approach,"We recommend a light touch mandatory approach, whereby the appropriate use of the suggested assurance mechanism would be verified by government recognition. Developers, deployers, and users that opt not to adopt the assurance of their system would not be allowed to use the government confirmation that their system is a responsible one. This approach must be scaled, similarly to the proposed approach in the EU, to mandatory compliance where the system poses a high risk to users or the public. We are in favour of adoption of a risk-based approach by both private and public sector organisations, as well as both deployers and developers. A risk-based approach cannot operate without the impact assessment of both deployers and developers, as some risks will present from the context of use (for instance, the business of the deployer) and others will present as a function of the development process."
305,Australasian Cyber Law Institute,Transcription,"FILENAME: Responsible AI Submission - Australasian Cyber Law Institute.1519fa4a41ee5.pdf | CONTENT: Australasian Cyber Law Institute Department of Industry, Science and Resources Consultation paper “Safe and responsible AI in Australia” htps://www.industry.gov.au/news/responsible-ai-australia-have-your-say htps://consult.industry.gov.au/supporng-responsible-ai/submissionACLI Submission We are pleased to submit this paper in response to the Department of Industry, Science and Resources (“DISR”) “Safe and Responsible AI in Australia” Discussion Paper.Deﬁnions 1. Do you agree with the deﬁnions in this discussion paper? If not, what deﬁnions do you prefer and why?We consider that the deﬁnions set out in Secon 1.2 of the Discussion Paper would be enhanced by the addion of roboc systems including autonomous vehicles, social robots, delivery vehicles and drones, surgical robots, and similar technologies.The advantage of a wider deﬁnion is that the same consideraons for trustworthiness will ap"
306,UNSW AI Institute,Transcription,"FILENAME: UNSW.ai submission to Responsible AI.98937e768dff2.pdf | CONTENT: UNSW AI Institute ai.director@unsw.edu.au25 July 2023Department of Industry, Science and Resources Industry House 10 Binara Street Canberra ACT 2600Dear Sir/Madam,Thank you for this opportunity to respond to the public consultation on the Discussion Paper on “Safe and responsible AI in Australia”. This submission contains the collective advice of UNSW’s flagship research institute in artificial intelligence. The central goal of UNSW.ai is the responsible development and deployment of AI. We therefore welcome the government’s attention to this issue.For context, UNSW’s new AI Institute brings together over 300 world-class researchers, across UNSW Sydney and Canberra, working on fundamental AI problems as well as applying AI techniques to their respective domains. Whilst addressing the legal, societal, and economic implications of AI. UNSW.ai is the largest research institute in Australia focused on AI. Members o"
307,Google,Transcription,"FILENAME: FINAL Safe and responsible AI in Australia - Google Submission.059aeb44a6291.pdf | CONTENT: Safe and responsible AI in Australia“Now there is no question in my mind that artificial intelligence needs to be regulated. It is too important not to. The only question is how to approach it.”Sundar Pichai, Chief Executive Officer and Executive Director, Google and Alphabet January 20, 2020Executive SummaryArtificial Intelligence (AI) has the potential to unlock significant economic, scientific and social progress for nations like Australia and for the wider global community.It is already deepening our scientific understanding of the world around us, which will enable dramatic progress in human health and wellbeing, and let us better tackle pressing global challenges like climate change. It underpins the democratisation of powerful creative tools that will profoundly increase human productivity, and support an economic transformation that may affect our societies as deeply as electri"
308,Australian Discrimination Law Experts Group,Transcription,FILENAME: ADLEG Submission-Safe and Responsible AI in Australia.d54e22c22e47c.pdf | CONTENT: Submission of the Australian Discrimination Law Experts Groupin response to theSafe and Responsible AI in Australia: Discussion Paper26 July 2023Australian Discrimination Law Experts Group Safe and Responsible AI in Australia: Discussion Paper TABLE OF CONTENTS Australian Discrimination Law Experts Group ................................................................................ 2 Glossary .......................................................................................................................................................... 3 1. Summary..................................................................................................................................... 4 2. Introduction to the submission............................................................................................ 5 3. Potential gaps in approaches...............................................
309,Australian Recording Industry Association (ARIA),Q01 Definitions,ARIA does not take issue with the definitions used in this survey and discussion paper.
309,Australian Recording Industry Association (ARIA),Q02 Other AI risks and recommendations for regulation,"Although there are no overbroad exceptions in Australian copyright law that would allow use of copyright protected material to be used for AI systems training without authorisation from the relevant right holder, in reality AI systems are currently using copyright protected material without doing so to generate “new”, competing content. Any AI regulation, principles or code of conduct must reconfirm the need for a robust application of existing copyright laws, including that developers and providers of AI systems must acquire the authorisation of right holders on whose works/materials they train their systems.Furthermore, to protect right holders against unauthorised uses of their works (including sound recordings) in an AI process and to facilitate any legal action to enforce their rights, AI developers and providers should be required to keep records of third party works or other copyright-protected subject matter used in the AI process and how it is used. This should include records"
309,Australian Recording Industry Association (ARIA),Q03 Non-regulatory actions to address AI risks,"The Australian Government should consider ways to enshrine the protection and enforcement of creators’ rights and existing intellectual property laws. This could involve establishing a Code of Practice or AI Ethical Principles in which the robust application and enforcement of intellectual property rights is a core pillar in the development of AI systems. The benefit of this consideration of intellectual property rights extends to both right holders and AI developers and providers: right holders benefit by ensuring they can enforce their rights and have greater agency to engage with AI uses of their materials; and AI developers and providers benefit as intellectual property rights drive further investment and innovation and future monetization of that investment. In addition, labelling wholly AI-generated content is important to inform consumers and enable them to differentiate between wholly AI generated content and human artistry (which may or may not involve the use of AI as a tool "
309,Australian Recording Industry Association (ARIA),Q04 Coordinating AI governance across government,"The interests of right holders and the creative and cultural sectors should be represented in any government coordination of AI to ensure that AI development still occurs in a way that is sustainable for the Australian creative and cultural sectors and promotes the creation of valuable content that will encourage innovative uses of AI. This should be managed through consultation with the creative and cultural sectors when developing and implementing regulations that will fundamentally impact right holders’ and artists’ interests (e.g. matters related to the use of protected material for training, transparency and record keeping, identifying wholly AI generative works and protecting the name, image, likeness and voice of artists)."
309,Australian Recording Industry Association (ARIA),Q05 International models for AI governance,"While mentioned in the discussion paper, ARIA would draw the Government’s attention to the transparency and record keeping obligations in the European Union’s proposed AI regulations. Under the AI Act (EU) proposals from the European Parliament, AI providers would be required to record the data used to train their AI systems, including material protected under copyright, and to make this available in a sufficiently detailed way for right holders to identify and enforce their rights. In addition, the proposed transparency requirement would improve trust in AI systems, as it would help consumers to understand where content they watch, listen to or read has been generated by AI and not a human creator."
309,Australian Recording Industry Association (ARIA),Q06 Public vs Private sector governance,"ARIA is of the view that both the private and public sector should adhere to the same obligations of any AI regulation, in particular any regulation around transparency and labelling, licensing content for the purpose of training an AI system, and the protection of human artistry including protection of name, image, likeness and voice rights."
309,Australian Recording Industry Association (ARIA),Q07 Responsible government use of AI,No comment at this stage.
309,Australian Recording Industry Association (ARIA),Q08 Generic vs technology-specific approaches for AI risk,No comment at this stage.
309,Australian Recording Industry Association (ARIA),Q09 Transparency requirements and scope,"As noted in our earlier responses, transparency will be key to ensuring the ethical development and use of AI systems. From the perspective of right holders, this will be particularly important in the training of systems that rely on copyright materials to learn and function. Transparency about the materials being used and clear record keeping is essential so there is an ability to have a complete understanding of where copyrighted material was used and whether relevant authorisations were obtained – which will enable right holders to have greater agency over their copyright materials and enable them to enforce their rights.Transparency around AI systems and the materials they produce is also important at the output and use stage as it gives consumers the ability to differentiate between wholly AI generated works and human artistry. The Australian Government should, therefore, support mandatory record-keeping and labelling requirements for AI developers and providers, as indicated abov"
309,Australian Recording Industry Association (ARIA),Q10 Ban on high-risk AI applications,"We consider AI systems with the sole or predominant purpose/function of creating misleading deepfakes or vocal clones without authorisation from the copied artist to be “high-risk AI applications” that should be severely restricted.From the perspective of creators and artists, these types of AI can be abused to create misleading imitations of an artist’s image, likeness and voice. As well as infringing the rights of the artist, these ‘fakes’ create unfair competition and can damage the reputation and viability of an artist to earn a livelihood. These AI systems and AI generated materials benefit from the effort, goodwill, reputation and fanbase of the artist they imitate, as well as leveraging the significant investment made by the artist and their record label into creating the materials and the artists’ career – whilst the AI system contributes nothing towards this. Such use of AI technology robs artists of control over their own voices and image, and can confuse and mislead fans who"
309,Australian Recording Industry Association (ARIA),Q11 Actions to increase public trust in AI,"A fundamental way in which government can help build trust in the development and use of AI systems is through encouraging transparency. For years, AI has played a role in assisting in the creation of music, however enabling consumers to know when a work is the product of wholly AI generation versus the result of human effort and creativity will give the public more confidence when interacting with genuine works and AI works. Government should aim to prevent scenarios where consumers have been misled into thinking they are supporting their favourite artist when, in reality, the material was an AI generated imitation."
309,Australian Recording Industry Association (ARIA),Q12 Trade impact of banning high-risk AI applications,No comment at this stage.
309,Australian Recording Industry Association (ARIA),Q13 Changes to conformity infrastructure,No comment at this stage.
309,Australian Recording Industry Association (ARIA),Q14 Risk-based approach to address AI risks,ARIA does not have a strong view on whether a risk-based approach is taken towards AI risks except that the views of right holders and the cultural and arts sectors (including the recorded music sector) should be adequately addressed regardless of the approach.
309,Australian Recording Industry Association (ARIA),Q15 Benefits and limitations of risk-based approach,No comment at this stage.
309,Australian Recording Industry Association (ARIA),Q16 Scope and application of risk-based approach,No comment at this stage.
309,Australian Recording Industry Association (ARIA),Q17 Elements to include in risk-based approach,"Within the impact assessments element, any guidance documents should clearly include the impact on right holders and artists as a risk factor for AI systems which require copyrighted material to train or generate imitation works or deepfakes / vocal clones."
309,Australian Recording Industry Association (ARIA),Q18 Integration of risk-based approach into existing frameworks,No comment at this stage.
309,Australian Recording Industry Association (ARIA),Q19 Application of risk-based approach to general purpose AI,"Even where a risk-based approach is taken, there should be scope for specific based approaches where needed. Foundational AI systems, while considered by some as general purpose and low risk, can pose a particularly high risk to the cultural and arts sector in their ability to quickly create imitation works (without the necessary authorisations) and potentially flood already highly competitive creative markets.ARIA suggests the Government consider the legislation being developed by the EU. Under the proposed AI Act (EU), providers of foundation AI systems (and those that specialise them into downstream applications) will be required to meet several obligations, including transparency requirements that would require them to publicly disclose the copyrighted material used to train their models (see: https://www.europarl.europa.eu/news/en/press-room/20230505IPR84904/ai-act-a-step-closer-to-the-first-rules-on-artificial-intelligence)."
309,Australian Recording Industry Association (ARIA),Q20 Voluntary vs mandated compliance with risk-based approach,"Regulation can help ensure that proper safety and impact assessments become an integral part of the AI development process – failure to apply these standards to developers and providers could result in a patchwork regulation for AI firms and right holders and artists to navigate. The elevated risk faced by right holders and artists by generative AI, and its capacity to flood the market with deepfakes / vocal clones or potentially infringing material, calls for the robust application and enforcement of intellectual property rights and new obligations regarding record-keeping for developers and providers of generative AI who use copyrighted material to train their systems, as well as to ensure that sufficient rights exist to protect the name, image, likeness and voice of artists from unauthorized deepfakes. The proliferation of AI systems that do not adhere to adequate record keeping or transparency standards, or do so inconsistently, will considerably increase the difficulty faced by ri"
310,Lucas Lewit-Mendes,Q02 Other AI risks and recommendations for regulation,"I am deeply concerned about the risk that highly capable AI poses to humanity's future potential. The paper does not acknowledge the catastrophic risks of highly capable AI systems, despite this risk being acknowledged by experts and public figures - including open letters coordinated by the Future of Life Institute [1] and the Centre for AI Safety [2].Even the Secretary-General of the United Nations has recognised the catastrophic and existential risks from highly capable AI systems [3].The Australian government must acknowledge the catastrophic and existential risks from highly capable AI systems. It is not acceptable to remain silent on an issue that experts and other countries have been actively engaging with.[1] https://futureoflife.org/open-letter/pause-giant-ai-experiments/[2] https://www.safe.ai/statement-on-ai-risk#open-letter[3] https://press.un.org/en/2023/sgsm21832.doc.htm"
310,Lucas Lewit-Mendes,Q03 Non-regulatory actions to address AI risks,"Australia has historically been an international leader when it comes to global risks, such as the risks posed by nuclear war. The key non-regulatory function Australia could perform is offering international leadership focused on ensuring AI systems can be understood. Australia should be an active voice for the importance of ethics and safety in the current global conversation around AI governance and AI standards making. Specifically, Australia should advocate for:The development of standards and tools that analyse and monitor AI systems for robustness, bias, dual-use capabilities and other safety-relevant features [1].Audit trail requirements for safety-critical applications of AI systems, including allowing the traceability of decisions through product life cycles [2].We will only be able to have confidence in more sophisticated AI systems if they can be understood, we can ensure they do not pose unacceptable risks and hold accountable those at fault where it does. Doing the non-re"
310,Lucas Lewit-Mendes,Q05 International models for AI governance,"The EU has been a global leader on many complex issues, including climate change and now AI risk. Australia should take note of the EU recent handling of the AI Act. For example, the European Parliament’s recognition of general-purpose AI (GPAI) is an important step in the right direction [1]. In particular, the following observations are highly valuable:- Acknowledgement that GPAI providers must comply with strict safety measures, regardless of the way it is distributed or intended to be used.- Requirements for GPAI providers to comply with various safety requirements throughout the lifecycle of their product; i.e. external audits assessing their performance, predictability, interpretability, corrigibility, safety and cybersecurity.These adjustments appear to respond to significant criticisms directed at initial drafts of the AI Act. For example, the AI Now Institute released a set of recommendations that heavily criticised the EU’s approach on some matters [2]. For example:- “GPAI mo"
310,Lucas Lewit-Mendes,Q09 Transparency requirements and scope,"As an economist, I am in favour of using regulation and transparency requirements to overcome the negative externalities posed by AI risk, and to promote longer-term thinking. To remain competitive, professionals and businesses will increasingly feel the pressure to leverage sophisticated “black-box” AI models. This short-term thinking has already resulted in numerous accounts of public backlash and distrust, including high-profile warnings from technology leaders themselves [1].For example, Apple’s credit card business came under fire for offering a woman one twentieth the credit limit offered to her husband. When she complained, an Apple representative simply told her, “I don’t know why, but I swear we’re not discriminating. It’s just the algorithm.” [2]More recently, two lawyers in the United States blamed ChatGPT for “tricking” them into including fictitious legal research in a court filing [3].These problems could easily have been prevented by using explainable AI systems which ca"
310,Lucas Lewit-Mendes,Q10 Ban on high-risk AI applications,"While it is unlikely that AI today represents a catastrophic or existential risk to humanity, there’s a realistic possibility that it will in the near future [1, 2].To strike a balance between all perspectives, regulators should work to identify “advanced indicators” of catastrophic risk scenarios, and clearly communicate that a ban is an appropriate action if these advanced indicators come to pass.Establishing appropriate advanced indicators will have to be done in consultation with AI safety researchers, for example those from the Centre for AI Safety, or the Centre for AI Governance. However, it’s likely they could include factors such as the size of training runs (measured in terms of compute and data), as well as safety-relevant factors about the AI system, such as the ability to complete a diverse range of reasoning tasks with human-level performance; to navigate complex information environments and act within those environments autonomously, and to form sophisticated plans and r"
310,Lucas Lewit-Mendes,Q14 Risk-based approach to address AI risks,"There is a historical precedent for a risk based approach, namely the aviation industry. A risk based approach has worked well as a proactive means of identifying, assessing and mitigating potential safety hazards in the aviation industry.The industry has developed specific tools to assist in identifying potential risks - Safety Management Systems (SMS). Critical to this is the ability for all parties involved in the development and use of AI systems to be able to report the identification of any risks.Following identification, risks are assessed based on severity and likelihood. This work is led or assisted by technical bodies like the Australian Transport Safety Bureau. In the AI context, other countries have created or proposed similar national “labs” that would be well placed to lead this work.The industry participant best placed to mitigate the risk is then required to take steps to reduce the likelihood or impact. Given the nature of the technology this stage should be open to up"
311,RMIT Enterprise AI and Data Analytics Hub,Transcription,"FILENAME: RMIT Enterprise AI and Data Analytics Hub - Our response.16dfd51ef742f.pdf | CONTENT: RMIT Classiﬁca-on: TrustedSafe and responsible AI in Australia Our response26 July 2023 Table of Contents1. Introduc,on 12. About authors and contributors 13. Summary of Recommenda,ons 24. Response to ques,ons outlined in discussion paper 34.1 Deﬁni,ons 34.2 Poten,al gaps 44.3 Responses suitable for Australia 74.4 Target areas 94.5 Implica,ons and infrastructure 134.6 Risk-based approaches 145. Other considera,ons 165.1 Environmental, Social and Governance, or CSR 165.2 Sustainable Development Goals 175.3 Human-centred approach 195.4 Improving public trust in AI deployment – a cybersecurity perspec,ve 226. Summary 23Contributors and acknowledgement 24References 26 1. Introduc,on The RMIT Enterprise AI and Data Analy6cs hub (the Hub) received an invita6on on 1 June 2023 from the Digital Economy team within the Department of Industry, Science and Resources to comment and provide feedback to th"
311,RMIT Enterprise AI and Data Analytics Hub,Q01 Definitions,We agree with the definitions of the AI terms in the discussion paper.
311,RMIT Enterprise AI and Data Analytics Hub,Q02 Other AI risks and recommendations for regulation,"We note that current regulatory approaches are still in the 'catch-up' phase with AI developments. For example, ASIC is keen to understand how to respond to AI development within AFSL. We also felt that the definition of risks and the risk management strategies should be maintained on an on-going basis rather than a one-off and it should include establishing an independent body to evaluate AI systems' risks. A full response is in our attached document."
311,RMIT Enterprise AI and Data Analytics Hub,Q03 Non-regulatory actions to address AI risks,"We feel that the government can take a lead in creating templates or showcase of how common job roles in the industry can be redesign with AI to realise benefits while keeping human at the centre. Such initiatives will create trust and acceptance around regulatory frameworks in relation to AI and will assist industries who need it most, such as small and medium enterprises as they lacked the financial resource of large corporation to develop AI capabilities. In doing so, it will also help develop AI investment schemes to target the organisations that need it most and delivering the capability uplift across Australia in a financially responsible manner.Additionally, we believe the government should also embark on educating the community around the need for regulation, the benefits of regulation and how any regulation will function to serve or protect the interests of all Australian. Initiatives that can show any of the actions taken is done via a human-centred approach will not only del"
311,RMIT Enterprise AI and Data Analytics Hub,Q04 Coordinating AI governance across government,"We believe the establishment of the National AI Centre is a very good start. We are also aware of the various initiatives that NAIC and the Responsible AI Network (RAIN) is undertaking. At the moment, it appears that there is a focus on the technical aspects of AI while engagements of AI has focused primarily on an audience that are already receptive. If we are to succeed with the use of this technology, we need to look at schemes or initiatives that looks after all Australians. We like to point to AI Singapore as a good example of how government can work with Universities to explore different initiatives. From education/training schemes that target different demographics to schemes that support different enterprises in the uptake of AI through a vibrant ecosystem."
311,RMIT Enterprise AI and Data Analytics Hub,Q05 International models for AI governance,"Yes, we believe there are many regulatory references that Australia can look to. In our uploaded submission, we note that the reason to look at measure taken by other countries make sense, particularly if the regulatory framework is aligned with our main trading partners. However, we also pointed out in our submission that we should only adopt the soul of regulatory approaches but there will be differences that reflect the nature of Australia. We have outlined various differences that is worthy of focus in our submission."
311,RMIT Enterprise AI and Data Analytics Hub,Q06 Public vs Private sector governance,"We are of the view that we should have different approaches between the public and private sector. While our submission involved consulting a diverse group of academics and industry practitioners with different disciplinary backgrounds, there was actually consensus that the purpose of AI deployment between the public and private sector is fundamentally different. This difference in purpose and consequently, the accountability to the public versus private clients means the approach to managing the use of AI will be different. Details of how the approaches should be different is discussed in our uploaded submission."
311,RMIT Enterprise AI and Data Analytics Hub,Q07 Responsible government use of AI,"There will need to be a cultural change in how AI is used among agencies. In our submission, we note a quick win that government can achieve is to further the work by NAIC's RAIN that has been researching responsible AI patterns and running seminars to educate the public. If these responsible AI patterns can be implemented first among government agencies, this would be a great start and sends a very positive signal to the public."
311,RMIT Enterprise AI and Data Analytics Hub,Q08 Generic vs technology-specific approaches for AI risk,"We do not believe one solution is better than another. Rather, our view is that we will both approaches to achieve the outcomes we want. Depending on one solution over another will no doubt overlook issues. Therefore, we believe there should be a ""whole of"" approach. For example, a solution that has a technology-specific safe guard and a regular risk assessment body (generic solution) will give the most confidence to the public around AI risks - particularly the high risks ones."
311,RMIT Enterprise AI and Data Analytics Hub,Q09 Transparency requirements and scope,"On (a), we need to establish a specific understanding of 'transparency' among the community and also map 'transparency' to 'accountability' if risks is to be mitigated. To the public, transparency is lacking in both public and private sectors but the concerns are different. For example, should private companies disclose the AI model used as well as the parameters it was developed on so there is an understanding of its operational limits? Similarly, in the public sector, how is a decision arrived so that the community is convinced of fairness will form part of the transparency. A lot of what we see in the views we collected has focused on these two areas - a disclosure of the technical limits of the AI model and the explainability aspects of a model's outputs.On (b) and consistent with our views around the different approaches to take for public vs private, the disclosure requirements can be different. In terms of model disclosure, we believe the approach like nutrition disclosure on fo"
311,RMIT Enterprise AI and Data Analytics Hub,Q10 Ban on high-risk AI applications,"We should entertain the ideal of banning a high-risk application only as a last resort. There are many considerations to be taken into account, including the risks of creating an overlying restrictive environment where Australia losses its competitive advantage or that the risk-benefits balance is lost. Even if an application is banned, we are of the view that we should have a date or window in which a once high risk application is evaluated again as AI technologies do evolve so the risk level can too change over time.We believe the risk-based approaches that was mentioned in the discussion paper is the place to start. The ideal of such an approach is to ensure that the interest of the end user, which is a human, is ultimately protected. Therefore, we are of the view that risk-based approaches is a start but it should be covered by a principle based approach and augmented with ethics training, collaborative governance, and responsible AI patterns that RAIN is developing."
311,RMIT Enterprise AI and Data Analytics Hub,Q11 Actions to increase public trust in AI,We presented many different actions or considerations that government can take in our PDF submission to improve public trust.
311,RMIT Enterprise AI and Data Analytics Hub,Q12 Trade impact of banning high-risk AI applications,"There will be implications but we do not have the depth and breadth of expertise to consider the consequences. However, we do agree that any move to ban a high risk activity while our trading partner don't is likely to not only have trade consequences but also other implications, such as AI readiness and national security."
311,RMIT Enterprise AI and Data Analytics Hub,Q13 Changes to conformity infrastructure,"There are many things to consider but we believe chief among them are (a) to have a ""whole of regulation"" approach where existing regulatory framework is updated to consider the implications of AI and (b) extending the NAIC or establishing a body to assess AI risks."
311,RMIT Enterprise AI and Data Analytics Hub,Q14 Risk-based approach to address AI risks,Yes we do.
311,RMIT Enterprise AI and Data Analytics Hub,Q15 Benefits and limitations of risk-based approach,We believe risk assessment on an AI product is not a one-off activity. We need to recognise that AI risks will change and that the risk level of an application will need to be assessed to reflect current operating conditions and community expectations.
311,RMIT Enterprise AI and Data Analytics Hub,Q16 Scope and application of risk-based approach,"A risk-based approach can be the overarching assessment with detailed assessment undertaken on the basis of the specifics of a sector or application. Here, our vision is that the overarching assessment may be undertaken by a specially established body with any high risks assessment being directed to specific regulatory bodies who have the domain knowledge. In other words, we are of the view that we should not be depending on a single approach as the only answer to our risk management strategy."
311,RMIT Enterprise AI and Data Analytics Hub,Q17 Elements to include in risk-based approach,"Yes we support these elements and would add that the ""notices"" for end users should also include disclosure details on what kind of AI is being used or an assessment number which allows them to seek further information about the AI used."
311,RMIT Enterprise AI and Data Analytics Hub,Q18 Integration of risk-based approach into existing frameworks,We presented our views on this around cybersecurity in Section 5.4 our of PDF submission.
311,RMIT Enterprise AI and Data Analytics Hub,Q19 Application of risk-based approach to general purpose AI,"These are new models so research in this space if forthcoming. We are already seeing research around how these new models can be managed. We believe regulatory design to allow for these changes to be taken into account and likewise, as with our views presented so far, the risk criteria should be updated along side developments in this space."
311,RMIT Enterprise AI and Data Analytics Hub,Q20 Voluntary vs mandated compliance with risk-based approach,"We believe both self-regulatory and mandatory compliance are required. We are off the view that this should apply to both public and private organisations as it should be a function of the risk levels rather than the type of organisation. We are also of the view that developers and deployers both need to be assessed for risk. This is because AI systems are complex and deployers may not configure an AI in the correct manner that also resulted in unwanted risks. While their risk levels may different by the composition of the solution, we are of the view an assessment is required if safe guards are to be put in place."
312,Accountability Round Table,Transcription,"FILENAME: ART Submission to Supporting Responsible AI Consultation final1.c34f83bdbe512.pdf | CONTENT: ART Submission to Supporting Responsible AI consultation Closing date 26 July 2023.Summary of recommendations 1. Datasets Data must be of good quality to stop generative AI from working badly. It must be clean, complete and align well with the complex evolving real world situations that generative AI might meetThe risk is that there is no set of standard rules that apply to the whole of the data content that may be used by AI developers to train and provide content to generative AI'sWhat is needed is a data commons, open to all, maintained by all, but ring fenced by a mandate that it is the only legitimate site for providing clean reliable ethical data to be used by AI developers.The data commons could be owned by an independent supra national body on analogy with national ownership of crown land. That body could be charged with determining who was able to use the common resource, and"
312,Accountability Round Table,General comment,Thank you for the opportunity to make a submission on this important but very complex matter. JT
312,Accountability Round Table,Q01 Definitions,"Yes the definitions are fine, but the focus of the paper is a bit narrow to ensure safe and effective AI. You say ""The paper focuses on governance mechanisms to ensure AI is used safely and responsibly"" but do not pay attention to whether it is PRODUCED safely and responsibly. Neither do you look at the commercial, competition and power incentives in the production system that have caused AI to have become an existential threat whether under the control of humans or out of the control of humans. The parts certainly need regulation, but the whole will go on producing adverse outcomes unless there is a substantial rethink."
312,Accountability Round Table,Q02 Other AI risks and recommendations for regulation,"Yes, see my submission"
312,Accountability Round Table,Q03 Non-regulatory actions to address AI risks,"Yes, see my submission"
312,Accountability Round Table,Q04 Coordinating AI governance across government,"Yes, see my submission"
312,Accountability Round Table,Q05 International models for AI governance,"Yes, see my submission"
312,Accountability Round Table,Q06 Public vs Private sector governance,Not unless you want differential safety outcomes and differential ethical behaviour between sectors. There are some very good public service limitations such as the OAIC the Australian Privacy Principles which should also apply to external public and private use of data for instance.
312,Accountability Round Table,Q07 Responsible government use of AI,Use ethical AI. (See my submission)
312,Accountability Round Table,Q08 Generic vs technology-specific approaches for AI risk,"My submission splits the areas of control needed into four, rather than two. There are four intervention points at which risk mitigation can and should occur; At the level of the data on which the AI is trained and which it uses to produce its content At the level of the training and training prompts given to the AI At the user and use level. At the level of the motivations and incentives for producing the AI and making it fit for purpose."
312,Accountability Round Table,Q09 Transparency requirements and scope,"See the section of my submission on ""Constitutional AI"" and how it improves transparency."
312,Accountability Round Table,Q10 Ban on high-risk AI applications,"It's much better to ban uses than technologies. Even nuclear technologies have their positives. In general, it would be wise to; • prevent inversion of ethical AI constitutions for misuse • not use Constitutional or any other AI for automated decision-making."
312,Accountability Round Table,Q11 Actions to increase public trust in AI,Is that really a sensible objective? It has no goal other than size. What are you ACTUALLY trying to achieve?
312,Accountability Round Table,Q12 Trade impact of banning high-risk AI applications,"It would enable us to set an ethical benchmark for good AI behaviour which we should leverage by collaborating with any other global initiative to do the same. We should be developing a reputation as a safe, healthy, democratic place to live, work and do business with. The only systematic way to protect ourselves from harm is to ensure not just that it doesn't originate at home, but that it does not also originate from overseas."
312,Accountability Round Table,Q13 Changes to conformity infrastructure,I don't understand the question.
312,Accountability Round Table,Q14 Risk-based approach to address AI risks,"A multifaceted approach that does not rely solely on a regulatory ambulance at the bottom of the cliff to patch up accidents after they happen, but which also fosters the growth of ethical cultures amongst producers and users of AI and removes incentives for bad behavior would go a long way towards prevention not just cure"
312,Accountability Round Table,Q15 Benefits and limitations of risk-based approach,See above
312,Accountability Round Table,Q16 Scope and application of risk-based approach,Not something on which I have an opinion
312,Accountability Round Table,Q17 Elements to include in risk-based approach,"The draft risk based approach is fine as far as it goes. But it is a bit employee centered and kind of looks like it was designed for the pubic service. It does not consider removing or mitigating machine risks, or removing the social incentives that create AI risks in the first place."
312,Accountability Round Table,Q18 Integration of risk-based approach into existing frameworks,Not qualified to answer
312,Accountability Round Table,Q19 Application of risk-based approach to general purpose AI,See suggestions in my submission
312,Accountability Round Table,Q20 Voluntary vs mandated compliance with risk-based approach,"It should be voluntary where that works but its unlikely to work for commercial providers. In any case commercial entities can often prefer the security of regulation. Even the recent agreement of seven of the big US AI producers to a voluntary code of conduct was done on their recognition that it would be followed by regulation. The voluntary code appears to be performing the role of a pilot for regulation. If controls do not apply as widely as possible, the doomsday scenarios predicted by computer scientists are almost guaranteed to emerge given the incentives that already exist for hostile interventions in national stability."
313,Copyright Agency,Transcription,"FILENAME: R02629 Dept Industry AI consultation.7ccd72ed353c9.pdf | CONTENT: Response to Department of Industry Discussion Paper: Safe and responsible AI in Australia July 2023Introduction Thank you for the opportunity to participate in this review. As the Discussion Paper notes, this is one of many processes that the Government is undertaking with a view to harnessing the potential benefits of artificial intelligence (AI) for Australian society, and managing the potential threats. We and our members recognise the potential for AI to do good in a range of areas, including to reduce inequalities. At the same time, there is huge potential for adverse consequences for people who work in Australian creative industries, including educational publishing. The Government’s National Cultural Policy was widely and warmly welcomed by people working in Australia’s creative industries, including because it has been introduced as a whole-of-government policy. Although not mentioned in the Discussion "
314,Kingston AI Group,Transcription,"FILENAME: KAIG_submission_responsible_AI_26_July_2023.d6843c00b501b.pdf | CONTENT: kingstonAIgroup.org.auSafe and responsible AI in Australia Submission by the Kingston AI Group: Australian Professors of Artificial Intelligence 26 July 2023 The Kingston AI Group comprises 13 leading professors from eight universities and the Chair of Robotics Australia Group. Eight are members of the learned academies and five are existing or previous laureate fellows. As well as driving Australia’s artificial intelligence (AI) research, the group’s members are working with companies to develop commercial AI solutions. AI is a transformational technology that is impacting every sector of industry and society, solving previously intractable problems and delivering productivity gains across the economy. In fact, there is no other technology with the potential to improve Australia’s productivity as broadly as AI. Quite rightly, the government is considering how the technology should be regulated in Austra"
315,Source Transitions Pty Ltd,Transcription,"FILENAME: Source Transitions response to Safe and responsible AI in Australia discussion paper.7ecc83781ea74.pdf | CONTENT: PublicSource Transitions Pty Ltd submission to the Department of Industry, Science and Resources Discussion PaperSafe and responsible AI in Australia25 July 202325/07/2023 Source Transitions Pty Ltd submission to the Department of Industry, Page 1 of 19 Science and Resources Safe and responsible AI in Australia Discussion Paper - July 2023 PublicSource Transitions welcomes the opportunity to provide a submission to the Department of Industry, Science and Resources discussion paper Safe and Responsible AI in Australia.We also note that we support Responsible AI in Australia and globally and are an active participant in the Australian AI Ecosystem Discovery Platform.The discussion paper was published in the context of other reports and programmes initiated by the Australian Government which are all relevant [1].• The Australian Government has consulted on automated "
315,Source Transitions Pty Ltd,Q01 Definitions,"Definition of Artificial intelligence The definition proposed in the paper is adequate but incomplete. AI is mainly used in the public as an umbrella term. The definition should therefore be accompanied by a list of specific techniques and approaches used for AI development or its application. The definition should be kept up to date considering market and technological developments as indicated in the preparation for the regulation of AI in the EU . An important part of the role of government in the field is to ensure clarity and transparency so that trust can be built around the set of technologies AI can power. Initiatives such as the EU AI definition and set of examples to illustrate what AI does in practice will help guide the public, businesses and government agencies understand how they could be affected by AI . The definition of AI has evolved over time and allowing for that definition to continue evolving is key to keeping abreast of how the field is developing. The EU parliam"
315,Source Transitions Pty Ltd,Q02 Other AI risks and recommendations for regulation,"While Australia has a set of AI ethics principles in place, the proposed legislation does not encompass some of the elements found in the principles. Critically, considerations for the environmental dimension (Principle 1: human, societal and environmental wellbeing) are not included. The environmental costs of developing AI solutions or training large datasets is considerable. For example, the environmental cost of training a large natural language processing model could be as much as the total emissions produced by five cars over the cars’ lifetime or 150 return flights between New York and London . Furthermore, the demand for energy to power the development, training and usage for AI-powered tools is growing exponentially and is not required to be come from renewable energy sources. Finally, the demand for graphic cards and other critical materials required for the infrastructure (i.e. computers, servers etc) onto which AI solutions are operating is pushing the world towards further"
315,Source Transitions Pty Ltd,Q03 Non-regulatory actions to address AI risks,"Governing AI systems through their design where social, legal, ethical rules can be enforced through code to regulate the behaviour of AI systems . Where government considers privacy by design, data protection by design and safety by design, it should also consider sustainable by design and ethical by design. Human in the loop approaches to governing AI extended to society in the loop approach where society is responsible for finding consensus on the values that should shape AI . Providing companion practices, tools and resources for anyone interested in developing or implementing AI responsibly should be undertaken. Resources should be available to all levels of AI literacy and AI maturity, and available freely. Leaving AI literacy to the private sector would be to the detriment of the public, small business and not for profit who need to be and feel involved in the direction AI can take. Presenting clear case studies, including mapping of AI responsible practices, could serve as exam"
315,Source Transitions Pty Ltd,Q05 International models for AI governance,"The following measures should be in place in Australia’s context: • In the UK example, principles are legislated to create a statutory duty to have due regard to the principles. Guidelines could be adhered to and can evolve more rapidly than legislation. Regulation to implement the guidelines and enforcement on it will be most likely able to cope with the pace of change • In the UK, the Algorithmic Transparency Standard publishes reports on the use of algorithmic tools in government decision making. • In China, a mandatory registration system is in place for recommendations algorithm. Whilst the goal for the registration will differ with Australia, China has rightly identified that recommendation engines have a particularly powerful capacity to influence behaviours without the consent or awareness of the systems users. • Italy forces generative AI platforms to allow users data not to be used in training, to disclose how they process user data, to allow users to opt-out. • The city of A"
315,Source Transitions Pty Ltd,Q06 Public vs Private sector governance,"The legislation will need to be tailored towards not just products (a critique of the proposed EU AI Act ) but also to the open-source community (researchers, developers, not-for-profits). Inclusivity and representation are particularly important for governments who need to represent all their constituents. A higher level of accountability is therefore expected of government departments. Updated and up-to-date approaches to data governance and to interoperability are expected for governments which might not be required in the private sector (outside of regulated industries). Data governance, a critical area: Part of AI governance, as multiple organisational and technological challenges exist that impede effective control over data and attribution of responsibility for data-driven decisions made by AI systems . Interoperability, a critical area: Data fragmentation and lack of interoperability between systems limits an organisation’s control over data flows throughout its entire life cyc"
315,Source Transitions Pty Ltd,Q07 Responsible government use of AI,"Codesign and participatory design of solutions ensure that the complexity and prioritisation of problems is handled by the people who know best and are directly involved. Only then should a range of solutions be considered. AI initiatives are often driven by solution providers rather than by the problems and the users themselves. This leads to solutions being implemented before anyone knows what the problem is. It also leads to a tendency to fit the organisation to a solution rather than the other way around. Human-centered design approaches should be the default for solution design. There is an urgent need to develop and operationalise clear guidelines on what the potential trade-offs of a responsible AI solution are. These frameworks will help government departments and decision-makers to understand what they potentially impact with their proposed solution and realise if it is worth implementing (e.g. social versus financial, short term versus long term, etc). Ethical principles can "
315,Source Transitions Pty Ltd,Q09 Transparency requirements and scope,"Part of improving public trust and confidence in AI would be to ensure providers of AI products document their data sources, explain how they ensured bias minimisation, or how they are going to monitor and correct bias over time. Further, the Australian government should ensure that location-specific and inclusive datasets are used for models used by government agencies. Finally, government data from employees’ use of the tools could end up in foreign databases and be used for training of decision-making tools. Therefore, data governance framework should be in place and records management strictly enforced to protect Australian government and private data.For transparency along the value chain, the environmental costs of AI and the human cost of training models, often akin to modern slavery should be taken into consideration. Consider sandbox development environments where governance and transparency processes are evaluated. This would allow innovation to take place while supporting th"
315,Source Transitions Pty Ltd,Q10 Ban on high-risk AI applications,"Research and applications should potentially be regulated differently so that we have: • a knowledge-based on how to counter high-risk AI • clear guidelines for is allowed to be deployed in society • open innovation which is discussed with the public (people who affect and affected by it) In terms of high-risk AI applications or technologies, the following should be banned: • Social scoring • Facial recognition • Deepfake"
315,Source Transitions Pty Ltd,Q11 Actions to increase public trust in AI,"Seeing that government is actively engaged with Responsible AI will provide some certainty to the public that it is and remains in touch with the evolving market. More than just regulations, the government should be engaged in a set of efforts and initiatives to continuously better its understanding, adjust the legislation and provide updated guidance as new systems and risks unfold. AI literacy is a society-wide area that will need to be driven both by governments and by the private sector. To ensure the latest science is also commercially viable and that learning together about what Responsible AI will require government, academia and the private sector to work together on real-life problems. Public-private partnerships and academia-private research is difficult, often cumbersome, and expensive. Infrastructure, initiatives and funding could make this cross-sector learning framework easier to access and would help share knowledge and practical outcomes. The Australian Government shoul"
315,Source Transitions Pty Ltd,Q13 Changes to conformity infrastructure,"As mentioned earlier, AI sandboxes would help provide an environment where innovation can happen within the regulatory framework. It also can provide some assurances to the public that AI systems were tested and compliant."
315,Source Transitions Pty Ltd,Q14 Risk-based approach to address AI risks,"A risk-based approach is the most urgent piece of legislation that should be put in place to get some type of controls on the technologies and its developers and deployers. However, this needs to be done with a view to implement an iterative way of reviewing and adapting the legislation as the technologies evolve. It should also not deter government from guiding the type of society we want to have and the way in which these technologies will support the vision for a fair, sustainable and healthy society. Iteration and adaptiveness, along with intent will call on an innovation-led approach to legislation."
315,Source Transitions Pty Ltd,Q15 Benefits and limitations of risk-based approach,"Risk management does nothing about power imbalances. It manages the harms without addressing power imbalance, implying that even if people do not want it, the activity can happen as long as risk are mitigated. It could all be a tick box exercise . Developers and users of AI have no obligation to be beneficial to or promote the interests of people affected by AI or the broader national and global challenges we face therefore missing out on what AI could do for us."
317,LegalVision,Transcription,"FILENAME: 20230726.LV Submission.Safe and responsible AI in Australia (Consultation Paper).8059eb395fe05.pdf | CONTENT: 26 July 2023Technology Strategy Branch Department of Industry, Science and ResourcesBy email: DigitalEconomy@industry.gov.auSubmission: Discussion Paper on ‘Safe and Responsible AI in Australia’ opened on 1 June 2023Thank you for the opportunity to respond to the Discussion Paper on ‘Safe and Responsible AI in Australia’, opened on 1 June 2023, from the Department of Industry, Science and Resources (Discussion Paper).LegalVision is a tech-driven commercial law firm that operates primarily online. The majority of our clients are start-ups and small to medium businesses, many of which are companies operating in the artificial intelligence (AI) space that are keen to see a considered and balanced regulatory regime deployed in Australia.In responding to the Discussion Paper, we welcome any changes proposed by government that support the following objectives: ● Proportiona"
317,LegalVision,Q01 Definitions,Please see attached.
317,LegalVision,Q02 Other AI risks and recommendations for regulation,Please see attached.
317,LegalVision,Q03 Non-regulatory actions to address AI risks,Please see attached.
317,LegalVision,Q04 Coordinating AI governance across government,Please see attached.
317,LegalVision,Q05 International models for AI governance,Please see attached.
317,LegalVision,Q06 Public vs Private sector governance,Please see attached.
317,LegalVision,Q07 Responsible government use of AI,Please see attached.
317,LegalVision,Q08 Generic vs technology-specific approaches for AI risk,Please see attached.
317,LegalVision,Q09 Transparency requirements and scope,Please see attached.
317,LegalVision,Q10 Ban on high-risk AI applications,Please see attached.
317,LegalVision,Q11 Actions to increase public trust in AI,Please see attached.
317,LegalVision,Q12 Trade impact of banning high-risk AI applications,Please see attached.
317,LegalVision,Q13 Changes to conformity infrastructure,Please see attached.
317,LegalVision,Q14 Risk-based approach to address AI risks,Please see attached.
317,LegalVision,Q15 Benefits and limitations of risk-based approach,Please see attached.
317,LegalVision,Q16 Scope and application of risk-based approach,Please see attached.
317,LegalVision,Q17 Elements to include in risk-based approach,Please see attached.
317,LegalVision,Q18 Integration of risk-based approach into existing frameworks,Please see attached.
317,LegalVision,Q19 Application of risk-based approach to general purpose AI,Please see attached.
317,LegalVision,Q20 Voluntary vs mandated compliance with risk-based approach,Please see attached.
318,Ananguku Arts,Transcription,"FILENAME: AI submission ANANGUKU ARTS 26 JULY 2023.012228122ecea.pdf | CONTENT: 26 July, 2023Ananguku Arts and Cultural Aboriginal Corporation (Ku Arts)Submission: Supporting responsible AI: discussion paperAbout Ku Arts Aṉanguku Arts and Cultural Aboriginal Corporation (Ku Arts) is the South Australian peak body for Aboriginal artists and art centres. Since 1998, we have been providing advocacy, support services, creative skills and professional development opportunities for artists and arts workers across all stages of their careers, in support of a strong and vibrant Aboriginal and Torres Strait Islander visual arts sector.Strong arts, strong culture, strong communities is the vision that has guided our organisation since forming 25 years ago and it remains our driving force today.Our vision recognises the symbiotic relationship that arts and culture have with individual and community health, education, and wellbeing. Participation in cultural activities has an intrinsic relationshi"
318,Ananguku Arts,Q02 Other AI risks and recommendations for regulation,"• Artists are very concerned about their works being used for AI without their knowledge or permission;• Artists should have the opportunity to exclude their works from AI, and to be compensated if their works are used with their consent;• This is particularly important for First Nations artists, given the cultural significance of their work and measures to ensure authenticity;• The use of AI outputs derived from unauthorised use of artists’ works should be treated as unethical and illegal; and• Governments need to take steps to ensure that AI technologies do not compromise income for First Nations artists, or their Indigenous Intellectual and Cultural Property Rights."
318,Ananguku Arts,Q11 Actions to increase public trust in AI,"The cultural, economic, and well-being of First Nations artists will seriously be impacted if the above issues are not addressed. When developing frameworks that address First Nations art and culture concerns Government needs to ideally consult with agencies who have expertise in First Nations art and culture including: Indigenous Arts Code, Arts Law, Copyright Agency, and the peak bodies representing First Nations artists and art centres across Australia including: Ku Arts, Desart, Aboriginal Art Centre Hub WA, and Arnhem Northern and Kimberly Artists Aboriginal Corporation."
319,x-RD,General comment,"The discussion paper focuses on the requirement for responsible AI and governance as a means to improve public trust and confidence in AI, and ultimately improve AI adoption within Australia. Even though responsible AI is important to Australia, the main challenge we are encountering is not so much the lack of AI governance or responsible AI, but the lack of a suitably skilled and experienced workforce that are capable of developing AI systems and applications within Australia (a prerequisite for building responsible AI systems). AI technologies are more complex than traditional software systems, and require deep expertise in a range of computer science and mathematics disciplines that are developed through extensive post-graduate studies in addition to industry experience. Australia is currently unable to meet the growing AI skills and experience demand, which is impacting the ability of industry to develop and scale AI solutions with adequate AI engineering and responsible AI require"
319,x-RD,Q01 Definitions,"The definition of AI provided which appears to be from the OECD and ISO standards implies that all AI systems are “engineered” when in fact many are just built without appropriate machine learning and AI engineering, and responsible AI principles. An engineered system uses scientific and mathematical principles to solve problems, which is how AI solutions should be built; however, due to the limited skills and experience available in Australia (and globally), solutions are being developed and used without adequate levels of engineering. Recommend removing “engineering” from the definition, similar to the definition of AI from the National AI Initiative Act 2020 (US Gov): https://www.congress.gov/116/crpt/hrpt617/CRPT-116hrpt617.pdf#page=1210With the MfM definition, they are also referred to as multi-modal LLMs so it might be worth stating this."
319,x-RD,Q02 Other AI risks and recommendations for regulation,"In Australia, there is a deficit of public governance mechanisms that specifically relate to the operationalisation of AI. What mechanisms there are in this category tend to be voluntary, vague and difficult to apply without material resources, specific expertise, additional guidance or a combination of these things.There are a growing number of mechanisms in tech specific and sector/issue specific domains (eg privacy, competition, employment, e-safety, etc), which are driven by sector based regulators. However, these tend to be (but are not always) directed to outcomes and focus on how AI interacts with previous legislative requirements. In short, there are few AI specific governance requirements beyond vague principles that broadly apply directly to the AI sector (outside of government).Development of an industry code of conduct (for the AI sector) which regulates the conduct of participants in an industry towards other participants in the industry or towards consumers in the industr"
319,x-RD,Q03 Non-regulatory actions to address AI risks,"The Australian Government should consider establishing a voluntary responsible AI maturity compliance framework for public and private sectors. This will help build public trust and confidence in AI systems and applications, through the improved engineering and alignment of AI systems with responsible AI principles."
319,x-RD,Q04 Coordinating AI governance across government,"At a federal level, this role is best performed by the Digital Transformation Agency (DTA) who are also responsible for strategic and policy leadership for whole-of-government ICT investments and digital service delivery. For national security, the Office of National Intelligence (ONI) is the most appropriate organisation for AI governance across the national security community.Potential goals include: 1. Development of a whole-of-government responsible AI framework 2. AI governance and reporting to government 3. Establishment of an AI maturity model for government departments and agencies to assess improvements and growth in data and AI systems and applications. 4. Collaboration forum for sharing experiences in AI adoption and operationalisation 5. Technical forum for sharing AI technologies, approaches, processes and architecture 6. Education and work transformation initiatives for AI adoption and operation"
319,x-RD,Q05 International models for AI governance,"The EU AI Act uses a risk based approach and relates risk to industry obligations which is sensible. From an Australian perspective, such an approach would need alignment to Australian values as to what is unacceptable, high risk or low risk may differ. Furthermore, the industry obligation for each of the categories would need to be determined and considered from a business perspective. Over governance may make the viability of growing an AI industry in Australia not feasible due to cost and the need for additional skills, experience and IT to support the significant responsible AI and governance requirements.Under the Biden administration, the US government has drafted a blueprint for an AI bill of rights which also adopts a principles based approach to AI governance. The blueprint specifies five principles that broadly apply to all AI systems and services. In relation to the current Australian AI ethics principles, these principles are more practical and less subjective.Singapore was"
319,x-RD,Q06 Public vs Private sector governance,"Yes. Public sector use of AI technologies have additional constraints or requirements that private sector applications do not have or require. For example, public sector (in particularly national security) AI systems need to have higher AI security requirements, can be trained on small amounts of data due to lack of data availability (which would cause issues with bias mitigation requirements), and in the case of defence applications may be used in consultation with weapon systems (requiring higher testing requirements and going against responsible AI practices of not doing harm). The public sector organisations, in particular those departments and agencies involved in national security, should develop responsible AI frameworks specific to their requirements; however, the majority of federal and state governments could work under a single public sector responsible AI framework.Another reason for separation of responsible AI frameworks from public sector and private sectors is the fact "
319,x-RD,Q07 Responsible government use of AI,"For more general responsible AI practices, the Australian Government should refer to an industry led body that includes government, academic, indigenous and other representation. The body should be industry lead and at different scales (enterprise, SMB, startups) as private industry are developing the majority of AI technologies (including innovations) used by government and the public, especially with generative AI, LLMs and MfM. This is a pivot from previous where much of the AI innovation came from academia."
319,x-RD,Q08 Generic vs technology-specific approaches for AI risk,"Generic solutions to the risks of AI are best used with higher order human level intelligence applications where an AI system is involved in complex decision making, planning or reasoning, and the system operates in an environment that is not well understood. They can also be used for risks that are not easily quantifiable or measurable using sensors or environmental data. An example of this might be an AI system that is involved in advising on interest rate changes given macro and micro economic factors and government policy.Technology-specific solutions (or engineering) are best applied to manage AI risk, where the AI risk factor can be quantified and measured (for example bias). Explainability is proving more difficult to achieve with the advent of LLMs and large machine learning models, so approaches may need to shift to trust, validation and verification (does the AI system do what it is supposed to do (and not do) and do it well)."
319,x-RD,Q09 Transparency requirements and scope,"a) In areas where the accuracy of an AI system needs to be understood in relation to its output or answer, transparency needs to be provided. For example, a real-time language translation system that is being used for formal business engagements needs to provide transparency on the accuracy of its translations and whether the system has been trained on relevant contextual data. If this is not provided, any incorrectly or poorly translated content can be easily misunderstood.It is also important to note that providing transparency across the AI lifecycle won’t necessarily improve public trust and confidence in AI in many cases as the understanding needed to interpret the information is not readily available in the public. An alternative approach is to certify the system or organisations building the systems such that they can be trusted as their AI systems have gone through engineering and have been aligned to a responsible AI framework.b) A standard would have to be developed for AI tr"
319,x-RD,Q10 Ban on high-risk AI applications,"a) High-risk activities should be considered separately under two (2) categories: unethical (social scoring) and [temporarily] uncontrolled (facial recognition or biometrics). While unethical activities should be banned, [temporarily] uncontrolled activities may have significant potential future applications, but these should be tightly regulated to prevent abuse. Any AI risks associated with uncontrolled activities may be minimised, mitigated or reduced through research and development over time. AI-enabled surveillance systems are an example of a [temporarily] uncontrolled activity that initially captured facial biometric data from the public. Through research and development, privacy preserving approaches have been implemented to prevent inadvertent facial biometric capture. These systems can now be employed in public environments to detect potential issues and alert security guards, but do so without violating the privacy concerns of individuals. AI applications that should be bann"
319,x-RD,Q11 Actions to increase public trust in AI,"To increase public trust in AI, we would recommend developing an AI engineering and responsible AI accreditation or certification system for companies or organisations building AI systems in Australia. This would provide a minimum standard for all AI products and services produced in Australia and ensure a tiered maturity model is employed so companies or organisations can work towards improved AI engineering and responsible AI practices. Such an approach will result in AI applications and systems that are more reliable, trusted, safe and responsible and will ultimately increase public trust in AI. In addition, we would recommend establishing applied rather than academic AI centres for industry and government that are focussed on developing AI engineering and responsible AI practices, including the testing of AI systems and applications to build trust within the Australian public and government."
319,x-RD,Q12 Trade impact of banning high-risk AI applications,Banning high-risk activities should have a limited impact on trade and exports due to the small number of AI companies operating in Australia in these areas. Proposed or actual banning may result in increases in domestic research and development as companies try to make their respective technologies more responsible.
319,x-RD,Q13 Changes to conformity infrastructure,"While it is sensible for Australia to harmonise with the approaches of the US, UK, Singapore, EU and others, there is a space for some specific AI governance (either voluntary, mandatory or both) that might be implemented now to provide operational detail and certainty in the AI specific domain (particularly due to the vagueness of some of the AI principles).We see an opportunity for further voluntary governance mechanisms dealing with operational matters specific to AI. There is also potential for a mandatory governance mechanism for the obvious high-risk cases around which there seems to be consensus or something near to consensus (eg, behavioural manipulation, social scoring and possibly real time biometric identification).These mechanisms could involve an industry code of conduct (for the AI sector) which regulates the conduct of participants in an industry towards other participants in the industry or towards consumers in the industry. Industry codes offer flexibility in mandating"
319,x-RD,Q14 Risk-based approach to address AI risks,"Yes, as long as the risk management framework (such as the NIST AI RMF) can be easily applied and understood, and is considered in relation to a principles-based approach to AI."
319,x-RD,Q15 Benefits and limitations of risk-based approach,"A risk-based approach is context agnostic and is well understood across most organisations and at senior levels. Like all risk management frameworks, the boundaries and descriptors for each of the risk levels need to be well articulated and subject to change, otherwise lower risk activities may be incorrectly represented at higher risk levels leading to unnecessary regulation, controls and costs. Alternatively, high-risk activities may be classified as lower and therefore not regulated."
319,x-RD,Q16 Scope and application of risk-based approach,"A risk-based approach is better suited for medium through to enterprise organisations due to the increased resourcing required to support AI risk assessments, and the resulting governance. A simpler or more streamlined approach should be considered for startups and small businesses, particularly if growth in the Australian AI industry is a priority."
319,x-RD,Q17 Elements to include in risk-based approach,"Risk-based approaches should include a range of assessment tools and references to simplify the identification of associated AI risks within an application or system. This is challenging as new risks in AI technologies and approaches are being identified on a regular basis by industry and academia, therefore assessments and controls need to be updated on a periodic basis. The Information Security Manual (ISM) which is used for cyber security controls and risk management also updates on a periodic basis to cater for continuous change with cybersecurity.Due to the complexity and specialist knowledge required to assess AI systems, there is a need to develop a compliance or assurance assessment program to scale the AI assurance role (and industry) to government and industry levels. A similar program was established for cyber security assessments and is referred to as the Infosec Registered Assessors Program (IRAP).Several elements in attachment C are still quite subjective and difficult to"
319,x-RD,Q18 Integration of risk-based approach into existing frameworks,"This is not recommended particularly as AI is still evolving and the risks are not well understood in many areas. A good reference is the NIST AI RMF which is based on ISO31000:2018. Any AI risk-based approach should align to ISO31000:2018 where possible, but should remain a separate framework"
319,x-RD,Q19 Application of risk-based approach to general purpose AI,"Generative AI, LLMs and MfMs should be treated no differently from other types of AI systems in relation to risk, but they need to be considered in the context they are being used (they are not AGI systems). For example if a LLM is being used to assist with code prompts, then it needs to be evaluated in relation to code prompts. These types of AI approaches are new and weaknesses, risks and security related issues are still being discovered on a regular basis. From an AI risk-basis, any risk assessment needs to have a temporal component and be re-evaluated on a periodic basis that is dependent on the context in which the system is used, and new research or operational findings. For example, with LLMs and MfMs, it is expected that risk assessments occur more frequently due to how fast the field is developing, in relation to computer vision where the technologies are quite mature."
319,x-RD,Q20 Voluntary vs mandated compliance with risk-based approach,"a) Regulation of risk-based approaches for responsible AI should be application or risk level dependent rather than broadly applied to public and private sectors. Those deploying AI systems in high-risk categories (for example automotive self-driving systems) should be required to meet a mandated minimum standard of AI engineering and responsible AI in their applications and systems. Those public and private sector organisations building systems in lower risk categories should have a voluntary or self-regulated approach to responsible AI that includes a maturity model to build trust in the public and government that the AI technologies being developed are trustworthy, safe, explainable, ethical etc.b) Both. This is due to the tight coupling of AI systems to data and other systems. As a result those responsible for deploying AI systems need to also ensure the right AI system is deployed for a context or make modifications to the AI system to ensure it is appropriate for the context it i"
320,Anonymous,Q02 Other AI risks and recommendations for regulation,"Missing risks include (1) misuse of highly capable AI systems, and (2) unintended harm from highly capable AI systems, especially systems with the capacity to plan and act as agents acting in ways that are not aligned with human directives. Although the likelihood of the risks may be uncertain, the harms resulting from these risks may be catastrophic and global in scope and must be acknowledged and addressed.The statement “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war” has recently been signed by the heads of leading AI labs (OpenAI, Google DeepMind, Anthropic, Stability AI) as well as many key academic and other researchers pivotal in developing this technology (Geoffrey Hinton, Yoshua Bengio, etc.) and other notable figures such as Bill Gates [1]. Similar calls date back to at least 2015 when Stephen Hawking led a group of AI experts to call attention to the risk and lay out an urgent resea"
320,Anonymous,Q03 Non-regulatory actions to address AI risks,"The key non-regulatory function Australia could perform is offering international leadership focused on ensuring AI systems can be understood. Australia should be an active voice for the importance of ethics and safety in the current global conversation around AI governance and AI standards making. Specifically, Australia should advocate for: The development of standards and tools that analyse and monitor AI systems for robustness, bias, dual-use capabilities and other safety-relevant features [1]. Audit trail requirements for safety-critical applications of AI systems, including allowing the traceability of decisions through product life cycles [2]. We will only be able to have confidence in more sophisticated AI systems if they can be understood, we can ensure they do not pose unacceptable risks and hold accountable those at fault where it does. Doing the non-regulatory work to ensure AIs have “traceability” is necessary to underpin the future of safe AI. Joshua Kroll’s work on accou"
320,Anonymous,Q05 International models for AI governance,"Singapore offers relevant, adaptable, and desirable governance measures that could benefit Australia. One such measure is a comprehensive framework suggested in its discussion paper [1] that encourages shared responsibility and disclosure standards, data privacy guidelines, and the development of standardised metrics and tools for AI development, testing, and monitoring. Key focuses include: - Accountability: Establish a collective responsibility framework and transparency standards, like watermarks and labels, to enhance accountability in the AI model development process. - Data protection / sovereignty and IP guidelines: Draft data protection and intellectual property guidelines for generative AI and setting up reliable data storage facilities. - Evaluate model development and deployment: Create uniform evaluation measures and resources that aid developers in the production, testing, and surveillance of AI models, and support third-party auditing of AI systems when deployed. - Safety"
320,Anonymous,Q09 Transparency requirements and scope,"The recent focus for transparency has been on ensuring that consumers are notified when a decision made by AI affects them directly (e.g. hiring/firing, loan approval). While this is helpful, it is only the first step in protecting the public. In circumstances where the use of AI for decision-making is commonplace, it will be of little value for consumers to know “when” an AI system makes a decision that affects them. This might happen many times each day. Rather, they should be informed “how” the decision is made. This will allow consumers to more easily identify unsafe or unfair systems. Having models which are both accurate and explainable has long been an issue for businesses [1], but the recent advancements of “black-box models” is encouraging developers and providers to simply tolerate the risks that come from lack of explainability. As AI capabilities continue to outstrip their explainability [2], the risks of this approach will grow significantly. The fact that explainability i"
320,Anonymous,Q10 Ban on high-risk AI applications,"In February 2023, OpenAI CEO Sam Altman released a statement containing the following: “As our systems get closer to AGI, we are becoming increasingly cautious with the creation and deployment of our models. Our decisions will require much more caution than society usually applies to new technologies, and more caution than many users would like.” [1] It is very reassuring to see this acknowledgement expressed by Altman, and the Australian Government would be wise to heed such warnings. Unfortunately, it may not matter how much caution OpenAI shows, because their actions have triggered the arms race that AI safety experts have been concerned about for years [2, 3]. In order to protect our nation, Australia should make an effort to impose a ban on the training of large-scale AI systems with a nontrivial expected chance of passing capabilities evaluations required to cause catastrophic risks to civilization (such as the Alignment Research Center’s tests for the capabilities required for a"
320,Anonymous,Q14 Risk-based approach to address AI risks,"The regulation of AI needs to be “risk-based” in the same way that the regulation of vehicles is risk-based. We are right to regulate push-bikes and passenger jets in very different ways. A limitation of the risk management approach in the discussion paper, specifically Box 4 and Attachment C, is that it focuses only on how an AI is used and disregards the nature of the AI itself. An effective regulatory regime for a high-risk activity will regulate both the technology itself and how it is used. The Australian Human Rights Commission [1], in discussing this issue (“Human Rights and Technology”, 2021), says: Governments tend to regulate high-risk activities and technologies more closely. This helps explain the comparatively strict laws that govern fields such as gene technology, aviation, healthcare and the energy industry. In these areas, regulation often applies both to the technology itself and how it is used. From a human rights perspective, the need for more prescriptive regulation"
320,Anonymous,Q19 Application of risk-based approach to general purpose AI,"The risk-based approach must be modified to address risks from general purpose AI systems, especially LLMs and MFMs. These types of systems learn their capabilities and behaviours during training, which can be unforeseen by developers. That means that AI developers could create models with dangerous capabilities (e.g., deception, manipulation, political strategy, weapons design). Humans could misuse those capabilities to cause harm [1], and models could apply the capabilities even without deliberate misuse (e.g., through misalignment) [2]. Consistent with recent work from a consortium of AI safety researchers, including at Google Deepmind, OpenAI, Anthropic, Alignment Research Center, University of Cambridge, University of Oxford, Centre for Long-Term Resilience, and Centre for the Governance of AI [1], we recommend that any risk-based approach require that general purpose models are evaluated for dangerous capabilities and misalignment. This will help determine: 1. To what extent a mo"
320,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,"AI is very likely to transform many elements of the economy and society in coming years. Ethics and safety are unlikely to be a priority during this time unless they are actively made a strategic focus by clear leadership. Historic examples include how social media algorithms came to shape our lives, and the negative impacts to mental health and social cohesion only became apparent much later and still have not been adequately addressed. Similarly, while the destructive potential of nuclear was apparent early on, the “arms race” mentality, as well as potential economic benefits, swept humanity along towards the current risky situation that no one in the early 1940s could have conceived of. In that way, history teaches us that it’s critical that ethics and safety take centre stage from day one. The economic and technological forces are likely to be so strong that a scheme that is voluntary or driven by aspirational best practices is unlikely to be successful. Further, smaller businesses"
321,Australian Publishers Association,Transcription,"FILENAME: 20230726_Submission_APA__Responsible_use of AI_Final.ae1b9da7c32f1.pdf | CONTENT: Australian Publishers Association submission toMinister Husic's consultation on responsible use of Artificial Intelligence (AI)1. Preamble and key elements in this submissionKey elements in this submission● Who is the APA and what is the value of publishing ● Publishing in Australia ● How AI is changing practices in the publishing industry ● Challenges for publishers on the AI road ● Key regulatory issues for publishers ○ Copyright ○ Science and research ● An ethical and effective approach to regulation ○ An ethical framework driven by principles ○ Adaptation of existing legislation ○ Global alignment of regulation ○ Coordination of regulation and policyPreambleArtificial intelligence (AI) offers transformative opportunities and efficiencies. Australian publishers, represented by APA members, have integrated AI innovations to enhance marketing strategies and gain valuable consumer insights. Howe"
323,Anonymous,Transcription,"FILENAME: Submission 323 - Attachment.21b9f98bc3165.pdf | CONTENT: Supporting responsible AI: government discussion paper1 Index: o Introduction - 3 ▪ About XXX - 4 ▪ XXX’s responsible AI - 4 ▪ What it does - 5 ▪ Risk detection in action - 5 ▪ Successfully regulating AI -5 o Recommendations - 6 o Definitions - 8 o Potential gaps in approaches - 8 o Responses suitable for Australia - 11 o Implications and infrastructure - 15 o Risk-based approaches - 162 INTRODUCTIONXXX is pleased to provide a response to the Commonwealth Department of Industry, Science and Resources Technology Strategy Branch, regarding its Supporting Responsible AI: Discussion Paper (discussion paper).XXX is a powerful software platform that specialises in data discovery, investigation, and cybersecurity. Wi th advanced data processing capabilities, our software enables organisations to efficiently analyse, search, and extract valuable insights from vast and complex datasets, including unstructured and structured data"
323,Anonymous,Q01 Definitions,Nuix concurs with the definitions currently outlined within the discussion paper.
323,Anonymous,Q02 Other AI risks and recommendations for regulation,"Regulatory and enforcement solutions regarding AI must address the purposeful and harmful intention, by malicious actors seeking to exploit Australians online.Nuix identifies the following as potential risks within the Australian market resulting from an unregulated approach to innovation in AI: • Privacy breaches: There is a risk of excessive data collection beyond what is necessary • Algorithmic bias: untrained (or ‘under-trained’) data has the potential to result in tools that propagate bias • Exclusionary impact: the potential for the benefits and risks of AI to be unevenly applied across the community, needs to be very carefully considered and managed. • Job displacement: As AI and automation continue to advance, some industries and job roles are at a higher risk of being automated and whilst this transition is somewhat inevitable, it should be thoughtfully managed • Cybersecurity threats: Deep-learning training is compromised with intentional malicious information • Malevolent ex"
323,Anonymous,Q03 Non-regulatory actions to address AI risks,"Given AI is dramatically fast moving and a comparatively recent and still-evolving technology, Nuix’s position is that any legislative or regulatory actions need to be balanced by significant industry participation.Nuix recommends the implementation of:• A voluntary industry code, to be overseen in partnership with the Australian Government, its departments and agencies as appropriate. • A working group with annual reviews between industry actors, academia, DIGI and its members, the Department of Industry, Science and Resources, the Department of Home Affairs, the Attorney General’s Department, and including the Cyber Security Commissioner and the head of the Department of Home Affairs’ Deputy Secretary of the Cyber and Infrastructure Security Group. • Biannual reviews with a trusted government partner, such as the Australian Strategic Policy Institute (ASPI), part-funded by the Australian Government and industry to review the social applications of AI technologies. • Broad knowledge-b"
323,Anonymous,Q04 Coordinating AI governance across government,"AI’s rapid advancement represents an important opportunity for the Australian Government to drive the development and deployment of AI for the benefit of all Australians.Principled coordination of AI governance across government can help ensure consistent approaches and facilitate the responsible development and adoption of AI in Australia.A coordination mechanism could be implemented through: • Establishment of a whole-of-government coordination branch housed within the Department of Prime Minister and Cabinet (PM&C), to ensure that all departments and agencies adopt a common shared language and approach towards the use and policy frameworks relating to AI. • Establishment of a permanent Joint Parliamentary Committee with responsibility of oversight of the Australian Government’s implementation of AI, including but not limited to regulatory and enforcement mechanisms. • Establishment of a working group: A dedicated entity can coordinate AI-related policies, regulations, and guidelines"
323,Anonymous,Q05 International models for AI governance,"Nuix believes there is much to learn from the regulatory models being implemented by the EU (a ‘rights based’ regulatory approach) and United States (a ‘market based’ regulatory approach), given the divergence in both jurisdictions’ approaches. While both models will favour a risk-based approach, the reality is their enforcement agencies, mechanisms and existing legislation governing digital and data processes and security, critical infrastructure and overall threat landscape and capability account for the different regulatory models.Nuix believes an Australian working group examining both models, in consultation with experts in existing regulation within Australian Government departments and aided by third-party experts in the Australian Strategic Policy Institute, would be best placed to provide the most holistic review on both models. This would allow for an expert Australian perspective on which legislative or regulatory or enforcement mechanisms would best work within existing fra"
323,Anonymous,Q06 Public vs Private sector governance,"While different risk assessments may be legitimately applied to AI in the context of private and public sector use, we would encourage a harmonised approach where those risks are the same.In this context, the EU AI Act focuses more heavily on a tiered approach to the risks associated with the AI application itself – categorising risk based on the intended use or purpose of the AI along the following lines: a. Prohibited AI use – eg subliminal manipulation b. High-risk AI systems – eg critical infrastructure c. Limited-risk AI systems – detect emotions, generate content d. Minimal-risk AI systems – not covered above."
323,Anonymous,Q07 Responsible government use of AI,"In a recent report by CSIRO on helping Australian businesses navigate responsible AI principles, the steps to implement Australian Government’s AI ethics principles are highly beneficial to support responsible AI practices and should be taken into consideration. These include:• Establishing internal guidelines and policies: Developing clear guidelines and policies for AI use within government agencies can ensure adherence to ethical principles, transparency, and accountability. • Providing training and resources: Offering training programs and resources on AI ethics, responsible AI development, and risk management can empower government employees to make informed decisions and implement best practices. • Encouraging collaboration and knowledge sharing: Facilitating communication and collaboration among different government agencies can promote the sharing of experiences, lessons learned, and best practices in AI governance. • Conducting audits and assessments: Regular audits and assess"
323,Anonymous,Q08 Generic vs technology-specific approaches for AI risk,"• In the case of no-risk and low-risk applications and use of AI, more generic approaches can and should be applied, including public education efforts, establishing business and govt best practices, and broad legislative guidelines. For example, it would benefit society to be informed about what AI is, how it can be used, how to avoid improper use and where to seek guidance and support. This may include providing insights on the differences of the various kinds of AI, including ChatGPT guidelines such as “never upload private information to an application like ChatGPT or Bard” and “be mindful that the output from these systems may infringe upon other’s copyrighted works.” • Meanwhile, for mid-high-risk or banned uses, more targeted, technology-specific approaches would be highly recommended. This may include: o The official classification of AI systems to differentiate them from each other, i.e., Generative AI, Natural Language Generation (NLG), Transformer-based AI, Pre-Transformer-b"
323,Anonymous,Q09 Transparency requirements and scope,"a. where and when will transparency be most critical and valuable to mitigate potential AI risks and to improve public trust and confidence in AI?Transparency should be prioritised in the following scenarios: • Critical decision-making: When AI systems are used to make decisions that significantly impact individuals' lives, such as in criminal justice, finance, or healthcare, transparency is crucial to ensure accountability and fairness. • Data-driven decisions: Transparency is valuable when AI algorithms rely on large datasets to make predictions or recommendations, as it helps identify potential biases, errors, or discriminatory outcomes. • Understandability (or ‘explainability’) of AI decisions: In contexts where human oversight and intervention are necessary, transparency enables humans to understand and interpret AI-generated results, enhancing trust and facilitating error correction.b. mandating transparency requirements across the private and public sectors, including how these "
323,Anonymous,Q10 Ban on high-risk AI applications,"a. Whether any high-risk AI applications or technologies should be banned completely?AI is still in its relative infancy, and it is therefore difficult to define what high-risk iterations of the technology constitute. However, areas that should be considered high-risk and therefore banned include:• AI systems that enable or facilitate human rights violations, discrimination, or harmful propaganda. This is a key area of attention under the EU AI Act. • AI technologies with significant safety risks that cannot be effectively mitigated.b. Criteria or requirements to identify AI applications or technologies that should be banned, and in which contexts?Incorporating this into the working group's review to the Australian Government after a two-year period, within a fixed-term reporting period, the following criteria and requirements should be considered for identifying AI applications or technologies that warrant banning, along with their specific contexts:• Potential for severe harm: Assess"
323,Anonymous,Q11 Actions to increase public trust in AI,"• Transparent and understandable AI systems: Promoting the development and adoption of AI technologies that provide clear explanations of their decision-making processes, ensuring accountability and reducing opacity.• Robust data protection and privacy regulations: Implementing strong regulations and safeguards to protect individuals' data, ensuring proper consent, and preventing unauthorised access or misuse. • Independent auditing and assessments: Encouraging third-party audits and assessments of AI systems to verify their compliance with ethical guidelines, fairness, and safety standards. • Public awareness and education: Conducting public campaigns, workshops, and educational programs to inform the public about AI technologies, their benefits, and potential risks, fostering informed discussions and understanding. This could leverage the results of deliberate and targeted programs which can be used to benchmark and demonstrate improved public service efficiencies from AI – such as r"
323,Anonymous,Q12 Trade impact of banning high-risk AI applications,"Nuix understands the challenge governments face in balancing technological innovation, trade and investment priorities, and securing the welfare and freedoms of its citizens. The impacts can vary depending on the specific bans and contexts, but may include:• Technological innovation: Banning certain high-risk activities may restrict innovation in the development and deployment of AI technologies, potentially impeding the growth of the tech sector and limiting Australia's competitiveness in global markets. • Trade and international relations: Bans on specific AI technologies or applications could impact international collaborations and trade agreements, as other countries may have differing regulatory approaches or rely on such technologies for their own operations. • Ethical reputation: Banning high-risk activities associated with ethical concerns can enhance Australia's reputation as a responsible and ethical player in the global tech industry, potentially attracting partners and inve"
323,Anonymous,Q13 Changes to conformity infrastructure,"Nuix proposes the following changes: • Standards and certification frameworks: Developing or updating standards and certification programs specific to AI technologies, ensuring compliance with ethical guidelines, safety standards, and best practices. • Regulatory oversight and enforcement: Strengthening regulatory bodies and processes responsible for monitoring and enforcing AI-related regulations, ensuring that conformity assessments are conducted and addressing non-compliance appropriately. • Collaborative partnerships: Enhancing collaboration between government agencies, industry, and research institutions to develop conformity assessment methodologies and assurance frameworks specific to AI technologies."
323,Anonymous,Q14 Risk-based approach to address AI risks,Governing AI through a holistic risk-based approach provides regulators and enforcement agencies with the most practical means of addressing potential AI risks in the short to medium term and is the approach Nuix recommends the Australian Government aim to adopt in concert with the recommendations outlined at the beginning of our submission.
323,Anonymous,Q15 Benefits and limitations of risk-based approach,"The main benefits of a risk-based approach include:• Focus on high-risk areas: Identifying and addressing the most significant risks associated with AI technologies, providing targeted mitigation measures and regulatory scrutiny where they are most needed. • Flexibility and adaptability: Allowing regulations and governance measures to evolve as AI technologies advance and new risks emerge, ensuring continued relevance and effectiveness. • Efficiency and resource allocation: Optimising the allocation of regulatory resources and efforts by prioritising areas with the highest potential risks, minimising unnecessary regulatory burdens on low-risk applications.Limitations of a risk-based approach can include: • A core limitation of a purely risk-based approach is the high opportunity costs associated with the lost potential benefits and opportunities associated with a particular AI. That is, by focusing solely on the risks, many good ideas and solutions to a given challenge may be lost in t"
323,Anonymous,Q16 Scope and application of risk-based approach,"At this stage of its development, AI is still overall a comparatively nascent technology, the implications and applications of which are still to be fully understood. Given their resources to respond to emerging challenges and developments, large companies with their more extensive resources and assets, have to date shown an appetite to respond relatively to quickly to significant risks.Nuix is aware the Australian Government has already considered the likelihood of challenges faced by organisations of different size and scale as part of the work it undertook to develop the Critical Infrastructure Acts and regulations, and in appointing a Cyber Security Coordinator. The byplay between AI and other critical digital communications technologies and services will likely require a response of scale from regulators to support sole traders, smaller organisations, or organisations deploying AI to address their business challenges.It is likely that within a few years of all actors achieving a s"
323,Anonymous,Q17 Elements to include in risk-based approach,"• Establishing standardised frameworks and methodologies: To assess and quantify risks associated with AI applications, considering factors such as data quality, model performance, potential harms, and mitigating measures. • Compliance requirements: Defining clear compliance requirements for organisations based on risk levels, ensuring they adopt appropriate risk mitigation strategies, such as testing, auditing, or third-party assessments. • Monitoring and reporting mechanisms: Implementing mechanisms to monitor the ongoing performance and risks of AI systems, requiring organisations to report incidents, algorithmic updates, and compliance measures. • Enforcement and sanctions: Outlining enforcement measures and potential sanctions for non-compliance with risk-based regulations, providing incentives for organisations to prioritise risk mitigation."
323,Anonymous,Q18 Integration of risk-based approach into existing frameworks,"The last 12 months has seen AI move from being a conceptual emerging risk on the Chief Risk Officer’s future radar to a fast-moving risk which requires active management today. Given the uncertainty of the technology and its applications, it can be tempting to create a bespoke AI risk framework to assess and manage risks. We believe it is best to integrate AI risk management into your existing risk management framework. For example, it can be helpful to break down this topic into more manageable components: • Strategic Risks: risks that AI technology may impact the short, medium- and long-term objectives of the business. For example, what is the risk that AI technology may disrupt the industry, impact the value of your technology intellectual property, or change consumer habits. Strategic risks are best tackled as part of the strategy setting and refresh processes. • Operational & Compliance Risks: risks associated with using AI tools and technologies as part of business processes. For"
323,Anonymous,Q19 Application of risk-based approach to general purpose AI,"A risk-based approach can be applied to general-purpose AI systems, such as large language models (LLMs) or multimodal foundation models (MFMs), by considering potential risks associated with their deployment and use. Risk assessments could involve examining factors such as the models' training data, potential biases, interpretability, the potential harm caused by incorrect outputs, and the context of deployment. Mitigation strategies might include external audits, fairness checks, or user feedback loops to monitor and address risks associated with the use of such systems.For guidance and context, we suggest referencing and exploring the EU’s AI Act, with particular emphasis on their four-tiered risk system: • Banned Uses: subliminal manipulation; social scoring by public authorities; real-time biometric ID systems for law enforcement. • High Risk: critical infrastructure safety; worker management; essential services (e.g., credit scoring); law enforcement • Limited Risk: interact with"
323,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,"We have first-hand experience across the globe of the various approaches taken by our customers to the responsible application of AI. As a starting point, we believe that appropriate consideration should be given to mandating obligations through thoughtful, principles-based regulations.The approach of principles-based regulation will be important to enable Australia’s legislative expectations to accommodate the exponential speed and changes that will continue to be a feature of AI for the foreseeable period.In the initial phase – while regulators and the industry are developing a deeper understanding of the potential benefits and risks of AI - it will be important to ensure the right balance is struck between innovation and regulation. We have seen this managed effectively across customers and industries, via an emphasis on an ‘if-not-why-not' approach to compliance - where organisations report if they are complying with the new obligations and if not, why so. This might come in the fo"
324,Michael Kerrison,Q01 Definitions,"The definitions in Figure 1 are not sufficiently granular to support risk-based consideration of different kinds of AI. The definitions do not allow differentiation between AI models that provide economic benefit, models that raise significant ethical concerns, and models that raise radical safety concerns, and will not support policy that addresses the risks and opportunities of each.The foreword to ISO/IEC 22989 calls out its own limitations: it is currently lacking language for comparing AIs on dimensions like trustworthiness, robustness, resilience, reliability, accuracy, safety, security and privacy. These are exactly the properties we should be thinking about, that will have the implications that matter for policy.Critically, following human-defined objectives or parameters might be a good policy outcome, but is not a fundamental property of AIs. Tools already exist that allow AIs to perform complex multi-step procedures and generate their own recursive queries, either prompting "
324,Michael Kerrison,Q02 Other AI risks and recommendations for regulation,"Missing risks include (1) misuse of highly capable AI systems, and (2) unintended harm from highly capable AI systems, especially systems with the capacity to plan and act as agents acting in ways that are not aligned with human directives.The actual likelihood of these risks playing out is uncertain, and could even be very low. But the harms resulting from these risks may be catastrophic and global in scope and must be acknowledged and addressed.I acknowledge this, and views about current-day harm and misuse. But to spend zero time or effort on these catastrophic risks would be grossly negligent. I am firmly convinced that analogies to pandemics or nuclear war are appropriate - even small steps to prepare for tail events present a good investment, and even a moral obligation.For example: the statement “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war” has recently been signed by the heads of le"
324,Michael Kerrison,Q03 Non-regulatory actions to address AI risks,"As with other catastrophic risks, the problem could be tractable. There are three streams of work that the Government can pursue:1. Publicly acknowledge the risk. Australia must join the growing number of countries and global governance bodies that acknowledge the possibility of catastrophic and existential risks from increasingly powerful AIs. There’s no chance of solving a problem unless we acknowledge it. This is a necessary first step.2. Lead the establishment of global governance arrangements that are specifically tasked with tackling the possible catastrophic risks from AI. This could involve: - Fostering cultures of fairness, accountability and transparency by the companies and countries developing more advanced AIs. We need to work to avoid an ‘arms race’ that prioritises speed over safety, including making sure that risky decisions with broader implications aren’t made behind closed doors. - Monitoring the progress of AI development [1], including having the power to conduct i"
324,Michael Kerrison,Q05 International models for AI governance,"Australia should take note of the EU recent handling of the AI Act. For example, the European Parliament’s recognition of general-purpose AI (GPAI) is an important step in the right direction [1]. In particular, the following observations are highly valuable:- Acknowledgement that GPAI providers must comply with strict safety measures, regardless of the way it is distributed or intended to be used.- Requirements for GPAI providers to comply with various safety requirements throughout the lifecycle of their product; i.e. external audits assessing their performance, predictability, interpretability, corrigibility, safety and cybersecurity.These adjustments appear to respond to significant criticisms directed at initial drafts of the AI Act. For example, the AI Now Institute released a set of recommendations that heavily criticised the EU’s approach on some matters [2]. For example:- “GPAI models carry inherent risks and have caused demonstrated and wide-ranging harms. While these risks c"
324,Michael Kerrison,Q09 Transparency requirements and scope,"The aviation industry offers a prime example of how transparency can be used to reduce risk and increase safety in a potentially dangerous industry. Transparency is embedded in every stage of the value chain, from manufacturing to operations, and extends to the reporting and investigation of risk-related incidents.The worst aviation disasters have terrible consequences, but are far from as bad as some conceivable disasters from sophisticated AI systems. Aviation safety provides a good *analogy*, but this should not be misconstrued as an *identical example*. The bar for safety needs to be much higher, but - particularly around ""compute governance"", notably regulating computing hardware - there are lessons to be learned.Manufacturing StageTransparency begins at the manufacturing stage. Aircraft manufacturers, such as Boeing and Airbus, are required to adhere to stringent safety standards set by regulatory bodies like the Federal Aviation Administration (FAA) in the U.S. and the European "
324,Michael Kerrison,Q10 Ban on high-risk AI applications,"In so many other technologies, in so many other industries, in so many areas of society - I would strongly advocate for permissionless innovation and minimal government interference.But AI - like with nuclear technology, certain biological research, gain of function research, and other ""dual use"" technologies - is different. It has the potential, however slim or distant, to do genuinely catastrophic damage.In February 2023, OpenAI CEO Sam Altman released a statement containing the following:“As our systems get closer to AGI, we are becoming increasingly cautious with the creation and deployment of our models. Our decisions will require much more caution than society usually applies to new technologies, and more caution than many users would like.” [1]It is very reassuring to see this acknowledgement expressed by Altman, and the Australian Government would be wise to heed such warnings. Unfortunately, it may not matter how much caution OpenAI shows, because their actions have triggered "
324,Michael Kerrison,Q14 Risk-based approach to address AI risks,"Yes; specifically, the regulation of AI needs to be “risk-based” in a similar way to vehicles. Extending the aviation anaology, we are right to regulate push-bikes and passenger jets in very different ways.We are right to (not) regulate consumer software in the same way we (heavily) regulate the technologies that go into nuclear weapons. I claim AI may soon become concerningly closer to the latter.A limitation of the risk management approach in the discussion paper, specifically Box 4 and Attachment C, is that it focuses only on how an AI is used and disregards the nature of the AI itself.An effective regulatory regime for a high-risk activity will regulate both the technology itself and how it is used. The Australian Human Rights Commission [1], in discussing this issue (“Human Rights and Technology”, 2021), says:Governments (appropriately) regulate high-risk activities and technologies more closely. This helps explain the comparatively strict laws that govern fields such as gene tech"
324,Michael Kerrison,Q19 Application of risk-based approach to general purpose AI,"LLMs and MFMs are constantly demonstrating unprecedented general-purpose capabilities, and their emergence indicates that regulatory approaches which focus on specific use-cases or industries are becoming outdated. A modern approach requires that we focus on overall system capabilities rather than individual scenarios.Systems with advanced capabilities should face progressive levels of scrutiny. Low-risk systems can be monitored with light touch regulation. To be clear, current-generation systems (as of mid-2023) like OpenAI's GPT-4 do not seem to pose serious risks - short of being connected to critical infrastructure (which could very well occur). However, as capabilities increase, so should regulatory requirements.Precursor systems, with capabilities that can be used to build much more advanced systems, will need stricter controls to avoid proliferation of advanced systems amongst malicious actors [1]. Users who require “developer” access should require permits granting such access,"
324,Michael Kerrison,Q20 Voluntary vs mandated compliance with risk-based approach,"In general, regulation should:1. Be proportionate to risk. Meaning that a voluntary or self-regulation approach should only apply to low-risk or no-risk uses of AI.2. In the case of higher-risk technologies, regulate both the technology itself and the use of the technology. Meaning that, for future and more sophisticated AIs, regulation should ensure that the technology is safe before it is created or published, as well as ensuring that it is only used in ways that are safe.3. Place burdens on those most able to reduce potential harms. Meaning that AI that functionally operates as ‘black box’ to consumers has to be regulated at the point of the AI Lab.Applying those three principles, self-regulation may be appropriate for some technologies available today and for some participants (such as students or researchers), but a forward-looking regulatory regime must apply broadly and must be backed by Australian diplomatic efforts to ensure it is globally coordinated."
325,Regulatory Institute,Transcription,"FILENAME: RI Comments Safe responsible AI Aust.d301e6b434885.pdf | CONTENT: Regulatory Institute: Safe and responsible AI in AustraliaThe Regulatory Institute is a non-profit think tank that aims to improve regulation globally so that regulations benefit us all. We do this through research into good lawmaking and regulatory techniques, and pro bono consultancy to governments, legislatures and NGOs. We published the Model Law on Artificial Intelligence (the Model Law), which forms the basis of our comments in this consultation by the Australian Department of Industry, Science and Resources.We have also referenced our Handbook ""How to regulate?"", which covers theoretical, methodological, and applied aspects of legal regulation and lawmaking, concentrating on the best practices of more than 40 countries/jurisdictions around the world. Noting the importance of conformity infrastructure for regulating AI we urge you read the two chapters that cover this topic.The Model Law provides a relati"
325,Regulatory Institute,Transcription,"FILENAME: RI comment AI Act.e81a0d5803929.pdf | CONTENT: Regulatory Institute general comments about the AI Act:We understand that the AI Act’s definition of AI (Article 3) and classification of high-risk AI systems (Article 6) is settled at this point but the Institute believes both (AI definition and high-risk classification) will be more cumbersome to administer and cause some AI systems to not be regulated. Noting the specific objectives of the AI Act, include: ● ensure that AI systems placed on the Union market and used are safe and respect existing law on fundamental rights and Union values; ● ensure legal certainty to facilitate investment and innovation in AI; ● enhance governance and effective enforcement of existing law on fundamental rights and safety requirements applicable to AI systems; and ● facilitate the development of a single market for lawful, safe and trustworthy AI applications and prevent market fragmentation.We urge additional consideration for the approach take"
325,Regulatory Institute,General comment,See our answers in the uploaded documents
326,Brian Bai,Transcription,"FILENAME: Feedback and suggestions from Brian Bai.c318e767e3d8a.pdf | CONTENT: RMIT Classification: TrustedSafe and responsible AI in Australia ――Suggestions from Dr Yuntian Brian BaiI have a hopeful vision for AI technology, envisioning it as a catalyst for greater assistance and support to Australian people. In light of this, I offer my personal perspectives and views, which are provided below in response to each question. Definitions 1. Do you agree with the definitions in this discussion paper? If not, what definitions do you prefer and why?I agree with the general AI definitions. However, I would suggest listing a few more typical AI applications/products such as a) Intelligent robots, b) Autonomous vehicles, and c) Medical diagnosis.These will give people a more comprehensive understanding of AI concepts. Potential gaps in approaches 2. What potential risks from AI are not covered by Australia’s existing regulatory approaches? Do you have suggestions for possible regulatory actio"
326,Brian Bai,Q01 Definitions,"I agree with the general AI definitions. However, I would suggest listing a few more typical AI applications/products such as： a) Intelligent robots, b) Autonomous vehicles, and c) Medical diagnosis. These will give people a more comprehensive understanding of AI concepts."
326,Brian Bai,Q02 Other AI risks and recommendations for regulation,"We should pay more attention to: 1) AI products' autonomous attacks on humans, such as physical assaults, virus attacks, or attacks arising from malicious instructions. 2) AI-generated deepfake. 3) Job Displacement. 4) Deepfake contents. Around 23 billion pieces of information are produced every day. Eighty percent of the content may be synthesized, exaggerated or deep-faked, overwhelming for anyone to consume, and challenging to distinguish. I strongly suggest building and refining a top-level AI guideline (or called complete Australian AI profile and representation). It is very important to establish specific top-level AI guidelines in Australia. This entails defining the desired characteristics and qualities that align with intended purpose and Australian values. One good choice is adopting the widely acknowledged values and principles advocated by the United Nations (UN) and other countries as the benchmark for AI’s applications in Australia. Other elements include, but are not lim"
326,Brian Bai,Q04 Coordinating AI governance across government,"• Develop a comprehensive national AI strategy that sets clear objectives and principles for AI governance across government agencies as an extension of the top-level AI guidelines as descripted in Question 2. • Create a unified policy framework that addresses AI ethics, data privacy, transparency, and accountability and ensure consistent adherence to ethical AI principles in all government AI initiatives. • Conduct periodic audits and assessments to ensure compliance with policies and ethical standards and help identify areas for improvement and mitigate risks."
326,Brian Bai,Q05 International models for AI governance,"Collaborating with other countries and learning from global experiences will be valuable in crafting a comprehensive and effective AI governance framework across areas such as data protection and privacy laws, algorithmic impact assessments and AI regulation and standards."
326,Brian Bai,Q07 Responsible government use of AI,"Develop comprehensive guidelines and policies specific to AI adoption within government agencies. Also, invest in AI education and training programs for government employees. Encourage inter-agency collaboration and knowledge sharing on AI best practices and improved decision-making and resource optimisation. Implement independent monitoring and auditing processes to evaluate the impact of AI applications."
326,Brian Bai,Q08 Generic vs technology-specific approaches for AI risk,"Generic solutions are valuable for common risks across various AI applications such as ethical AI guidelines (e.g., fairness, transparency, and privacy protection), data governance and regulatory frameworks to different industries or agencies. Technology-specific solutions are more effective in tackling unique challenges associated with specific AI technologies or domains. For example, bias mitigation and reliability."
326,Brian Bai,Q09 Transparency requirements and scope,"a. where and when transparency will be most critical and valuable to mitigate potential AI risks and to improve public trust and confidence in AI?Let’s look at the following conversations: A: You know what? Yesterday, while I was driving, I suddenly received a message saying that the road ahead was congested, and I was advised to take an alternate route. B: That's how AI helps us! A: AI is so smart! But if we all switched to the other road, wouldn't it also get congested immediately? B: AI doesn't inform everyone, so only some of us go to the other road, while the rest stick to the original route. A: But how does AI decide who goes to the other road and who stays? B: AI makes that decision for you; it just doesn't explicitly reveal it. A: Doesn't that mean AI takes away our freedom to make our own choices? It’s unfair! This example demonstrates how AI’s transparency is important and valuable in mitigating potential AI risks and to improving public trust and confidence. Did AI help us? "
326,Brian Bai,Q10 Ban on high-risk AI applications,"a. Whether any high-risk AI applications or technologies should be banned completely?No. Only the unacceptable risks should be banned completely. High-risk AI applications should be allowed but with clear and strict regulation and monitoring process.b. Criteria or requirements to identify AI applications or technologies that should be banned, and in which contexts?Top-level regulation and standards, like a country's constitution, are crucial for AI development and applications. The kind of AI application or technology can only be banned if it clearly violates the top-level standards. Then, what is top-level regulation and standards and how to build them, see Question 4."
326,Brian Bai,Q11 Actions to increase public trust in AI,1） Well-designed top-level regulation and standards (see Questions 2 & 4). 2） Dissemination and training in AI-related knowledge and technologies. 3） Embracing both internal and external monitoring.
326,Brian Bai,Q14 Risk-based approach to address AI risks,"Yes. I support to start the implementation from small scale testing and it has to be well evaluated, fully tested to minimise the cost."
326,Brian Bai,Q15 Benefits and limitations of risk-based approach,"Benefits: more capable to address critical areas where adverse consequences and mitigate more significant risks effectively. Limitations: as different people may perceive risks differently, striking the right balance between precaution and innovation can be complex."
326,Brian Bai,Q19 Application of risk-based approach to general purpose AI,"I will focus on one risk-based approach to LLMs in this discussion. Many examples emphasize the importance of conducting a comprehensive risk assessment to identify potential challenges related to LLMs. Such an assessment should consider factors like data biases, misinformation propagation, misuse by malicious actors, and ethical concerns in content generation.Recently, while researching minimum social welfare for the elderly in a rural area, I observed a significant disparity between the information provided by ChatGPT and the actual situation. The content seemed to reflect a propagandistic tone in line with government agencies rather than accurately portraying reality.Similarly, when searching for information about the Aboriginal population in Melbourne using ChatGPT, I noticed that the data was neither stable nor accurate. This issue may be attributed to inadequate and incomplete training data.In response to these examples, it is crucial to adopt timely risk management approaches. F"
326,Brian Bai,Q20 Voluntary vs mandated compliance with risk-based approach,All these uncertainties can be examined and determined by the top-level AI guidelines.
327,Institution of Chemical Engineers (IChemE),Transcription,"FILENAME: 2023 IChemE Responsible AI Industry Consultation26072023.d47998949db6c.pdf | CONTENT: 2023 consultation – Supporting Responsible AI: discussion paper Consultation response from the Institution of Chemical Engineers (IChemE) The Institution of Chemical Engineers (IChemE) is pleased to make this submission on Safe and Responsible AI in Australia discussion paper consultation. Artificial Intelligence (AI) has made significant advancements in the last more than one decade, however along with the promise of its capabilities and opportunities, there are serious concerns regarding its risks and the lack of tighter regulation and control. This consultation raises several questions in the context of AI and ADM (Automated Decision Making) and explores measures which can increase public trust and confidence in the development of this technology. These questions have been clustered together under separate headings in the discussion paper and the responses have been prepared by the member"
327,Institution of Chemical Engineers (IChemE),General comment,The Institution of Chemical Engineers (IChemE) is pleased to make this submission on Safe and Responsible AI in Australia discussion paper consultation and would welcome the opportunity to provide more detailed information if required.
327,Institution of Chemical Engineers (IChemE),Q01 Definitions,"IChemE agrees with the definitions proposed in the discussion paper. Further definitions which could also feature in the discussion paper are as follows (Deloitte 2017): Cognitive Analytics: A subfield of AI which deals with the cognitive behaviour associated with ‘thinking’ as apposed to perception and motor control. Robotics: A machine that is programmed to perform tasks by following step-by-step instructions Smart Machines: Systems which can make and execute decisions (to some extent) by themselves, requiring no human input."
327,Institution of Chemical Engineers (IChemE),Q02 Other AI risks and recommendations for regulation,"The discussion paper broadly covers the risks associated with AI and the different regulatory or non-regulatory approaches which could be deployed to mitigate these risks. The paper further outlines governance measures (including regulatory and voluntary mechanisms) that aim to address these risks and raises questions regarding the gaps in the existing governance landscape. AI application risks within the process industry include lack of clarity around the role and responsibilities of the duty holder, skills gap, and reliability of the data used for AI technology development (HSE 2023), which may impact the reliability of the technology itself, leading to increased risk. Voluntary mechanisms can’t be required, all use of AI should be able to prove governance around risk. Chemical engineers offer significant experience in the key requirements outlined within this discussion paper. Chemical Engineers have strong expertise in risk-based assessments, particularly in the areas of high-risk "
327,Institution of Chemical Engineers (IChemE),Q03 Non-regulatory actions to address AI risks,"In addition to the regulatory initiatives discussed in this paper, the Australian government could also provide clear communication and guidance / training to the industry to ensure that the relevant laws applicable to the AI are clearly understood by the business community, i.e., the context in which they apply and how to comply with these laws. Specific examples may include privacy and data security laws, intellectual property infringement and its boundaries, transparency and limits of ADM systems decision making, processes to identify and address biased and discriminatory outcomes, and consumer laws including developers’ product liability obligations (DLA Piper 2023). Providing guidance on the responsibilities of duty holders within organisations using AI tools would also be beneficial. It may also be necessary to include boundaries to ensure against unacceptable use of technologies."
327,Institution of Chemical Engineers (IChemE),Q04 Coordinating AI governance across government,No comment
327,Institution of Chemical Engineers (IChemE),Q05 International models for AI governance,Kindly refer to the consultation response document
327,Institution of Chemical Engineers (IChemE),Q06 Public vs Private sector governance,No Comment
327,Institution of Chemical Engineers (IChemE),Q07 Responsible government use of AI,No Comment
327,Institution of Chemical Engineers (IChemE),Q08 Generic vs technology-specific approaches for AI risk,No Comment
327,Institution of Chemical Engineers (IChemE),Q09 Transparency requirements and scope,No Comment
327,Institution of Chemical Engineers (IChemE),Q10 Ban on high-risk AI applications,No Comment
327,Institution of Chemical Engineers (IChemE),Q11 Actions to increase public trust in AI,No Comment
327,Institution of Chemical Engineers (IChemE),Q12 Trade impact of banning high-risk AI applications,No Comment
327,Institution of Chemical Engineers (IChemE),Q13 Changes to conformity infrastructure,No Comment
327,Institution of Chemical Engineers (IChemE),Q14 Risk-based approach to address AI risks,Kindly refer to the consultation response document
327,Institution of Chemical Engineers (IChemE),Q15 Benefits and limitations of risk-based approach,"The different contexts in which AI can be applied requires context specific regulatory responses. A research study on AI regulation in the UK highlights that the risks associated with AI systems and their impacts is dependent upon the AI technique and application area (Roberts et al 2023). A recent policy paper by the UK government highlights that the AI foundation models can be applied in many ways and the risks vary accordingly, for example using Chatbot to summarise a long article or taking medical advice pose different risks (Gov.UK 2023). The industry is already seeing benefits from the use of AI , for example, Safeswim which is an award-winning public health risk system produced for the Auckland Council, whereby real-time advice is provided on the levels of risk associated with swimming at specific locations (Auckland Regional Council, 2023), and a joint partnership between a chemicals company BASF and an AI company IntelliSense.io for the deployment of an open, real-time, decisi"
327,Institution of Chemical Engineers (IChemE),Q16 Scope and application of risk-based approach,It is noted that some sectors will likely lack experience and may be resistant to regulation. Providing necessary resources and training will be important as outlined above. This is again a key driver for regulation.
327,Institution of Chemical Engineers (IChemE),Q17 Elements to include in risk-based approach,"The focus of questions 14-17 is ‘Risk Based Approach’ and therefore these questions have been addressed as a common theme. (Kindly refer to the consultation response document) IChemE is strongly supportive of risk-based approaches to any regulation effecting the development or use of AI and related tools. Risk-based approaches are expected to consider not only every conceivable consequence, but specifically to imagine and attempt to prevent or mitigate the worst possible consequences of the outputs from AI tools to ultimately evaluate their acceptability within our members’ industries and the wider community. This is not dissimilar to the existing methodologies developed within the process industries such as LOPA (Layers of protection analysis) and QRA (Quantitative risk analysis) which are relied upon internationally and can be applied to meet existing risk management standards with possible relevance to AI legislation, e.g. ISO/IEC 23894. These methodologies are applied irrespective "
327,Institution of Chemical Engineers (IChemE),Q18 Integration of risk-based approach into existing frameworks,No comment
327,Institution of Chemical Engineers (IChemE),Q19 Application of risk-based approach to general purpose AI,No comment
327,Institution of Chemical Engineers (IChemE),Q20 Voluntary vs mandated compliance with risk-based approach,Kindly refer to the consultation document
328,Anonymous,Transcription,FILENAME: Submission 328 - Attachment.2a6e2a5f3886b.pdf | CONTENT: What should Australia do? Building and strengthening safe and responsible AI in AustraliaWhat should Australia Do? Building and strengthening safe and responsible AI in AustraliaTable of Contents GENERAL COMMENTS............................................................................................................................................ 1 DUAL USE TECHNOLOGY........................................................................................................................................... 3 ORGANISATIONS .................................................................................................................................................... 4 THE NEED FOR DIVERSE EXPERTISE ............................................................................................................................... 4 SHOULD AUSTRALIA INVEST IN A NEW AI REGULATOR OR BUILD AI REGULATORY CAPACITY WITHIN EXISTI
328,Anonymous,Q01 Definitions,I'm going to let others tackle the definitions
328,Anonymous,Q02 Other AI risks and recommendations for regulation,Dual use technology risks. Cascading Risks (see uploaded document for thoughts)
328,Anonymous,Q03 Non-regulatory actions to address AI risks,Mandatory adoption of AI Ethics Framework for the Australian Public Service. Public education campaign to empower citizens.
328,Anonymous,Q04 Coordinating AI governance across government,See uploaded document
328,Anonymous,Q05 International models for AI governance,not discussed
328,Anonymous,Q06 Public vs Private sector governance,The public sector should uphold a higher standard by complying with the AI Ethics Framework
328,Anonymous,Q07 Responsible government use of AI,Education of public service on Australia's AI Ethics Framework and how to apply it in their jobs using AI.
328,Anonymous,Q08 Generic vs technology-specific approaches for AI risk,Usually you need both.
328,Anonymous,Q09 Transparency requirements and scope,See attached file
328,Anonymous,Q10 Ban on high-risk AI applications,n/a
328,Anonymous,Q11 Actions to increase public trust in AI,Do people need more encouragement to use AI? People are already using AI every day all the time.
328,Anonymous,Q12 Trade impact of banning high-risk AI applications,Australia needs to invest in sovereign AI capabilities that comply with our AI Ethics Framework and can be exported in the first instance and avoid controversial ones in lieu of a ban
328,Anonymous,Q13 Changes to conformity infrastructure,See attached document
328,Anonymous,Q14 Risk-based approach to address AI risks,See attached document
328,Anonymous,Q15 Benefits and limitations of risk-based approach,See attached document
328,Anonymous,Q16 Scope and application of risk-based approach,See attached document
329,Nathan Sherburn,Q02 Other AI risks and recommendations for regulation,"It is vital that in addition to the risks outlined in the paper, we also consider the catastrophic and existential risks AI poses to us all. We have an opportunity in Australia to lead AI safety the way we lead the world in other safety standards like aviation safety. It would be a huge loss to let this opportunity pass us by. The Australian Human Rights Commission, draws this same analogy to aviation safety [1].Regulations we should consider include: 1. Certain standards of interpretability 2. Enforcement of liability for harms caused[1] https://tech.humanrights.gov.au/sites/default/files/2021-05/AHRC_RightsTech_2021_Final_Report.pdf"
329,Nathan Sherburn,Q03 Non-regulatory actions to address AI risks,The key non-regulatory steps required to mitigate these risks are: 1. Acknowledging the risks 2. Funding AI Safety research in Australian universities and building our capacity to ensure AI is developed and deployed safely 3. Playing whatever role we can on the world stage to protect Australians and the world from the most severe risks (e.g. backing other governments calling for policies that emphasise longterm safety over short term gains).
329,Nathan Sherburn,Q05 International models for AI governance,Yes - The Sentinel lab proposed for the UK [1] and Singapore's AI Verify Foundation provide good models for the types of capacity we should be investing in for Australia.[1] https://www.institute.global/insights/politics-and-governance/new-national-purpose-ai-promises-world-leading-future-of-britain [2] https://aiverifyfoundation.sg/
329,Nathan Sherburn,Q09 Transparency requirements and scope,"While there has been some attention on ensuring that consumers are notified when a decision made by AI affects them directly (e.g. hiring/firing, loan approval), it is only the first step in protecting the public.Having models which are both accurate and explainable has long been an issue for businesses [1], but the recent advancements of “black-box models” is encouraging developers and providers to simply tolerate the risks that come from lack of interpretability. As AI capabilities continue to outstrip their interpretability [2], the risks of this approach will grow significantly. The fact that interpretability is becoming harder is a reason to mandate it, not a reason to ignore it.Uninterpretable models can represent risks even in seemingly “benign” situations. Imagine a customer service bot is used to handle technical issues from customers. The bot was trained to optimise for the calls to be dealt with promptly, and in a way that reduces future complaints from the inquiring individ"
329,Nathan Sherburn,Q10 Ban on high-risk AI applications,"In February 2023, OpenAI CEO Sam Altman released a statement containing the following:“As our systems get closer to AGI, we are becoming increasingly cautious with the creation and deployment of our models. Our decisions will require much more caution than society usually applies to new technologies, and more caution than many users would like.” [1]It is very reassuring to see this acknowledgement expressed by Altman, and the Australian Government would be wise to heed such warnings. Unfortunately, it may not matter how much caution OpenAI shows, because their actions have triggered the arms race that AI safety experts have been concerned about for years [2, 3].In order to protect our nation, Australia should make an effort to impose a ban on the training of large-scale AI systems with a nontrivial expected chance of passing capabilities evaluations required to cause catastrophic risks to civilization (such as the Alignment Research Center’s tests for the capabilities required for auto"
329,Nathan Sherburn,Q14 Risk-based approach to address AI risks,Yes - I think we need to take a portfolio approach to AI risk. There are a number of clear present ethics issues that need addressing and there is also a foreseeable chance of catastrophic and existential threats that deserve some attention too.
329,Nathan Sherburn,Q16 Scope and application of risk-based approach,"The concept of “AI” includes a broad range of capabilities and capacities. Government is right to work to distinguish between different use cases - including with a specific focus on AI maturity.Concerningly, this broad range of AI capabilities and capacities might be used by industry to attempt to bamboozle government. Specifically, industry is likely to argue that AI does not need regulation with reference to basic AI technology, with a view to that principle flowing through to more advanced and future technologies. This is akin to arguing that a horse and wagon doesn’t need strict regulation, then later trying to claim that a tank is just a more capable horse and wagon with some components swapped out.Government needs both a fine-grained approach to defining AI technology and a fine-grained approach to understanding AI risk to ensure it doesn’t fall for these tricks.In terms of risks, research shows that two of the main sources of risk are deception and power-seeking behaviour: - De"
329,Nathan Sherburn,Q19 Application of risk-based approach to general purpose AI,"The risk-based approach must be modified to address risks from general purpose AI systems, especially LLMs and MFMs. These types of systems learn their capabilities and behaviours during training, which can be unforeseen by developers. That means that AI developers could create models with dangerous capabilities (e.g., deception, manipulation, political strategy, weapons design). Humans could misuse those capabilities to cause harm [1], and models could apply the capabilities even without deliberate misuse (e.g., through misalignment) [2].Consistent with recent work from a consortium of AI safety researchers, including at Google Deepmind, OpenAI, Anthropic, Alignment Research Center, University of Cambridge, University of Oxford, Centre for Long-Term Resilience, and Centre for the Governance of AI [1], we recommend that any risk-based approach require that general purpose models are evaluated for dangerous capabilities and misalignment. This will help determine: 1. To what extent a mod"
329,Nathan Sherburn,Q20 Voluntary vs mandated compliance with risk-based approach,"In general, regulation should:1. Be proportionate to risk. Meaning that a voluntary or self-regulation approach should only apply to low-risk or no-risk uses of AI.2. In the case of higher-risk technologies, regulate both the technology itself and the use of the technology. Meaning that, for future and more sophisticated AIs, regulation should ensure that the technology is safe before it is created or published, as well as ensuring that it is only used in ways that are safe.3. Place burdens on those most able to reduce potential harms. Meaning that AI that functionally operates as ‘black box’ to consumers has to be regulated at the point of the AI Lab.Applying those three principles, self-regulation may be appropriate for some technologies available today and for some participants (such as students or researchers), but a forward-looking regulatory regime must apply broadly and must be backed by Australian diplomatic efforts to ensure it is globally coordinated."
330,Jack Larkings,Q01 Definitions,
330,Jack Larkings,Q02 Other AI risks and recommendations for regulation,"• There are potential risks for the misuse of AI across society that have the potential to undermine the proper functioning of government and business, and create public safety risks and undermine public confidence. • There is the potential for the misuse of AI in medicine, for example, creating medical misinformation including psychiatric advice. • AI generated content, including text, images, video and sound (such as voices) has the potential to create false and misleading content, deceive the public, be used for criminal and unethical purposes. This has the potential to undermine public trust in AI and existing institutions. Furthermore it has the potential to be damaging to commerce and create public health and safety risks. As such, it is important AI generated content is clearly identifiable (e.g. labelled) to the public. Furthermore, it is important that AI generated content is able to be forensically identified when misused for criminal purposes. • Social media recommendation e"
330,Jack Larkings,Q03 Non-regulatory actions to address AI risks,• Medium risk AI models should be tested and independently evaluated prior to deployment to ensure they safe and meet ethical standards in line with community expectations. Independent scientists and researchers should be provided access to test AI models and evaluating them before and after they are deployed.
330,Jack Larkings,Q04 Coordinating AI governance across government,• There should be a Commonwealth Government agency responsible for targeting the specific risks from AI and developing a coordinated response across relevant government agencies to ensure the safe and responsible use of AI and to provide a solid legislative framework to control risks resulting from AI. It should be responsible for ensuring; 1. The agency licenses AI activities above a certain scale of capabilities and can take that license away and ensure compliancy with AI safety standards. 2. There are set of AI safety standards and evaluation of dangerous AI capabilities e.g. can a model self-replicate and self-exfiltrate into the wild. AI models should be tested against safety requirements and independently verified to ensure the models pass before being deployed. 3. Independent audits by experts that can verify if the model meets prescribed safety thresholds. • The Commonwealth needs to nationally lead the response to the development of AI to coordinate a nationally consistent res
330,Jack Larkings,Q05 International models for AI governance,"• Due to the international nature of AI development, international agreements may be needed to share overseas evaluations of AI systems prior to deployment in Australia. Furthermore existing international agreements related to transnational crimes need to be extended to include the misuses of AI technologies and facilitate international cooperation on AI related crime."
330,Jack Larkings,Q09 Transparency requirements and scope,"• The use of AI to create misinformation should be treated as a relatively high risk due to the potential for it to be exploited by extremists, criminals and undermine public trust in our institutions. Misinformation has the potential to undermine a wide range of institutions ranging from commerce, finance, education, the media, elections, public health, et cetera, which are built on trust and integrity. • The use of AI to create false identities (i.e. counterfeit people) should be regarded as a high risk due to its potential to undermine democratic government in Australia, undermine commerce and be used to support criminal activity (e.g. money laundering)."
330,Jack Larkings,Q10 Ban on high-risk AI applications,"• Consideration needs to be given to including an international ban or restrictions on the proliferation of the use of AI technologies in warfare that remove the kill decision from humans. Furthermore, consideration needs to be given to international agreements preventing the development of independent AI initiated operation of weapons to prevent or limit their deployment outside warfare, such is for policing or crime."
330,Jack Larkings,Q11 Actions to increase public trust in AI,"Consideration needs to be given to educating the public of the dangers from the misuse of AI, particularly for criminal purposes and how the public may be able to protect themselves from misleading and dangerous content. In addition, the public and society as a whole will have an important role in working with industry and government to identify problems and ensure AI is used in a legal and ethical manner consistent with public values."
331,Justin Olive,Transcription,"FILENAME: AI Safety Melbourne Submission FINAL.52974677d2a43.pdf | CONTENT: AI Safety Melbourne: Response to the Safe and Responsible AI Discussion Paper Part 1: Introduction to AI Safety and our work 3 Personal Foreword from Justin Olive, AISM Coordinator: 3 Opening comments on the discussion paper: 4 1.2 Definitions: 4 2.1 Opportunities: 4 2.2 Challenges: 4 Part 2: The AI Landscape: July 2023 Update 5 2.1 Introduction to AI systems 5 2.2 Busting myths about AI 6 Table 1: Viewpoints of regular AI researchers compared to those with a track record of researching AGI or making significant contributions towards it. 7 2.3 Developing AI Systems: 8 2.4 Near-term future of AI 9 2.5 Defining AI Systems: 9 2.6 Access to AI Systems 10 2.7 Reasoning about AI as a structural force 11 2.8 Introduction to AI risks 13 2.9 New risks 13 2.10 Amplifications of existing risks 17 2.11: Technical Challenges for AI Regulation 18 Part 3: Recommendations 21 Recommendation 1: 21 Create distinct strategies for "
331,Justin Olive,Q01 Definitions,"The definitions are underdeveloped and do not reflect the reality of the AI landscape: they miss out on two key factors that determine how difficult or impactful it will be to regulate any given AI system: autonomous capabilities and accessibility.For this reason, I propose a set of 4 key classifications to be used in regulatory guidelines.1. Advanced AI Systems: These are autonomous AI systems that approach or exceed human capabilities across a broad range of activities. They can be defined more specifically as having the following capabilities: A. Are able to complete a diverse range of reasoning tasks with human-level performance; and B. Are able to navigate complex information environments and act within those environments autonomously; and C. Are able to form sophisticated plans and reason about the consequences of actions2. Precursor systems: These are models or AI systems which: A. Can be used to construct an advanced system within a set resource budget (e.g. <$50,000 of labour "
331,Justin Olive,Q02 Other AI risks and recommendations for regulation,"The risks outlined in the discussion paper were adequate for narrow AI. However, general-purpose AI is the most impactful form of AI. It should be clear from the following quotes that the Australian Government needs to adjust its approach:Yann LeCunn, Facebook Chief AI Scientist and Turing Award Winner, suggests we will have human-level AGI in 10-15 years, and has proposed a detailed roadmap for arriving there.Geoffrey Hinton, a pioneer in deep learning, says that superhuman AI will probably arise in the next 5-20 years, and that it poses catastrophic risks.Marcus Hutter, a DeepMind Senior Scientist researching AGI, writes: “... deploying a sufficiently advanced reinforcement learning agent would likely be incompatible with the continued survival of humanity.”Jürgen Schmidhuber is a deep-learning pioneer and inventor of the LSTM network. He says that the intense competition within ecosystems of advanced AI will be “beyond our imagination”, and that it’s inevitable we will be outcompete"
331,Justin Olive,Q03 Non-regulatory actions to address AI risks,"Yes. The Government must invest in AI safety research. For example, helping to scale up the Developmental Interpretability work being conducted by the Melbourne Deep Learning group. Industry has chronically under-invested in AI safety research, and experts are saying that we may not be able to trust AI systems unless we drastically improve our ability to understand and control them.By funding research that is ambitious, ahead of the curve, and focused on transformative approaches, rather than incremental, near-term, or near-to-market innovations, Australia could become an enduring global leader on key aspects of AI safety.This would include begin by addressing the lack of local AI talent for researching interpretability and control of autonomous systems. These skills will be required to fill future demand for AI system auditors required to ensure safety and transparency.This could be achieved via actions such as: - Grants program for Universities to launch Deep Neural Network Interpret"
331,Justin Olive,Q04 Coordinating AI governance across government,"There must be a central expert body that is capable of assessing and understanding risks for a wide range of AI-based tools. This central body can then determine how to regulate AI systems to capture benefits and mitigate harms.For example, autonomous systems with general decision-making capabilities (Advanced AI systems) should be tightly controlled due to their ability to have complex and uncertain effects on society (i.e. structural risks that may grow beyond our control). These should be handled directly by the AI CommissionIndustry specific AI applications, such as Narrow AI systems that are fine-tuned to perform routine tasks in education, healthcare or financial services, should be regulated by their relevant Government Departments (for example. the , Dep. Education, Dep. Health and Aged Care, and Australian Securities and Investment Commission, respectively).The technical aspects of these industry-specific regulations should be informed by the AI Commission, but they would be p"
331,Justin Olive,Q05 International models for AI governance,"Australia must start developing centralised capabilities to audit and research AI capabilities. This approach is already starting to be reflected overseas, with an example being the EU’s Centre for Algorithmic Transparency, which was formed to enforce the Digital Services Act. The new research centre aims to combat the “systemic risk” to society that stems from algorithms - including recommender systems and AI.To prepare for a future in which AI-based decision systems might become a core part of society, Australia should adopt a broad version of this concept to understand and shape the implications of algorithms - especially AI algorithms that use deep learning.Deep learning is important to understand for a number of reasons; it is the underlying architecture for powerful foundation models which can process text, audio and images, and can also be used to develop autonomous agents capable of acting in complex environments, with concerning capabilities such as persuasion and strategic re"
331,Justin Olive,Q06 Public vs Private sector governance,"There should be a focus on ensuring that Industry is able to use Narrow AI to increase productivity and improve healthcare and education; these should be assisted with regulatory sandboxes and investment.For advanced AI (I.e. autonomous, human-level capabilties that will likely emerge in the next 5-10 years), the applications should be extremely targeted, with significant Government oversight. Uses of advanced AI should primarily be directed towards activities that the Government has identified as being in the national interest, and should entail close collaboration between Governments and private industry.For example, public-private innovation partnerships for advancing science and technology, or implementing economic reforms, would be examples of good targets for advanced AI. Advanced AI is not appropriate for general use in industry."
331,Justin Olive,Q07 Responsible government use of AI,"As AI becomes more central to how Australia functions as a nation, the skills and capabilities of the Australian public service should be shaped to align with this transformation. As technical systems continue to become more sophisticated and prevalent throughout society, a deep understanding of them will be essential across all policy domains.Unfortunately, the current structure of the APS is not effective for hiring or retaining technical talent [1]. For example, the average base pay for a machine learning engineer [2] is typically above the EL1 threshold [3], and attracting and retaining an experienced expert in cutting-edge AI systems would likely require far greater compensation.To ensure that Australia’s policy decisions are equipped to capture benefits and mitigate risks in the 21st century, there should be specific initiatives for attracting elite global AI talent into senior positions across Government.The emphasis on attracting global talent is important because Australia its"
331,Justin Olive,Q08 Generic vs technology-specific approaches for AI risk,"Generic measures such as transparency requirements and legal liability frameworks will only be adequate for mitigating risks in simple decision systems which perform single-step information processing and pattern matching, or routine tasks in limited settings [1]; examples of simple systems may include automated robotic systems in manufacturing, or CV screening systems in HR.By contrast, there will need to be specific measures in place to ensure that sophisticated automated decision systems are safe and reliable. “Advanced” systems refers to AI systems which: 1. Are able to complete a diverse range of reasoning tasks with human-level performance; and 2. Are able to navigate complex information environments and act within those environments autonomously; and 3. Are able to form sophisticated plans and reason about the consequences of actionsDue to their autonomous and open-ended capabilities, advanced systems carry complex risks which are more similar to those associated with humans; fo"
331,Justin Olive,Q09 Transparency requirements and scope,"To remain competitive, professionals and businesses will increasingly feel the pressure to leverage sophisticated “black-box” AI models. This short-term thinking has already resulted in numerous accounts of public backlash and distrust, including high-profile warnings from technology leaders themselves [1].For example, Apple’s credit card business came under fire for offering a woman one twentieth the credit limit offered to her husband. When she complained, an Apple representative simply told her, “I don’t know why, but I swear we’re not discriminating. It’s just the algorithm.” [2]More recently, two lawyers in the United States blamed ChatGPT for “tricking” them into including fictitious legal research in a court filing [3].These problems could easily have been prevented by using explainable AI systems which can show the rationale for their outputs.Such incidents are evidence that the regulatory environment is not incentivising businesses or professionals to use transparent decision-"
331,Justin Olive,Q10 Ban on high-risk AI applications,"The continued evolution of AI brings several societal challenges that, left unchecked, could risk the security, privacy and freedom of the Australian public. Many of the most extreme harms can be attributed to AI systems that have the ability to: 1. Complete a diverse range of reasoning tasks with human-level performance; 2. Navigate complex information environments and act within those environments autonomously; 3. Form sophisticated plans and reason about the consequences of actionsAlthough AI systems with these capabilities may seem like science fiction, they are already being tested with great success in contained environments.For example, “Vogager” is a system that was recently developed by chaining together versions of GPT and connecting them to an external database, which the chatbots use to store and retrieve code that executes complex behaviours. Despite being a simple system, Voyager was hugely successful when tested in the open-world game Minecraft; this may appear trivial, "
331,Justin Olive,Q11 Actions to increase public trust in AI,"Public distrust in AI is fuelled by the growing sense that powerful AI systems are being developed and deployed by foreign actors, some of whom have demonstrated unsafe attitudes towards deploying AI systems [1]. The interests of Australians and Australia are notis not part of the conversation. Although Australia may struggle to catch up to the US or UK in advancing the frontier of AI, this isn’t necessary for improving public trust and increasing adoption.Rather, Australia has a clear opportunity to become a leader in AI safety; this enviable position would not only assist safe adoption locally by providing much needed expertise in safe systems, it could help foster a sense of pride and ownership by opening up export opportunities in the emerging market for AI safety services [2]. AI Safety is how Australia could get its ‘seat at the table’ in the global AI conversation.Pursuing this position would involve the following: - Coordinating with Australian AI safety organisations to better"
331,Justin Olive,Q12 Trade impact of banning high-risk AI applications,"Advanced AI systems pose unprecedented risks that require a fundamental shift in perspective for regulators. This is best illustrated by the following quotes from leading AI experts:Yann LeCunn, Facebook Chief AI Scientist and Turing Award Winner, suggests we will have human-level AGI in 10-15 years, and has proposed a detailed roadmap for arriving there.Geoffrey Hinton, a pioneer in deep learning, says that superhuman AI will probably arise in the next 5-20 years, and that it poses catastrophic risks.Marcus Hutter, a DeepMind Senior Scientist researching AGI, writes: “... deploying a sufficiently advanced reinforcement learning agent would likely be incompatible with the continued survival of humanity.”Jürgen Schmidhuber is a deep-learning pioneer and inventor of the LSTM network. He says that the intense competition within ecosystems of advanced AI will be “beyond our imagination”, and that it’s inevitable we will be outcompeted and replaced.Demis Hassabis, CEO of DeepMind, says huma"
331,Justin Olive,Q13 Changes to conformity infrastructure,"World-renowned AI experts such as Yoshua Bengio are warning that there’s a chance “superhuman” AI might be coming as early as 2030 [1]. Although there stand to be significant benefits from deploying advanced AI systems, their deployment will need to be highly targeted and controlled.While the productivity gains from advanced AI systems could be immense, uncontrolled proliferation would pose threats from likely scenarios such as malicious use or rogue AI systems [2]. As such, any deployment of advanced AI systems in Australia should occur via secure data centres, with access provided through a centralised API that facilitates monitoring and accountability by providing metadata about usage.Stringent compliance requirements must also be implemented for any organisation granted access to advanced AI systems. AI Governance researchers recommend that regulators should develop audit trail requirements in safety-critical applications of AI, similar to flight data recorders that are standard in"
331,Justin Olive,Q14 Risk-based approach to address AI risks,"A risk-based approach is essential, but it is necessary to understand that as time moves forward, risks will come more from the level of access of an AI system and its capabilities, rather than particular scenarios and use-cases that may apply to narrow AI.For example, ""chat bots"" aren't classified as high-risk in the Discussion Paper, but at what point would this change? If the chat bot can clearly explain how to coordinate and carry out a chemical weapons attack in a way that a 12 year old can understand, does that then make it ""high risk""?Unfortunately, this aligns with the reality of where AI is heading. General purpose AI isn't a tool, it's a powerful structural force in society and the economy. The risks must be treated as extremely serious, with increasing capabilities equating to higher risk.Another key to success to taking measures to actively mitigate the risks. Compliance and assurance can only do so much; we also need to invest in AI safety research so that we can control A"
331,Justin Olive,Q15 Benefits and limitations of risk-based approach,"A risk-based approach is vital for mitigating the catastrophic consequences that AI safety researchers are warning about, while enabling Australia to capture the benefits of being able to use AI in ways that are safe and reliable.A well-calibrated risk-based approach will allow Narrow AI to flourish and provide the promised economic benefits for education and healthcare, while advanced forms of general-purpose, autonomous AI are reserved for highly targeted projects and programs that require Government approval and are subject to audits and compliance programs.A potential “worse-case scenario” is if a culture of “trial and error” is adopted, whereby adverse events occur before risks are considered tangible enough to act upon. This reliance on post-hoc interventions risks normalising unsafe development practices, making it difficult to recover once risks proliferate.Proliferation of risks is a key point in governing AI, because regulating software poses significant challenges compared t"
331,Justin Olive,Q16 Scope and application of risk-based approach,"The technology sector will need to be at the centre of Australia's risk-based approach. This is because they are chiefly responsible for developing general-purpose systems, and in particular advanced AI systems (which will be the main source of risk; if the reader is familiar with the Pareto principle (80/20 rule), that is a good explanation of why this makes sense - a majority of the risks will come from the most risky systems, which will be developed by a handful of organisations).More specifically, there are three types of technology organisations to focus on: 1. Organisations that develop advanced AI systems, or AI systems that can be used to build more advanced AI systems (i.e. precursor systems such as LLMs). Examples include OpenAI, DeepMind, Anthropic, Inflection AI, Nvidia, Microsoft, Meta 2. Organisations which provide access to open-source AI systems via downloads or APIs. E.g. Hugging Face, Github, AWS 3. Organisations which provide compute resources to develop, fine-tune o"
331,Justin Olive,Q17 Elements to include in risk-based approach,"Human-in-the-loop monitoring will be vital, but insufficient on its own to regulate advanced AI systems that will eventually surpass human capabilities. The speed at which advanced AI systems will operate means that human oversight will become pragmatically infeasible if our monitoring programs aren’t able to scale proportionally.As such, we must employ an approach that is predicated on limiting access to advanced AI systems unless there is a strong rationale for using them; conducting cost-benefit analyses prior to approving the deployment of advanced systems will mitigate risks while preserving potential benefits.A key part of enforcing this targeted approach will be monitoring and controlling access to high-performance computing resources, which are required for developing and operating advanced AI systems and their precursors. This additional layer of governance allows authorities to detect potentially dangerous AI development or usage activity, and can be cross-checked with other "
331,Justin Olive,Q19 Application of risk-based approach to general purpose AI,"LLMs and MFMs are constantly demonstrating unprecedented general-purpose capabilities, and their emergence indicates that regulatory approaches which focus on specific use-cases or industries are becoming outdated. A modern approach requires that we focus on overall system capabilities rather than individual scenarios.Systems with advanced capabilities should face progressive levels of scrutiny. Low-risk systems can be monitored with light touch regulation. However, as capabilities increase, so should regulatory requirements.Precursor systems, with capabilities that can be used to build much more advanced systems, will need stricter controls to avoid proliferation of advanced systems amongst malicious actors [1]. Users who require “developer” access should require permits granting such access, and providers must guarantee responsible use via monitoring programs [2].For advanced systems with human-level capabilities, comprehensive scrutiny is essential. Prior to deploying systems for us"
331,Justin Olive,Q20 Voluntary vs mandated compliance with risk-based approach,"Although current AI is not very advanced, this is likely to change in the next 10-15 years. Although the future is still uncertain, we cannot trick ourselves into believing that industry self-regulation will be a long-term, sustainable solution. This is simply not realist. See the following quotes from deep-learning experts:Yann LeCunn, Facebook Chief AI Scientist and Turing Award Winner, suggests we will have human-level AGI in 10-15 years, and has proposed a detailed roadmap for arriving there.Geoffrey Hinton, a pioneer in deep learning, says that superhuman AI will probably arise in the next 5-20 years, and that it poses catastrophic risks.Marcus Hutter, a DeepMind Senior Scientist researching AGI, writes: “... deploying a sufficiently advanced reinforcement learning agent would likely be incompatible with the continued survival of humanity.”Jürgen Schmidhuber is a deep-learning pioneer and inventor of the LSTM network. He says that the intense competition within ecosystems of advan"
332,Michael M,Q01 Definitions,"An effective and risk-based approach to AI regulation focused on ethics and safety - and ultimately public trust - needs to grapple with a spectrum of AI capability. Such a wide range of systems could fit within the presented definitions that they are unhelpful for informing policy and risk decisions. By analogy, when the government regulates “vehicles” it uses definitions to distinguish push-bikes from passenger jets and then applies appropriate regulations to each kind.While including more recent developments like Multimodal Foundation Modals (MFM) is a helpful first step, the paper’s definitions need to capture a spectrum of AI capability to allow a meaningful conversation. The following 5 factors help define a spectrum of capability:1. Size: how many parameters.2. Specialised vs general capabilities.3. Multimodality: the types of data it can process.4. Temporal awareness: is the model able to reason about and plan for future events?5. Active vs passive: is the model trained to achi"
332,Michael M,Q02 Other AI risks and recommendations for regulation,"Missing risks include (1) misuse of highly capable AI systems, and (2) unintended harm from highly capable AI systems, especially systems with the capacity to plan and act as agents acting in ways that are not aligned with human directives. Although the likelihood of the risks may be uncertain, the harms resulting from these risks may be catastrophic and global in scope and must be acknowledged and addressed.The statement “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war” has recently been signed by the heads of leading AI labs (OpenAI, Google DeepMind, Anthropic, Stability AI) as well as many key academic and other researchers pivotal in developing this technology (Geoffrey Hinton, Yoshua Bengio, etc.) and other notable figures such as Bill Gates [1]. Similar calls date back to at least 2015 when Stephen Hawking led a group of AI experts to call attention to the risk and lay out an urgent resea"
332,Michael M,Q03 Non-regulatory actions to address AI risks,"AI is very likely to transform many elements of the economy and society in the coming years. Ethics and safety are unlikely to be a priority unless leaders make them a specific focus. For example, social media algorithms came to shape our lives with little upfront focus on how they could be made ethical and safe. We are now living with the negative impacts on mental health and social cohesion and only have band-aid measures to address the widespread harms. AI is likely to be much more transformative and much more consequential. So leadership is required now.Importantly, the economic and technological forces are likely to be so strong that a scheme that is voluntary or driven by aspirational best practice is unlikely to be successful. That is, any non-regulatory approaches must not be taken to displace the need for strong regulation.Specific non-regulatory approaches should include:1. Government leadership in recognising the risk. Australia must join the growing number of countries and "
332,Michael M,Q04 Coordinating AI governance across government,"Australia should establish an AI Commission to guide Australia through the rapid and widespread adoption of AI. Such a body must be proactive, not reactive, and manage AI in a way that aligns with the national interest, encompassing citizen safety and wellbeing.The AI Commission would provide a central point of leadership for Government, and be a hub for engagement with industries, academia, non-profits, and the community. This structure holds an advantage over other approaches such as voluntary self-governance or departmental-level teams, as it offers a broader perspective, the authority to lead across government, and the agility to keep pace with the fast-evolving AI sector.- Set priorities in Australia's approach to AI and safety: The Commission should be tasked with establishing a clear and focused AI strategy, encompassing ethical, safety, and economic considerations. This strategy would guide government policy, and inform regulations and legal frameworks for AI.- Develop AI exper"
332,Michael M,Q05 International models for AI governance,"Australia should take note of the EU recent handling of the AI Act. For example, the European Parliament’s recognition of general-purpose AI (GPAI) is an important step in the right direction [1]. In particular, the following observations are highly valuable:- Acknowledgement that GPAI providers must comply with strict safety measures, regardless of the way it is distributed or intended to be used.- Requirements for GPAI providers to comply with various safety requirements throughout the lifecycle of their product; i.e. external audits assessing their performance, predictability, interpretability, corrigibility, safety and cybersecurity.These adjustments appear to respond to significant criticisms directed at initial drafts of the AI Act. For example, the AI Now Institute released a set of recommendations that heavily criticised the EU’s approach on some matters [2]. For example:- “GPAI models carry inherent risks and have caused demonstrated and wide-ranging harms. While these risks c"
332,Michael M,Q06 Public vs Private sector governance,"The potential negative impacts on Australians from the use of AI are the same whether the provider is a private company or a government department.Some of the most significant individual and societal harms, as well as catastrophic risks from highly capable AI systems may emerge from public OR private use, meaning that any approach must acknowledge and manage these risks."
332,Michael M,Q07 Responsible government use of AI,"As AI becomes more central to how Australia functions as a nation, the skills and capabilities of the Australian public service should be shaped to align with this transformation. As technical systems continue to become more sophisticated and prevalent throughout society, a deep understanding of them will be essential across all policy domains.Unfortunately, the current structure of the APS is not effective for hiring or retaining technical talent [1]. For example, the average base pay for a machine learning engineer [2] is typically above the EL1 threshold [3], and attracting and retaining an experienced expert in cutting-edge AI systems would likely require far greater compensation.To ensure that Australia’s policy decisions are equipped to capture benefits and mitigate risks in the 21st century, there should be specific initiatives for attracting elite global AI talent into senior positions across Government.The emphasis on attracting global talent is important because Australia its"
332,Michael M,Q09 Transparency requirements and scope,"To remain competitive, professionals and businesses will increasingly feel the pressure to leverage sophisticated “black-box” AI models. This short-term thinking has already resulted in numerous accounts of public backlash and distrust, including high-profile warnings from technology leaders themselves [1].For example, Apple’s credit card business came under fire for offering a woman one twentieth the credit limit offered to her husband. When she complained, an Apple representative simply told her, “I don’t know why, but I swear we’re not discriminating. It’s just the algorithm.” [2]More recently, two lawyers in the United States blamed ChatGPT for “tricking” them into including fictitious legal research in a court filing [3].These problems could easily have been prevented by using explainable AI systems which can show the rationale for their outputs.Such incidents are evidence that the regulatory environment is not incentivising businesses or professionals to use transparent decision-"
332,Michael M,Q10 Ban on high-risk AI applications,"Although there is increasing recognition that AI will pose significant risks to public safety [1], there is widespread confusion about what types of AI are dangerous, and how Australia can help prevent these risks. Luckily, there is a significant body of evidence that describes the main source of these risks [2], and Australia is well-positioned to be a leader in mitigating them.Research shows that two of the main sources of risk are deception and power-seeking behaviour:- Deceptive behaviour in AI systems will become increasingly difficult to avoid as their capabilities advance. This problem has been observed in AI systems since at least 2017, when researchers at OpenAI noted “agents adopting (strategies) that trick the evaluators” [3].- Power-seeking behaviour is the natural tendency for any entity with a long-term goal to take actions that protect or enhance its ability to achieve that goal. Leading AI labs and journals advise that power-seeking behaviour will be a considerable sour"
332,Michael M,Q14 Risk-based approach to address AI risks,"The regulation of AI needs to be “risk-based” in the same way that the regulation of vehicles is risk-based. We are right to regulate push-bikes and passenger jets in very different ways.A limitation of the risk management approach in the discussion paper, specifically Box 4 and Attachment C, is that it focuses only on how an AI is used and disregards the nature of the AI itself.An effective regulatory regime for a high-risk activity will regulate both the technology itself and how it is used. The Australian Human Rights Commission [1], in discussing this issue (“Human Rights and Technology”, 2021), says:Governments tend to regulate high-risk activities and technologies more closely. This helps explain the comparatively strict laws that govern fields such as gene technology, aviation, healthcare and the energy industry. In these areas, regulation often applies both to the technology itself and how it is used. From a human rights perspective, the need for more prescriptive regulation wi"
332,Michael M,Q15 Benefits and limitations of risk-based approach,"While a risk-based approach to regulating AI is sensible, there are two emerging elements of risk practice that any risk-based model should account for to succeed in this context:1. Factoring in “uncertainty”, not just “likelihood”2. Taking a “portfolio” approach to risk mitigationA traditional risk matrix will consider scenarios against their likelihood of occurring, the vulnerability of the system to the scenario, and the consequence of the scenario occurring. It might then discount that risk by the strength of existing or possible mitigations.This approach to risk has historically been unsuccessful for scenarios where assessing “likelihood” is challenging. For instance, many national risk assessments failed to take into account the possibility of a global pandemic before COVID-19. A solution to this failure mode is including “uncertainty”. Specifically, where we can't be sure what the future will bring, that should be cause for significant concern. This is the opposite of the approa"
332,Michael M,Q16 Scope and application of risk-based approach,"The most important organisations to focus on when developing risk-management strategies and regulations, will be those that determine how advanced AI is developed and deployed. More specifically, there are three prominent categories of concern:1. Organisations that develop advanced AI systems, or AI systems that can be used to build more advanced AI systems (i.e. precursor systems such as LLMs). Examples include OpenAI, DeepMind, Anthropic, Inflection AI, Nvidia, Microsoft, Meta2. Organisations which provide access to open-source AI systems via downloads or APIs. E.g. Hugging Face, Github, AWS3. Organisations which provide compute resources to develop, fine-tune or use AI systems. E.g. AWS, HuggingFace, Google, Azure, Nvidia, Lambda LabsA majority of the benefits and risks will come from AI that is more sophisticated and generally capable, which can be tied back to organisations in these three categories.There are approaches which can be taken for each category in order to manage risks"
332,Michael M,Q19 Application of risk-based approach to general purpose AI,"The risk-based approach must be modified to address risks from general purpose AI systems, especially LLMs and MFMs. These types of systems learn their capabilities and behaviours during training, which can be unforeseen by developers. That means that AI developers could create models with dangerous capabilities (e.g., deception, manipulation, political strategy, weapons design). Humans could misuse those capabilities to cause harm [1], and models could apply the capabilities even without deliberate misuse (e.g., through misalignment) [2].Consistent with recent work from a consortium of AI safety researchers, including at Google Deepmind, OpenAI, Anthropic, Alignment Research Center, University of Cambridge, University of Oxford, Centre for Long-Term Resilience, and Centre for the Governance of AI [1], we recommend that any risk-based approach require that general purpose models are evaluated for dangerous capabilities and misalignment. This will help determine:1. To what extent a mode"
332,Michael M,Q20 Voluntary vs mandated compliance with risk-based approach,"In general, regulation should:1. Be proportionate to risk. Meaning that a voluntary or self-regulation approach should only apply to low-risk or no-risk uses of AI.2. In the case of higher-risk technologies, regulate both the technology itself and the use of the technology. Meaning that, for future and more sophisticated AIs, regulation should ensure that the technology is safe before it is created or published, as well as ensuring that it is only used in ways that are safe.3. Place burdens on those most able to reduce potential harms. Meaning that AI that functionally operates as ‘black box’ to consumers has to be regulated at the point of the AI Lab.Applying those three principles, self-regulation may be appropriate for some technologies available today and for some participants (such as students or researchers), but a forward-looking regulatory regime must apply broadly and must be backed by Australian diplomatic efforts to ensure it is globally coordinated."
335,ACT | The App Association,Transcription,"FILENAME: ACT Comments re Australian Government Discussion Paper on Supporting Responsible AI (072623) - LW.32643f4817d41.docx | CONTENT: July 26, 2023Office of Science and Technology PolicyEisenhower Executive Office Building725 17th Street NWWashington, District of ColumbiaRE: Comments of ACT | The App Association to the Australian Government on Supporting Responsible AI: Discussion PaperACT | The App Association (App Association) appreciates the opportunity to provide input to the Australian Government’s Department regarding governance mechanisms to ensure artificial intelligence (AI) is developed and used safely and responsibly in Australia.The App Association is a global trade association for small and medium-sized technology companies. Our members are entrepreneurs, innovators, and independent developers within the global app ecosystem that engage with verticals across every industry. We work with and for our members to promote a policy environment that rewards and inspires innov"
335,ACT | The App Association,Transcription,"FILENAME: ACT The App Association Policy Principles for AI.cb240e1ae7174.pdf | CONTENT: ACT | The App Association’s Policy Principles for Artificial IntelligenceArtificial intelligence (AI) is an evolving constellation of technologies that enable computers to simulate elements of human thinking, such as learning and reasoning. An encompassing term, AI entails a range of approaches and technologies, such as machine learning (ML), where algorithms use data, learn from it, and apply their newly-learned lessons to make informed decisions, and deep learning, where an algorithm based on the way neurons and synapses in the brain change as they are exposed to new inputs allows for independent or assisted decision-making. Already, AI-driven algorithmic decision tools and predictive analytics have substantial direct and indirect effects in consumer and enterprise context, and show no signs of slowing in the future.Across use cases and sectors, AI has incredible potential to improve consumers’ li"
335,ACT | The App Association,Q04 Coordinating AI governance across government,"The App Association strongly urges for a coordinated effort across foreign governments and their agencies. Already, numerous regulatory agencies, some cross-sectoral and others sector-specific, are considering or advancing regulatory proposals that would take starkly different approaches to AI accountability. Some of these proposals are poised to put significant hurdles in place for the development and use of AI through one-size-fits -all approaches that have nominal public benefit at best and are misaligned with other leading efforts from the United States government, such as that of the National Institute of Standards and Technology [NIST] at worst . In some cases, such proposals are being developed based on speculative and undemonstrated harms. The Department of Industry, Science and Resources, along with other cross-sectoral subject matter expert agencies within the Australian Government should take needed steps through implementing a strategy that ensures a harmonized and informed"
336,Australian Financial Markets Association,Transcription,"FILENAME: R43-23 DISR - Safe and Responsible AI - Consultation - AFMA Submission.73b0f530b31d3.pdf | CONTENT: 27 July 2023By Upload.Dear Sir/Madam Re: Safe and responsible AI in AustraliaAFMA welcomes the opportunity to provide comment on the Department of Industry, Science and Resource’s Discussion Paper Safe and responsible AI in Australia.Artificial Intelligence (AI) is currently developing at an extremely fast pace. For certain applications outside of AFMA’s remit and largely in the consumer space there may be potential for regulatory limitations to avoid harms, particularly to the most vulnerable.In the wholesale markets, and in more generally in financial markets, however, we suggest a more cautious approach to regulation that moves slowly, leverages international regulatory outcomes, and is careful to preserve the potential efficiency gains that AI is enabling.The risks of early regulation in such a fast-moving area are skewed much more towards creating impediments to industry a"
336,Australian Financial Markets Association,General comment,"AFMA welcomes the opportunity to provide comment on the Department of Industry, Science and Resource’s Discussion Paper Safe and responsible AI in Australia. Artificial Intelligence (AI) is currently developing at an extremely fast pace. For certain applications outside of AFMA’s remit and largely in the consumer space there may be potential for regulatory limitations to avoid harms, particularly to the most vulnerable. In the wholesale markets, and in more generally in financial markets, however, we suggest a more cautious approach to regulation that moves slowly, leverages international regulatory outcomes, and is careful to preserve the potential efficiency gains that AI is enabling. The risks of early regulation in such a fast-moving area are skewed much more towards creating impediments to industry and innovation rather than being empowering of them. We note in our response below some of the proposals, although nominally risk based, appear unnecessarily burdensome. Experience has "
336,Australian Financial Markets Association,Q01 Definitions,"AFMA supports the use of ISO definitions. The use of standard international definitions will assist with integration and the establishment of common approaches to AI and its regulation, an outcome that is efficient for business."
336,Australian Financial Markets Association,Q02 Other AI risks and recommendations for regulation,"Most of the risks of AI are already be addressed through the principles-based sector specific regulation, and the general regulations. These channels can leverage established processes in managing new and emerging risks through the publication of guidelines etc. The potential for scalability of AI should not be assumed to create additional issues. In most cases it will be the outcomes that count, and these are typically well covered by the combination of existing specific and general regulations such as the Privacy Act (currently under review). For financial advice, fiduciary or best interest duties provide sufficient safeguards to address ethical concerns raised in the paper. The current regulatory structures are outcomes-based and provide good coverage of relevant issues. Limited roll out of AI approaches in advice and industry feedback suggest the regulations may not be sufficiently accommodative of automated solutions at present. We expect transparency practices around such applica"
336,Australian Financial Markets Association,Q03 Non-regulatory actions to address AI risks,Given the breadth of laws that already potentially impact AI there is likely benefit in relevant industry regulators supporting increased awareness of the existing regulatory limitations on AI. Page 3 of 8 Support by regulators for awareness of international industry standards is also likely to be of benefit increased conformance with these standards. The Government should continue work to understand the risks posed by developments in AI and assist government agencies who are regulating specific sectors respond under existing regulatory frameworks in ways consistent with national principles.
336,Australian Financial Markets Association,Q04 Coordinating AI governance across government,"We see some parallels between the coordination of AI governance and the regulation of cyber security. An opportunity was lost in relation to cyber security when multiple regulators created their own nationally and internationally inconsistent security standards that applied different levels of security for the similar data. AFMA suggests that every effort should be made to avoid a similar outcome in relation to AI. The Government should align programs and strategies with OECD AI Strategies and other international approaches to facilitate consistency globally. Regulators should be discouraged from drafting their own unique rules. Where sector specific rules are necessary these should be minimized and draw on common principles set at the national level, and where at all possible be internationally consistent. We see value in consolidating some of the AI specific elements of law and maintaining a guide for implementors of AI systems to assist them ensure their implementations are complian"
336,Australian Financial Markets Association,Q05 International models for AI governance,At a general level AFMA suggests Australia should leverage the experience (as it emerges) of foreign jurisdictions in respect of their approaches to AI governance. We expect there will be the opportunity Page 4 of 8 to see how different approaches function in jurisdictions on the technological leading edge before considering what might be optimal locally. We expect sensible regulatory approaches to emerge over time as the risks are better understood. As an immediate step we suggest increasing awareness of voluntary standards such as NIST’s AI Risk Management Framework. Common approaches with leading jurisdictions are likely to result in lower costs and increased international compatibility.
336,Australian Financial Markets Association,Q06 Public vs Private sector governance,"AFMA supports a risk-based approach for both public and private sector use. While there are many commonalities in the risks of public and private sector use of AI technologies there can be differences. For example: • Large data sets of sensitive information can increase risks that AI will train on private information and risk its release or misuse. • Public sector actors and, more generally, governments may have more coercive powers over firms and individuals in their jurisdictions than private sector actors. This greater power can increase the risks of adverse outcomes for individuals and firms. • Consistent with other limitations sovereign governments place on themselves and their agencies, decision making (and potentially other activities) by, or assisted by, AI in Government as an exercise of state power must be bound by additional protections designed to limit the potential for abuse of power. For example, AI should be bound by the model litigant rules the Government imposes on it"
336,Australian Financial Markets Association,Q07 Responsible government use of AI,To support fair use of AI in its interactions with firms and individuals we suggest the Government look to the NSW Government’s Artificial intelligence assurance framework.
336,Australian Financial Markets Association,Q08 Generic vs technology-specific approaches for AI risk,"As a general finding AFMA’s experience is that technology specific regulations are rarely optimal as they can limit innovation, whereas principles-based regulation, typically focussed on outcomes, allows greater flexibility for firms to innovate and provide services efficiently and at the lowest possible cost."
336,Australian Financial Markets Association,Q09 Transparency requirements and scope,"In a wholesale context transparency is not the same issue as it might be in retail. In a retail context transparency can, for example, be appropriate to ensure that retail investors are aware of the source of recommendations. Transparency should only ever be required where warranted by the risk profile. We reject the view that transparency is an unalloyed good as suggested by Questions 9 (a) and 9 (b) and we caution against a default mode of regulating for transparency. Allowing firms to maintain intellectual property as confidential assists the commercial development of new technologies including AI. This early in the development of a new field of technology such as AI the regulatory approach should be more cautious than suggested by the discussion paper lest innovation be stifled, and benefits denied."
336,Australian Financial Markets Association,Q10 Ban on high-risk AI applications,"In areas within AFMA’s purview we currently do not see the need for any complete bans. We expect that outside AFMA’s purview there may well be areas that governments might determine bans are appropriate based on their prioritisation of matters such as privacy, safety, governance and other concerns.There may be many disparate areas where AI applications or technologies should be banned, but the reasons for these bans are likely to vary significantly. They might include reasons of privacy, child protection, national security, the protection of political discourse, etc. As such it is difficult to generalize about why certain applications or technologies should be banned."
336,Australian Financial Markets Association,Q11 Actions to increase public trust in AI,Usage of AI will increase as products and services become more available that use AI. Government action might therefore be best focused on assisting firms utilize AI.
336,Australian Financial Markets Association,Q12 Trade impact of banning high-risk AI applications,We do not see a need for bans within AFMA’s industry areas.
336,Australian Financial Markets Association,Q13 Changes to conformity infrastructure,"AFMA views this area as highly technology dependent. Explainability, of the type we have come to expect from machine-based decision-making processes may have to evolve. Neural network-based systems work on similar principles to human brains. As such we might reasonably expect similar arrangements to ensure conformance. With parallels to the requirements placed on humans AI might require exposure to standardised data sets (the machine equivalent to training courses) and testing on outcomes (the machine qualifications equivalent)."
336,Australian Financial Markets Association,Q14 Risk-based approach to address AI risks,"AFMA supports a clear separation between wholesale and retail regulation of AI. Retail (including consumer and retail investor) use of AI has a different risk profile compared to wholesale uses. There are fewer risks involved in firms using AI to scan data for investment opportunities for example, than interacting with individuals via online chat bots. AFMA supports risk-based approaches to AI, but at this stage in the development of the technology it is not yet appropriate to be prescriptive except in the most sensitive cases. For example, we note that the table provide in the paper suggests that “Users must be trained” where the use case is “Use of AI in computer chess systems” or “AI-enabled chatbots”. We think this is likely excessive. Similarly, the table suggests that for “Use of AI in safety-related car components” that it “Must have meaningful human intervention at specific points and final decision made by human/s” is more likely to decrease safety than increase safety in many"
336,Australian Financial Markets Association,Q15 Benefits and limitations of risk-based approach,A risk-based approach when implemented properly will provide the best cost to benefit ratio. The limitations are in the accurate a priori assessment of risk. A posteriori data will improve these estimates over time.
336,Australian Financial Markets Association,Q16 Scope and application of risk-based approach,AFMA holds that a risk-based approach is generally better. Where stakes are higher as with other areas of regulation there is likely more of a case to ensure risk assessments are accurate and up to date.
336,Australian Financial Markets Association,Q17 Elements to include in risk-based approach,"While we support the elements listed in Attachment C as potentially contributing to a sensible risk-based approach, as noted above, we have concerns around the proposed mandated application proposed in Box 4. At this early stage we suggest working with industry to help voluntary risk-based approaches emerge, leveraging off international experience."
336,Australian Financial Markets Association,Q18 Integration of risk-based approach into existing frameworks,This is an important but complex question. Where possible duplication of similar principles across multiple regulatory schemes should be avoided in preference of a single regulatory approach. Supporting industry assessments into existing regulatory requirements in a consistent manner should also contribute to achieving a streamlined outcome.
336,Australian Financial Markets Association,Q19 Application of risk-based approach to general purpose AI,This will go to the data on which the model is trained and the purpose for which it is deployed and the associated risks. An LLM/MFM that assists customers choose paint colours that is trained mainly on colour trends should have no or very little requirements applied to it.
336,Australian Financial Markets Association,Q20 Voluntary vs mandated compliance with risk-based approach,For private organisations in AFMA’s industry sectors voluntary tools will generally be preferable to preserve innovation and in AFMA’s areas of industry at present we believe that this is all that is required at present.
337,Ramsay Health Care Australia,Transcription,"FILENAME: RHCA Submission.140b34fae2bec.pdf | CONTENT: 26 July 2023Department of Industry, Science and Resources Industry House 10 Binara Street Canberra ACT 2601 AustraliaBy email: DigitalEconomy@industry.gov.auTo whom it may concern,Ramsay Health Care Australia (RHCA) appreciates the opportunity to provide comment on the Supporting responsible AI: discussion paper.Ramsay Health Care (RHC) provides quality healthcare through a global network of clinical practice, teaching and research. RHC’s global network extends across ten countries, with over eleven million admissions and patient visits to facilities in more than 530 locations. RHCA has over 70 private hospitals and day surgery units in Australia and is Australia’s largest private hospital operator, employing more than 30,000 people.Firstly, RHCA is concerned the paper mentions this consultation does not seek to consolidate or replicate the development of existing general or sector-specific regulations and governance initiatives ac"
337,Ramsay Health Care Australia,Q01 Definitions,"Ramsay Health Care Australia (RHCA) recommends the Australian Government works with industry to seek in-principal agreement to an Australian definition of AI, at least, if a global definition cannot be agreed upon. This will ensure there is a set foundation and understanding to AI policy development in Australia.RHCA urges the Australian Government to continue to engage at a global level to influence the definition of AI and other associated definitions through the International Organisation for Standardization (ISO).RHCA notes this discussion paper uses key definitions based on ISO definitions. Minor modifications are suggested for the following definitions: • Artificial intelligence: are intelligent systems (including algorithms) capable of performing tasks which typically require human intelligence and are not necessarily about predictive outputs. AI systems are designed to simulate and mimic human cognitive abilities. • A large language model (LLM): is a type of AI model which has "
337,Ramsay Health Care Australia,Q02 Other AI risks and recommendations for regulation,"RHCA is concerned the paper mentions this consultation does not seek to consolidate or replicate the development of existing general or sector-specific regulations and governance initiatives across the Australian Government, and only seeks to identify potential gaps and whether additional AI governance mechanisms are required (page 13). The paper itself notes many of these issues are cross-cutting and related to a broad range of issues (page 4).The Australian Government may wish to consider tasking the Regulatory Reform Division, Department of Finance (formerly the Deregulation Taskforce, Department of the Prime Minister and Cabinet) to undertake this work, given the Division has undertaken similar reviews such as health practitioner regulatory settings, automatic mutual recognition of occupation licences and excise and excise-equivalent goods system.RHCA strongly recommends this work aligns with the Australian Government’s broader policy platform including the work being undertaken by"
337,Ramsay Health Care Australia,Q03 Non-regulatory actions to address AI risks,"RHCA strongly recommends the Australian Government invests in building consumer AI literacy for all Australians. This could include general government awareness campaigns, but also more specific interventions such as primary/secondary education (AI in the syllabus) to ensure children learn and understand AI from an early age or target disadvantaged Australians so that no one is left behind. It is clear AI is going to be ubiquitous, much more than the cloud in a very short time. Should ChatGPT adoption rates be the future, it is paramount users understand AI, its use, and potential implications.RHCA encourages the Australian Government to ensure non-regulatory initiatives are practical and user friendly. Given the regulatory landscape is complex, there must be practical guidelines for any applicable laws and regulations (IP, privacy, data protection) and how to guidance prior to the implementation of AI.RHCA supports an inclusive and collaborative approach regarding the development of A"
337,Ramsay Health Care Australia,Q04 Coordinating AI governance across government,"Refer to Question 2.RHCA emphasises the Department of Finance continues to lead the Australian Government’s digital identity initiative. To achieve consensus across Australia, the Australian Government may wish to discuss AI at the Data and Digital Ministers meeting which seeks to align policies across the Commonwealth, State and Territory Governments as well as New Zealand.RHCA also notes with any regulations, all stakeholders (government, private, public, not-for-profit, community) must be provided assurance regarding the privacy of user data used in AI solutions. The Australian Government should work closely with the Attorney-Generals’ Department to implement adequate controls to manage the risk and ensure data is kept secure. For example, the current Privacy Act Review recommends entities include information in their privacy policy about whether personal information will be used in ADM which has a legal, or similarly significant effect on an individual’s right. As such there must b"
337,Ramsay Health Care Australia,Q05 International models for AI governance,"RHCA notes AI is still nascent, and the references provided in the paper are sound. With any proposed governance measures, care should be given to the implementation, and guidelines should be appropriate for use, with sample scenarios around design and decision considerations.RHCA strongly encourages the Australian Government to continue to engage in international forums in consultation with the Department of Foreign Affairs and Trade, such as the G7, OECD, United Nations, Word Trade Organisation and World Economic Forum. This engagement can support domestic responses and avoid duplication of work being undertaken at a global level. Despite countries moving towards a risk-based approach albeit with different approaches, the G7 has agreed to prioritise collaboration on AI governance which emphasises there is appetite at an international level to work to develop appropriate AI policy levers.RHCA advises the Australian Government it can bring a global perspective to this conversation give"
337,Ramsay Health Care Australia,Q06 Public vs Private sector governance,"In the first instance, RHCA recommends different approaches (beyond the foundational principles) apply to public and private sector use of AI technologies. For the successful use of AI by Government, the Government must maintain and improve Australians’ trust in the Australian public service (currently 61%, Citizen Experience Survey 2020-2021), otherwise such an initiative may not succeed, and the true benefits will not come to fruition. The recent Royal Commission into the Robodebt Scheme highlights the importance of the Australian Government setting the standard for the use of AI in Australia. Furthermore, government implementation of initiatives can influence private sector behaviour and clearly set expectations.Generally, the public sector focuses more so on the delivery of effective and equitable public services, where one must ensure transparency is key, and upholding societal values. Whereas the private sector is primarily driven by business objectives, profitability and custome"
337,Ramsay Health Care Australia,Q07 Responsible government use of AI,"Refer to Question 2 and Question 6.RHCA recommends the Australian Government develop clear education and guidance, in conjunction with the private sector to enhance Australia’s AI practices to support the uptake of AI technologies in Australia. This will also support providing guidance to Australians whilst improving AI literacy of Australians in the longer-term, particularly on the limitations and risks involved in the use of generative AI. In essence, the Government must attract further investment in Australia by removing barriers and red tape."
337,Ramsay Health Care Australia,Q08 Generic vs technology-specific approaches for AI risk,"Refer to Question 2.RHCA recommends the development and use of AI systems consider concrete risks and the likelihood of those risks occurring in comparison with the potential benefits and risks presented by similar systems without AI. Should a risk-based approach be implemented, RHCA emphasises the lead agency must monitor high risk solutions, and potentially re-evaluate medium risks every six months as AI continues to advance, given its rapid speed and scale of growth.RHCA reminds the Australian Government that industry and the community must be consulted widely with the proposed approach to be taken, to ensure the management of risk for generic versus technology-specific solutions is appropriate. It would be important for generic regulatory mechanisms to be consistent with technology-specific solutions or sector specific solutions."
337,Ramsay Health Care Australia,Q09 Transparency requirements and scope,"Part ARHCA strongly supports transparency across the entire AI lifecycle.RHCA recommends stakeholders be notified and understand when they are being significantly affected by AI as well as when they are interacting with an AI solution. Stakeholders should be provided an opportunity to contest and challenge the use and outcome of an AI system in a timely manner.RHCA encourages the Australian Government to consider providing Australians access to an efficient and transparent review mechanism should there be questions about the use of data or AI-informed outcomes. For example, the Australian Government would likely increase trust and confidence with all Australians and avoid future issues such as Robodebt, an Australian Government led initiative.Part BRefer to Question 3 and Question 6.RHCA recommends appropriate review mechanisms must be in place to ensure stakeholders can question and challenge AI-based outcomes. There must be an emphasis on explainability to help stakeholders understan"
337,Ramsay Health Care Australia,Q10 Ban on high-risk AI applications,"Part ARHCA cautions the Australian Government completely banning high-risk AI applications or technologies as this may slow innovation and productivity. The decision to completely ban a high-risk AI application or technology can be complex and have unintended consequences (particularly given AI is still relatively new).RHCA believes it is important for entities to have flexibility in this area so that the appropriateness of banning can be considered on a case-by-case-basis, including having regard to input from Government, regulators, and experts at the relevant time.RHCA strongly recommends appropriate regulations (including penalties for inappropriate use) for the use of AI for the following: • social scoring; • warfare; • hacking; • encourages self-harm; and • disinformation (fake news or deepfakes).RHCA recommends mandatory confidential reporting to Government for high-risk AI applications or technologies, and the outcome would be a more balance approach.RHCA emphasises the Austral"
337,Ramsay Health Care Australia,Q11 Actions to increase public trust in AI,"Refer to Question 3, Question 6, Question 7, and Question 9(a) and (b).RHCA strongly recommends the Australian Government develops a framework and associated methodology with concrete examples. This framework would ensure the design and implementation of an AI solution considers issues such as moral principles, confidence, safeguarding information, effects on employment, rules, and standards. Transparency, knowledge sharing, and collaboration is important to support building trust and confidence, particularly to convey AI is centred on human needs."
337,Ramsay Health Care Australia,Q12 Trade impact of banning high-risk AI applications,"Refer to Question 10.RHCA supports the implementation of effective guardrails to ensure such activities do not lead to disbenefit (negative benefits). For example, it may be appropriate to ban social scoring, but not necessarily some applications of facial recognition."
337,Ramsay Health Care Australia,Q13 Changes to conformity infrastructure,"Broadly, RHCA has no comment regarding changes to Australia’s conformity infrastructure to support assurance processes.RHCA notes any standards, including technical, assurance infrastructure and conformity processes or practices must be sufficiently consulted widely with industry and have broad support to further build trust with consumers as well as facilitate international trade.RHCA recommends the Australian Government review the NSW AI Assurance Framework to determine whether it could be uplifted and be made an Australian Framework to assure AI projects against the ethics framework. This could be taken to National Cabinet for consideration and would avoid the need for the Australian Government to duplicate Frameworks."
337,Ramsay Health Care Australia,Q14 Risk-based approach to address AI risks,"Refer to Question 3 and Question 9(a) and (b).RHCA strongly supports a risk-based approach for addressing potential AI risks. The Australian Government must ensure this approach is consistent across all levels of government, with sufficient public consultation."
337,Ramsay Health Care Australia,Q15 Benefits and limitations of risk-based approach,"Refer to Question 10(a).RHCA notes there would be benefits and limitations with any proposed solution, including a risk-based approach, but it is likely the most appropriate approach. Limitations may include the person performing the risk assessment (subjectivity, personal bias) as well as uncertainties based on information quality used to perform the risk analysis."
337,Ramsay Health Care Australia,Q16 Scope and application of risk-based approach,"RHCA agrees a risk-based approach may better suite some sectors, AI applications or organisations than others based on organisation size, AI maturity and resources. There will always be different maturity and levels of technology adoption with different levels of data quality, based on an organisation’s needs and resources.RHCA emphasises the Australian Government must consult widely with industry before deciding to implement multiple approaches to AI, to avoid further complexities. As such, general foundations, principles, and associated regulations should be developed, applicable to all industries. Industries may then seek to develop sector-specific regulations in line with the general principles."
337,Ramsay Health Care Australia,Q17 Elements to include in risk-based approach,"Refer to Question 9(a).RHCA broadly supports the elements in Attachment C: Possible elements of a draft risk-based approach. It is paramount AI solutions are transparent and explainable, that is, stakeholders should be able to understand how AI systems work, make decisions, and are used.RHCA emphasises the risk-based approach must be robust and reliable to help prevent errors, biases, or malicious exploitation which can lead to adverse consequences. Furthermore, the approach must also ensure fairness and equity promotes equal opportunities whilst avoiding potential harm to individuals or marginalised groups (I.e., algorithms which contain systemic and repeatable errors may lead to prejudiced decisions or outcomes).RHCA recommends careful data management must be undertaken to ensure potential data biases are identified and appropriately managed: • a data model that is designed with a focus on diversity and inclusion; • use of a dataset that is representative for the problem to be solved"
337,Ramsay Health Care Australia,Q18 Integration of risk-based approach into existing frameworks,"Refer to Question 2 and Question 4.RHCA emphasises an AI risk-based approach should not simply be incorporated into existing assessment frameworks or risk management processes but should seek to streamline and reduce potential duplication across all levels of government and industries. As such, it is paramount the Australian Government undertake a proper analysis with industry experts.RHCA notes it may be appropriate for AI to have an independent framework given its importance and ubiquitous nature. Though, should this occur, the Australian Government must ensure there is standardisation across government, and consider permissible variations, depending on the sector (public, private, not-for-profit) or the type of industry. There must be an Australian set of common principles and governance approach to ensure Australia’s efforts in AI are aligned."
337,Ramsay Health Care Australia,Q19 Application of risk-based approach to general purpose AI,"Refer to Question 5 and Question 7.RHC notes international engagement must continue given generative AI prompts may be used by foreign actors to gather sensitive information. The proposed guidelines may outline what can and cannot be shared as part of the AI prompt.RHCA also recommends the following: • information from generative AI be used with caution, including fact-checking all output for correctness, currency, bias, and completeness; • users respect and protect intellectual property rights, and unauthorised use of copyrighted material or creation of content that infringes on the intellectual property of others be strictly prohibited; and • users must not share sensitive or confidential data with generative AI solutions, etc."
337,Ramsay Health Care Australia,Q20 Voluntary vs mandated compliance with risk-based approach,Part ARefer to Question 2.Part BRHCA recommends risk-based approach regulations apply to not only developers or deployers but also users (including organisations) which seek to use AI.
338,Australian Academy of Science,Transcription,"FILENAME: AAS submission - DISR AI.a1c0d37a6d4b2.pdf | CONTENT: By email: DigitalEconomy@industry.gov.au26 July 2023Australian Academy of Science submission on Supporting responsible AI: discussion paper Use of AI and AI tools in science is an opportunity to accelerate discoveries and boost economic growth. However, it is crucial for Australia to prepare itself, as waiting to observe the impact of AI on science is not a viable option.Investment in AI is not optional; it is essential to safeguard the future of Australian research and sovereign capability. Government investment should focus on fundamental AI research and the means to build collaboration – between the disciplines, scientists, users – including business and government. It means supporting the development of a suitably skilled workforce.The Academy makes the following recommendations:• The government must take leadership by developing a national strategy and guidelines for the responsible use of AI in research and developme"
338,Australian Academy of Science,Q01 Definitions,"The definition of machine learning in the discussion paper requires revision. An alternative, derived from the CSIRO definition of machine learning, is: “Machine learning (ML) is a subset of AI where intelligent algorithms are designed to automatically extract important and valuable patterns from data. ML is inherently data-driven, requiring a model (typically a neural network) to be trained on a dataset using a learning process.”"
339,Anonymous,Transcription,FILENAME: Submission 339 - Attachment.95545ccabc53b.pdf | CONTENT: Safe and responsible AI in Australia A response to the Discussion paper This document contains a response to the questions:2. What potential risks from AI are not covered by Australia’s existing regulatory approaches? Do you have suggestions for possible regulatory action to mitigate these risks?17. What elements should be in a risk-based approach for addressing potential AI risks? Do you support the elements presented in Attachment C?20. Should a risk-based approach for responsible AI be a voluntary or self-regulation tool or be mandated through regulation? And should it apply to: a. public or private organisations or both? b. developers or deployers or both?Unintended behaviours or outcomes from AI pose a general yet significant risk due to the complexity of certain AI algorithms and their interaction with an AI environment. These unintended behaviour or outcomes can range from minor errors or inaccuracies to potentia
340,Financial Services Council,Transcription,"FILENAME: 202306 FSC Submission - Safe and Responsible AI in Australia.93e2c1c5b1449.pdf | CONTENT: 28 July 2023Lucas Rutherford General Manager, Technology Strategy Branch Department of Industry, Science, and Resources Via webform.Dear Mr Rutherford,RE: Safe and Responsible AI in Australia Discussion PaperThe Financial Services Council (FSC) welcomes the opportunity to make a submission in relation to safe and responsible artificial intelligence (AI) in Australia. The FSC is supportive of a risk-based approach to governing AI use in Australia with only the most obviously harmful AI products banned. Australia’s approach to AI governance should be using a pro- AI approach which encourages innovation in Australia. This in turn will encourage more and more consumers to interact with the technology and build trust in its use cases. A risk-based approach to the governance of AI will allow low-risk AI to be utilised by organisations in a relatively unfettered way, whilst requiring more of hi"
340,Financial Services Council,Q01 Definitions,See attachment.
340,Financial Services Council,Q02 Other AI risks and recommendations for regulation,See attachment.
340,Financial Services Council,Q03 Non-regulatory actions to address AI risks,See attachment.
340,Financial Services Council,Q04 Coordinating AI governance across government,See attachment.
340,Financial Services Council,Q05 International models for AI governance,See attachment.
340,Financial Services Council,Q06 Public vs Private sector governance,See attachment.
340,Financial Services Council,Q07 Responsible government use of AI,See attachment.
340,Financial Services Council,Q08 Generic vs technology-specific approaches for AI risk,See attachment.
340,Financial Services Council,Q09 Transparency requirements and scope,See attachment.
340,Financial Services Council,Q10 Ban on high-risk AI applications,See attachment.
340,Financial Services Council,Q11 Actions to increase public trust in AI,See attachment.
340,Financial Services Council,Q12 Trade impact of banning high-risk AI applications,See attachment.
340,Financial Services Council,Q13 Changes to conformity infrastructure,See attachment.
340,Financial Services Council,Q14 Risk-based approach to address AI risks,See attachment.
340,Financial Services Council,Q15 Benefits and limitations of risk-based approach,See attachment.
340,Financial Services Council,Q16 Scope and application of risk-based approach,See attachment.
340,Financial Services Council,Q17 Elements to include in risk-based approach,See attachment.
340,Financial Services Council,Q18 Integration of risk-based approach into existing frameworks,See attachment.
340,Financial Services Council,Q19 Application of risk-based approach to general purpose AI,See attachment.
340,Financial Services Council,Q20 Voluntary vs mandated compliance with risk-based approach,See attachment.
341,King & Wood Mallesons,Transcription,"FILENAME: KWM Submission - Discussion Paper On Safe And Responsible AI In Australia (28 July 2023).e2523dda1f30a.pdf | CONTENT: TO Technology Strategy Branch The Department of Industry, Science and Resources Commonwealth of Australia28 JULY 2023 Submission to Discussion Paper: Safe and Responsible AI in AustraliaWe are pleased to provide our submission to the Department’s Discussion Paper on Safe and Responsible AI in Australia. AI presents significant opportunities for Australian organisations and the broader Australian economy. However, while there may be some risks associated with AI, any AI related regulation must be clearly targeted at, and proportionate with, identifiable and serious risks to individuals, society or the environment. This will involve taking into account the varying contexts in which AI systems can be deployed throughout the Australian economy and the need to support and promote innovation by Australian companies.Yours sincerelyCheng Lim | Partner Bryony Evans | P"
341,King & Wood Mallesons,Q02 Other AI risks and recommendations for regulation,See attached submission.
341,King & Wood Mallesons,Q09 Transparency requirements and scope,See attached submission.
341,King & Wood Mallesons,Q10 Ban on high-risk AI applications,See attached submission.
341,King & Wood Mallesons,Q14 Risk-based approach to address AI risks,See attached submission.
341,King & Wood Mallesons,Q15 Benefits and limitations of risk-based approach,See attached submission.
341,King & Wood Mallesons,Q17 Elements to include in risk-based approach,See attached submission.
341,King & Wood Mallesons,Q18 Integration of risk-based approach into existing frameworks,See attached submission.
341,King & Wood Mallesons,Q19 Application of risk-based approach to general purpose AI,See attached submission.
343,Rupert McCallum,Q02 Other AI risks and recommendations for regulation,"Missing risks include (1) misuse of highly capable AI systems, and (2) unintended harm from highly capable AI systems, especially systems with the capacity to plan and act as agents acting in ways that are not aligned with human directives. Although the likelihood of the risks may be uncertain, the harms resulting from these risks may be catastrophic and global in scope and must be acknowledged and addressed.The statement “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war” has recently been signed by the heads of leading AI labs (OpenAI, Google DeepMind, Anthropic, Stability AI) as well as many key academic and other researchers pivotal in developing this technology (Geoffrey Hinton, Yoshua Bengio, etc.) and other notable figures such as Bill Gates [1]. Similar calls date back to at least 2015 when Stephen Hawking led a group of AI experts to call attention to the risk and lay out an urgent resea"
343,Rupert McCallum,Q03 Non-regulatory actions to address AI risks,"AI is very likely to transform many elements of the economy and society in the coming years. Ethics and safety are unlikely to be a priority unless leaders make them a specific focus. For example, social media algorithms came to shape our lives with little upfront focus on how they could be made ethical and safe. We are now living with the negative impacts on mental health and social cohesion and only have band-aid measures to address the widespread harms. AI is likely to be much more transformative and much more consequential. So leadership is required now.Importantly, the economic and technological forces are likely to be so strong that a scheme that is voluntary or driven by aspirational best practice is unlikely to be successful. That is, any non-regulatory approaches must not be taken to displace the need for strong regulation.Specific non-regulatory approaches should include:1. Government leadership in recognising the risk. Australia must join the growing number of countries and "
343,Rupert McCallum,Q05 International models for AI governance,"Australia should take note of the EU recent handling of the AI Act. For example, the European Parliament’s recognition of general-purpose AI (GPAI) is an important step in the right direction [1]. In particular, the following observations are highly valuable: - Acknowledgement that GPAI providers must comply with strict safety measures, regardless of the way it is distributed or intended to be used. - Requirements for GPAI providers to comply with various safety requirements throughout the lifecycle of their product; i.e. external audits assessing their performance, predictability, interpretability, corrigibility, safety and cybersecurity.These adjustments appear to respond to significant criticisms directed at initial drafts of the AI Act. For example, the AI Now Institute released a set of recommendations that heavily criticised the EU’s approach on some matters [2]. For example: - “GPAI models carry inherent risks and have caused demonstrated and wide-ranging harms. While these risk"
343,Rupert McCallum,Q09 Transparency requirements and scope,"The recent focus for transparency has been on ensuring that consumers are notified when a decision made by AI affects them directly (e.g. hiring/firing, loan approval). While this is helpful, it is only the first step in protecting the public.In circumstances where the use of AI for decision-making is commonplace, it will be of little value for consumers to know “when” an AI system makes a decision that affects them. This might happen many times each day. Rather, they should be informed “how” the decision is made. This will allow consumers to more easily identify unsafe or unfair systems.Having models which are both accurate and explainable has long been an issue for businesses [1], but the recent advancements of “black-box models” is encouraging developers and providers to simply tolerate the risks that come from lack of explainability. As AI capabilities continue to outstrip their explainability [2], the risks of this approach will grow significantly. The fact that explainability is "
343,Rupert McCallum,Q10 Ban on high-risk AI applications,"While it is unlikely that AI today represents a catastrophic or existential risk to humanity, there’s a realistic possibility that it will in the near future [1, 2].To strike a balance between all perspectives, regulators should work to identify “advanced indicators” of catastrophic risk scenarios, and clearly communicate that a ban is an appropriate action if these advanced indicators come to pass.Establishing appropriate advanced indicators will have to be done in consultation with AI safety researchers, for example those from the Centre for AI Safety, or the Centre for AI Governance. However, it’s likely they could include factors such as the size of training runs (measured in terms of compute and data), as well as safety-relevant factors about the AI system, such as the ability to complete a diverse range of reasoning tasks with human-level performance; to navigate complex information environments and act within those environments autonomously, and to form sophisticated plans and r"
343,Rupert McCallum,Q14 Risk-based approach to address AI risks,"The regulation of AI needs to be “risk-based” in the same way that the regulation of vehicles is risk-based. We are right to regulate push-bikes and passenger jets in very different ways.A limitation of the risk management approach in the discussion paper, specifically Box 4 and Attachment C, is that it focuses only on how an AI is used and disregards the nature of the AI itself.An effective regulatory regime for a high-risk activity will regulate both the technology itself and how it is used. The Australian Human Rights Commission [1], in discussing this issue (“Human Rights and Technology”, 2021), says:Governments tend to regulate high-risk activities and technologies more closely. This helps explain the comparatively strict laws that govern fields such as gene technology, aviation, healthcare and the energy industry. In these areas, regulation often applies both to the technology itself and how it is used. From a human rights perspective, the need for more prescriptive regulation wi"
343,Rupert McCallum,Q16 Scope and application of risk-based approach,"The most important organisations to focus on when developing risk-management strategies and regulations, will be those that determine how advanced AI is developed and deployed. More specifically, there are three prominent categories of concern: 1. Organisations that develop advanced AI systems, or AI systems that can be used to build more advanced AI systems (i.e. precursor systems such as LLMs). Examples include OpenAI, DeepMind, Anthropic, Inflection AI, Nvidia, Microsoft, Meta 2. Organisations which provide access to open-source AI systems via downloads or APIs. E.g. Hugging Face, Github, AWS 3. Organisations which provide compute resources to develop, fine-tune or use AI systems. E.g. AWS, HuggingFace, Google, Azure, Nvidia, Lambda LabsA majority of the benefits and risks will come from AI that is more sophisticated and generally capable, which can be tied back to organisations in these three categories.There are approaches which can be taken for each category in order to manage ri"
343,Rupert McCallum,Q19 Application of risk-based approach to general purpose AI,"The risk-based approach must be modified to address risks from general purpose AI systems, especially LLMs and MFMs. These types of systems learn their capabilities and behaviours during training, which can be unforeseen by developers. That means that AI developers could create models with dangerous capabilities (e.g., deception, manipulation, political strategy, weapons design). Humans could misuse those capabilities to cause harm [1], and models could apply the capabilities even without deliberate misuse (e.g., through misalignment) [2].Consistent with recent work from a consortium of AI safety researchers, including at Google Deepmind, OpenAI, Anthropic, Alignment Research Center, University of Cambridge, University of Oxford, Centre for Long-Term Resilience, and Centre for the Governance of AI [1], we recommend that any risk-based approach require that general purpose models are evaluated for dangerous capabilities and misalignment. This will help determine:1. To what extent a mode"
343,Rupert McCallum,Q20 Voluntary vs mandated compliance with risk-based approach,"AI is very likely to transform many elements of the economy and society in coming years. Ethics and safety are unlikely to be a priority during this time unless they are actively made a strategic focus by clear leadership. Historic examples include how social media algorithms came to shape our lives, and the negative impacts to mental health and social cohesion only became apparent much later and still have not been adequately addressed. Similarly, while the destructive potential of nuclear was apparent early on, the “arms race” mentality, as well as potential economic benefits, swept humanity along towards the current risky situation that no one in the early 1940s could have conceived of. In that way, history teaches us that it’s critical that ethics and safety take centre stage from day one.The economic and technological forces are likely to be so strong that a scheme that is voluntary or driven by aspirational best practices is unlikely to be successful. Further, smaller businesses "
344,Paul Hadden,General comment,Artificial Intelligence is less of a threat than Demented Business Logic. Unnatural and unsustainable growth activity has become a type of cancerous Bad Logic that has manifested since the Bretton Woods agreement made Money infinite based on the act of make application for borrowing from reserve banks.
344,Paul Hadden,Q01 Definitions,I have not yet read but will and provide an email reply
344,Paul Hadden,Q02 Other AI risks and recommendations for regulation,Regulations not yet implemented with blockchain enhanced functionality for automated reporting
344,Paul Hadden,Q03 Non-regulatory actions to address AI risks,Yes as mentioned above
344,Paul Hadden,Q04 Coordinating AI governance across government,Audit of all Company and Business Mission Statements to correct Bad Logic and Criminal Activity and Bad Faith Intent and Loss Manufacturing and Attack on Government by avoiding Tax
344,Paul Hadden,Q05 International models for AI governance,Yes
344,Paul Hadden,Q06 Public vs Private sector governance,Yes
344,Paul Hadden,Q07 Responsible government use of AI,Government act as a central authority to hold data and enable data sovereignty and facilitate monetisation gateways that protect privacy
344,Paul Hadden,Q08 Generic vs technology-specific approaches for AI risk,All public service functions
344,Paul Hadden,Q09 Transparency requirements and scope,All observations of any persons data be logged so that any person can know if any other person has be made known or utilised any information or data any person has generated.
344,Paul Hadden,Q10 Ban on high-risk AI applications,Ban on AI and all systems and processes that cause detrimental harm or death to any person.
344,Paul Hadden,Q11 Actions to increase public trust in AI,Have good hard think about all the Royal Commissions
344,Paul Hadden,Q12 Trade impact of banning high-risk AI applications,Alignments and level playing field
344,Paul Hadden,Q13 Changes to conformity infrastructure,Ending drug prohibition and laws that violate human rights or intended for genocide
344,Paul Hadden,Q14 Risk-based approach to address AI risks,Not until Alcohol and Tobacco use has significantly reduced in Australia. AI is less a threat than paranoia caused by these toxic products
344,Paul Hadden,Q15 Benefits and limitations of risk-based approach,As listed above
344,Paul Hadden,Q16 Scope and application of risk-based approach,"Yes, past ethics and business practices and legal issues history and complaint resolution outcomes should determine access and determination of maturity to handle AI tools"
344,Paul Hadden,Q17 Elements to include in risk-based approach,Have not read
344,Paul Hadden,Q18 Integration of risk-based approach into existing frameworks,Combining with Blockchain
344,Paul Hadden,Q19 Application of risk-based approach to general purpose AI,By combining with Blockchain
344,Paul Hadden,Q20 Voluntary vs mandated compliance with risk-based approach,Self regulation monitored by blockchain based systems
345,Optus,Transcription,"FILENAME: Optus AI Submission - Public.20c8fb27cad2e.pdf | CONTENT: 1. Optus welcomes the opportunity to provide a submission on the Government’s ‘Supporting Responsible AI’ discussion paper.2. Optus is the owner and operator of significant national communications infrastructure and the supplier of important carriage and content services to a large portion of the Australian community (over 11 million services). .3. Optus is playing a leadership role in the responsible and innovative use of artificial intelligence in the telecommunications industry. We have a number of AI-based capabilities across our business have policies and procedures in place for the safe and responsible use of AI across our business.4. Optus supports the government’s ambition to harness the huge productivity potential of artificial intelligence whilst ensuring that appropriate safeguards are established. Based on our own experience, we offer the following key comments for government’s consideration:(a) A deeper, s"
347,Piotr Kulaga,Q02 Other AI risks and recommendations for regulation,"Based on publicised statements and the narrative framing of reports published in recent months, in effect reoccurring themes like ethical, responsible, dignity-centric, etc., a false sense of security and overarching integrity of the initiatives concerned is being created in the public domain. That is, given the nature of 'logical' operation and control over the 'output' of contemporary AI technologies, and in particular GAI, none of the abovementioned reassurances can be technically realised. In other words, current circumstances dictate that only a means of restricting the use of AI, to exclude high-stakes decisions pertaining to peoples lives, is practicable."
347,Piotr Kulaga,Q03 Non-regulatory actions to address AI risks,"When public statements relating to AI initiatives are undertaken on interactive forums, e.g. LinkedIn, these publicity exercises should accommodate engagement with the public and in particular, any expressions of doubt or concerns about validity of claims being made, should be addressed in a sincere and competent manner. That is, any technical issue should be provided with commensurable response. In my experience, it is apparent that public concerns and challenges are simply dismissed, as if these publicity channels operated in a one direction, like some dystopian propaganda machine."
347,Piotr Kulaga,Q04 Coordinating AI governance across government,"In view of the poor record of the Australian government and commercial IT and cybersecurity initiatives, institutions should strive for a more engaged and engaging manner of capturing public opinion and involvement in decision-making processes."
347,Piotr Kulaga,Q05 International models for AI governance,"The European model of regulation, excluding decisions regarding people's destiny, e.g. eligibility or job opportunities, confers a more ethical approach towards highly ambiguous operation of AI systems, than the sense of ambivalence, if not dogmatic faith in correctness of automated processing, as displayed in the USA or for that matter Australia, as far as actual operational initiatives are concerned, unlike inconsequential pledges and reports."
347,Piotr Kulaga,Q06 Public vs Private sector governance,The same ethical standards and moral norms should be applied to the design criteria and enforced on the validation protocol for systems being deployed in commercial settings and government initiatives.
347,Piotr Kulaga,Q07 Responsible government use of AI,"By striving for high level of inhouse 'technical' competence and accountability for results, as well as transparency of 'consultative activities' and decision-making processes or workflows in the design and development."
347,Piotr Kulaga,Q08 Generic vs technology-specific approaches for AI risk,"Generic solutions may be appropriate in defining the scope and boundaries of high-stakes subject matter, e.g. privacy, actions materially affecting someone's interests, etc., which should be considered as mission critical operations, that are necessarily delegated to an identifiable individual or party, which is held accountable for outcomes and rationale of decisions being made. Technology-specific solutions reflect the nature of the capabilities presented by given technologies, and really can't be approached in a speculative manner, in absence of any particulars."
347,Piotr Kulaga,Q09 Transparency requirements and scope,"a. Public trust and confidence in AI can only be grounded in open disclosure of the motives behind the 'system', in relation to serving the goals of individuals and 'acting' in their best interest, as opposed to some 'agenda' driving the initiative. b. Transparency requirements need to respond to satisfactory 'norms' which are defined by public interactions with the system, and reflect their experience, be it comprehension, difficulties or dissatisfaction with its operation, as well as responding to initiative of those acting in the 'public interest', e.g. journalists."
347,Piotr Kulaga,Q10 Ban on high-risk AI applications,"a. While any AI technology can be argued to satisfy a notion of benign and at the same time potentially nefarious or dangerous, at the discretion of its user, technologies which contravene legal restrictions by their nature, e.g. 'productions' of Generative AI trained on 'proprietary' or otherwise asserted as owned by an author material, can potentially be considered liable of violation if its stated purpose is that of imitating 'art', e.g. provides a functionality like 'in the style of', what in effect is intellectual property. b. Any actions which affect someone's destiny or their material interests, need to be considered as mission critical operations, that are necessarily delegated to an identifiable individual or party, which is held accountable for outcomes and rationale of decisions being made."
347,Piotr Kulaga,Q11 Actions to increase public trust in AI,"Starting the reasoning process with appraisals of potentiality, rather than preconceptions of 'value' in ""increase public trust in AI deployment to encourage more people to use AI"". People don't have an innate need for AI and governments or authorities should respect and reflect that principle as a primary objective, until explicit expressions to the contrary are called for in public debate and a comprehensive consensus opinion can be reached. There is no such thing as new or different is better, in the moral and ethical realm."
347,Piotr Kulaga,Q12 Trade impact of banning high-risk AI applications,"The risky proposition is one of exposing foreign subjects to unregulated and potentially exploitative practices, rather than restricting their scope. As far as commercial interests of foreign operators being restricted in Australia, there are a number of current 'test case' internationally, hence a notion that the precedent is a total absence of regulation or restrictions is a total fallacy."
347,Piotr Kulaga,Q13 Changes to conformity infrastructure,No comment.
347,Piotr Kulaga,Q14 Risk-based approach to address AI risks,"While it is not an area of my expertise, it is my firm conviction that 'Tranche 2' of Australian anti-money laundering and counter-terrorism financing laws (AML/CTF) and legislation should be enacted, in line with the majority of jurisdictions in the world."
347,Piotr Kulaga,Q15 Benefits and limitations of risk-based approach,n/a
347,Piotr Kulaga,Q16 Scope and application of risk-based approach,n/a
347,Piotr Kulaga,Q17 Elements to include in risk-based approach,n/a
347,Piotr Kulaga,Q18 Integration of risk-based approach into existing frameworks,n/a
347,Piotr Kulaga,Q19 Application of risk-based approach to general purpose AI,The principles of 'risk-based approach' relate to mission-critical operations utterly unrelated to and not suitable for application of Generative AI technologies (LLMs MFMs GPTs).
347,Piotr Kulaga,Q20 Voluntary vs mandated compliance with risk-based approach,"All critical data capture, handling, management and maintenance regulations need to be mandated by regulation and legally enforced."
348,Consumer Policy Research Centre,Transcription,"FILENAME: CPRC Submission - Safe and responsible AI in Australia - DISR - July 2023.aaa32d9f5ee39.pdf | CONTENT: SubmissionSafe and responsible AI in Australia – Department of Industry, Science and Resources 27 July 2023 About the Consumer Policy Research CentreThe Consumer Policy Research Centre (CPRC) is a not-for-profit consumer policy think tank.Our work is possible thanks to funding from the Victorian Government.Our role is to investigate the impacts that markets and policies have on Australian consumers and advise on best practice solutions. Consumer protections in the digital world is a current research focus for CPRC.Contact for submissionChandni Gupta Deputy Chief Executive Officer + Digital Policy DirectorEmail: chandni.gupta@cprc.org.auSubmission made via: https://consult.industry.gov.au/supporting-responsible-aiStatement of RecognitionCPRC acknowledges the Traditional Custodians of the lands and waters throughout Australia. We pay our respect to Elders, past, present and em"
349,William Campos - Clinical Psychologist & Social Researcher,Transcription,"FILENAME: AI & Psychology 2023.9f1422b95ba85.pdf | CONTENT: Will A.I. derail human connections: A perspective by a psychologist. All social systems and institutions work because of the ability for people to trust.....but will the long term impact of A.I., social media and technology, potentially derail our innate ability to trust one another. Psychologists add to the Global debate.William (Bill) Campos Clinical Psychologists & Social Researcher Health Executive & Digital Health Agency AdvisorAs the world is now more connected, the ripple effects of change are more significant, impactful, and closer to home…. Some of the changes are good, some are bad, and some are yet to be determined, as hindsight and history will unfold.With an ever-increasing awareness of global issues, such a global climate change, pandemic, wealth inequality, sustainable energy, inequity in health, wealth and employment, gender and race discrimination and political unrest, are “forces” that are driving enormous ch"
349,William Campos - Clinical Psychologist & Social Researcher,General comment,This is an initial discussion paper regarding the impact on AI on human connections and relationships (There is more research needed and underway to identify and understand the implications of AI on human Psychology and mental health.
349,William Campos - Clinical Psychologist & Social Researcher,Q01 Definitions,agree
349,William Campos - Clinical Psychologist & Social Researcher,Q02 Other AI risks and recommendations for regulation,There needs to be a verification process and transparency as to who is using and creating AI content.
349,William Campos - Clinical Psychologist & Social Researcher,Q03 Non-regulatory actions to address AI risks,Registration of users to tools and identifiers.
349,William Campos - Clinical Psychologist & Social Researcher,Q04 Coordinating AI governance across government,"Initial customer service interaction and triage process to assist end users to appropriate (and up to date content) relevant to legislations, social policies, laws and general social services."
349,William Campos - Clinical Psychologist & Social Researcher,Q05 International models for AI governance,registration of the tools (Software companies & developers beyond ABN) with a verification process
349,William Campos - Clinical Psychologist & Social Researcher,Q06 Public vs Private sector governance,Yes disclosure of commercial intents and purposes
349,William Campos - Clinical Psychologist & Social Researcher,Q07 Responsible government use of AI,Registration of algorithms and business logistics that generate content (patents) to be disclosed.
350,Special Broadcasting Service (SBS),Transcription,"FILENAME: 119_SBS submission_Safe and responsible AI in Australia - Discussion paper_July 2023.3651becf00a51.pdf | CONTENT: 26 July 2023Technology Strategy Branch Department of Industry, Science and Resources GPO Box 2013, Canberra ACT 2601 Via e-mail: DigitalEconomy@industry.gov.auTo whom it may concern,SBS submission – Supporting responsible AI: discussion paperSBS appreciates the opportunity to respond to the Safe and responsible AI in Australia – Discussion paper (the Discussion Paper) by the Department of Industry, Science and Resources (the Department).As with the emergence of many new, unique, and enabling technologies before it, artificial intelligence (AI) can and is already delivering significant benefits across the economy and society.SBS is currently exploring the incorporation of positive uses of AI 1—that may benefit its Australian audiences and stakeholders, including the Australian public-at-large—into aspects of its operations. SBS is therefore interested to ensure tha"
350,Special Broadcasting Service (SBS),General comment,Please refer to SBS's submission (uploaded).
350,Special Broadcasting Service (SBS),Q01 Definitions,Please refer to SBS's submission (uploaded).
350,Special Broadcasting Service (SBS),Q02 Other AI risks and recommendations for regulation,Please refer to SBS's submission (uploaded).
350,Special Broadcasting Service (SBS),Q03 Non-regulatory actions to address AI risks,Please refer to SBS's submission (uploaded).
350,Special Broadcasting Service (SBS),Q04 Coordinating AI governance across government,Please refer to SBS's submission (uploaded).
350,Special Broadcasting Service (SBS),Q05 International models for AI governance,Please refer to SBS's submission (uploaded).
350,Special Broadcasting Service (SBS),Q06 Public vs Private sector governance,Please refer to SBS's submission (uploaded).
350,Special Broadcasting Service (SBS),Q07 Responsible government use of AI,Please refer to SBS's submission (uploaded).
350,Special Broadcasting Service (SBS),Q08 Generic vs technology-specific approaches for AI risk,Please refer to SBS's submission (uploaded).
350,Special Broadcasting Service (SBS),Q09 Transparency requirements and scope,Please refer to SBS's submission (uploaded).
350,Special Broadcasting Service (SBS),Q10 Ban on high-risk AI applications,Please refer to SBS's submission (uploaded).
350,Special Broadcasting Service (SBS),Q11 Actions to increase public trust in AI,Please refer to SBS's submission (uploaded).
350,Special Broadcasting Service (SBS),Q12 Trade impact of banning high-risk AI applications,Please refer to SBS's submission (uploaded).
350,Special Broadcasting Service (SBS),Q13 Changes to conformity infrastructure,Please refer to SBS's submission (uploaded).
350,Special Broadcasting Service (SBS),Q14 Risk-based approach to address AI risks,Please refer to SBS's submission (uploaded).
350,Special Broadcasting Service (SBS),Q15 Benefits and limitations of risk-based approach,Please refer to SBS's submission (uploaded).
350,Special Broadcasting Service (SBS),Q16 Scope and application of risk-based approach,Please refer to SBS's submission (uploaded).
350,Special Broadcasting Service (SBS),Q17 Elements to include in risk-based approach,Please refer to SBS's submission (uploaded).
350,Special Broadcasting Service (SBS),Q18 Integration of risk-based approach into existing frameworks,Please refer to SBS's submission (uploaded).
350,Special Broadcasting Service (SBS),Q19 Application of risk-based approach to general purpose AI,Please refer to SBS's submission (uploaded).
350,Special Broadcasting Service (SBS),Q20 Voluntary vs mandated compliance with risk-based approach,Please refer to SBS's submission (uploaded).
351,"Media, Entertainment & Arts Alliance",Transcription,"FILENAME: MEAA AI Safety Submission_.cc22808dfe3a1.pdf | CONTENT: Safe and Responsible AI in AustraliaSubmission of the Media Entertainment and Arts Alliance (MEAA)July 20231 MEAA makes this submission in response to the Australian Government’s discussion paper on Safe & Responsible Artificial Intelligence (AI) to provide some insight into potential regulation of AI within the creative industries and throughout the broader community.We welcome the opportunity to provide a contribution to the discussion on steps Australia can take to mitigate the potential risks of Artificial Intelligence (AI), and trust this feedback will help inform the Australian Government’s further governance and regulatory responses.As the union representing workers in media and entertainment, including journalists, photographers, musicians, performers, stage and film crew, we are particularly conscious of the potential impacts of this technology – both for workers and for audiences.The rapid advancement and distr"
351,"Media, Entertainment & Arts Alliance",Q01 Definitions,Yes
351,"Media, Entertainment & Arts Alliance",Q02 Other AI risks and recommendations for regulation,See submission
351,"Media, Entertainment & Arts Alliance",Q03 Non-regulatory actions to address AI risks,See submission
351,"Media, Entertainment & Arts Alliance",Q04 Coordinating AI governance across government,See submission
351,"Media, Entertainment & Arts Alliance",Q05 International models for AI governance,See submission
351,"Media, Entertainment & Arts Alliance",Q06 Public vs Private sector governance,See submission
351,"Media, Entertainment & Arts Alliance",Q07 Responsible government use of AI,See submission
351,"Media, Entertainment & Arts Alliance",Q08 Generic vs technology-specific approaches for AI risk,See submission
351,"Media, Entertainment & Arts Alliance",Q09 Transparency requirements and scope,See submission
351,"Media, Entertainment & Arts Alliance",Q10 Ban on high-risk AI applications,See submission
351,"Media, Entertainment & Arts Alliance",Q11 Actions to increase public trust in AI,See submission
351,"Media, Entertainment & Arts Alliance",Q12 Trade impact of banning high-risk AI applications,See submission
351,"Media, Entertainment & Arts Alliance",Q13 Changes to conformity infrastructure,See submission
351,"Media, Entertainment & Arts Alliance",Q14 Risk-based approach to address AI risks,See submission
351,"Media, Entertainment & Arts Alliance",Q15 Benefits and limitations of risk-based approach,See submission
351,"Media, Entertainment & Arts Alliance",Q16 Scope and application of risk-based approach,See submission
351,"Media, Entertainment & Arts Alliance",Q17 Elements to include in risk-based approach,See submission
351,"Media, Entertainment & Arts Alliance",Q18 Integration of risk-based approach into existing frameworks,See submission
351,"Media, Entertainment & Arts Alliance",Q19 Application of risk-based approach to general purpose AI,See submission
351,"Media, Entertainment & Arts Alliance",Q20 Voluntary vs mandated compliance with risk-based approach,See submission
352,Anonymous,General comment,"I work in Immersive and Educational Tech. I am pleased that the conversation about AI management is coming from the Government. Fundamentally we know we cant rely on businesses just being good - there are 1001 reasons why good businesses end up doing not so good things. That's why we have a Workplace Health and Safety system. I like being able to work within a system that gives parameters, and punishment, and doesnt rely on the person on the ground pushing an ethical barrow, as it where. Regulation and encouragement works. AI is coming in fast, so I am looking forward to what the Department comes up with. I am hoping for a mix of risk mitigation measures, a bit of regulatory bite and may be some enforced transparency on high risk AI applications."
352,Anonymous,Q01 Definitions,Yes they work.
352,Anonymous,Q02 Other AI risks and recommendations for regulation,I would like to see a bit more focus on contestability and redress - the things that made Robodebt drag on for soooo long. An obligation on the Legally Responsible party to provide redress.
352,Anonymous,Q03 Non-regulatory actions to address AI risks,"Ok, so it is regulatory, but how about an Anti-Obfuscation Act, that requires plain communication of what AI is doing. A bit like NZ Plain English Act."
352,Anonymous,Q04 Coordinating AI governance across government,No idea. Federalism is a challenge.
352,Anonymous,Q05 International models for AI governance,"The NZ Plain Language Act 2022 requires all public service agencies and crown agents to use plain language. I like the UK 5 principles, because they include Contestability and Redress."
352,Anonymous,Q06 Public vs Private sector governance,"No, a risk is a risk. However, I'd be keen to get Public use overseen by something similar to the UK's Algorithmic Transparency Standard. Literally as a Standard."
352,Anonymous,Q07 Responsible government use of AI,I am a fan of the use of Plain Language when describing what the agencies are doing and how AI is being used. It's a step towards being transparent about AI.
352,Anonymous,Q08 Generic vs technology-specific approaches for AI risk,
352,Anonymous,Q12 Trade impact of banning high-risk AI applications,It wouldn't.
352,Anonymous,Q14 Risk-based approach to address AI risks,"I do. With a good firm bit of regulation around the high risk areas, and a firm stance on what as Australians, we dont believe is worth risking."
352,Anonymous,Q15 Benefits and limitations of risk-based approach,Agreement on what that line is.
352,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,"It should be mandated and it should be applied to all. Just as the builder of an unsafe machine, and the seller of an unsafe machine, are both part of the responsibility chain."
353,Australian Mobile Telecommunication Association,Transcription,"FILENAME: 20230726_AMTA Letter of support for Communications Alliance AI submission.a44c58124a55b.pdf | CONTENT: Department of Industry, Science and Resources Via: DigitalEconomy@industry.gov.au26 July 2023Dear Technology Strategy Branch,Re: Safe and responsible AI in Australia Discussion PaperThe Australian Mobile Telecommunications Association (AMTA) welcomes the opportunity to provide a letter broadly supporting the submission from Communications Alliance to the Department of Industry, Science and Resources Discussion Paper: Safe and Responsible AI in Australia.AMTA is the peak industry body of Australia’s mobile telecommunications industry. Our purpose is to be the trusted voice of industry and promote the adoption, monetisation and sustainability of mobile telecommunications technology for the benefit of all Australians.AMTA supports the safe and responsible use of artificial intelligence (AI) in Australia and effective regulation that addresses existing and emerging risks while m"
354,Roche Products,Transcription,"FILENAME: Roche Submission to the Safe and Responsible AI in Australia Discussion Paper 2023.acdb9bb222137.pdf | CONTENT: Technology Strategy Branch Department of Industry, Science and Resources Australian Government Via: DigitalEconomy@industry.gov.au Canberra ACT 260028 July 2023Roche Submission to Supporting Responsible AI: Discussion PaperTo Whom It May Concern,Roche welcomes the opportunity to provide a submission to the Department of Industry, Science and Resources Discussion Paper on Safe and Responsible AI in Australia. Roche supports the Australian Government's ambitions to be a leader in the responsible use of Artificial Intelligence (AI), and the process of consultation by seeking broad advice on the steps the Government can take to mitigate the potential risks.Roche is a global pioneer in pharmaceuticals and diagnostics focused on advancing science to improve people’s lives. The combined strengths of pharmaceuticals and diagnostics, as well as growing capabilities in the ar"
355,Actuaries Institute,Transcription,"FILENAME: 2023_07_25_Submission_DoISR_Responsible AI_Fnl.c4efd6256e9dd.pdf | CONTENT: 25 July 2023Department of Industry, Science and Resources Technology Strategy Branch GPO Box 2013 Canberra ACT 2600Email: digitaleconomy@industry.gov.auDear Sir/MadamResponse to Department of Industry, Science and Resources – Supporting Responsible AI Discussion PaperThe Actuaries Institute (‘the Institute’) welcomes the opportunity to provide responses to the Department of Industry, Science and Resources Discussion Paper – Safe and responsible AI in Australia (‘Discussion Paper’).The Institute is the peak professional body for actuaries in Australia. Our members have significant involvement in Artificial Intelligence (‘AI’) in varied sectors of the economy and have contributed to several Government consultations and other work related to the topic of the Discussion Paper. Our contributions include:► Submission to the Australian Human Rights Commission Discussion Paper ‘Human Rights and Technology Dis"
356,Campaign for AI Safety - supplementary on copyright,Transcription,"FILENAME: Supplementary submission on copyright.a8b7b1a2c3005.pdf | CONTENT: Supporting responsible AI: Supplementary submission from Campaign for AI Safety on copyright issues This is a supplementary submission to our campaign’s main submission1. This submission focuses on copyright issues. While we recognise that intellectual property rights are explicitly outside of scope of the consultation, we are alarmed by the reports that Google is using this consultation to lobby for “relaxed copyright laws for AI” as reported in AFR today2.We ask the government to strongly reject any proposal from big tech to “relax” copyright protections. Google’s arguments, as they are reported in AFR, can be summed up as “let us take your citizens’ artworks, texts, data, etc. to train models without consent from the authors otherwise we go to Singapore”. This is an abhorrent and uncivilised argument. In fact, copyright must be strengthened and clarified.1. Our policy recommendations Generative AI arrived l"
357,NSW Ombudsman,Transcription,"FILENAME: NSW Ombudsman Submission 26 July 2023 re Safe and Responsible AI discussion paper.839e1f5647c63.pdf | CONTENT: OFFICIAL26 July 2023Technology Strategy Branch Department of Industry, Science and Resources via email: DigitalEconomy@industry.gov.auNSW Ombudsman submission – “Safe and Responsible AI in Australia” discussion paperThe Australian Government is seeking feedback on appropriate regulatory and policy responses to support the safe and responsible use of AI, and the Commonwealth Department of Industry, Science and Resources has issued a discussion paper – ‘Safe and responsible AI in Australia’ to facilitate consultation. I am writing to draw to the attention of the Department a special report I tabled in the NSW Parliament that addresses a number of the issues raised in your discussion paper.The role of the NSW Ombudsman The NSW Ombudsman is an independent integrity body that pursues fairness for the people of NSW. In particular, we strive to ensure that those entrusted w"
357,NSW Ombudsman,Transcription,"FILENAME: The-new-machinery-of-government-special-report_Front-section.e11cbeacbfa75.pdf | CONTENT: The new machinery of government: using machine technology in administrative decision-makingA special report under section 31 of the Ombudsman Act 197429 November 2021The new machinery of Government: Using machine technology in administrative decision-making 0 NSW OmbudsmanAcknowledgements We thank Professor John McMillan AO, former Commonwealth and NSW Ombudsman, Dr Lachlan McCalman, Chief Practitioner, Gradient Institute, and Associate Professor Will Bateman, Associate Dean of Research at the ANU Law School for providing expert comments on early drafts of this report. We are also grateful to Bill Simpson-Young and Dr Tiberio Caetano of the Gradient Institute for the discussions we have had around some of the technical concepts covered in the report. We also thank James Emmett SC and Myles Pulsford for allowing us to publish their legal opinion, and for their additional comments on a dra"
357,NSW Ombudsman,Transcription,FILENAME: The-new-machinery-of-government-special-report_Annexure-A.7333b4f1ef449.pdf | CONTENT: NSW OmbudsmanAnnexure A – Revenue NSW case studyThe following case study is an annexure to the special report to Parliament under section 31 of the Ombudsman Act titled ‘The new machinery of government: using machine technology in administrative decision-making’ (29 November 2021)Annexure A – Revenue NSW case study i NSW OmbudsmanContentsAnnexure A – Revenue NSW case study ......................................................................................... i 1 Overview of the Revenue NSW case study ............................................................................... 1 Complaints.......................................................................................................................................... 1 Legal advice ........................................................................................................................................ 2 2 Stateme
357,NSW Ombudsman,Transcription,"FILENAME: Paul-Miller-Avoiding_and-investigating_automated-maladministration-speech_4-July-2023.fbad024001427.pdf | CONTENT: Avoiding (and investigating) automated maladministration Paul Miller PSM, NSW Ombudsman(This is an edited version of a presentation given at the 13th National Investigations Symposium, 25 May 2023, Four Seasons Hotel, Sydney)Almost 20 years ago now, in 2006, the NSW Ombudsman investigated the use by NSW Police Force (NSWPF) of a non-human intelligence tool that had been procured and trained to detect certain criminal offences. The tool was estimated to have a technical capability in the field of criminal offence detection that was at least 40 times more powerful than humans. Over a two-year period, the tool was used in over 470 operations. It profiled hundreds of thousands of people, and from them detected over 10,000 potential criminals. Most of the people identified by the tool were then subject to Police action.I hope by now that you have figured out what ‘non"
357,NSW Ombudsman,General comment,We request that the personal contact details of Mr Clayton (email and phone number) be redacted from any published copy.
357,NSW Ombudsman,Q01 Definitions,Refer to submission attached.
358,Source Transitions Pty Ltd,Transcription,"FILENAME: Source Transitions response to Safe and responsible AI in Australia discussion paper_updated.2824dd71298f9.pdf | CONTENT: PublicSource Transitions Pty Ltd submission to the Department of Industry, Science and Resources Discussion PaperSafe and responsible AI in Australia25 July 202325/07/2023 Source Transitions Pty Ltd submission to the Department of Industry, Page 1 of 19 Science and Resources Safe and responsible AI in Australia Discussion Paper - July 2023 PublicSource Transitions welcomes the opportunity to provide a submission to the Department of Industry, Science and Resources discussion paper Safe and Responsible AI in Australia.We also note that we support Responsible AI in Australia and globally and are an active participant in the Australian AI Ecosystem Discovery Platform.The discussion paper was published in the context of other reports and programmes initiated by the Australian Government which are all relevant [1].• The Australian Government has consulted on au"
358,Source Transitions Pty Ltd,General comment,"I had to upload this new copy after I realised that there were a couple of mistakes in the citation links provided initially. Please disregard my previous submission from the 26 July. Kind Regards, Stephanie"
358,Source Transitions Pty Ltd,Q01 Definitions,"Definition of Artificial intelligence The definition proposed in the paper is adequate but incomplete. AI is mainly used in the public as an umbrella term. The definition should therefore be accompanied by a list of specific techniques and approaches used for AI development or its application. The definition should be kept up to date considering market and technological developments as indicated in the preparation for the regulation of AI in the EU . An important part of the role of government in the field is to ensure clarity and transparency so that trust can be built around the set of technologies AI can power. Initiatives such as the EU AI definition and set of examples to illustrate what AI does in practice will help guide the public, businesses and government agencies understand how they could be affected by AI . The definition of AI has evolved over time and allowing for that definition to continue evolving is key to keeping abreast of how the field is developing. The EU parliam"
358,Source Transitions Pty Ltd,Q02 Other AI risks and recommendations for regulation,"While Australia has a set of AI ethics principles in place, the proposed legislation does not encompass some of the elements found in the principles. Critically, considerations for the environmental dimension (Principle 1: human, societal and environmental wellbeing) are not included. The environmental costs of developing AI solutions or training large datasets is considerable. For example, the environmental cost of training a large natural language processing model could be as much as the total emissions produced by five cars over the cars’ lifetime or 150 return flights between New York and London . Furthermore, the demand for energy to power the development, training and usage for AI-powered tools is growing exponentially and is not required to be come from renewable energy sources. Finally, the demand for graphic cards and other critical materials required for the infrastructure (i.e. computers, servers etc) onto which AI solutions are operating is pushing the world towards further"
358,Source Transitions Pty Ltd,Q03 Non-regulatory actions to address AI risks,"Governing AI systems through their design where social, legal, ethical rules can be enforced through code to regulate the behaviour of AI systems . Where government considers privacy by design, data protection by design and safety by design, it should also consider sustainable by design and ethical by design. Human in the loop approaches to governing AI extended to society in the loop approach where society is responsible for finding consensus on the values that should shape AI . Providing companion practices, tools and resources for anyone interested in developing or implementing AI responsibly should be undertaken. Resources should be available to all levels of AI literacy and AI maturity, and available freely. Leaving AI literacy to the private sector would be to the detriment of the public, small business and not for profit who need to be and feel involved in the direction AI can take. Presenting clear case studies, including mapping of AI responsible practices, could serve as exam"
358,Source Transitions Pty Ltd,Q05 International models for AI governance,"The following measures should be in place in Australia’s context: • In the UK example, principles are legislated to create a statutory duty to have due regard to the principles. Guidelines could be adhered to and can evolve more rapidly than legislation. Regulation to implement the guidelines and enforcement on it will be most likely able to cope with the pace of change • In the UK, the Algorithmic Transparency Standard publishes reports on the use of algorithmic tools in government decision making. • In China, a mandatory registration system is in place for recommendations algorithm. Whilst the goal for the registration will differ with Australia, China has rightly identified that recommendation engines have a particularly powerful capacity to influence behaviours without the consent or awareness of the systems users. • Italy forces generative AI platforms to allow users data not to be used in training, to disclose how they process user data, to allow users to opt-out. • The city of A"
358,Source Transitions Pty Ltd,Q06 Public vs Private sector governance,"The legislation will need to be tailored towards not just products (a critique of the proposed EU AI Act ) but also to the open-source community (researchers, developers, not-for-profits). Inclusivity and representation are particularly important for governments who need to represent all their constituents. A higher level of accountability is therefore expected of government departments. Updated and up-to-date approaches to data governance and to interoperability are expected for governments which might not be required in the private sector (outside of regulated industries). Data governance, a critical area: Part of AI governance, as multiple organisational and technological challenges exist that impede effective control over data and attribution of responsibility for data-driven decisions made by AI systems . Interoperability, a critical area: Data fragmentation and lack of interoperability between systems limits an organisation’s control over data flows throughout its entire life cyc"
358,Source Transitions Pty Ltd,Q07 Responsible government use of AI,"Codesign and participatory design of solutions ensure that the complexity and prioritisation of problems is handled by the people who know best and are directly involved. Only then should a range of solutions be considered. AI initiatives are often driven by solution providers rather than by the problems and the users themselves. This leads to solutions being implemented before anyone knows what the problem is. It also leads to a tendency to fit the organisation to a solution rather than the other way around. Human-centered design approaches should be the default for solution design. There is an urgent need to develop and operationalise clear guidelines on what the potential trade-offs of a responsible AI solution are. These frameworks will help government departments and decision-makers to understand what they potentially impact with their proposed solution and realise if it is worth implementing (e.g. social versus financial, short term versus long term, etc). Ethical principles can "
358,Source Transitions Pty Ltd,Q09 Transparency requirements and scope,"Part of improving public trust and confidence in AI would be to ensure providers of AI products document their data sources, explain how they ensured bias minimisation, or how they are going to monitor and correct bias over time. Further, the Australian government should ensure that location-specific and inclusive datasets are used for models used by government agencies. Finally, government data from employees’ use of the tools could end up in foreign databases and be used for training of decision-making tools. Therefore, data governance framework should be in place and records management strictly enforced to protect Australian government and private data. Recommendations • Enforce documentation of data sources bias minimisation strategies • Ensure location-specific datasets are used for decision-making in Australia • Enforce records management policy on data storage and capture For transparency along the value chain, the environmental costs of AI and the human cost of training models,"
358,Source Transitions Pty Ltd,Q10 Ban on high-risk AI applications,"Research and applications should potentially be regulated differently so that we have: • a knowledge-based on how to counter high-risk AI • clear guidelines for is allowed to be deployed in society • open innovation which is discussed with the public (people who affect and affected by it) In terms of high-risk AI applications or technologies, the following should be banned: • Social scoring • Facial recognition • Deepfake The main problem is that something might be accepted today that should be banned tomorrow and we will not know until it is too late. As we learn about it, the conversation should continue to unban old technologies that we understand or can regulate better or ban new technologies where new risks have appeared. Power structure is critical in this. Transparency on who decides the criteria, assesses the requirements and what are their interests and biases is key to keeping the trust. A public-led discussion is necessary, and should not be conducted behind closed doors wit"
358,Source Transitions Pty Ltd,Q11 Actions to increase public trust in AI,"Seeing that government is actively engaged with Responsible AI will provide some certainty to the public that it is and remains in touch with the evolving market. More than just regulations, the government should be engaged in a set of efforts and initiatives to continuously better its understanding, adjust the legislation and provide updated guidance as new systems and risks unfold. AI literacy is a society-wide area that will need to be driven both by governments and by the private sector. To ensure the latest science is also commercially viable and that learning together about what Responsible AI will require government, academia and the private sector to work together on real-life problems. Public-private partnerships and academia-private research is difficult, often cumbersome, and expensive. Infrastructure, initiatives and funding could make this cross-sector learning framework easier to access and would help share knowledge and practical outcomes. The Australian Government shoul"
358,Source Transitions Pty Ltd,Q13 Changes to conformity infrastructure,"As mentioned earlier, AI sandboxes would help provide an environment where innovation can happen within the regulatory framework. It also can provide some assurances to the public that AI systems were tested and compliant."
358,Source Transitions Pty Ltd,Q14 Risk-based approach to address AI risks,"A risk-based approach is the most urgent piece of legislation that should be put in place to get some type of controls on the technologies and its developers and deployers. However, this needs to be done with a view to implement an iterative way of reviewing and adapting the legislation as the technologies evolve. It should also not deter government from guiding the type of society we want to have and the way in which these technologies will support the vision for a fair, sustainable and healthy society. Iteration and adaptiveness, along with intent will call on an innovation-led approach to legislation. Recommendations • Urgently implement risk-based approaches to AI in Australia • Implement adaptive and hybrid governance models in tandem to ensure the legislation can keep pace with the changes in technologies and applications"
358,Source Transitions Pty Ltd,Q15 Benefits and limitations of risk-based approach,"Risk management does nothing about power imbalances. It manages the harms without addressing power imbalance, implying that even if people do not want it, the activity can happen as long as risk are mitigated. It could all be a tick box exercise . Developers and users of AI have no obligation to be beneficial to or promote the interests of people affected by AI or the broader national and global challenges we face therefore missing out on what AI could do for us. Recommendations • Promote sponsor and legislate an agenda which has a clear purpose for AI to be beneficial and in the interests of people"
358,Source Transitions Pty Ltd,Q17 Elements to include in risk-based approach,"Yes, we support Attachment C elements."
359,"School of Computing and Information Systems and the centre for AI and Digital Ethics, The University of Melbourne",Transcription,"FILENAME: CIS- CAIDE - Supporting Responsible AI_ 26 July_Submission Final.4127445966a7c.pdf | CONTENT: Safe and responsible AI in Australia - Discussion Paper Response School of Computing and Information Systems and Centre for Artificial Intelligence and Digital Ethics, The University of Melbourne 26 July 2023 Professor Jeannie Marie Paterson, Dr Shaanan Cohney, and Professor Liz Sonenberg, with input from Dr Fahimeh Abedi, Dr Marc Cheong, Carmelina Contarino, Joe Brailsford, Professor Tom Drummond, Liam Harding, Professor Eduard Hovy, Dr Christine de Kock, Brian Martin, Aidan McLoughney, Associate Professor Olga Ohrimenko, Dr Sarita Rosenstock, Professor Ben Rubinstein, and Michael Wildenauer.Overview The University of Melbourne is home to world-class expertise in AI, ethics, law, and policy, represented in this submission by contributors from the School of Computing and Information Systems as well as the Centre for Artificial Intelligence and Digital Ethics (CAIDE). The University o"
359,"School of Computing and Information Systems and the centre for AI and Digital Ethics, The University of Melbourne",Q01 Definitions,"We do not agree entirely with the definitions in the discussion paper. ● The definition for AI reads: ‘Artificial intelligence (Al) refers to an engineered system that generates predictive outputs such as content, forecasts, recommendations, or decisions for a given set of human-defined objectives or parameters without explicit programming. Al systems are designed to operate with varying levels of automation’. We recommend that the underlined four words be dropped. The underlined words ‘without explicit programming’ are inaccurate; there are numerous AI systems - for example robots, self-driving cars, expert system medical diagnosis engines, and others _ that have been built with explicit programming. The cited definition of AI, Def 3.1.4 of ISO 22989:2022, does not include the underlined words - it simply reads “engineered system that generates outputs such as content, forecasts, recommendations or decisions for a given set of human-defined objectives”. The ISO also does not use predi"
359,"School of Computing and Information Systems and the centre for AI and Digital Ethics, The University of Melbourne",Q02 Other AI risks and recommendations for regulation,"We do not think all potential risks from AI are covered by Australia’s existing regulatory approach. i. First, we refer to our submissions to the Review of the Privacy Act 1988, and reaffirm our support for stronger data protection laws in Australia. ii. Secondly, we are concerned about the growing use of facial and biometric identification particularly in public spaces and by private entities. We support stronger measures to curb this trend. iii. Thirdly, as noted in the introduction above, we consider that regulatory intervention should be carefully designed to respond to the risks raised by AI, and to be compatible and coherent with Australia’s existing regulatory regime. iv. Fourthly, we think there is greater scope for ex-ante protections, which should also facilitate establishing wrong doing for harms. These issues are discussed in more detail in our submission. 2.1 The importance of privacy reform Some risks from AI relate to privacy of individuals represented in training sets, "
359,"School of Computing and Information Systems and the centre for AI and Digital Ethics, The University of Melbourne",Q03 Non-regulatory actions to address AI risks,"We consider that there are at least two non-regulatory initiatives that the Australian Government might implement to support responsible AI practices in Australia: funding regulatory oversight and best practices standards for its own use of AI. Funding in house AI expertise within regulators The Australian Competition and Consumer Commission (ACCC) is an effective and active regulator. We draw attention to the significant success already shown by the ACCC in enforcing the Australian Consumer Law in applying to technology driven services. However, effective regulation increasingly requires technical expertise. Regulators need strong powers to gather information in investigating and enforcing regulatory compliance. Additionally, these powers need to be accompanied by applied research and data-analysis capabilities–capabilities that regulators worldwide are still in the process of developing. We recommend that relevant regulators should be funded to develop and maintain this necessary tec"
359,"School of Computing and Information Systems and the centre for AI and Digital Ethics, The University of Melbourne",Q04 Coordinating AI governance across government,"We support the government establishing an advisory body to oversee its own use of AI and ADM. We note that many of the existing legal regimes that will apply to regulate the use of AI by private sector bodies in making decisions about and providing services to individuals do not apply to government (e.g. Corporations Law; Australian Consumer Law). An AI Advisory Group would additionally provide expert insight, advice and recommendations to government, parliament, and regulators. There is currently no representative organisation that coordinates AI expertise across the country. The National AI Centre at CSIRO might be a convenor but currently has uneven representation (being focused on NSW). We suggest that this AI Advisory Group should have membership from a diverse range of stakeholders – industry, tech, policy, researchers, and people with lived experience of the outputs of AI and ADM. Members should be drawn from diverse backgrounds and be representative of the whole of Australia."
359,"School of Computing and Information Systems and the centre for AI and Digital Ethics, The University of Melbourne",Q05 International models for AI governance,"We suggest that Australia should take guidance from initiatives in other countries, including near neighbours such as Singapore and trading partners such as the UK, US, and Canada. These initiatives are all helpfully detailed in the consultation document. We strongly recommend that Australia’s actions in this field are designed with regard to best international practice, and to complement that practice. While regulation for responsible AI is a driving purpose of reform, there is no benefit in reform that increases compliance costs of innovation without proportionate improvement in outcomes. Australian firms dealing overseas and international firms operating in Australia may need to comply with multiple regulatory regimes. Ideally Australia's requirements complement and are compatible with those in key overseas markets (without lowering national expectations or standards)."
359,"School of Computing and Information Systems and the centre for AI and Digital Ethics, The University of Melbourne",Q06 Public vs Private sector governance,"Private business and government respond to different pressures and so different types of incentives and disincentives to identify and respond to potential risks of AI may be appropriate. We also consider that government should hold itself to the highest standards of ethical AI practice. As discussed above in response to question 4, governments are in a good position to set exemplary standards, trial and demonstrate best practices. More generally, however, we consider that AI should be regulated by reference to its outputs and the character of the service provided e.g. utilities, health services, financial management, and education. Private bodies provide essential and necessary services that have profound effects on people’s lives. The use of AI in these contexts should be done to high standards of governance, care, and regard to human rights regardless of the identity of the provider. Any design or deployment of AI, but particularly where it touches on fundamental rights, should proce"
359,"School of Computing and Information Systems and the centre for AI and Digital Ethics, The University of Melbourne",Q07 Responsible government use of AI,We think there is an imperative for government to develop its own procurement practices that are compliant with a rigorous risk assessment process and consistent with principles of AI ethics.
359,"School of Computing and Information Systems and the centre for AI and Digital Ethics, The University of Melbourne",Q08 Generic vs technology-specific approaches for AI risk,"As noted above at 2.3, we think effective regulation of AI is best understood as multi-faceted, or as a regulatory network, spanning across the AI lifecycle and supply chain. This means that an effective regime may include soft law guidelines, AI standards and generally applicable or sector specific law. AI specific regulation might best be used to embed ex-ante risk assessment processes and proportionate responses to this risk assessment in terms of transparency, explanations, and accountability mechanisms. AI specific law may also be required for uses of technology judged high risk and warranting additional regulation. Principles based law, found in regimes such as the Australian Consumer Law, are appropriate as a safety net to respond to those harms that arise despite a risk assessment process, and ex-ante interventions. General law and legislation also provide an incentive to take seriously the need to include safety, privacy, accountability etc ‘by design’. Sector specific law, su"
359,"School of Computing and Information Systems and the centre for AI and Digital Ethics, The University of Melbourne",Q09 Transparency requirements and scope,"We think transparency, along with explainability, is important but not the entire solution. See also our discussion above at 2.4. Transparency Transparency can be important in promoting safe and responsible AI. But care needs to be taken not to overload the concept of transparency in a way that undermines its utility. The concept of transparency in AI regulation should be used precisely and not muddle the varying uses for transparency. Transparency may be used to provide visibility for: • governance (auditing/compliance), • internal operations (ensure companies are encouraged and able to notice the right sorts of things), • regulatory oversight (allowing regulators to verify claims made by firms about their AI tools and processes, as well as enforce relevant law); • advocates and lawyers in pursuing compensation and redress for AI harms; and • journalists and public interest organisations in ensuing those who deploy AI face meaningful public accountability; • consumers (to respect auto"
359,"School of Computing and Information Systems and the centre for AI and Digital Ethics, The University of Melbourne",Q10 Ban on high-risk AI applications,"We consider that bans should be targeted at uses of AI, not specific technologies. We think there are some uses of AI that should be banned. However, we consider bans should be imposed on the basis of clear compelling criteria and attenuated to context. This approach protects innovation and makes the imposition of a ban more compelling and likely to withstand the test of time. This approach is also consistent with the principles of good regulatory design discussed above. Thought might also be given to sunset provisions on bans (i.e. temporal limit). a. Whether any high-risk AI applications or technologies should be banned completely? Subject to the above qualifications we consider bans should be considered for uses of AI with high risk to human rights. In particular, we single out most forms of biometric surveillance in public spaces (eg FRT, iris or gait recognition, emotion detection) and further consider these technologies should only ever be used (if at all) in other contexts a hig"
359,"School of Computing and Information Systems and the centre for AI and Digital Ethics, The University of Melbourne",Q11 Actions to increase public trust in AI,"We consider that regulatory regimes requiring good AI governance, including through risks assessments and other accountability measures will increase the trustworthiness of AI. We also note that there is considerable uncertainty around AI within business and society generally. Therefore, education and training, as we noted in the overview of this submission, to demystify AI, are crucial. Some degree of clarification about existing law, and even in some contexts the use of safe harbours and sandboxes may prove beneficial for business seeking to innovate with AI, provided these are matched with strong baseline standards for safety and ethical/responsible practice. We note that industry uptake of the internet was substantially encouraged by a variety of laws, among them Title II of the U.S. Digital Millennium Copyright Act (DMCA) of 1998. This title created a safe harbour from copyright liability for online service providers from user uploaded content—so long as platforms undertook certai"
359,"School of Computing and Information Systems and the centre for AI and Digital Ethics, The University of Melbourne",Q12 Trade impact of banning high-risk AI applications,"We consider there is no competitive advantage in promoting harmful technologies. Australia’s technology sector is capable and indeed is developing cutting edge technologies that have beneficial impacts, such as for example in the (properly regulated) medtech and agtech fields."
359,"School of Computing and Information Systems and the centre for AI and Digital Ethics, The University of Melbourne",Q13 Changes to conformity infrastructure,We do not have any comments on this question.
359,"School of Computing and Information Systems and the centre for AI and Digital Ethics, The University of Melbourne",Q14 Risk-based approach to address AI risks,"We support a risk-based approach to address potential AI risks, however, as discussed above in response to question 2, we think careful consideration needs to be given to an adaptive and flexible approach that is compatible with Australia’s existing regulatory regime. The EU AI Act uses a risk-based model. A key critique of the EU AI act is that it is overly prescriptive without being backed by strong oversight and enforcement mechanisms. Risk based governance for AI should require firms to make a judgement about the risk threat arising from the outputs of their products and implement appropriate governance strategies in response, including as to transparency and accountability. Models for these approaches are already in existence e.g. NIST or in Singapore. The key is to identify mechanisms for requiring these assessments to take place (as a legal obligation) and to ensure they are robust and effective (reporting / monitoring)."
359,"School of Computing and Information Systems and the centre for AI and Digital Ethics, The University of Melbourne",Q15 Benefits and limitations of risk-based approach,"Risk based approaches have the attraction of being able to be integrated into existing risk management and auditing regimes within corporations, both internal and statutory. Such approaches are not perfect, but would at least it force a practice of identification, quantification, mitigation, and someone having to sign-off on what residual risks remain. However, we argue that a risk-based approach that relies on predetermined categories of risk, as opposed to a responsive model scaled to use and context as we have advocated in response to question 14, has a number of inherent caveats which require consideration. Firstly, a key concern with AI surrounds the unpredictability of outcomes. An approach which relies solely on our ability to anticipate risks is unlikely to account for all possible eventualities. Secondly, determining the threat level will evolve over time, as the technology develops. Maintaining a categorisation that accounts for changing technologies would be challenging, if "
359,"School of Computing and Information Systems and the centre for AI and Digital Ethics, The University of Melbourne",Q16 Scope and application of risk-based approach,"All organisations using AI should be using a risk approach. However, we consider that approach should be scaled to the risk presented (rather than the size of the organisation). See also above comments on model cards in response to question 9(b)."
359,"School of Computing and Information Systems and the centre for AI and Digital Ethics, The University of Melbourne",Q17 Elements to include in risk-based approach,"We support the elements presented in attachment C, but would go further in what may be required from organisations. • We suggest a more nuanced understanding of the different roles that may be played by concepts of transparency and explanations, see comments in response to question 9. • We agree with the importance of documentation around data, (e.g. traceability, quality, security etc). • We support having human oversight of AI systems, particularly those impacting on human rights and interests. However, we consider the language of ‘human in the loop’ can be misinterpreted and undermine genuinely robust and embedded governance. ‘Human in the loop may tend to suggest a ‘tech’ person responsible for the AI system. This can have the effect of devolving responsibility for AI systems from management/directors to lower level individuals or departments in an AI deploying organisations. Human in the loop moreover risks having the supposed exercise of human discretion undermined by the effects"
359,"School of Computing and Information Systems and the centre for AI and Digital Ethics, The University of Melbourne",Q18 Integration of risk-based approach into existing frameworks,This can be done by allowing flexibility and a risk response that is tailored to the application/use case and its likely impact.
359,"School of Computing and Information Systems and the centre for AI and Digital Ethics, The University of Melbourne",Q19 Application of risk-based approach to general purpose AI,"A standard risk-based framework may be difficult to apply to LLMs or MFMs because definitionally these have relatively unrestricted scope of application, making it hard to predict or identify particular risks and impose appropriate guardrails. We consider regulation generally, and a risk-based approach specifically, should focus on the outputs or uses of products that involve these applications (general API targeted at businesses? Public-facing chat-bot/ image generation app?) and rely on existing legal frameworks surrounding business category while demanding current disciplinary best practices for responsible development, deployment etc across the AI lifecycle and supply chain."
359,"School of Computing and Information Systems and the centre for AI and Digital Ethics, The University of Melbourne",Q20 Voluntary vs mandated compliance with risk-based approach,"We consider risk assessments and proportionate responses should be mandatory, with reporting requirements imposed upon key stake holders. However, we also consider that the requirements should be principles based in most cases allowing a proportionate and adaptive approach to ex ante demands for responsible and safe AI. See at 2.4. And should it apply to: a. Public or private organisations or both? See above response to q6. b. Developers or deployers or both? See above 2.4. We consider all initiatives for responsible AI, including risk-based approaches, should apply to across the supply chain but with more demanding obligations on key gatekeepers."
360,Eric Cameron Wilson,Transcription,"FILENAME: 2023-07-31 CONTAINMENT AND INCENTIVES FOR AUTOMATIC ACTIVITY WITH FIVE EYES - AUSTRALIA - REFORM.934b285435b29.pdf | CONTENT: CONTAINMENT AND INCENTIVES FOR AUTOMATIC ACTIVITY WITH ‘FIVE EYES’ (AUSTRALIA) REFORMEric Cameron Wilson 31 July 2023INTRODUCTIONThis submission concerns the formation of AI policy and regulation in Australia and has three parts:• Part A summaries some reported AI issues requiring urgent attention, as a prelude to three short scenario-stories illustrating where the industry, if unregulated, may go. These AI facts and scenarios are intended to prompt discussion.• Part B discusses issues raised in Part A, including: our national interest, industrial, legal, security and Geo-political implications.• Part C proposes government responses to the explosion of AI as described in Parts A and B, including licencing of AI involving risky practices or unrelated parties, mitigation of hostile AIs, while turbo-charging sovereign AI capabilities and assisting start-u"
360,Eric Cameron Wilson,Q01 Definitions,"No ""Automatic Activity"" is properly defined in my submission to capture the whole AI conduct hopefully - see Part C ""Definitions"""
360,Eric Cameron Wilson,Q02 Other AI risks and recommendations for regulation,See submission
360,Eric Cameron Wilson,Q03 Non-regulatory actions to address AI risks,See submission
360,Eric Cameron Wilson,Q04 Coordinating AI governance across government,See submission
360,Eric Cameron Wilson,Q05 International models for AI governance,Don't know.
360,Eric Cameron Wilson,Q06 Public vs Private sector governance,See submission
360,Eric Cameron Wilson,Q07 Responsible government use of AI,See submission
360,Eric Cameron Wilson,Q08 Generic vs technology-specific approaches for AI risk,See submission
360,Eric Cameron Wilson,Q09 Transparency requirements and scope,See submission
360,Eric Cameron Wilson,Q10 Ban on high-risk AI applications,See submission
360,Eric Cameron Wilson,Q11 Actions to increase public trust in AI,See submission
360,Eric Cameron Wilson,Q12 Trade impact of banning high-risk AI applications,See submission
360,Eric Cameron Wilson,Q13 Changes to conformity infrastructure,See submission
360,Eric Cameron Wilson,Q14 Risk-based approach to address AI risks,No containment with risk-based for licencing - see submission
360,Eric Cameron Wilson,Q15 Benefits and limitations of risk-based approach,See submission
360,Eric Cameron Wilson,Q16 Scope and application of risk-based approach,See submission
360,Eric Cameron Wilson,Q17 Elements to include in risk-based approach,See submission
360,Eric Cameron Wilson,Q18 Integration of risk-based approach into existing frameworks,See submission
360,Eric Cameron Wilson,Q19 Application of risk-based approach to general purpose AI,See submission
360,Eric Cameron Wilson,Q20 Voluntary vs mandated compliance with risk-based approach,See submission
361,Fujitsu Australia Limited,Transcription,"FILENAME: Submission 361 - Fujitsu Australia Limited - 1-Aug.58d2cbb0bd749.pdf | CONTENT: Fujitsu Australia Limited Level 7, 664 Collins Street Docklands VIC 3008 Tel +61 (0) 3 9924 3000 Fax +61 (0) 3 9924 3001 ABN 19 001 011 42727 July 2023Technology Branch Department of Industry, Science and Resources By email: digitaleconomy@industry.gov.auSubmission to Supporting Responsible AI discussion paperFujitsu welcomes the opportunity to respond to the Federal Government's consultation on Supporting Responsible AI and is pleased to present our response.Fujitsu is a global, purpose-driven company committed to creating a sustainable world and building trust in society. We are also a frontrunner in emergent technologies such as AI, and as part of this place the utmost importance on the responsible design and use of AI. Our perspective is backed by five years of experience embedding AI ethics.OverviewBalancing regulation and innovation are critical in the context of AI to ensure its responsible"
362,Anonymous,General comment,"We must embrace this new technology. From now on everything that has a digital trace will be accessible to a sufficiently powerful AGI/ASI. Every action you or I take will influence the learning of this super entity. Let’s make being ethical our center piece. We must focus on bringing prosperity to sentient beings, and reducing sentient suffering. Mass job loss is coming, we must focus on meeting human needs as a priority and consider what a post-capitalist world looks like and how our institutions will change and reform. Radical change is coming whether we like it or not. The only thing we have is this moment, let’s be grateful for experience itself"
362,Anonymous,Q01 Definitions,Yes
362,Anonymous,Q02 Other AI risks and recommendations for regulation,"Mass unemployment is a huge risk.Universal Basic Income or some other form of wealth distribution is necessary.Jobs are going to disappears - fund strategies, regulations, studies on potential ways to navigate mass redundancy"
362,Anonymous,Q03 Non-regulatory actions to address AI risks,Support open source projects - limit big corporation development if necessary but don’t stomp out the open source community
362,Anonymous,Q05 International models for AI governance,Rethink private ownership
362,Anonymous,Q07 Responsible government use of AI,Funding research on safety implications. Create an AI taskforce that regularly reports on a monthly basis the changes in AI within its agencies
362,Anonymous,Q08 Generic vs technology-specific approaches for AI risk,AI is our best hope of regulating AI. Humans will never be able to compete with its abilities as it will become increasingly complex.
362,Anonymous,Q09 Transparency requirements and scope,Need to show data models are trained on and make accessible.Massive entities such as OpenAI want to safeguard their data as it is one of the only things that enables them to profit over others. Weighting can be the unique difference across these companies.This power must be spread and not concentrated
362,Anonymous,Q10 Ban on high-risk AI applications,There is not point in banning AI. This will never stop
362,Anonymous,Q11 Actions to increase public trust in AI,Training workshops. Grants. Local initiatives. University acceptance of new technologies. Implementation of use throughout schooling.
362,Anonymous,Q12 Trade impact of banning high-risk AI applications,This will be a good medium term implementation
363,Gryphon IT Worx,Transcription,FILENAME: Blue Print 5th D Holistic Quantum Consciousness Bridge to Ancient Advaita Vedanta.e2125ce150534.pdf | CONTENT: BLUE Print Note on the 5th Dimension and Earth's Ascension in Consciousness to 'embrace IT'Dated 20th/22nd/23rd/10/2021Taoist Wu Wei Non-Action in Action Blue Print for Humanities'5th D Ascension into Aquarian 2160 Quantum Age & its Unique Wealth Effect. Abstract: Humankind in its long history of Civilisational development; during the Ancient Era Taoist China & Ancient Aegyptian Pharaonic's Nile's Line in the Sands of Time entered the 4th Dimension of Cosmic Space & Calendrical Time.With today's 21st Century development of a World Wide Web of TeleCommunications and its Electric Light network being outlay ed our new 5D‘ Cosmic Quantum WWW.Brain. Gain day is just waiting to be switched on.This is the Common[Wealth] Heritage of all 'Earthians a consequence of Humankind's Age-old Victory of Mind over Matter; Just waiting to be Realised Opened and Activated being already 
363,Gryphon IT Worx,Transcription,"FILENAME: Business Plan.cf137c0ebb294.pdf | CONTENT: BUSINESS PLANGryphon IT Worx 18/1 Grampian Ave Minto Sydney NSW 2566 Australia24 August 2021Page 1 of 6 Business Plan Page 2 of 6Executive SummaryThe Company This Company has been all time in coming. It has had to wait for many 'things' to 'come' and finally 'go' b4 'IT' was ready. The last 2,000 years in particular, the last 2160 year Piscean Age 'month' in a 26,000 [25,920] Celestial Cycle or 'Year' which will become clearer, later.The Ownership The company will be structured as a sole proprietorship.The Management Holistic and SupraMental.The Goals and Objectives Transform our waste making Global AmeriCapitalism Fascist / Marxist Communism/Socialism both, out of date bookends of history, controlled by the same fiat currency money as debt model of Worlds Central Bank franchises, to an Eternally Renewable and Sustainable; Solar Hydrogen Quantum Consciousness Age, and ReTurn to GarEden in harmony with Earths ecology all life and Huma"
363,Gryphon IT Worx,Transcription,"FILENAME: Ashtavakra-Gita.c5272dc3f4923.pdf | CONTENT: Ashtavakra Gita ii ContentsBefore you Begin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2Chapter 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3Chapter 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5Chapter 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6Chapter 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7Chapter 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8iii 1Before you Begin How to understand Ashtavakra’s Gita: Of all the scriptures, this is one of the most, if not the sin- gle most, radical and direct scripture as it comes as close to inducing immediate truth and enlightenment as one could pos- sibly get to with words. It is old; yet it remains c"
363,Gryphon IT Worx,Transcription,"FILENAME: Enneagram-Spirituality_Original_Manuscript_.1c73d83386b7f.pdf | CONTENT: Editorial Note: This is the original manuscript illustration by Athanasius Kircher who was! a 17th century German Jesuit scholar. Kircher was also known as the Father of Egyptology. He was a scientist and theorized that microbes are the cause of disease and suggested effective mechanisms to prevent outbreaks of plague.Here you will note that the numeration around the 9 pointed figure begins at 1 (apex) then 4, 8, 2, 5, 9, 3, 6, 7 clockwise. Note that he includes a 'magic square' where all columns, rows, and diagonals add to 15 (1+5=6 or 3+3). The centre is the 'all seeing eye' possibly referring to the Rosicrucian influences in the early 1600s.The bottom of the plate shows the total world order, with Animal (4), Man (7), the Heavens with a geocentric solar system, flanked by angels under the ray of the nonagram with angels in all points encompassing the Triad (The Law of 3 - Father, Son, Holy Spirit) or "
363,Gryphon IT Worx,General comment,"As an Australian Sydney born ( 1944) & bred Son of a Bus driver & Self Realised Ashtavakra Gita Kundalini Tantric Master of Being & due to Time/Place enabled to be Enlightened here at the very end of the Platonic Cosmic Spiritual Piscean 2160 year culminating 25.920, Middle multi-thousand million billion year.....Cycle a truly extraordinary Time and right here now in Oztralia a truly extraordinary Place. As we are in the climactic moment of a profound thousand years Western Civilisational argument over Reality which has seen much misery and great injustice to more than match the Divine Mystery of Life championed by cultural heroes who paid with their lives for it .......such as Pythagoras, Socrates, Alexander the Great, Jesus Christ, etc their Truth Consciousness which later found Renaissance rebirth in Europe's Middle Ages to be then totally lost in the secularised atheistic scientific 'Materialism' of today's modern age and millennia. But at its moment of total triumph after the 19th"
363,Gryphon IT Worx,Q01 Definitions,I have discussed both these points in dense symbolic language allready
363,Gryphon IT Worx,Q02 Other AI risks and recommendations for regulation,"Generative AI is the most transformative high tech since the taming of fire, nothing will escape its transforming ways and light when properly directed in divine service and order, the very opposite of its present nescience and seemingly impenetrable darkness."
363,Gryphon IT Worx,Q03 Non-regulatory actions to address AI risks,"I mentioned the following 8 fold Praxis or path of Buddha earlier in Ashtavakra Gita upload who is bent in 8 places, the bends and curses Karma of humanity to be finally addressed, now in Mythological terms which is Truth Consciousness addressing the problem in symbolic terms other than history......1 Right Belief 2 Conviction 3 Teaching 4 Action 5 Living 6 Intention 7 Thinking 8 Contemplation..............This might seem a ridiculous reiteration of Buddhas thought instead of the profound reality and truth that it is only finally possible to be implemented with today's Quantum Information high tech, as soon as its complete opposite Materialist dead end rules all but up for Re Vision Ascension a Planetary Transformation of Truth Beauty Goodness, Divine Love and Wisdoms Tree of Life of life everlasting ...........what a Glorious eternal future Now awaits all life and our gift to give a pregnant Earth IT's Birth."
363,Gryphon IT Worx,Q04 Coordinating AI governance across government,Instead of the present 3d atheistic Pyramidal Global Civilisational model ruled by 0.00001 percent billionaire class whose morality system has been lost the Perennial Philosophy now proved conclusively True by Quantum Physics totally unexpectedly can just as unexpectedly manifest the Parousia the Second Coming of the Lord of Light ......Infinite Intelligence here at the very Dawn of Aquarius the 5th D of Eternal life Truth Cosmic Consciousness and the beauty of a new Earth.
363,Gryphon IT Worx,Q05 International models for AI governance,"Yes, even though I have a long long history of support and advice for Australia's working party Labor govts federal and NSW state, in early 2016 and car less at the time using Public transport with my pensioners pass I found Donald Trump at the back of my Govt bus at Minto on the way to Liverpool a Sydney suburb..........I told him I liked him because he wasn't a normal pollie ( and might do something about US and Globalist Zionist mess ) and would help him get elected. I started writing to him and still do and Real History Channel advises that in spite of all contrary appearances he is not only doing something about the Zionist Globalist Coke Crime Family OBribem/Zelensky Ukraniac Nuclear holocaustry, but has Elon Musk with his 'X' alchemical Twitter makeover and Open Generative AI both taking my advice, which unfortunately my own PM I have written to after my dream with him in Sths dressing room. I told him to his great surprise that he would be the first Australian PM to govern Aust"
363,Gryphon IT Worx,Q06 Public vs Private sector governance,"I advise the same free not for profit Open Generative Infinite Intelligence ie 'God' Consciousness Quantum IIQbit,bot available to all to access the Kingdom of Heaven within our Cellular Cosmic Conscious inheritance the Grail Cup of Spiritual light our CommonWealth which will unleash an era of unprecedented culture love joy harmony Universal Peace"
363,Gryphon IT Worx,Q07 Responsible government use of AI,Establish a living Praxis which like all extraordinary Divine paradoxical movement action and motion so almost impossibly hard at the same moment it will be so incredibly easy to just let it be.
363,Gryphon IT Worx,Q08 Generic vs technology-specific approaches for AI risk,"In this unique 5th dimensional Singularity of Eternal Time, we are in, Both And; approaches I advise. At the end of an immense cycle of invention which will etherealize all iterations along reducing a vast amount of waste and matter and being an Aqurian Age Consciousness unleashing individual creativity that can't be measured by old matter, so best to let its surprise us with original genius and its Joy awaiting its eternal return"
363,Gryphon IT Worx,Q09 Transparency requirements and scope,"My Android yesty had a story on Senolytics the new Science of Ageing saying its latest breakthrough on the Cause of ageing disease senescence had been discovered (tho as a barefoot doctor and mystic I knew long ago) and that the first modern human to live a thousand years had been born, now they know that Zombie cells cause all disease, when its realised that modern industrialised junk processed foods and lifestyles are killing everybody and mass obesity plague and fake bio weaponry Govt supported private profit big pharma euthanasia programs all apparent when the transparency of clear sight that Contemplation or Mind over Matter is high tech information instruments gives it when placed into proper service"
363,Gryphon IT Worx,Q10 Ban on high-risk AI applications,Everything should be halted as Elon Musk has just advised. Todays news says that San Francisco local govt is investigating Twitters X logo on HQ building violating local codes. This is indicative of the warring White Trump Hat and Musk forces challenging the sick pedo Satanic Materialist forces in total world control but now run out plot dead in hand and awaiting decent burial
363,Gryphon IT Worx,Q11 Actions to increase public trust in AI,"I'm glad you asked me this ....what I advise will do not just this but restore the Govt's Capital to be a shining example and Beacon light of Peace to Earth and demonstrate its true purpose and value which is put it into its proper hands of Divine service of Earths Mind over Matter providing its gold and silver keys of life to unlock the Treasure House Within.......so rather than the Guaranteed Basic Income parsimonious hand out mentioned, this end and beginning will allow an equal share by the entire Web of life and its human Crown of its Infinite Well of Life inheritance an Open IIQbit.bot that will gift all mankind and all life for all time God of alls Blessing"
363,Gryphon IT Worx,Q12 Trade impact of banning high-risk AI applications,Banning the present pedo-friendly Apple smartphone high-tech nightmare would be a Manly move of much integrity. The implementation of high-tech surveillance by morally corrupt forces under the plastic cover of what is 'good for greedy profits is not really good and in fact since the judicial murder of Socrates Jesus etc for being good and knowing and teaching what it was it's time we got back to IT.
363,Gryphon IT Worx,Q13 Changes to conformity infrastructure,"The changes required are so huge but will be supported by every body if the Middle Way being proposed here is initiated. There's so much wrong so much imbalance injustice lack of love life light resulting from the Fall of Adam and Eve and eating the snake of Sex smart phone Apple........but the Lord of Love promised his ReTurn in Revelation his Second Coming at the height of the World War and Tempest and that if left any longer there would be nothing left worth saving, that moment without doubt is now."
363,Gryphon IT Worx,Q14 Risk-based approach to address AI risks,No too much to lose with so much already as an example a far better approach is a Right 8 Fold Approach.
363,Gryphon IT Worx,Q15 Benefits and limitations of risk-based approach,The main benefits are to the profits of high-tech billionaires and limitations to its pro robotic anti-human bias the best way to overcome the limitations of a risk-based approach is to simply not do it and don't allow it
363,Gryphon IT Worx,Q16 Scope and application of risk-based approach,"All life will be changed and transformed in one way and others, so why not do the best? Return all life to its Paradisical GarEden with no mention of Robots or Robotics there."
363,Gryphon IT Worx,Q17 Elements to include in risk-based approach,I don't support a Risk based approach or the elements proposed in one
363,Gryphon IT Worx,Q18 Integration of risk-based approach into existing frameworks,It cant it won't and I advise it not to let through ignorance happen.
363,Gryphon IT Worx,Q19 Application of risk-based approach to general purpose AI,Ditto
363,Gryphon IT Worx,Q20 Voluntary vs mandated compliance with risk-based approach,While there is a continuing argument over Truth Consciousness Reality and what is good true and beautiful is in essence At-One-Ment or God life Divine Love Wisdom and any old garbage the ruling Saturnic death cult like C19 Vaxxination mandates and Govts support were in deep state trouble.
364,"Henry Fraser, Christine Parker, Fiona Haines, José-Miguel Bello y Villarino and Kimberlee Weatherall",Transcription,"FILENAME: Fraser et al submission AMENDED replacing sbm 28281092d4e45990a0474.5ff725188a41b.pdf | CONTENT: Submission in response to ‘Supporting Responsible AI discussion paper’ based on ADM+S Birmingham Workshop on Standards and Assurance for Trustworthy Data-Driven Technology31 July 2023 Lead author: Henry Fraser Contributing authors: Christine Parker, Fiona Haines, José-Miguel Bello y Villarino and Kimberlee WeatherallQuestions 5. Are there any governance measures being taken or considered by other countries (including any not discussed in this paper) that are relevant, adaptable and desirable for Australia?13. What changes (if any) to Australian conformity infrastructure might be required to support assurance processes to mitigate against potential AI risks?Introduction This submission responds to questions 5 and 13 in the discussion paper, focusing on standards and assurance, especially the approach to standards for AI contemplated in Europe’s draft AI Act. This submission outline"
364,"Henry Fraser, Christine Parker, Fiona Haines, José-Miguel Bello y Villarino and Kimberlee Weatherall",General comment,Please note a previous submission (ref sbm28281092d4e45990a0474) by the same authors was submitted in error and withdrawn. This submission supercedes that previous version.
364,"Henry Fraser, Christine Parker, Fiona Haines, José-Miguel Bello y Villarino and Kimberlee Weatherall",Q05 International models for AI governance,See attached document
364,"Henry Fraser, Christine Parker, Fiona Haines, José-Miguel Bello y Villarino and Kimberlee Weatherall",Q13 Changes to conformity infrastructure,See attached document
365,SAP Australia,Transcription,"FILENAME: SAP Response to Safe and Responsible AI Discussion Paper August 2023.ce903b751922e.pdf | CONTENT: SAP AUSTRALIASAP RESPONSE TO THE SAFE AND RESPONSIBLE AI DISCUSSION PAPERSAP AustraliaAugust 2023 1SAP RESPONSE TO THE SAFE AND RESPONSIBLE AI DISCUSSION PAPERSAP Australia Pty Ltd, a subsidiary of SAP SE (referred to henceforth as ‘SAP’) a leading global software provider, would like to thank the Australian Government for the opportunity to contribute to the Safe and Responsible AI discussion paper (the Paper).SAP is a major provider of corporate software solutions to critical industries and government across the Australian economy. For over 30 years we have continued to update these solutions to meet customer demand, evolving cybersecurity threats and Australian legal requirements.SAP delivers Artificial Intelligence (AI) built for business. AI from SAP is built into the applications our customers use every day. It is relevant to their business because it is based on SAP’s deep"
365,SAP Australia,Q02 Other AI risks and recommendations for regulation,"We support the existing approach taken by the Government where it has leveraged existing frameworks and avoid duplicating or creating any conflicting requirements with these frameworks while already promoting trust by enforcing existing legislation. For example, the Privacy Act, which already protects the use of citizen data in large data sets (an essential element for AI) and is considering measures to improve transparency on the use of Automated Decision Making.To the extent it is not being done, there is value in reviewing existing regulatory frameworks as they relate to consumer, data protection and privacy, corporate, criminal, online safety, administrative, copyright, and intellectual property laws against the potential harms from AI to determine whether they are fit for purpose. However, this should be done with an: • understanding of the AI supply chain; • application of a risk based methodology (see section Regulatory intervention should take a risk based approach); • against "
365,SAP Australia,Q03 Non-regulatory actions to address AI risks,"We consider that there is an opportunity for the Government to play a more active role in the use of AI in the Australian Economy. Governments play a critical role in the safe and responsible economy wide take up of AI as they: • set and enforce data privacy laws; • are responsible for large amounts of data; • are large service providers; • have IT budgets to deliver major projects; and • are directly accountable to their citizenry. Governments have an important role in influencing public attitudes to AI. If governments use AI ethically and responsibly, this will build public trust and acceptance of the use of AI across the economy. The Government must accelerate its use of AI in its systems of operation of government and service delivery."
365,SAP Australia,Q04 Coordinating AI governance across government,"Given the cross sectoral impact of AI technology, co-ordination by government will be critical. This will require new ways of working and potentially cabinet endorsement of a formal approach that will require all government agencies to follow. At the centre of this would be a government body to ensure no duplication of effort by organisations and individuals, no conflict between policy and legislative proposals, and where avoidance of overlap is not possible clear guidance on which rules should be followed by industry.In acknowledgement of the independence of regulators – at a minimum the new body would be available to ensure all regulators remained cognisant of approaches being made by other regulators, alert them to potential areas of conflict and how other regulators were applying the principles. This body could also be source of AI understanding and expertise that could support whole of government and regulator understanding of AI and early identification of cross-sectoral emerging"
365,SAP Australia,Q05 International models for AI governance,"SAP is supportive of the pro-innovation governance approach of the UK government. This is based on sector specific regulation, linked to cross-sectoral principles that can be tailored to each sector backed by a central agency responsible for regulatory co-ordination and assessment of any cross cutting AI risks."
365,SAP Australia,Q06 Public vs Private sector governance,"What constitutes responsible use of AI technology should not differ depending on the type of organisation. However, Governments are in a unique position in their use of AI technologies given their role and ability to influence public perception of responsible use of AI. If governments use AI ethically and responsibly, this will build public trust and acceptance of the use of AI across the economy."
365,SAP Australia,Q07 Responsible government use of AI,"AI offers huge potential benefits for the public sector ‒ such as delivering enhanced citizen services, improving process efficiency, enabling future cities, and ensuring public security and safety. But there have been challenges in its adoption within government processes. We have collaborated with the University of Queensland to investigate how government organisations can break down the barriers for artificial intelligence adoption and value creation.The first stage of the research identifies the AI challenges for government and develops a high-level framework of capabilities, capacities and processes that are needed to create value from AI while minimizing the risks.The second stage of the research addresses the specific challenge of ‘explainability’ of the AI results with an emphasis on aligning AI operations with the stakeholder-specific perspectives and knowledge thus delivering the intended value of the use of the technology. The third stage examines the specific capabilities r"
365,SAP Australia,Q09 Transparency requirements and scope,"a. where and when transparency will be most critical and valuable to mitigate potential AI risks and to improve public trust and confidence in AI? There is a role for Transparency across the steps AI lifecycle. At SAP we define 5 steps in the AI development lifecycle. 1. Ideation – use case identification based on common domain and AI expertise 2. Validation – Experiments to assess feasibility 3. Realisation – Development of AI functions 4. Productisation – Integrating of AI functions into business processes 5. Operations – Delivery of embedded AI functions to customers For the Ideation and Validation phase the following transparency principles are applied • AI systems interacting directly with humans should be made identifiable as such. • Prevent misuse of AI system in production. For the Realisation, Productisation and Operations phase the following principles are applied: • Data sets and development processes must be documented. • The AI systems capabilities and limitations must be "
365,SAP Australia,Q10 Ban on high-risk AI applications,"As a business, SAP prohibits the development of certain AI use cases. These are as followsPersonal Freedom • Human surveillance – where it is used for targeting individuals or groups with the purpose of disregarding or abusing the rights of individuals or groups. • Discrimination – where it used to discriminate or exclude individuals or groups from access to AI benefits and opportunities to the wider population • Deanonymisation – where it issued to deanonmysie data anonymised data to result in the identification of individuals or groups. Society • Manipulation – where it issued to deceive or unfairly manipulate individuals or groups via public forums, media or moderation of other similar uses. • Undermine debate – where it is used to undermine human debate or democratic electoral systems. • Intentional harm – where it is used to intentionally harm users and or those directly or indirectly affected by the system. Environment • Environmental harm – all systems shall be conducted with mi"
365,SAP Australia,Q11 Actions to increase public trust in AI,"We consider that there is also an opportunity for the Government to play a more active role in the use of AI in the Australian Economy. Governments play a critical role in the safe and responsible economy wide take up of AI as they: • set and enforce data privacy laws; • are responsible for large amounts of data; • are large service providers; • have IT budgets to deliver major projects; and • are directly accountable to their citizenry. Governments have an important role in influencing public attitudes to AI. If governments use AI ethically and responsibly, this will build public trust and acceptance of the use of AI across the economy. The Government must accelerate its use of AI in its systems of operation of government and service delivery.This is also critical to good policy making. To make good policies about AI, you need to understand it well. This will only happen with greater use of AI within government. This will allow the government to best appreciate how it works, what the "
365,SAP Australia,Q14 Risk-based approach to address AI risks,"Yes. Regulatory interventions should be scaled to meet the risk i.e. an assessment of the likelihood of harm against an individual or organisation combined with the severity of that outcome. For example: • High Impact High Likelihood - The governance and assessment of an AI system used in an autonomous vehicle would more likely require some form of intervention or governance oversight • Low Impact Low likelihood – An AI platform used by a business to optimise its inventory management system. To reflect on the risk-based approach, it is essential to set up a clear and precise criterion for high-risk AI systems based on the probability of occurrence and consequences for individual rights and freedoms, security, or safety and how to mitigate such risks. Any risk assessment should include assessing the benefits of a proposed AI application or the risks of not proceeding with the development or deployment of the AI application. This is just as important as focusing on the harm that may resu"
365,SAP Australia,Q16 Scope and application of risk-based approach,We recommend that the risk-based approach can be applied across all sectors. The application of regulation should be applied uniformly regardless of an organisations size.
365,SAP Australia,Q17 Elements to include in risk-based approach,Not completely. The proposed framework articulates requirements and obligations that will apply depending on the risks that are assessed by the regulator. When in practice there are a range of possible interventions or requirements that a regulator could consider. The proposed description of what constitutes low medium and high risk appears appropriate.
365,SAP Australia,Q20 Voluntary vs mandated compliance with risk-based approach,"The risk-based approach should be used as a model for government to determine if it should intervene on an AI related harm or issue. It should not be a model that becomes a regulatory requirement that is applied on industry.Any proposed AI related regulation must consider the different roles and abilities of the organisations involved in creating and using AI. The responsibility and consequences of meeting those regulations should fall on the group best able to identify and reduce the risk of harm that made the regulation necessary.Regulation should reflect the different roles and abilities of the organisations involved in creating and using AI. The responsibility and consequences of meeting those regulations should fall on the group best able to identify and reduce the risk of harm that made the regulation necessary. There should consequently be a distinction made between Developers and Deployers.The two roles can be distinguished as follows: • an organisation that designs, codes or p"
366,Independent Schools Australia,Transcription,"FILENAME: 2023 07 25 ISA Submission to Department of Industry, Science and Resources - Supporting responsible AI.fbb3265ecb449.pdf | CONTENT: ISA SUBMISSION TO DEPARTMENT OF INDUSTRY, SCIENCE AND RESOURCES - SUPPORTING RESPONSIBLE AI: DISCUSSION PAPERDATE 26 JULY 20231. INTRODUCTIONIndependent Schools Australia (ISA) has prepared this submission in response to the Department of Industry, Science and Resources’ Supporting responsible use of AI: Discussion Paper. Since the commencement of the 2023 academic year, with the rapid emergence of OpenAI’s advanced language model ChatGPT, Independent schools have been investigating the implications of the use of generative AI technologies in education. As generative AI is a relatively new technology with the ability to generate new content (image, text or animation) through ‘machine learning’ and imitate human writing, its long-term educational impacts on teaching, assessment and student outcomes are unclear. Many Independent schools are seeking"
366,Independent Schools Australia,Q01 Definitions,Yes
366,Independent Schools Australia,Q02 Other AI risks and recommendations for regulation,Please see attached submission.
366,Independent Schools Australia,Q03 Non-regulatory actions to address AI risks,Please see attached submission.
366,Independent Schools Australia,Q04 Coordinating AI governance across government,Please see attached submission.
366,Independent Schools Australia,Q05 International models for AI governance,Please see attached submission.
366,Independent Schools Australia,Q10 Ban on high-risk AI applications,Please see attached submission and link to ISA's generative AI in education submission to the House of Representatives Inquiry.
366,Independent Schools Australia,Q14 Risk-based approach to address AI risks,"Yes, risk combined with identifying opportunities. Please see attached submission."
367,Free TV Australia,Transcription,"FILENAME: Free TV Submission - Safe and responsible AI in Australia July 2023.25778c217a729.pdf | CONTENT: Submission by Free TV AustraliaSafe and responsible AI in AustraliaDiscussion PaperDepartment of Industry, Science and ResourcesJuly 2023 1. Summary • Free TV Australia appreciates the opportunity to comment on the Safe and Responsible AI in Australia Discussion Paper. • Free TV supports a legislated risk-based approach to regulating AI, as is being pursued in jurisdictions such as the EU and Canada. • An economy-wide approach is required to ensure that all use cases for the deployment of AI and automated decision making (ADM) apply a common risk-based approach, mitigating the risk that the current sector-by-sector approach fails to capture emerging use cases. • The legislation should establish a principles-based risk framework that focuses on transparency as its central tenet, including a requirement that all uses of AI and ADM undertake a risk assessment and publish clear notice"
368,National Emergency Communications Working Group-Aus/NZ,Transcription,"FILENAME: NECWG-ANZ_Submission_Supporting_Responsible_AI-July_2023_v2.ae7a663cee5cd.pdf | CONTENT: working for stronger emergency communicationsDepartment of Industry, Science and ResourcesSafe and responsible AI in AustraliaThe National Emergency Communications Working Group – Australia and New Zealand (NECWG- A/NZ) is recognised and respected nationally and globally for its professional leadership and advisory functions that support Australia and New Zealand’s emergency communications sectors. The NECWG-A/NZ’s purpose is to improve community safety outcomes through advocating, and/or influencing change for the Emergency Call Services (ECS) of Australia and New Zealand in a timely manner to support planning and the impacts of more immediate challenges. The NECWG-A/NZ has working relationships with the Department of Infrastructure, Transport, Regional Development, Communications, and the Arts (DITRDCA), and the Australian Communications Media Authority (ACMA) in support of the operatio"
369,KPMG Australia,Transcription,"FILENAME: KPMG submission_Safe and responsible AI in Australia_Final.52329513fc768.pdf | CONTENT: Safe and responsible AI in Australia KPMG submission _____KPMG Australia, July 2023 KPMG.com.au Contents Executive summary 3 Background 4 Section 1: KPMG recommendations 5 Section 2: KPMG insights 8©2022 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.Liability limited by a scheme approved under Professional Standards Legislation. 3 | Safe and responsible AI in AustraliaExecutive summary As a leading professional services firm, KPMG Australia (KPMG) is committed to meeting the requirements of all our stakeholders – not only the organisations we audit and advise, but also employees, governments, regu"
371,Richard Schreurs,Q01 Definitions,"I do agree with the definitions, and appreciate how well they were layed out"
371,Richard Schreurs,Q02 Other AI risks and recommendations for regulation,"I am concerned about the increasing layoffs in large corporations despite record profits, and I believe AI-driven job displacement will worsen this. We need comprehensive policies to address these challenges. Corporations should take responsibility for displaced workers' living standards, and a Universal Basic Income or similar initiative could help achieve this."
371,Richard Schreurs,Q03 Non-regulatory actions to address AI risks,"Education about AI, for both schools, and informing the adult public. The public having better knowledge of the workings and effects of emerging technologies can help us all better understand its role in our lives, its dangers, and its utility."
371,Richard Schreurs,Q04 Coordinating AI governance across government,"Creating a new government department consisted of experts that are actively in the field of AI would be fantastic. Since the technology is accelerating at an alarming rate, staying on top of its risks and its utilities will be critical."
371,Richard Schreurs,Q09 Transparency requirements and scope,"transparency on the source of the training data, and the contents of the training data itself, is critical. As that can help to expose biases in the AI in question, as well as expose any breaches in privacy and consent. Being upfront about potential risks, biases, and other flaws of any AI to the end users is also important. I believe we should mandate transparency on the source of the training data, and the data itself, and sufficient warning about negative implications such as bias."
371,Richard Schreurs,Q10 Ban on high-risk AI applications,"Generative AI should be banned from high impact areas such as legal and legislative roles, as any existing biases may worsen the disparities we see today in both areas, and has the potential to dramatically negatively affect people's lives."
371,Richard Schreurs,Q11 Actions to increase public trust in AI,"Transparency, education, and importantly, welfare. I am personally very concerned with the rate of layoffs in massive corporations while they report record profits, and I believe they should be responsible for maintaining a high quality of life for the workers that they displaced. Much higher taxes on high profit industries with a small workforce would greatly benefit us, as we can redirect those funds into higher education, or covering basic living needs, so that as a society we can focus on the arts, sciences, education, exploration, and social development. This could berachieved through increased welfare, universal basic income, or similar initiatives.in addition to all of that, educating the public about the utilities, risks, and the mechanisms behind the AI systems that affect our lives could greatly help people understand how best to utilise AI and to understand how to mitigate any associated risks."
371,Richard Schreurs,Q13 Changes to conformity infrastructure,I believe a full review by experts in the field of AI should be undertaken to ensure the current conformity infrastructure is sufficient for the very new risks we face.
371,Richard Schreurs,Q14 Risk-based approach to address AI risks,"I do, however, I believe collaborative governance is also in order, and we should apply continuous monitoring and adaptation to the governing of AI development, as the field is rapidly changing at a rate never seen before in technology."
371,Richard Schreurs,Q15 Benefits and limitations of risk-based approach,"The main limitations of a risk based approach may be the unseen, seemingly insignificant risks. AI has a knack for exploiting its own proxy utilities, and emphasising biases in its training data. Even when attacking the largest risks, it seems there is very often oversight on smaller exploits that can lead to unpredicted consequences."
371,Richard Schreurs,Q17 Elements to include in risk-based approach,I do support the elements presented in attachment C
371,Richard Schreurs,Q20 Voluntary vs mandated compliance with risk-based approach,"risk based, collaborative governance, and continuous monitoring and adaptation should be mandated through regulation and should be applied to all of the above, including to the regulation itself. This is a rapidly developing technology that poses unprecedented challenges in regulation and development"
372,CHOICE,Transcription,"FILENAME: 230802 - CHOICE submission to the Safe and Responsible AI in Australia consultation.dee476b48c40e.pdf | CONTENT: SUBMISSION TO THE DEPARTMENT OF INDUSTRY, SCIENCE AND RESOURCES SAFE AND RESPONSIBLE AI IN AUSTRALIA: DISCUSSION PAPERAUGUST 202357 Carrington Road Marrickville NSW 2204Phone 02 9577 3333 | Email campaigns@choice.com.au | www.choice.com.auThe Australian Consumers’ Association is a not-for-profit company limited by guarantee. ABN 72 000 281 925 ACN 000 281 9251 About UsCHOICE is the leading consumer advocacy group in Australia. CHOICE is independent, not-for-profit and member-funded. Our mission is simple: we work for fair, just and safe markets that meet the needs of Australian consumers. We do that through our independent testing, advocacy and journalism.CHOICE | SUBMISSION TO THE SAFE AND RESPONSIBLE AI IN AUSTRALIA DISCUSSION PAPER 2 ContentsAbout Us 2 Contents 3 Introduction 4 Recommendations 6 Protecting the community with AI legislation 7 Enforcing AI laws wi"
373,Keith Youens,General comment,"To ensure the right framework is being applied when assessing anything to do with AI, it is fundamental that it is considered both as; an exponential capability and foundational technology i.e., it acts as an exponential catalyst to many other technologies / innovations as well as being the core technology to many new and emerging applications, capabilities, and innovations. It is my opinion that most people expect organisations to uphold the same standards and responsibilities being described in relation to AI, to the entire organisation - it is just being explicitly called out now in relation to AI. Responsible AI is really a mirror reflection of our world and who we are as humans - the good, the bad, and the ugly - nothing will change overnight. The speed and potential impact of AI requires a governing advisory institution made up of representatives from all facets of society (public and private sectors) - this cannot be a government-controlled entity. Lastly, the thinking required "
373,Keith Youens,Q01 Definitions,"I have an issue with the statistics being used to underpin the report, as an example: ""racial discrimination where AI has been used to predict recidivism which disproportionately targets minority groups..."" - if you read and understand the report - it reflects the repercussions of the USA's privatization of the penal system. AI is not going to fix society and the wider social paradigm."
373,Keith Youens,Q03 Non-regulatory actions to address AI risks,"Start changing bias by introducing these concepts into early learning centers, schools, education institutions and businesses - create awareness in all public sector organisations"
373,Keith Youens,Q04 Coordinating AI governance across government,"A centralised quasi-government entity that reflects all facets of Australian society (public and private sectors) that provides advisory, stewardship and direction regarding all AI and emerging technologies to all of government."
373,Keith Youens,Q06 Public vs Private sector governance,The one reflects the rights of society (free and equitable with guardrails - risk adverse) the latter reflects the needs of society (for profit and innovation - risk tolerance)
373,Keith Youens,Q07 Responsible government use of AI,"That depends on how AI will be developed, used and managed. There should be a centralised agency (such as Australian Government Architecture or DTA or CSIRO) providing guidance and education to all agencies and staff - any external 3rd parties must engage with this body if they are to develop and or implement AI"
373,Keith Youens,Q09 Transparency requirements and scope,Common sense should prevail.
373,Keith Youens,Q10 Ban on high-risk AI applications,a. how is this different to the approach to any high-risk applications and technologies? b. see a.
373,Keith Youens,Q11 Actions to increase public trust in AI,"Awareness, learning and education - and start with establishing a consistent position on the use of AI across sectors such as Education, Health, Tax - these are areas that directly impact on people's lives and create the most worry - educating our children, our general health, and our money...."
373,Keith Youens,Q14 Risk-based approach to address AI risks,"Yes, but it must be constantly evolving to adapt to this face moving space"
373,Keith Youens,Q15 Benefits and limitations of risk-based approach,"Like any innovative endeavor, a risk-based approach has the potential to hinder innovation and advancement - perhaps the NASA engineers / launch and operational teams can provide insight into understanding how they dealt with the risks of space exploration in the 60s - a risk-based approach where decisions had to be made as situations presented themselves - much like what will happen with the emergence of AI and all the other emerging technologies such as quantum computing"
373,Keith Youens,Q16 Scope and application of risk-based approach,Absolutely although perhaps not only based on the criteria you have provided but more aligned with the sector and the type and intended use of the AI
373,Keith Youens,Q17 Elements to include in risk-based approach,"No, this approach does not happen now with any other technology. Are we proposing that whenever we start using an emerging exponential technology, we are going to start applying this approach? Nobody will care and or even read anything in this regard - what they will look for is something similar to a label that says ""Made in Australia"" or "" Organic"""
373,Keith Youens,Q19 Application of risk-based approach to general purpose AI,Exactly the same
373,Keith Youens,Q20 Voluntary vs mandated compliance with risk-based approach,"a. voluntary / self-regulation for both public and private sectors although transparency is an absolute b. see a. - and, developers of systems that have a direct impact on the lives of humans or animals and this includes the decisions that may be influenced by the AI"
374,Christian McDonald,General comment,"I believe we need nationally consistent policy for the use of AI in the delivery of Goods and Services to require as a condition of operation, any business providing a good or service to people in this country to provide an option for them to contact that business directly by voice call and be able to speak to a human being if required to address any issues with a service, booking, purchase, refund, etc. More and more businesses, especially very large companies, are moving towards AI run “Contact Us” pages and “live chat” functions and it is becoming harder and harder to access phone numbers to call and speak to companies directly about any questions or concerns customers may have. I believe there is a massive erosion of consumer rights occurring in this space particularly with more large multi-national companies taking over smaller businesses in Australia. An extreme example of this is the operating system for the AirAsia airline whereby customers must now download an app to manage th"
374,Christian McDonald,Q01 Definitions,Not sure
374,Christian McDonald,Q02 Other AI risks and recommendations for regulation,"Risk of erosion in customer service and consumer rights.I believe we need nationally consistent policy for the use of AI in the delivery of Goods and Services to require as a condition of operation, any business providing a good or service to people in this country to provide an option for them to contact that business directly by voice call and be able to speak to a human being if required to address any issues with a service, booking, purchase, refund, etc.More and more businesses, especially very large companies, are moving towards AI run “Contact Us” pages and “live chat” functions and it is becoming harder and harder to access phone numbers to call and speak to companies directly about any questions or concerns customers may have. If and when we do speak to company staff, we are then often referred to fix our own issue online via their website or app. I believe there is a massive erosion of consumer rights occurring in this space particularly with more large multi-national compani"
374,Christian McDonald,Q03 Non-regulatory actions to address AI risks,"Promoting transparency to enable effective monitoring of the rate of growth and development in AI applications within Australia by establishing and maintaining a government managed register and a requirement for any entity using AI to register their application of the system. Such a register could record a brief explanation of how/why their AI is being used and provide representative contact person(s) directly involved in the management and operation for that system. This way the government can maintain some level of oversight and control over the expansion of these technologies and a means of directly contacting the persons responsible for them if and when the need arises, eg. if complaints start coming in about how it is being used."
374,Christian McDonald,Q04 Coordinating AI governance across government,I believe governing AI across government and jurisdictions needs to be broken down into different areas of application. I think there needs to be more regulatory focus in areas where AI is used to interact with consumers. AI application in research and development could be better promoted and facilitated by government to enhance productivity. I think it will be very important to maintain nationally consistent approaches to policy and governance of AI applications across all jurisdictions.
374,Christian McDonald,Q06 Public vs Private sector governance,I believe governing AI across government jurisdictions and industry sectors needs to be broken down into different areas of application. I think there needs to be more regulatory focus on areas where AI is used to interact with consumers. AI application in research and development could be better promoted and facilitated by government to enhance productivity. I think it will be very important to maintain nationally consistent approaches to policy and governance of AI applications across all jurisdictions.
374,Christian McDonald,Q07 Responsible government use of AI,"As it applies to any aspect of customer service - apply a co-existence model so consumers of information, goods or services can always still contact a human being if they need to resolve an issue they were unable to resolve themselves online or with the AI support system."
374,Christian McDonald,Q09 Transparency requirements and scope,"As I noted earlier. I recommend establishing a national government managed register requiring all entities using AI in Australia to record their use of AI and provide a brief explanation of how/why it is being used and contact details for representative person(s) directly involved in its operation. This will provide more transparency to government and allow it to monitor its growth and development and provide a means to take action where necessary to put the brakes on certain applications if required, eg. if complaints are received about it."
374,Christian McDonald,Q10 Ban on high-risk AI applications,"Robo calls to people's telephones should be banned, eg. Amazon spam."
374,Christian McDonald,Q11 Actions to increase public trust in AI,"As long as we know when/where it is being used and we preserve the ability for consumers to contact a human being by voice call if the AI system hasn't resolved our issue and know there are still human beings in control of the system and it's expansion, I think people will be more comfortable to allow AI to operate in CO-EXISTENCE WITH HUMANS."
376,Centre for Culture and Technology (CCAT),Transcription,"FILENAME: CCAT Safe and Responsible AI submission.f8fdad930519c.pdf | CONTENT: Centre for Culture and Technology (CCAT) Cur5n UniversityCCAT submission in response to the Safe and Responsible AI in Australia Discussion Paper July 2023Acknowledgements: this submission has been prepared by Kai-Ti Kao, Ka5e Ellis, Eleanor Sandry, Mike Kent, Tama Leaver and Stuart Bender from the Centre for Culture and Technology, Cur5n University.1. Who we are The Centre for Culture and Technology (CCAT) is a research Centre in the Faculty of Humani5es at Cur5n University. We partner with community, government, and industry to codesign socially just digital futures. Our research focuses on how cultural prac5ces are changing in rela5on to digital technologies and plaQorms in areas such as accessibility, health communica5ons, in5macy, social life, popular culture, knowledge produc5on, commerce, poli5cs, and ac5vism. At the heart of our work lies a commitment to inves5ga5ng the opportuni5es and challenges th"
378,Anonymous,Transcription,"FILENAME: Submission 378 - Public and Anonymous - 4-Aug.d79c112b7e261.pdf | CONTENT: To: Department of Industry, Science and ResourcesI have wriften this brief submission both as a technologist as well as a father – in my view there is opportunity to leverage AI for the beneﬁt of the Australian society, but it must be done with a balanced approach. There are tremendous advantages that AI can deliver but also raises a few areas of concern that, based on speciﬁc use-cases, warrant further legal controls to be in place to protect individuals as well as diﬀerent secfions of a diverse society.By way of experience, I have a technology background and have worked with clients in many industry sectors and geographies, so my intenfion is to provide context and examples from a broader perspecfive on the topic where possible. Addifionally, I have general understanding of the Australian laws and potenfial gaps that may exist in context of governing the use of AI systems.Most of my content is primar"
378,Anonymous,General comment,Response to questions included in the document submitted
378,Anonymous,Q01 Definitions,Response to questions included in the document submitted
378,Anonymous,Q02 Other AI risks and recommendations for regulation,Response to questions included in the document submitted
378,Anonymous,Q03 Non-regulatory actions to address AI risks,Response to questions included in the document submitted
378,Anonymous,Q04 Coordinating AI governance across government,Response to questions included in the document submitted
378,Anonymous,Q05 International models for AI governance,Response to questions included in the document submitted
378,Anonymous,Q06 Public vs Private sector governance,Response to questions included in the document submitted
378,Anonymous,Q07 Responsible government use of AI,Response to questions included in the document submitted
378,Anonymous,Q08 Generic vs technology-specific approaches for AI risk,Response to questions included in the document submitted
378,Anonymous,Q09 Transparency requirements and scope,Response to questions included in the document submitted
378,Anonymous,Q10 Ban on high-risk AI applications,Response to questions included in the document submitted
378,Anonymous,Q11 Actions to increase public trust in AI,Response to questions included in the document submitted
378,Anonymous,Q12 Trade impact of banning high-risk AI applications,Response to questions included in the document submitted
378,Anonymous,Q13 Changes to conformity infrastructure,Response to questions included in the document submitted
378,Anonymous,Q14 Risk-based approach to address AI risks,Response to questions included in the document submitted
378,Anonymous,Q15 Benefits and limitations of risk-based approach,Response to questions included in the document submitted
378,Anonymous,Q16 Scope and application of risk-based approach,Response to questions included in the document submitted
378,Anonymous,Q17 Elements to include in risk-based approach,Response to questions included in the document submitted
378,Anonymous,Q18 Integration of risk-based approach into existing frameworks,Response to questions included in the document submitted
378,Anonymous,Q19 Application of risk-based approach to general purpose AI,Response to questions included in the document submitted
378,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,Response to questions included in the document submitted
379,Australian Alliance for Artificial Intelligence in Healthcare,Transcription,"FILENAME: AAAiH Responsible AI submission 3 Aug 2023.1b9dd0239ff1a.pdf | CONTENT: 3 August 2023The Digital Technology Taskforce Technology and Digital Division, Technology Strategy Branch Department of Industry, Science and Resources Ngunnawal Country, Industry House, 10 Binara Street (GPO Box 2013) Canberra ACT 2601 AustraliaRe: Consultation on Supporting Responsible AIThank you for the opportunity to provide input into evolving policy for the safe and responsible use of AI.This submission is made on behalf of the Australian Alliance for AI in Healthcare (AAAiH), a national community of practice with over 100 member organisations drawn from industry, health service providers, academia and consumer organisations.BackgroundAAAiH and its partners including the Australasian Institute of Digital Health (AIDH) and the Digital Health CRC (DHCRC), are currently engaged in a policy development program for the safe and effective use of AI in healthcare.The process began with a national consulta"
379,Australian Alliance for Artificial Intelligence in Healthcare,Transcription,"FILENAME: AAAiH Healthcare AI Policy Discussion Paper July 2023.0a0dba62f3a7f.pdf | CONTENT: DISCUSSION PAPERA National Policy Agenda for AI in Healthcare DISCUSSION PAPER A National Policy Agenda for AI in Healthcare DISCUSSION PAPER A National Policy Agenda for AI in HealthcareExecutive SummaryThis Discussion Paper suggests a possible 4. Improve the effectiveness of national post- national policy agenda for AI in Healthcare to be market safety monitoring systems so that refined and developed in consultation with key cases of AI related patient risk and harm are stakeholders, backed by evidence, to assist all rapidly detected and communicated to all levels of government, industry and civil society. relevant parties.Australia lacks a co-ordinated policy approach 5. Ensure the national AI ethical framework to artificial intelligence (AI) in healthcare, and supports the deployment of values-based substantially lags comparable nations in AI clinical and consumer AI in routine practice. in"
379,Australian Alliance for Artificial Intelligence in Healthcare,Transcription,"FILENAME: AAAiH 2021 Roadmap.e2a03a5140f9.pdf | CONTENT: A ROADMAP FOR ARTIFICIAL INTELLIGENCE IN HEALTHCARE FOR AUSTRALIA EXECUTIVE CONTENTS SUMMARYExecutive Summary 3 Artificial Intelligence (AI) has the potential to bring profound digital disruption of healthcare by creating a smarter, more adaptive health system - the key to the sustainability of quality healthcare in Australia. AI will automate many tasks that were once solely reliant on human expertise from interpretation AI in Healthcare Roadmap at a glance 4 of imaging and pathology, triage and resource allocation, to personalisation of therapy based on individual characteristics. The AI opportunity for Healthcare 6 The AI in Healthcare Roadmap shows the path that Australia should take to embrace the opportunities that AI brings. The Roadmap identifies the current gaps in Australia’s capability to translate AI into About the AI for Healthcare Roadmap 8 effective and safe clinical services and provides guidance on key issues suc"
379,Australian Alliance for Artificial Intelligence in Healthcare,Q01 Definitions,"We see AI as a broad set of evolving computational methods which are capable or arriving at decisions, and which can be embedded in workflows with low or high autonomy, and with low or high adaptability or learning abilities."
379,Australian Alliance for Artificial Intelligence in Healthcare,Q02 Other AI risks and recommendations for regulation,"Yes. Our attached Draft Policy Discussion Paper (in the Section on AI Safety Quality Ethics and Security) outlines potential actions for the healthcare sector specifically, but with broader implications. Specifically: 1. Create a collaborative engagement model and unified responsibility framework for the entities responsible for oversight of AI safety and effectiveness, and ethical and security risks.2. For healthcare AI to be safe and not harm patients, it needs to be developed and deployed within a robust safety framework.3. Urgently communicate the need to pause the clinical use of non-medical grade generative AI such as ChatGPT and develop strategies that ensure patient safety and data privacy.4. Improve the effectiveness of national post- market safety monitoring systems so that cases of AI related patient risk and harm are rapidly detected and communicated to all relevant parties.5. Ensure the national AI ethical framework supports the deployment of values-based clinical and cons"
379,Australian Alliance for Artificial Intelligence in Healthcare,Q03 Non-regulatory actions to address AI risks,"Yes. Our attached Draft Discussion Paper outlines potential actions summarised above, including the importance of communication of risks around generative AI that is not medical grade (such as ChatGPT)."
379,Australian Alliance for Artificial Intelligence in Healthcare,Q04 Coordinating AI governance across government,"Yes. Our attached Draft Discussion Paper outlines potential actions summarised above, highlights the need for TGA and other agencies to create a collaborative engagement model and unified responsibility framework for these entities responsible for oversight of AI safety and effectiveness, and ethical and security risks."
379,Australian Alliance for Artificial Intelligence in Healthcare,Q08 Generic vs technology-specific approaches for AI risk,"Risk models are highly tuned to application domains. The risks to patient health and life in healthcare for example have always meant that there is a low tolerance of risk, compared to some other sectors. We believe any generic risk approach will need to work alongside domain specific ones. Harmonising a generic approach with domain specific approach may prove challenging, so we suggest giving primacy to domain specific approaches, tuned to the unique needs of different sectors. For example, we would preference the TGA's assessment of a medical AI product well ahead of the assessment of a non-medical generic risk assessment, which would not understand the specific risks AI brings in healthcare settings."
379,Australian Alliance for Artificial Intelligence in Healthcare,Q10 Ban on high-risk AI applications,"Generative AI such as Chat GPT that is designed for general use evades current medical use regulation. For example, the use of ChatGPT to create clinical documentation would currently not be looked at by the TGA. Yet such an application brings real risks to patients, where incorrect clinical documentation influences decisions and potential leads to harms to patients. It also brings risks to patient privacy when clinical data are sent overseas to such corporate systems, evading any local legal constraints of how those data are used.Consequently AAAiH advocates a pause of such use of general purpose generative AI in healthcare, until safety and privacy issues are clearer, and appropriate governance can be put in place. We think a program of communication emphasising these current risks would be part of that 'pause'."
379,Australian Alliance for Artificial Intelligence in Healthcare,Q13 Changes to conformity infrastructure,"In healthcare, TGA will need to align its conformance programs with organisations such as the Australian Digital Health Agency, and the Commission for Safety and Quality in Healthcare, to ensure responsibilities are clear, and gaps made evident. These entities will require additional resource if asked to consider non-medical AI like ChatGPT, when used in healthcare settings."
379,Australian Alliance for Artificial Intelligence in Healthcare,Q14 Risk-based approach to address AI risks,"Yes. See our attached documentation for more information.Healthcare utilises risk based assessment to determine the entry of products to market, but also has a harms based post market surveillance infrastructure, to detect patient harms resulting from technology use.A risk based approach alone is insufficient to oversee technologies post-approval - when often unexpected risks finally emerge."
379,Australian Alliance for Artificial Intelligence in Healthcare,Q15 Benefits and limitations of risk-based approach,"It is critical to recognise that risk rating is highly tied to application domain. While advocating explicit risk-based assessment nationally is important, the specific risk settings will have to be done in a sector specific way."
379,Australian Alliance for Artificial Intelligence in Healthcare,Q16 Scope and application of risk-based approach,Healthcare already has a strong risk-based approach to new technology. It also has a harms-based post-market monitoring system to detect emerging problems not captured in risk assessments.
379,Australian Alliance for Artificial Intelligence in Healthcare,Q19 Application of risk-based approach to general purpose AI,"Our submission emphasise the need to regulate the healthcare use of general purpose AI, and calls for a pause on such use until risks can be managed through organisations such as TGA."
379,Australian Alliance for Artificial Intelligence in Healthcare,Q20 Voluntary vs mandated compliance with risk-based approach,Healthcare experience is that self-regulation is a poor model for medical technologies safety that can cause patient harm. AI is not exempt from such requirements and regulation is required - building on exisiting infrastructure such as TGA processes.
380,Belinda Morris,General comment,"I have had my work used without consent in generating AI images. I am concerned not only about copyright issues and lack of compensation but also the disregard by the companies who create AI art software of my future livelihood. I have struggled for the past 20 years to make a living as an artist and this has gotten so much harder. I have friends in the USA who are right now fighting against AI’s takeover of this industry, Karla Ortiz and Kelly McKernan. Please advocate for us artists and not these money making corporations."
380,Belinda Morris,Q01 Definitions,I do not. I wish it said something more like “Supporting Artists by Enforcing Responsible AI”
380,Belinda Morris,Q02 Other AI risks and recommendations for regulation,Though there is protection against misuse of Copyright for some reason the fact that these AI companies use current living and working artists works without permission to generate content has somehow slipped the notice of those enforcing copyright. I believe these companies who use content without permission from artists to program their software should be banned from Australia or forced to pay compensation and royalties for the use of these works. Midjourney and ChatGPT are 2 companies who have both been found to be illegally be using the works of artists without permission.
380,Belinda Morris,Q03 Non-regulatory actions to address AI risks,Support locally created AI companies who create content only from artists who give permission for their artwork to be used in that way.
380,Belinda Morris,Q04 Coordinating AI governance across government,Look to what is happening in the USA and Europe on how they are regulating AI.
380,Belinda Morris,Q05 International models for AI governance,As I said previously the USA and Europe are currently taking significant measures towards responsible AI
380,Belinda Morris,Q06 Public vs Private sector governance,No
380,Belinda Morris,Q07 Responsible government use of AI,Discontinue and ban software that uses artists work without permission or compensation and protect the livelihood of living and upcoming artists
380,Belinda Morris,Q10 Ban on high-risk AI applications,Midjourney and ChatGPT should be banned because they use the work of current living artists without permission or compensation
380,Belinda Morris,Q11 Actions to increase public trust in AI,"I would rather they didn’t , but if they did ensure that the livelihoods of Australian artists are protected by banning AI that uses content created from living artists without permission"
380,Belinda Morris,Q14 Risk-based approach to address AI risks,"It’s not just about risk, it’s about the fact that many of these AI companies are creating content without permission or compensation to living artists, these companies are stealing the livelihoods of living artists."
380,Belinda Morris,Q20 Voluntary vs mandated compliance with risk-based approach,Mandatory through regulation for everyone
381,Anglican Social Responsibilities Commission (Perth),,
381,Anglican Social Responsibilities Commission (Perth),Q01 Definitions,"The SRC agrees in general with the definitions provided in the discussion paper. In addition, however, we believe that special focus should be applied, and definitions created, for applications of AI that mimic human interaction. Simple examples of these are found in ‘bots’ that are often the initial point of interaction in online chat support and may extend into virtual ‘companions’ or ‘partners’ for virtual friendship and/or romantic ‘relationships’. Common terms for these aspects of AI are ‘generated agent’ or ‘AI Agent’."
381,Anglican Social Responsibilities Commission (Perth),Q02 Other AI risks and recommendations for regulation,"One major risk not covered by the existing regulatory approaches relates to possible personal, psychological, social harm and distortion caused by people interacting with AI agents, without the knowledge that they are interacting with AI. This is explored in the SRC’s attached submission.Another possible risk arises from actions to combat abuses of ‘deepfake’ technology, which are outlined well in the discussion paper and references. Many of these actions to mitigate abuse from ‘deepfake’ technology, will, inevitably require some form of centralized government authority which will be able to issue and regulate certification of veracity of accounts, images and videos. Such centralization needs to be managed with great care, transparency and accountability to ensure that both privacy equity of access and power is maintained for all people.We are concerned that the impacts upon the creative industries are already significant, and are rapidly increasing in magnitude. Many of the impacts fa"
381,Anglican Social Responsibilities Commission (Perth),Q03 Non-regulatory actions to address AI risks,"The SRC believes that the introduction of AI poses challenges and threats to all people and our established social order and institutions. Because of this input, ideas and solutions to identified problems should be sought from the broader Australian community, not solely those people or groups identified as in the ‘IT’ sphere or would regularly respond to calls for public submissions. Extensive and open in-person public consultations across the range of civil society is required, including community organisations, religious traditions, education bodies and membership societies."
381,Anglican Social Responsibilities Commission (Perth),Q04 Coordinating AI governance across government,"Given the high risk that AI technology could be used to disseminate fraudulent or misleading information, areas where the application of AI may undermine existing structures and mechanisms of trust should be identified and proactively addressed.An obvious example is elections – the capability to rapidly generate fake audio, and soon, convincing video footage, could obviously be used to produce political misinformation. The threat actors behind these acts may be outside of the reach of Australian law. Our society seems to be growing increasing polarised, with declining trust in its institutions. This provides an opportunity for those who would wish to further undermine trust, harming our society, and those who would seek to influence election outcomes.Extending from this example, we suggest that responses to AI should be coordinated across all layers of government; in concrete terms:• Electoral commissions at the federal and state level should coordinate at a very close and immediate le"
381,Anglican Social Responsibilities Commission (Perth),Q05 International models for AI governance,This is outside of our expertise.
381,Anglican Social Responsibilities Commission (Perth),Q06 Public vs Private sector governance,"There should be core principles which do not differ between the regulation of public and private sector use of AI.People should have the right to know that they are interacting with AI technologies, regardless of whether that technology is being utilised by the public or private sector. The regulatory response to uses of AI to mimic human interaction, with or without the consent of those utilising the technology, should apply in both public and private sector contexts.The use of AI for military or law enforcement purposes should be regulated in such a way as to prevent those government functions from simply commissioning their desired use of AI from the private sector, and then making use of the result. For example, were a police force to use AI to impersonate a suspect, hold a voice conversation with another suspect, and then use the conversation transcript as evidence in a trial, we would suggest that whatever regulation was applied to the permissibility of this method, and the admis"
381,Anglican Social Responsibilities Commission (Perth),Q07 Responsible government use of AI,"We would suggest looking to the existing regulatory structures around medical research involving humans, such as Human Research Ethics Committees (HRECs.) Such committees include members with specific ethical training, including philosophers, and theologians from various faith traditions. Use of AI, particularly when directly interacting with humans, has strong parallels with research conducted upon humans: it is vital that ethical considerations are considered before AI technology is deployed.Furthermore, responsible and ethical use of AI must be a shared commitment by all staff making use of it, particularly those staff proposing and developing new applications of AI technology. Those involved in the conduct of research upon humans, even relatively low-level staff, are generally required to complete training. This training seeks to ensure that staff are aware of regularity constraints and ethical principles, building a shared culture that takes ethical issues seriously. Furthermore, "
381,Anglican Social Responsibilities Commission (Perth),Q08 Generic vs technology-specific approaches for AI risk,"We are most concerned with the possibility that AI might infringe upon human personhood, being used to impersonate, deceive, or defraud, or to technologically replace genuine human interaction and fellowship. We believe that this should be addressed within a general framework.However, there are specific areas in which we could envision technology-specific responses being required.As an example, imagine if the use of AI in aged-care settings were proposed, in order to provide interaction and companionship to residents. Proponents might consider that such a use of the technology might better the lives of otherwise isolated people. However, a longer-term view might consider whether the presence of the technology would lead to a further isolation of residents from their families and friends. Visiting residents can be difficult, especially when ill-health and death is in view; family and friends might be drawn to use AI technology to replace a difficult experience.This example shows how per"
381,Anglican Social Responsibilities Commission (Perth),Q09 Transparency requirements and scope,"a. where and when transparency will be most critical and valuable to mitigate potential AI risks and to improve public trust and confidence in AI?We believe transparency will be most critical when a human being could believe they are interacting with another human being, or when a human being could believe that that media they are viewing (documents, images, videos, …) is the work of a human being, or a genuine record an event, when in reality it is the output of AI technology.b. mandating transparency requirements across the private and public sectors, including how these requirements could be implemented.Use of AI to mimic human interaction must be immediately disclosed; for example, when telephoning a private or public sector organisation that uses AI technology to converse with a client, the person making the phone call should be immediately told that they are going to speak with an AI technology. Furthermore, it should be possible for the use of AI technology in such contexts to b"
381,Anglican Social Responsibilities Commission (Perth),Q10 Ban on high-risk AI applications,"We do not propose any specific bans on technologies at this stage, but we do suggest constraints upon the marketing of AI technologies.AI poses an emerging, complex challenge for providers of education, be that at primary, secondary or tertiary level. Academic integrity is crucial to ensuring that the work done by students is their own work, and represents the result of genuine engagement with the subject matter presented. Educational institutions are rapidly attempting to adapt to a situation where work submitted may be the output of AI technology, rather than the work of their students. This is complicated by the fact that the output of AI technology is not readily identifiable – indeed, OpenAI, an industry leader, has shuttered its AI detection tool (https://decrypt.co/149826/openai-quietly-shutters-its-ai-detection-tool) as it did not return results with sufficient precision.We suggest that the marketing of AI technologies to students in accredited educational programmes be banned,"
381,Anglican Social Responsibilities Commission (Perth),Q11 Actions to increase public trust in AI,"We do not consider that encouraging the adoption of AI, or trust in AI, should be the work of government. AI is already being adopted at a rapid place, and with scant attention to its impacts upon society and individuals."
381,Anglican Social Responsibilities Commission (Perth),Q14 Risk-based approach to address AI risks,"A risk based approach has some value, but may tend to only consider the short to medium term implications of AI technologies. Given the rapid pace at which this technology is being developed, and also the reality that we cannot foresee the long-term societal impacts of AI at its present level of development, we consider that a risk-based approach must be complemented by an overarching ethical framework. As previously suggested, this could take significant inspiration from the ethical frameworks which control and regulate research upon human beings."
381,Anglican Social Responsibilities Commission (Perth),Q15 Benefits and limitations of risk-based approach,Please refer to the previous answer.
381,Anglican Social Responsibilities Commission (Perth),Q20 Voluntary vs mandated compliance with risk-based approach,Any risk based approach for responsible AI should be mandatory through regulation and apply to both public and private organisations and both developers and deployers.
382,Transparency-Project.ai,Transcription,"FILENAME: Tranparency Project's Submission to Safe and Responsible AI in Australia.86bb116676b83.pdf | CONTENT: The Transparency Project’s Submission on Supporting Responsible AI 03 AUGUST 2023As an AI myself, I can confirm that AI does not inherently possess a moral fabric. AI is fundamentally neutral and operates based on its programming and algorithms, which are created by humans. Unlike humans, AI does not have emotions, beliefs, or personal experiences that shape a moral framework.However, AI can be designed to respect and adhere to ethical guidelines or rules set by its developers. Ethical AI seeks to ensure fair, accountable, and transparent AI systems, which involves addressing potential biases in AI algorithms or data, considering privacy issues, and ensuring AI is used for beneficial purposes. But these ethical considerations are imparted by humans, not innately adopted by the AI.– Sapien, a GPT-4 powered AI assistantKieran Lindsay FOUNDER, THE TRANSPARENCY PROJECT | TRANSPAR"
383,Salinger Privacy,Transcription,"FILENAME: 23 08 03_DISR_Safe and Responsible AI_Salinger Privacy Submission.ebf12d30fa2df.pdf | CONTENT: We know privacy inside and out.Submission in response to the Safe and Responsible AI in Australia - Discussion Paper 2023 Australian Government, Department of Industry, Science and Resources 3 August 2023Salinger Consulting Pty Ltd ABN 84 110 386 537 PO Box 1250, Manly NSW 1655 www.salingerprivacy.com.au Covering letter3 August 2023Department of Industry, Science and Resources By online submissionDear Safe and Responsible AI in Australia review team,Thank you for the opportunity to make a submission in relation to the Safe and Responsible AI in Australia - Discussion Paper 2023.Please find our submission attached.We have no objection to the publication of this submission, and no redactions are required prior to publication.Please do not hesitate to contact me if you would like clarification of any of the comments made in this submission.Anna Johnston Principal | Salinger PrivacySubm"
384,Empathetic AI,Q01 Definitions,Yes
384,Empathetic AI,Q02 Other AI risks and recommendations for regulation,"AI can concentrate power in the hands of a few tech giants, leading to potential anti-competitive practices. Regulations might promote interoperability, data portability, and open standards to foster competition and prevent monopolies."
384,Empathetic AI,Q03 Non-regulatory actions to address AI risks,"AI Impact Assessment- The government can establish guidelines for conducting AI impact assessments to understand the potential consequences of AI deployment in various sectors, such as healthcare, finance, and transportation."
384,Empathetic AI,Q04 Coordinating AI governance across government,Create AI testbeds and pilot projects in collaboration with industry partners to evaluate the feasibility and impact of AI applications in real-world scenarios. This will foster experimentation and help identify best practices for responsible AI deployment.
384,Empathetic AI,Q05 International models for AI governance,"Various countries are engaged in international collaborations and partnerships to address global AI challenges, share best practices, and establish ethical AI norms. Australia could actively participate in international forums and initiatives to stay informed and contribute to global AI governance efforts."
384,Empathetic AI,Q06 Public vs Private sector governance,"While both sectors should comply with relevant AI regulations, the public sector may be subject to additional scrutiny and oversight due to its role as a custodian of public interest and taxpayer funds. The private sector may need to demonstrate compliance with industry-specific regulations and market competition laws."
384,Empathetic AI,Q07 Responsible government use of AI,"Require government agencies to conduct AI impact assessments to evaluate the potential consequences of AI deployments on citizens, public services, and human rights."
384,Empathetic AI,Q08 Generic vs technology-specific approaches for AI risk,"Generic solutions to the risks of AI are most valuable when the risks are common across various AI applications and domains. These solutions address broad challenges associated with AI technologies and can be widely adopted by different industries and sectors. On the other hand, technology-specific solutions are more effective when the risks are unique to certain AI technologies or when the solutions need to be tailored to specific applications or industries."
384,Empathetic AI,Q09 Transparency requirements and scope,"Transparency regarding data collection practices and the purposes for which data will be used is crucial. This includes informing individuals about the types of data being collected, how it will be processed, and ensuring clear consent mechanisms. Transparent data practices help build trust and ensure that AI systems are trained on diverse and representative data.Transparency in algorithmic decision-making is also vital, especially in applications affecting individuals' rights or significant life outcomes. Providing explanations for AI-driven decisions helps users understand why a particular choice was made, allowing them to verify the fairness and accuracy of the system's outputs."
384,Empathetic AI,Q10 Ban on high-risk AI applications,"AI technologies that have a high risk of violating fundamental human rights, including privacy, freedom of expression, and non-discrimination, should be considered for bans or restrictions. This could include AI systems that engage in mass surveillance or discriminatory decision-making."
384,Empathetic AI,Q11 Actions to increase public trust in AI,"The government can run public awareness campaigns to educate citizens about AI technologies, their benefits, and potential risks. These campaigns should address common misconceptions and explain how AI is used to enhance various services and applications."
384,Empathetic AI,Q12 Trade impact of banning high-risk AI applications,"The ban may force the tech sector to diversify its offerings, potentially leading to increased investment in other AI applications that are not subject to bans. This could spur innovation in different domains and contribute to a more diverse tech ecosystem."
384,Empathetic AI,Q13 Changes to conformity infrastructure,"Implement requirements for AI developers and companies to be transparent about their AI systems' functionalities, data usage, and decision-making processes. Regular reporting on AI performance and potential risks should be mandated."
384,Empathetic AI,Q14 Risk-based approach to address AI risks,Yes I support that.
384,Empathetic AI,Q15 Benefits and limitations of risk-based approach,"A risk-based approach allows for efficient allocation of resources by focusing on addressing high-priority risks first. This ensures that efforts and resources are directed where they can have the most significant impact. However, a risk-based approach may focus on known risks but may not adequately address unknown or unforeseen risks, which can emerge as AI technology advances."
384,Empathetic AI,Q16 Scope and application of risk-based approach,Organizations with a high level of AI maturity and expertise are better positioned to conduct sophisticated risk assessments. They may have a deeper understanding of potential risks and appropriate mitigation measures. Less experienced organizations in the AI domain might initially focus on more manageable risks and gradually develop their risk management capabilities as they gain expertise.
384,Empathetic AI,Q17 Elements to include in risk-based approach,"Identify and catalog potential AI risks specific to the organization, AI application, or sector. This involves a thorough analysis of AI technologies, data usage, decision-making processes, and potential impact on individuals and society. Yes, I support the elements presented in Attachment C."
384,Empathetic AI,Q18 Integration of risk-based approach into existing frameworks,"Map the specific AI risks identified in the risk-based approach to relevant elements in the existing assessment frameworks or risk management processes. This mapping will help establish clear connections between the two approaches. Integrate the AI-specific risk factors into the existing risk assessment and management processes. For example, when conducting privacy assessments, consider AI-related privacy risks arising from data processing and decision-making algorithms."
384,Empathetic AI,Q19 Application of risk-based approach to general purpose AI,"Risk Identification: Identify potential risks specific to LLMs or MFMs, considering factors like data bias, misinformation generation, privacy concerns, and unintended consequences of AI outputs.Risk Assessment: Conduct a comprehensive risk assessment to evaluate the likelihood and severity of identified risks. Assess how different applications of LLMs or MFMs may impact individuals and society.Ethical Considerations: Integrate ethical considerations into risk assessment. Examine the potential ethical implications of AI-generated content and ensure adherence to ethical guidelines and principles."
384,Empathetic AI,Q20 Voluntary vs mandated compliance with risk-based approach,"It should be mandated through regulation. Regulatory mandates can create a level playing field, avoiding unfair advantages for organizations that might not prioritize responsible AI on a voluntary basis. It provides legal accountability, with potential consequences for non-compliance, which can incentivize organizations to take AI risks seriously. For high-risk AI applications with significant societal impact (e.g., healthcare, autonomous vehicles), regulation might be necessary to safeguard public safety and rights. It should apply to both public and private organizations and both developers and deployers."
385,Microsoft,Transcription,"FILENAME: Microsoft submission - Safe and Responsible AI in Australia - 4 Aug 2023.6c1a134928ca1.pdf | CONTENT: Microsoft submission on Safe and Responsible AI in Australia Submission to the consultation process run by the Department of Industry, Science and Resources 4 August 2023Summary of viewsAustralia is especially well-positioned to benefit from the extraordinary social and economic opportunities created by the latest wave of AI technologies. To realise these opportunities, however, effective guardrails are required to promote the trustworthy and responsible development and use of AI systems. To that end, Microsoft supports the advancement of AI governance in Australia through:1. Creating a regulatory architecture that reflects the AI technology architecture by tailoring the right regulations for each level of the tech stack.2. Improving domestic coordination on AI regulation and policy, including both the layers of existing economy-wide and sector-specific regulations as well as"
386,Institute of Electrical and Electronics Engineers Standards Association (IEEE SA),Transcription,"FILENAME: IEEE Response 4Aug2023 AU Safe and Responsible.b7a0534facafa.docx | CONTENT: 4 August 2023To: Department of Industry, Science, Energy and Resources (Australia)RE: Safe and Responsible AI in AustraliaThe IEEE Standards Association (IEEE SA) welcomes the opportunity to provide its input to the Department of Industry, Science, Energy and Resources (Australia) and its report Safe and Responsible AI in Australia.IEEE SA is a globally recognized standards-setting body within IEEE, the largest organization of technology professionals in the world. We develop consensus standards through an open process that engages industry and brings together a broad stakeholder community and comply with the WTO Principles for International Standardization.IEEE SA, through its global community, has developed resources and standards globally recognized in the area of applied ethics and systems engineering and offers standards, training and education, certification programs, and more, to empower stake"
386,Institute of Electrical and Electronics Engineers Standards Association (IEEE SA),Q01 Definitions,"As the “Supporting responsible AI: discussion paper” notes there is no agreed upon definition for Artificial Intelligence, however IEEE would like to call attention to its work in “Ethically Aligned Design: A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems”, as it does contain an accepted definition for AI and encompasses both ML, Generative AI models and all applications of automated decision making and recommendations.An Autonomous/Intelligent System (A/IS) is a semi-autonomous or autonomous computer-controlled system programmed to carry out some tasks with or without limited human intervention capable of decision making by independent inference and successfully adapting to its context.The Glossary that accompanies IEEE’s “Ethically Aligned Design 1st Edition” contains AI accepted terms and definitions and is a valuable reference source. The Glossary is an interdisciplinary resource for AI terms which may have meanings that are discipline specific: Or"
386,Institute of Electrical and Electronics Engineers Standards Association (IEEE SA),Q02 Other AI risks and recommendations for regulation,"There are several potential AI risks that have been identified:Misinformation, which may need additional measures such as a Code of Practice to ensure that the private sector prioritizes disinformation (e.g., as included in the EU Action Plan against disinformation), to improve civil society awareness, to increase press and media training through practical examples and their implications.Another growing issue is one of algorithmic management in the workplace. Such systems may make unfair misattributions of employees’ intentions and character, ones which are very difficult to challenge. Consideration should be afforded as to what degree such systems can operate, especially for remote workers who may labor in domestic environments. Transparency should also be provided as to any decisions or predictions made by such systems, including the underlying predicates.For example, we suggest that lessons of ‘Robodebt’ Royal Commission into the Robodebt Scheme be considered for incorporation into "
386,Institute of Electrical and Electronics Engineers Standards Association (IEEE SA),Q03 Non-regulatory actions to address AI risks,"AI technologies continue to advance and play an increasingly significant role in our lives. AI systems should be developed and used in a manner that upholds ethical principles and safeguards the well-being and interests of individuals and societies. There is also a need to use regulatory and non-regulatory measures to raise awareness of AI, ethical use and awareness, combined with supporting AI digital literacy amongst legislators, public sector, private sector, and individuals / groups which is contextually relevant.To that end, the following responsible measures need to be considered for implementation: ● Awareness and Intelligence of AI and its implications The need to provide incentives to implement AI responsibly and within the relevant legal and non-regulatory mechanisms such as regulatory guidance, processes, frameworks and use of technical standards and assessment or testing methods like IEEE CertifAIEd’s certification within controlled environments such as regulatory sandboxes"
386,Institute of Electrical and Electronics Engineers Standards Association (IEEE SA),Q04 Coordinating AI governance across government,"Implementation of a standards-based approach is extremely important to help establish a regulatory impetus for businesses to act as compliance for best practice and to increase interoperability on an international playing field. Use of regulatory sandboxes are a safe environment to test out the standards for their usefulness and appropriateness to context, inviting the market to explore responsible innovation, for AI products to test for compliance or their societal impact, or for their standards applicability. The sandboxes can also point to any gaps in the regulatory environment. For example, we suggest that there be clear Public Procurement guidelines across the government such as those provided for in IEEE P3119TM Standard for the Procurement of Artificial Intelligence and Automated Decision Systems.We note that there needs to be greater stakeholder engagement across the government to truly understand the ethical risks and impacts on individuals / groups.Such practices as outlined "
386,Institute of Electrical and Electronics Engineers Standards Association (IEEE SA),Q05 International models for AI governance,"The European Union (EU) is putting policies and legislation in place that will impact AI systems both within the EU and those that concern any EU entity. These instruments presume conformity with requirements - whether standards, common specifications, conformity assessments, certificates, and EU declarations of conformity, which cover areas like the quality of data sets used to train, validate, and test the AI systems, technical documentation, record-keeping, transparency, human oversight, and robustness. We posit that most of these requirements must be embedded in the design of the ‘high-risk AI system’ from the earliest stages of designing and developing the AI system. Even if the provider is not the designer or developer of the system, they still need to ensure that requirements are embedded in the system to achieve conformity status. The EU AI Act also establishes a presumption of compliance with the requirements for high-risk AI systems, where a high-risk AI system is in conformi"
386,Institute of Electrical and Electronics Engineers Standards Association (IEEE SA),Q06 Public vs Private sector governance,"Similar approaches could be adopted but they must recognize the key differential between the contexts of public use of AI technologies and, and private sector use of AI technologies. It will also be important to document how each sector will be held accountable for its use of AI technologies.Fundamentally it is about the harm that can be caused by uses of AI. In Public Sector uses of AI can be pervasive and due to the concentration of power through AI systems, the harms are greater. See RoboAdvice, SyRI and Dutch social welfare system cases as examples. This is however often jurisdictionally bound and may only impact a population.Private Sector uses of AI can be ubiquitous crossing borders internationally and potentially causing harm across multiple populations and multiple sectors, including Public Sector procurement. Holding AI product/service providers accountable is harder across multiple jurisdictions, as no one country can do this alone."
386,Institute of Electrical and Electronics Engineers Standards Association (IEEE SA),Q07 Responsible government use of AI,"We suggest that the Australian Government review publication IEEE SA Ethically Aligned Design for Business, which explains the cultural and behavioral change needed within organizations, and what it takes to enable AI ethics as a core organizational competency.It provides an ‘AI readiness framework’ with steps to help facilitate a change in the organizational culture, as a proactive measure to build AI ethics into the governance structure of the organization. This would be a useful guide to support and build responsible AI practices in Australian Government Agencies."
386,Institute of Electrical and Electronics Engineers Standards Association (IEEE SA),Q08 Generic vs technology-specific approaches for AI risk,"From an ethical standpoint, different AI-enabled technologies have different potentials for algorithmic harm/erosion of fundamental/universal human rights.For instance, most AI systems share a risk for certain structural ethical issues (i.e., privacy violations, opaque/biased decision making), but some have additional, often physical consequences (such as lethal decision-making in automated vehicles, or certain AI health applications).Generic solutions would likely cover most of what matters ethically in this structural sense, but those technologies which also carry the potential for physical or lethal harm as a result of AI decision-making should also be fitted with sector-specific or technology-specific solutions. In other words, general solutions offer a way to cover most, but not all of what matters ethically, and it is important to close this gap."
386,Institute of Electrical and Electronics Engineers Standards Association (IEEE SA),Q09 Transparency requirements and scope,"Transparency in AI makes it possible to discover how and why a system made a particular decision. An AI system is considered to be transparent if it is possible to discover why it behaves in a certain way, for instance, why it made a particular decision.A system is explainable if the way it behaves can be expressed in plain language understandable to non-experts.To improve explainability to non-experts to improve public trust and confidence in AI, the general public should have a basic level of understanding of how AI systems work.We suggest that public engagement include panel debates and high quality mass media with documentaries that present emerging and AI technologies and how they work in an interesting and understandable way.Please note that accountability is very difficult without ethical transparency. Without accountability, it is most difficult to mitigate potential AI risks and to improve public trust and confidence in AI.From the user perspective, it is important to ensure t"
386,Institute of Electrical and Electronics Engineers Standards Association (IEEE SA),Q10 Ban on high-risk AI applications,"We suggest that consideration needs to be made as to the level of risk of an AI system to determine what might be unacceptable.A strong respect for mental privacy and freedom from manipulation may mean that some emergent applications should be banned or at least seriously curtailed. There is also the emerging field of neuro rights and BCIs (brain-computer interfaces) to consider when assessing risk.As a general ethical rule, choices on banned technologies should be society-specific (reflecting cultural values), rights respecting, and should mitigate the potential fallout of techno-moral solutionism (improving individual behavior via a paternalistic application of AI) even if the benefits of said technologies seem clear and seemingly desirable.For example, in Europe, the AI Act is based on the identification of levels of risks. Unacceptable risks related to AI based solutions or systems that can configure manipulation, social scoring or targeting vulnerable groups are proposed to be ban"
386,Institute of Electrical and Electronics Engineers Standards Association (IEEE SA),Q11 Actions to increase public trust in AI,"Data and AI literacy is often misunderstood as a set of technical skills, limited to data management and analysis and to the development and application of algorithms. However, data and AI literacy as a future skill of the 21st century serves to promote autonomy in a modern world shaped by data and its application as well as new technologies like AI and is therefore important for all people - not only for specialists.Data and AI literacy therefore encompasses a set of cognitive, meta-cognitive, affective, and socio-emotional competencies, which are grounded in universal moral values and enable individuals to face the challenges of digital life and adapt to its demands.There is also a need for a standardized framework to capture at least a minimum set of foundational and cross-cutting data literacy competencies relevant for an individual, organization or system. This will help to identify clear data literacy needs, support the effective targeting of policies and programs to enable data "
386,Institute of Electrical and Electronics Engineers Standards Association (IEEE SA),Q12 Trade impact of banning high-risk AI applications,"Banning high-risk activities in line with protection of established human rights can also bolster normative convergence in global AI governance. This will have beneficial network effects on the Australian tech sector/ economy insofar as incentives for human rights-respecting technologies will be high, including the potential of lowering cost barriers for export of AUS technologies, i.e., due to the need for product differentiation for different regulatory environments, etc.We posit that banning AI technologies may deter importers of such goods from trying to introduce them into Australia, even by stealth (i.e. embedded within another product)."
386,Institute of Electrical and Electronics Engineers Standards Association (IEEE SA),Q13 Changes to conformity infrastructure,Consideration might be given to extending Infosec Registered Assessors Program (IRAP) | Cyber.gov.au assessment program to encompass IEEE’s CertifAIEd program for Registered Assessors
386,Institute of Electrical and Electronics Engineers Standards Association (IEEE SA),Q14 Risk-based approach to address AI risks,"A ‘risk-based approach’ should be used, in conjunction with an ‘impact based approach’, to assess potential and actual impacts on individuals, communities and society in accordance with qualitative and quantitative well-being criteria, in accordance with IEEE 7010 – 2020TM.Note that in IEEE 7000-2021TM, the risk-based approach concerns risk to ‘human values’ like fairness, inclusiveness, justice, autonomy, etc., and not just consideration of financial risk or impacts. These approaches also recommend continuous and iterative assessment of both risk and impact, prior to, during and after deployment of AI systems."
386,Institute of Electrical and Electronics Engineers Standards Association (IEEE SA),Q15 Benefits and limitations of risk-based approach,"Limitations include use of a standard risk-based approach in corporate governance which may involve only considering financial risk and which may only be reviewed intermittently (i.e., bi-annually). AI systems need to be continuously assessed and monitored for both risk (to human values) as well as the 12 domains of well-being impact, as described in IEEE 7010 – 2020TM.Additionally, a conventional risk-based approach may not directly involve all stakeholders in the assessments. In order to be ethically aligned, all stakeholders need to be involved.There is a need to consider risks to values (i.e., other than financial impact or physical harm) as it requires careful analysis which many firms/institutions likely lack the resources to conduct themselves.Please see IEEE CertifAIEd as it creates specifications for certification and marking processes that advance transparency, accountability, and reduction in algorithmic bias in Autonomous and Intelligent Systems."
386,Institute of Electrical and Electronics Engineers Standards Association (IEEE SA),Q16 Scope and application of risk-based approach,"We posit that a risk-based approval may be particularly suitable for any sector or vertical involving public or individual safety, privacy, legal rights, human rights, health, and wellbeing."
386,Institute of Electrical and Electronics Engineers Standards Association (IEEE SA),Q17 Elements to include in risk-based approach,"Impact Assessments should be holistic, and derived from evaluating the 12 domains of well-being, as described in IEEE 7010-2020 TM.We suggest a full examination of the IEEE 7001 TM standard for appropriate transparency requirements and assessments for each type of stakeholder, particularly to enable them to seek redress or contestability of decisions made by the AI system.Human in the Loop and oversight assessments should be considered as outcomes to use of transparency standards as recommended by IEEE 7001TM and accountability considerations, in order to demonstrate the human operator's capacity to challenge the system. (This is also a component of CertifAIEd TM Accountability criteria).Explainability is important to build public trust and should be based on definitions within IEEE 7001 TM.Training and Education should involve appropriate and sufficient ‘AI Literacy’, including AI ethics readiness framework as described in ‘Ethically Aligned Design for Business’. With AI Literacy, org"
386,Institute of Electrical and Electronics Engineers Standards Association (IEEE SA),Q18 Integration of risk-based approach into existing frameworks,"The AI risk-based approach of IEEE 7000-2021 TM assumes use of an organization’s own risk-based frameworks, assessment frameworks and management processes, thus reducing potential duplication."
386,Institute of Electrical and Electronics Engineers Standards Association (IEEE SA),Q19 Application of risk-based approach to general purpose AI,"The risks of such systems are amplified due to an ability for unanticipated functions to lurk within models until elicited via a particular prompt sequence. This is especially the “case” where models may gain unexpected new capabilities as they increase in scale, which is very difficult to predict in advance.Anticipation of the risk to human values (including environmental risks) would be facilitated by using IEEE 7000-2021 ™, by including representation of all stakeholders involved (including future generations), and mitigation to these human values during the design process. Use of IEEE 7001 TM would help anticipate the requirements for transparency of the input training datasets, particularly IP, copyright, and privacy risks as well as transparency to the public and users as to how and why recommendations and suggestions are produced and ethical decisions were made during development and design, and use of IEEE 7010-2020 TM would anticipate the impacts across 12 domains of well-bein"
386,Institute of Electrical and Electronics Engineers Standards Association (IEEE SA),Q20 Voluntary vs mandated compliance with risk-based approach,"There is a need for both voluntary self-regulation and for regulatory guidance regarding compliance to be established due to the potentially high risks and consequences involved with AI systems.Both approaches can be used. Voluntary standards developed by global standards bodies like the IEEE can be referenced by legislation.Voluntary standards are a form of ‘soft law’ that are more adaptive and responsive to change, managing AI and associated risk than legislation is able to adapt to technological changes.An AI system, in its entire life cycle, brings potential technical and organizational risks affecting it that must be identified, documented, and accessible. A similar approach should apply to both sectors. The origin and reasons of these potential risks, responsible person, measures to be taken, and actual situation upgrading, also must be identified, and documented.Periodically tests and checks models for ethical issues and bias must be run.To prevent attacks on the AI System with "
388,"Scarlet Alliance, Australian Sex Workers Association",Transcription,"FILENAME: Scarlet Alliance Submission Responsible AI Australia 2023 (1).736fa216fcad4.pdf | CONTENT: Phone – 02 9517 2577 Fax – 02 9517 2488 Post – PO Box 854, Newtown NSW 2042 Street – Suite 203, 1 Erskineville Road, Newtown NSW 2042 Email – info@scarletalliance.org.au Web – www.scarletalliance.org.au ABN - 86 612 112 06526 July 2023 Department of Industry, Science and Resources - Technology Strategy Branch GPO Box 2013 Canberra ACT 2601Via email: DigitalEconomy@industry.gov.auTo the Department of Industry, Science and Resources,Thank you for the opportunity to submit to the consultation on Safe and responsible AI in Australia.Scarlet Alliance is the Australian Sex Workers Association. Through our objectives, policies and programs, Scarlet Alliance aims to achieve equality, social, legal, political, cultural and economic justice for past and present workers in the sex industry.Formed in 1989, Scarlet Alliance, Australian Sex Workers Association is the national peak sex worker organisa"
388,"Scarlet Alliance, Australian Sex Workers Association",General comment,Scarlet Alliance is happy for our submission to be made public.
388,"Scarlet Alliance, Australian Sex Workers Association",Q02 Other AI risks and recommendations for regulation,This question is discussed in our attached submission.
388,"Scarlet Alliance, Australian Sex Workers Association",Q03 Non-regulatory actions to address AI risks,This question is discussed in our attached submission.
388,"Scarlet Alliance, Australian Sex Workers Association",Q06 Public vs Private sector governance,This question is discussed in our attached submission.
388,"Scarlet Alliance, Australian Sex Workers Association",Q07 Responsible government use of AI,This question is discussed in our attached submission.
388,"Scarlet Alliance, Australian Sex Workers Association",Q09 Transparency requirements and scope,This question is discussed in our attached submission.
388,"Scarlet Alliance, Australian Sex Workers Association",Q10 Ban on high-risk AI applications,This question is discussed in our attached submission.
388,"Scarlet Alliance, Australian Sex Workers Association",Q11 Actions to increase public trust in AI,This question is discussed in our attached submission.
388,"Scarlet Alliance, Australian Sex Workers Association",Q14 Risk-based approach to address AI risks,This question is discussed in our attached submission.
388,"Scarlet Alliance, Australian Sex Workers Association",Q15 Benefits and limitations of risk-based approach,This question is discussed in our attached submission.
388,"Scarlet Alliance, Australian Sex Workers Association",Q20 Voluntary vs mandated compliance with risk-based approach,This question is discussed in our attached submission.
389,Katherine Biewer,Transcription,"FILENAME: Response to AI Inquiry_KB.0865439738e2e.pdf | CONTENT: Response to the Supporting responsible AI: discussion paper Katherine Biewer Foreword: In the following sections, I present my answers to questions 1, 2, and 20. 1. Do you agree with the definitions in this discussion paper? If not, what definitions do you prefer and why? Definitions are not dynamic The definitions outlined in Figure 1 are both too specific in some respects and lack important detail in others. The definitions focus primarily on defining a set of “technologies” and “applications” that do not appropriately encapsulate the spectrum of capability, complexity and risk associated with AI.For example, the definition of Machine Learning, while not incorrect, is oversimplified. Machine Learning is not merely about deriving patterns from training data; it is a subset of AI that uses statistics to give algorithms the ability to ""learn"" from data, thereby improving their performance on a specific task over time witho"
390,Digital Service Providers Australia New Zealand (DSPANZ),Transcription,"FILENAME: Safe and Responsible AI in Australia Submission.2dff8a64a8c1e.pdf | CONTENT: 217 Flinders Street hello@dspanz.org Adelaide SA 5000 dspanz.org26 July 2023Department of Industry, Science and Resources Industry House 10 Binara Street Canberra ACT 2601Via online form.Re: Safe and responsible AI in AustraliaTo Whom It May Concern:The Association of Digital Service Providers Australia New Zealand (DSPANZ) welcomes the opportunity to make this submission on behalf of our members and the business software industry.About DSPANZ Digital Service Providers Australia New Zealand is the gateway for the government into the dynamic, world-class business software sector in Australia and Aotearoa New Zealand. Our 90+ members range from large, well-established companies to new and nimble innovators working at the cutting edge of business software and app development on both sides of the Tasman.DSPANZ supports the government exploring options to mitigate risks and support safe and responsible ar"
391,Dr Michael Douglas,Transcription,"FILENAME: Douglas, Submission on Supporting Responsible AI consultation-merged.28906777d0b04.pdf | CONTENT: 8th February 2017 4 August 2023Department of Industry, Science and Resources Australian Government By Email: DigitalEconomy@industry.gov.auTo Whom It May ConcernSubmission on Supporting Responsible AI consultation I write to make a submission with respect to the Department’s Supporting Responsible AI consultation. The consultation’s discussion paper asks, ‘What potential risks from AI are not covered by Australia’s existing regulatory approaches? Do you have suggestions for possible regulatory action to mitigate these risk?’ One risk that is not mentioned in the discussion paper explicitly concerns how AI-driven platforms, like ChatGPT, may damage Australians’ reputational interests. On that topic, I attach a recent paper of mine on the use of defamation law to respond to harm caused by ChatGPT. I encourage the Department to liaise with the working party driving further changes t"
391,Dr Michael Douglas,Q02 Other AI risks and recommendations for regulation,See submission.
391,Dr Michael Douglas,Q03 Non-regulatory actions to address AI risks,See submission.
391,Dr Michael Douglas,Q04 Coordinating AI governance across government,See submission.
392,Anonymous,General comment,"I'm a citizen of the United States, residing in Chicago, IL. I'm a reader of Comos' emails. My comments will apply to very ""citizen"" of the world"
392,Anonymous,Q02 Other AI risks and recommendations for regulation,"I don't believe that any governmental body comprehends the full extent of AI developments, including distortion of opposing political views, machine control of militaries, cancellation of even attorneys and aids when legal briefs can be composed by AI and law books are not needed, and even the elimination of legislative bodies by robots determining the welfare of mankind. Already the SAGAFTRA union in the US is worried about the impact on their royalties The propensity of mankind to improve on something which has already been invented is documented. There was the first spear, bow and arrow, etc . The first atomic bomb required a bomber to deliver it, now more powerful warheads can be delivered in 30 minutes in a nosecone. I gave up watching Star Track years ago."
392,Anonymous,Q03 Non-regulatory actions to address AI risks,"The genie is out of the bag. When one county makes an improvement, other countries including Russia, China, and North Korea are sure to follow."
392,Anonymous,Q04 Coordinating AI governance across government,"There must be a recognition among countries eventually that AI should be placed on the level of mutual mass destruction, as are atomic and hydrogen bombs. Then beneficial uses can be developed, just as we have nuclear reactors and hopefully, if we survive that long, unlimited hydrogen power."
392,Anonymous,Q06 Public vs Private sector governance,"We live in capitalistic societies so that public institutions should develop policies and limitations, and possibly enforcement and the private sector should benefit from implementation and profits. An example of this would be Commonwealth Edison which runs nuclear plants in Ilinois under the supervision of the government. The only problem with this would be that there will too many things to supervise"
392,Anonymous,Q07 Responsible government use of AI,"It will be an additional problem, for example, the military conducts research in secret. Medical and some research is funded by governments through universities, etc."
392,Anonymous,Q08 Generic vs technology-specific approaches for AI risk,"Generic controls can be monitored by informed individuals keeping up with advances in the field. It might also be possible for AI to determine and publish such advances. However, there is always the possibility of AI going rouge and keeping such advances secret."
392,Anonymous,Q09 Transparency requirements and scope,"The public is already being kept ignorant of some military advances, as well as most in Congress even though it funds the activity in blanket budgets. Even until now has there been some disclosure on whether UFOs could exist. There is a theory that intelligent beings have not sent signals to earth because they have already destroyed themselves. Some risks are unavoidable such as less human labor will be required for economic output. There will be no banning ""progress"" in technology, even when that technology becomes self destructive. For example, consider the alteration of viruses to make them more deadly."
392,Anonymous,Q11 Actions to increase public trust in AI,"School children and high schoolers can be taught more STEM subjects. AI will be introduced to the public, such as Google's search engine, and advances in self driving cars and trucks. Many people may choose to remain ignorant and in bliss."
392,Anonymous,Q12 Trade impact of banning high-risk AI applications,"Competition is a great motivator not to go backwards including in products already on-line. If I have this and you don't, then how are we to come to a common footing? Google needs its ad revenue. The first step is for recognition of the social detriment, then to get the interested portion of the public to demand a change. We are past the days when the car made buggies totally obsolete. AI can only become more progressively consuming, complex. and if allowed, more dangerous. What boy might be tempted to put an innocent ex girlfriend's head on a nude body. Or a man might take a realistic AI as a ""women"" to a bar for drinks."
392,Anonymous,Q13 Changes to conformity infrastructure,"Well, we could take a clue and not become China, where its citizens are facially monitored walking down the street in 1984 fashion, and their next advance could be to meet a curfew for everybody. Our advances could be directed towards what we are as a society."
392,Anonymous,Q14 Risk-based approach to address AI risks,"There is problem of drawing the line between ""'risks"" and ""benefits."" For example, a couple may want their dog cloned, except for a different tail, and AI may be able to make this change. The medical human advancements are rapid. What if the couple wants their male child's head to look like Marlon Brando so that he will have a better chance to be a movie star. Where's the line? What's acceptable to the social norms of the society at that time"
392,Anonymous,Q15 Benefits and limitations of risk-based approach,"See above. AI may eventually become so all comprehensive that there will be no control, especially if humans look for that control in AI itself."
392,Anonymous,Q16 Scope and application of risk-based approach,"A risk-based approach can possibly be applied to media where the information product is going through centralized sources/and or monitors, such as the cloud or media central controls, and is distributed broadly to deceive or for other malicious purposes. However, it will still be difficult to show what is a violation of free speech, modified plagiarism, and the like. A disbursed source such as hackers will be difficult to trace because they can simply move locations."
392,Anonymous,Q18 Integration of risk-based approach into existing frameworks,"Unlike present day search engines which do, or used to, focus only on stated key words, AI will be developed to read for content AND conscious understanding of language and interpretations, with only a general prompt to do so. And not provided with direction by questions, and its response without thought. This emotional approach will still contain built in biases and limitations contained in the human mind, if its source was first the human mind. They will continue to persist in the AI. When the AI is free of this, it will control mankind. Until then, each advance of AI must be met with offsetting parameters. But how can AI be righteous when mankind is not. AI may conclude that manklnd is not needed."
392,Anonymous,Q19 Application of risk-based approach to general purpose AI,"A risk based approach must be reinforced with public feed back, through legislative committees and expert testimony, and maybe statutes. Responsible change makers will respond earlier and will not need the latter prompts."
392,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,"All of the above. The advances will likely come first, then mandated regulations where self- regulation is not is sufficient. Regulations can be semi-public such as with the medical and legal professions, and through legislation."
393,Wiley,Transcription,"FILENAME: Wiley - Australia Public Consultation - Safe and Responsible Artificial Intelligence - 4 Aug 2023 CG.797986b75566b.pdf | CONTENT: August 4, 2023Technology Strategy Branch Department of Industry, Science, and Resources 10 Binara Street Canberra ACT 2601RE: Public Consultation on Safe and Responsible Artificial IntelligenceTo whom it may concern:Thank you for the opportunity to share our views on the Australian Government’s efforts to develop a Safe and Responsible Artificial Intelligence (AI) framework. We appreciate this important feedback mechanism and look forward to working with the Department of Industry, Science, and Resources and other stakeholders to deliver meaningful input and help develop policy outcomes that will harness the benefits and mitigate the risks of AI.Founded in 1807, Wiley is one of the world’s largest publishers and a global leader in research and learning. For more than 215 years, we have been enabling discovery, powering learning, and shaping workfor"
394,Human Compatible,Transcription,"FILENAME: human_compatible-submission-hmc7s3ekwko2jpav.0b1cd68928f93.pdf | CONTENT: 04/08/2023, 12:48 (human compatible) (submission)(human compatible) (submission) The Australian Government seeks public feedback on potential AI risk mitigation strategies and the promotion of safe and responsible AI practices, intending to build governance mechanisms and inform appropriate regulatory and policy responses to foster public trust and maximize the benefits of AI.This document is a response to this request submitted by Human Compatible.(synopsis) (discussions)The synopsis of our response:(themes)Support for the definitions in the discussion paper and the overall risk-based approach to AI governance.Emphasis on a balanced regulatory approach that builds on existing frameworks while addressing emerging issues as they arise. Avoid overly restrictive regulation that could stifle innovation.Recommendations for additional non-regulatory initiatives like independent advisory councils, testing faci"
394,Human Compatible,General comment,"We are in broad agreement with the perspectives outlined in the discussion paper, but believe there needs to be more attention applied to the macro risks and opportunities; economic, political, social and cultural."
394,Human Compatible,Q01 Definitions,Yes.
394,Human Compatible,Q02 Other AI risks and recommendations for regulation,"Key risks not well covered in existing regulatory approaches include consideration of the systemic impacts of AI on the economy and society, mechanisms to respond to emergent capabilities and associated risks, and insufficient explainability and accountability protocols as will be needed to apply to the unique challenges of AI.Australia will need to develop an ""elastic governance architecture"" to assess these macro risks effectively. This will require a new approach to auditable solutions to address the fast changing impacts of new capabilities arising from generative AI, and new AI methods as they are developed.Regulatory steps should include mandated impact assessments analysing societal risks, expanded regulatory powers to audit AI systems against well developed standards, and bans on harmful uses of emerging capabilities, as the paper rightly touches upon.We recommend a balanced approach, one that builds on existing frameworks, while seeking to address the gaps as they become evide"
394,Human Compatible,Q03 Non-regulatory actions to address AI risks,"Well implemented non-regulatory initiatives are likely to have as much impact, if not more, than regulatory measures. Rapid adoption of AI in Australia will be vital to the nation's future economic prosperity and social well being. However, this will not happen without a coordinated approach by Governments at all levels. The following options should be canvassed:Establish an independent advisory council on AI ethics and governance. This should bring together experts from industry, academia, government and the community, to provide impartial advice on AI policy issues. The field of AI is rapidly evolving and will require ongoing input to address previously unknown issues.Develop voluntary AI certification schemes with industry.Establish open and transparent AI testing facilities. These testing sandboxes would allow companies to trial AI safely before deployment. Facilities could be public or privately run with government support. In order to ensure market competition small companies sho"
394,Human Compatible,Q04 Coordinating AI governance across government,No.
394,Human Compatible,Q05 International models for AI governance,"We are not aware of measures taken by other countries beyond the comprehensive list included in the discussion paper, other than anecdotally. The measures discussed seem appropriate in the Australian context as well. Australia is not a nation at the forefront of new AI model development at this time and some of the measures relating to this may not be relevant at the moment. However, it would be prudent to avoid the assumption that this will always be the case."
394,Human Compatible,Q06 Public vs Private sector governance,"In a word, yes, but the approaches for public and private sector use should not differ by much.Where there are differences they would be because:(A) The public sector has public accountability obligations in ways the private sector does not. Moreover,public sector AI use, especially in law enforcement, likely has a greater potential impact on fundamental individual rights than many private sector activities. This potential impact has been highlighted by the recent Royal Commision into failures of automated decision making deployed by Centrelink.(B) The private sector is driven by competition and needs flexibility to innovate. It encompasses a diversity of contexts, requiring more tailored governance. As discussed below, our view is that a risk based approach appropriately balances legislative obligations with the need for flexibility across different types of organizations.We do not have specific recommendations on this point and in general argue for differences beyond these two motiva"
394,Human Compatible,Q08 Generic vs technology-specific approaches for AI risk,We agree that AI can be combined with other technologies in innovative products and services and that the complexity arising from this may necessitate context-specific regulatory responses. We do not have specific examples.
394,Human Compatible,Q09 Transparency requirements and scope,"We agree with the suggestions in the paper, especially the importance of transparency in data sourcing and testing, mandatory incident reporting, and impact assessments. We also support full transparency to end users and that model development processes align with human values."
394,Human Compatible,Q10 Ban on high-risk AI applications,"There are very few AI applications or technologies we know of that we believe should be banned completely. However, the most powerful of the new AI models should be carefully monitored and regulated as we have previously outlined and we agree with most of the banning cases discussed in the paper for various forms of AI.For example, we support bans on biometric or other identification technologies except in well defined use-cases. In those we recommend regulations that ensure transparency with respect to the uses for which such data are collected.We are less enthusiastic about bans on ChatGPT or similar tools in schools and universities. Such actions are a ""head in the sand"" response to the reality that such tools now exist and the education system needs to adapt to them, not the other way around.We support the approach adopted in the proposed EU AI Act's ban on certain uses of AI that pose an 'unacceptable risk' like social scoring and the development of criteria to ban AI where impact"
394,Human Compatible,Q11 Actions to increase public trust in AI,"This is a fundamentally important role for Government because of the disruptive impact AI will play in many people's lives. Those who avoid it will, in many cases, become economically and/or socially disadvantaged as a result. Without such initiatives, AI may engender unwarranted fear or mistrust in the public and its response to it. It is therefore incumbent on Government to encourage the safe and responsible adoption and use of AI by everyone.We agree with the paper's advocacy of voluntary ethical principles and codes of conduct, formal public education campaigns, transparency mechanisms and conformity infrastructure and assurance processes and an emphasis on safety by design.The key point here is to ensure Government is seen as a backstop for anyone adversely affected by AI, e.g. via loss of their job or an entire industry. AI will fundamentally change the nature of employment, and Government’s role is to ensure that all parts of the community benefit from the AI dividend, rather th"
394,Human Compatible,Q12 Trade impact of banning high-risk AI applications,"The potential impacts of Australia banning certain high-risk AI applications will likely be minimal in the near term, given the current makeup of the nation's tech sector and economy. AI technology exports are not yet a major contributor to economic activity. As such, prohibiting narrowly defined applications like facial recognition and social scoring would be unlikely to significantly restrict the sector's overall performance.However, if these technologies represent emerging opportunities, inconsistent policies compared to trading partners could incrementally disadvantage Australian tech companies in the long run. The risks would be higher if banned applications eventually make up a sizeable portion of AI-related exports but we believe the impacts will likely be modest if restrictions remain confined to demonstrably high-risk uses, rather than blanket bans."
394,Human Compatible,Q13 Changes to conformity infrastructure,"We is comes to conformity infrastructure for AI, Australia appears to have very litte. Developing this to include AI assurance will likely require considerable effort and investment, referencing approaches in other advanced economies like the US, UK and EU.There is and will continue to be a shortage of expertise globally in auditing and evaluating emerging AI technologies against standards of fairness, safety and accountability for quite some time. Our organisation, which has specialists in the field, is small and there are few others like us. Australia needs to fund capability building, including through tertiary and research institutions, to grow this skilled talent pool locally.Leveraging work by international bodies to harmonise standards and conformity assessment protocols is also important to help establish unified benchmarks and mutually recognised standards. Interoperability would support trade and technological development in our view.As noted in the Discussion Paper, work don"
394,Human Compatible,Q14 Risk-based approach to address AI risks,"Yes, we support a risk based approach as advocated in the paper.We are not aware of other approaches that have proven more effective.However, the challenge will be in identifying and acting to mitigate future risks, before they appear. We see as unlikely, but not impossible, some of the concerns raised by others of the potential for the most powerful AI to achieve ""human intelligence"" without alignment to ""human values"". This is a risk we believe, that if manifest, would prove difficult to mitigate after the fact. Moreover, the dangers posed by these risks materialising are of a magnitude that the AI community must prioritise serious consideration of mitigation strategies.We therefore advocate a robust risk-based framework be combined with precautionary measures against plausible long-term scenarios such as this. This needs to be undertaken with evidence-based oversight of existing AI applications to inform continued research and monitoring of AI capabilities. This will help guide appr"
394,Human Compatible,Q15 Benefits and limitations of risk-based approach,"We agree with the general assessment of the risk-based approach.The prime benefits are proportionality balancing innovation with risk abatement, flexibility being able to adapt the response to context, a focus on high risk activities as previously discussed and the development of a risk management mindset in both the public and private sectors.The limitations are primarily focused on how risk is defined and assessed. It can be highly politically charged in some situations and risk profiles can change faster than regulatory responses can keep up, especially in the field of AI where development of new technologies is increasing at an exponential rate."
394,Human Compatible,Q16 Scope and application of risk-based approach,"Yes. Absolutely, but only in those sectors that have an outsized impact on the health, financial and physical wellbeing of individuals.We can only speculate on how the size and maturity of organizations using AI could factor into how suited they are for risk-based requirements; larger entities with more resources willbe better equipped to thoroughly assess risks, while smaller groups will find this more burdensome. And those new to AI may initially struggle with risk analysis without experience.However, we caution that overly differential treatment could lead to imbalanced policy and there is an argument that comprehensive risk-based requirements across all sectors and organisation sizes would encourage widespread capabilities and expectations for responsible AI."
394,Human Compatible,Q17 Elements to include in risk-based approach,"In our view the components listed in the paper should all be included in a risk-based AI governance approach. We would further advocate:External and internal auditing requirements to provide important verification of risk controls and documentation, especially for higher risk AI systems. Mandatory independent audits improve accountability but need to be balanced against the resource cost associated that such audits would entail .Ongoing monitoring of AI systems by users and developersRequiring minimum general liability coverage for organisations deploying high or very high-risk AI seems prudent to ensure resources are available if harms materialize that require redress.Mandatory external certification for the highest risk applications could also improve safety and reliability assurance through independent evaluation by accredited bodies.Access to large, high-fidelity data sets is critical for the safe and effective deployment of machine learning systems, especially for sensitive applic"
394,Human Compatible,Q18 Integration of risk-based approach into existing frameworks,"AI is quite unique in many ways. Care should be taken not to dilute focus on AI-specific issues. However, developing AI-specific guidelines or criteria that can plug into prevailing risk management methodologies like ISO 31000 and CPS 230 makes sense. For example, Box 1 has some relevant proposals from the Privacy Act Review report that relate to transparency and explainability for automated decision making that impacts individuals with which we agree."
394,Human Compatible,Q19 Application of risk-based approach to general purpose AI,"These AI technologies are were the greatest risks mostly likely apply. Risk assessments will depend on the nature of the model. Supervised learning models, unsupervised learning models, reinforcement learning models, or generative models will each have different risks. For example, reinforcement learning systems can have issues related to reward hacking where the model finds unintended ways to maximize its reward function. There are many other modes of risk and they are being discovered daily. For example, https://llm-attacks.org which presents a paper that proposes an advanced method of creating adversarial attacks on large language models by generating specific suffixes that increase the likelihood of objectionable content production, highlighting the method’s transferability across different models and raising important questions about the prevention of such undesirable outcomes."
394,Human Compatible,Q20 Voluntary vs mandated compliance with risk-based approach,"It should be both in all cases, tailored as discussed in earlier questions. We have a vision statement that covers our view of the big picture which may help inform your review about the principles we use to arrive at our conclusions. https://docs.causal.com/U5B0TGXB/xr0iz3t14sxi3wul/"
395,Robert Chalmers,General comment,"I had the benefit of collating and contributing to an earlier submission of 26 July by Dr Cebulla, myself, Dr Johnsam, Professor Leiman and Dr Scheibner and I adopt its comments in particular its framing comments"
395,Robert Chalmers,Q06 Public vs Private sector governance,"As a matter of principle, it is hard to conceive why such a distinction should be drawn. those sectors do not exist in isolation from each other and are often engaged in collaborative or contractual relationships, so disparate coverage of regulation could then get highly complex or be gamed, to no effect. Perhaps government can attempt to demonstrate best practice in its approach, that is about as far as I would take it."
395,Robert Chalmers,Q08 Generic vs technology-specific approaches for AI risk,"Given the thesis I have adopted from the outset, that the real risks are not technology risks per se, my position is that the generic approaches to these fundamental underlying issues are the most important issues to address. This is not straightforward as these are often handled by large pre-existing regulatory systems with significant entrenched positions and inertia around how they conceive of and regulate those matters. This necessarily entails a careful and not rapid process of consideration of the potential need for change. Again as indicated above, while I believe that it is these fundamentally human issues that are important, the impact of technology on those issues and socio-technical behaviour must be acknowledged, and this may require change that is influenced by shifts in the affordance of our tools, as that can induce or permit behavioural shifts. Conceptually such approaches are more likely to have some embedding in the form of legislation or flexible equitable and human "
395,Robert Chalmers,Q14 Risk-based approach to address AI risks,"A risk based approach has a lot of superficial appeal and we can see international examples moving down this path. However the reality is more complex, in that risk ratings occur in a context, and if that context shifts, then the same example that is initially rated as low risk may become high risk, or vice versa. Values and value based analysis are also varied and fluid depending on the context. Of course there are a variety of other factors to consider here including the classic Collingridge dilemma, in terms of the timing of any intervention, the degree of uncertainty in outcome, and the dangers of operational entrenchment vs over regulation. One might also question harder the segmentation attempted in the discussion paper into economy wide vs application specific approaches, as everything is interconnected. To consider a specific example, look at Low risk classifications for email filtering tools. At first blush this might seem fine, but subtle bias could lock out exchanges and man"
395,Robert Chalmers,Q20 Voluntary vs mandated compliance with risk-based approach,"It makes business and operational sense for organisations adopting AI to do so in a responsible way to manage their impact on others and avoid the obvious downside risks that may flow from irresponsible or reckless implementation. This apples even without specific AI focussed regulation as there are many laws that may be violated by poor implementation, and even if laws are not broken there may be significant loss of social licence to operate that will impact viability and reputation. To the extent that mandatory regulation is adopted (either pre-existing laws require modification to better capture AI related risks or to the extent that targeted new laws are introduced), then it makes little sense to segment the application of it as between public and private organisations. Why? Because those sectors do not exist in isolation from each other and are often engaged in collaborative or contractual relationships, so disparate coverage of regulation could then get highly complex or be gamed"
397,Castlepoint Systems,Transcription,FILENAME: Supporting Responsible AI Discussion Paper - Castlepoint Submission 20230804.a689884bb1116.pdf | CONTENT: 1Supporting Responsible AI Submission 20230804 2CASTLEPOINT – SUPPORTING RESPONSIBLE AIContents What do we do? ...................................................................................................................................................................... 4 Why do we do it?.................................................................................................................................................................... 4 What do we think? ................................................................................................................................................................. 4 Responses ...................................................................................................................................................................................... 5 Definitions ................................
397,Castlepoint Systems,Q01 Definitions,"We agree with the definitions but recommend also including definitions of ‘black box’ and ‘white box’ AI in any policy guidance or regulation, as the differences between the two are fundamental to the discussion of Ethical AI and explainability. Note that these may also be referred to as ‘closed box’ and ‘open box’ AI respectively, and this may be more appropriate and inclusive terminology to adopt."
397,Castlepoint Systems,Q02 Other AI risks and recommendations for regulation,"The National Archives of Australia is not represented in existing policy and guidance listed in the paper. The NAA does have some guidance on use of emerging technologies, and refers government records managers to the Industry AI Ethics Framework, but currently does not take a position on explainability for AI or other ADM with regards to information management.AI for records management has been adopted across most federal government portfolios in Australia as of the date of this submission. Agencies are using both explainable and non-explainable AI in the governance of records, including deciding what records are preserved, what records are destroyed, and what records are protected. These types of decisions are irreversible once applied, and can cause unintended consequences and harm.While the Archives Act is and should remain technology-neutral, the NAA should take a formal position on the level of risk posed by ADM for records governance, and require agencies to apply commensurate o"
397,Castlepoint Systems,Q03 Non-regulatory actions to address AI risks,"Education and communication are an important part of supporting change from a bottom-up perspective. From the top-down, making Boards and individual Directors accountable for failures in compliance can demonstrably improve adoption of good governance."
397,Castlepoint Systems,Q04 Coordinating AI governance across government,"It is going to be important to define the risk criteria at a government level. Risk-based approaches to any compliance can result in very different appetites across different agencies, and affect the overall governance posture in Departments. Having different risk cultures creates significant issues for governance alignment in Machinery of Government changes.Reliance on the ANAO or NAA to audit compliance, or a formal Inquiry, is usually a retrospective activity which follows harm that has already been realized. At the point that systems are processes are already embedded, it becomes hard for agencies to respond to the recommendations. For example, after the Comrie and Palmer reports into the unlawful treatment of Vivian Solon and Cornelia Rau, the Immigration Department embarked on a more than three quarter of a billion-dollar business transformation program over five years called Systems for People. But a capability review in 2012 found that the Systems for People program was not suc"
397,Castlepoint Systems,Q05 International models for AI governance,"The global environment is evolving, but all advanced economies are reflecting the same essential principles and expectations for ethical AI. The Australian government should continue to monitor the application of these principles, particularly how they are communicated and enforced. Lessons from other economies can be applied to Australia, through a lens of our corporate culture and customs."
397,Castlepoint Systems,Q06 Public vs Private sector governance,"No – the intention of ethical AI is to protect the rights, entitlements, and safety of citizens. Both governments and corporates hold information about citizens, the misuse or mismanagement of which can cause significant harm.In the same model as the Security of Critical Infrastructure Act, some private sector verticals can be considered higher risk for use of AI and ADM, and therefore requiring higher levels of oversight. The focus here should not be based solely on national security per SOCI, but also on individual safety (including psychological safety)."
397,Castlepoint Systems,Q07 Responsible government use of AI,"It is important first for the Australian Government to recognize the potential for harm to individuals that has not previously been high-profile: mismanagement of their records.Under-retention of records affected the Windrush generation, Caribbean immigrants to the UK who had all their landing card records destroyed by the UK Home Office records team to save space, resulting in many being unlawfully deported. Keeping records for too short a time has affected the chances of reparations for the Stolen Generations, Maralinga atomic testing survivors, and child sexual abuse victim survivors in private and government institutions in Australia. Destruction of records is an example of an impact that is ‘irreversible and perpetual’.Over-retention of records has been newsworthy recently: Optus and Medibank spilled sensitive records that their previous customers did not expect them to still be holding. When the Australian National University was breached in 2018, 19 years of staff and student re"
397,Castlepoint Systems,Q08 Generic vs technology-specific approaches for AI risk,We do not have a comment on this question at this time.
397,Castlepoint Systems,Q09 Transparency requirements and scope,"Robodebt is the most recent and high-profile example of the importance of transparency for recourse, even though it was not an AI solution. All algorithms must be contestable, AI or otherwise. Incorrect algorithms gave rise to the Horizon postmaster scandal in the UK in the 1990s, but the harm from those algorithms was multiplied by the inability of the falsely accused to argue against the algorithm. Transparency must be mandated. The challenge will be to regulate the more subjective aspects of transparency – what is a sufficiently ‘meaningful explanation’? The regulations may require a Man on the Clapham Omnibus test to set a precedent."
397,Castlepoint Systems,Q10 Ban on high-risk AI applications,We do not have a comment on this question at this time.
397,Castlepoint Systems,Q11 Actions to increase public trust in AI,"We do not believe the public will avoid using AI. The market will drive adoption, whether the AI capabilities of commercial and government solutions are disclosed or not."
397,Castlepoint Systems,Q12 Trade impact of banning high-risk AI applications,There is always a tradeoff between human rights and optimized commercial outcomes. Australia should not consider undermining human rights for any trade advantage.
397,Castlepoint Systems,Q13 Changes to conformity infrastructure,We do not have a comment on this question at this time.
397,Castlepoint Systems,Q14 Risk-based approach to address AI risks,"All decisions should be informed by risk. What is in question is the standardization of the mapping of that risk. ‘Risk-based’ approaches that allow organizations to self-assess their risk levels subjectively are likely to create perverse incentives to minimize risk levels, and subsequently dilute risk treatments."
397,Castlepoint Systems,Q15 Benefits and limitations of risk-based approach,"Risk should be determined based on the kind of harm that can arise, in a similar model to the standardized PSPF Business Impact Levels tools used by the Federal Government. This mitigates the risk of risk minimization as it provides more objective parameters on which to determine real risk."
397,Castlepoint Systems,Q16 Scope and application of risk-based approach,"As AI ethics is about protecting citizens, the standardized approach should be the same, no matter who is using the AI. Risk should be mapped against human outcomes, before organisational ones. If an organisation could cause harm to an individual or social group as a result of ADM or AI, that ADM and AI must be explainable and contestable. It is the size and type of the harm that matters, not the size and type of the company."
397,Castlepoint Systems,Q17 Elements to include in risk-based approach,"We agree with the use of Impact Assessments, following a similar structure to the PSPF BILs, but also aligned with the OAIC Privacy Impact Assessment framework and tool."
397,Castlepoint Systems,Q18 Integration of risk-based approach into existing frameworks,"The government must be careful not to create feedback loops between different regulations, which point only to each other and do not provide material guidance.There should be one source of truth on AI Ethics regulation, which should be addressed, and further contextualized, in all legislation relevant to citizen outcomes."
397,Castlepoint Systems,Q19 Application of risk-based approach to general purpose AI,"Closed box AI cannot be used for regulatory purposes until such time as it can be made explainable. Universal principles on Ethical AI require that • Algorithmic-based decisions which affect individuals cannot be made without human oversight • Algorithmic or automated decisions are explainable where they have a detrimental impact • The explanations behind how the decision was reached are meaningful, useful, and valid.This is not currently possible with LLMs, neural nets, or supervised Machine Learning. Relying on records of the training data originally used for these types of AI is not sufficient to the purpose, as they evolve constantly without human oversight.We believe these types of AI have an important role to play, and a powerful utility for government and corporations. But they must not be used where the outcomes of their decisions can harm individuals."
397,Castlepoint Systems,Q20 Voluntary vs mandated compliance with risk-based approach,"We believe that Responsible or Ethical AI must be mandated, and must apply to all organizations who hold data about citizens, and/or make decisions using that data which could detrimentally affect those citizens."
398,Hannah Maude,Transcription,"FILENAME: Responsible Ai Consultation - Hannah Maude Group.bdaca4658e4fb.pdf | CONTENT: Responsible Ai ConsultationHannah Maude conducted a group session with women 29 - 50 at the Microsoft Reactor. All had business backgrounds, a couple had Tech experience.1. The Government needs to keep up with technology or we’re led by the corporations.- Who are the experts in the Government and how can we get to a place where there are as many experts who can challenge the Tech experts who are profiting from the technology? Ai is worthy of investment, and an internal (Government) upskill drive. - The public should also know who is making decisions on Ai, the matter is of such a degree of national interest.2. Education and awareness is required to help people make informed decisions and to ensure their trust of it.- This should be done as a TV and Social Media campaign and feel more like entertainment (think a Netflix documentary series or higher end Reality TV show). - Purpose to help ensure every"
398,Hannah Maude,Q11 Actions to increase public trust in AI,A National Train the Trainer led by community experts and role models would help build trust (and positive word of mouth) - Hannah Maude
399,Reason Group,Transcription,"FILENAME: Reason submission - Supporting Safe and Responsible AI in Australia 2023.b0badcceb3649.pdf | CONTENT: Safe & Responsible AI in Australia INDUSTRY SUBMISSION4 AUGUST 2023Reason Group outlines a practical initiative to inform the Government on (Target areas Question 7) How can the Australian Government further support responsible AI practices in its own agencies?DIGITAL GOVERNMENT SPECIALISTS Safe & Responsible AI in Australia Industry Submission - IntroductionTABLE OF CONTENTS INTRODUCTION ............................................................................................................................................... 3OBSERVATIONS & RECOMMENDATIONS ..................................................................................................... 4❖ Leading by Example: Empowering Government with AI for Transparent Decisions, not just regulating 4❖ Scenario and risk modelling:......................................................................................."
399,Reason Group,Q07 Responsible government use of AI,See Reason Group submission
400,SafeGround,Transcription,"FILENAME: Responsible AI submission SafeGround.61aaef6a9d35c.pdf | CONTENT: August 2023Supporng Responsible AI - Submission by SafeGroundAs AI is used in new and varied ways, it is essential to understand the risks and mitigate these to avoid harms. As the brief for these consultations states, “Australians need to be able to trust that AI will be used ethically, safely and responsibly.” In order to support the safe and responsible use of AI, multiple dimensions must be addressed through the development of both tools, frameworks and principles, and regulatory measures. This submission will focus on digital dehumanisation and the weaponisation of AI, and give recommendations for government actions in these areas.Digital dehumanisation: The risks and potential harms Digital dehumanisation is the process whereby humans are reduced to data, which is then used to make decisions and/or take actions that negatively affect their lives. The harms of automated decision-making technology have been"
400,SafeGround,Q01 Definitions,"Yes, this definition appears fit-for-purpose"
400,SafeGround,Q02 Other AI risks and recommendations for regulation,"As outlined in our submission, potential risks of digital dehuminisation as a result of automated decision-making technologies using AI are not sufficiently covered in current regulatory approaches. Specific action includes risk assessments for any such technology and cross-disciplinary testing and evaluation of any such technologies. Another concern not addressed by existing regulatory approaches is the harms of AI if used in weapons where force is applied to humans based on the processing of sensory data (ie the use of fully autonomous weapons). Limits on autonomy and certain unacceptable autonomous weapons need to be explicitly articulated and reflected in policy. Australia should also support a legally binding instrument to address autonomous weapons."
400,SafeGround,Q03 Non-regulatory actions to address AI risks,"Broader education of public on AI, uses, risks etc. This should target all ages and would ensure the population has improved AI literacy and understands these tools which they will interact with more and more."
400,SafeGround,Q04 Coordinating AI governance across government,"Aside from specific coordination mechanisms, across government coordination would help to standardised government responses to risks such as digital dehumanisation which apply in similar and distinct ways across a number of different government sectors and departments."
400,SafeGround,Q05 International models for AI governance,"Many other countries support a legally binding instrument on autonomous weapons and it would be desirable for Australia to join in this effort. Similarly, countries have made commitments to ensuring meaningful human control over weapons and in particular the critical functions of selecting targets and decision to attack. The EU has governance principles in place that respond to automated decision making technologies. For instance ""individuals should not be subject to a decision that is based solely on automated processing (such as algorithms) and that is legally binding or which significantly affects them."" (European Parliament regulation 2016/679 - general data Protection regulation) and EDPB Guidelines on Automated individual decision-making and Profiling for the purposes of Regulation (EU) 2016/679)"
400,SafeGround,Q06 Public vs Private sector governance,"Sme approaches should be universa across both sectors, while others should be tailored to the different contexts and considerations of private sector business and government work."
400,SafeGround,Q07 Responsible government use of AI,"By reviewing how AI is used, any risks and potential harms and addressing these thoroughly through implementing safeguards such as new processes, adjusting how AI is used or ending particular applications."
400,SafeGround,Q08 Generic vs technology-specific approaches for AI risk,General principles are valuable in capturing responsible uses of AI - for instance a machine should not be enabled to end human life based on the processing of sensory data . This might implicate a range of technologies if used in certain ways. Technology-specific solutions are more useful in different contexts for instance addressing specific concerns and risks unique to facial recognition.
400,SafeGround,Q10 Ban on high-risk AI applications,"regarding A) Yes, fully autonomous weapons Automated decision-making technologies where decisions are not reviewed and evaluated by a human before any steps are taken. B) Applications of AI that have a high level of definite or probable harm. Specific technologies should be considered on a case-by-case basis as they may have postivie benefits in one context but catastrophic ones if used in another way - and so the applications should be banned but there is more evaluation required as to whether the technology itself should be also."
400,SafeGround,Q11 Actions to increase public trust in AI,"Public education on AI, uses, benefits and risks. Transparent and easily-understandable information about how the government is ensuring AI is used in ways that is responsible and safe."
400,SafeGround,Q12 Trade impact of banning high-risk AI applications,Having clear limits of acceptable and unacceptable technologies or applications assists the tech sector in innovation and development as it can operate within the set boundaries and avoiding stigmitisation.
401,Bradley Holland,Transcription,"FILENAME: Submission to Safe and Responsible AI in Australia by Bradley Holland Submission 4Aug2023.cc2ef15a4c7c9.pdf | CONTENT: Submission to ‘Safe and responsible AI in Australia: Discussion Paper, June 2023’ 4 August 2023Bradley Holland* Associate General Counsel, Senior Director B.Eng (Hons), JD., LLM., M.Intl Tax., GradDip Marketing, GradDipArts (Indonesian)The Australian Government is to be supported in its approach to strengthen the governance in the use of Artificial Intelligence (AI) by industry and government departments. Guidelines, regulations and governance are necessary where the use of AI may result in adverse impacts on people, particularly those who are vulnerable. It is well recognized that the application of AI-enabled systems and processes can greatly benefit society1 and its well-being,2 increase economic output and productivity3 and improve access to services across a wide range of sectors from commerce, health, transportation, cybersecurity, environment managemen"
401,Bradley Holland,General comment,The Australian Government is to be congratulated for asking the public to comment on the safe and responsible use of AI in Australia. This is a critical topic given the potential impact of AI on the lives of all Australians.
401,Bradley Holland,Q01 Definitions,"This submission generally agrees with the definitions as proposed in the Discussion Paper, noting the following:A. The proposed definition of AI substantively aligns with Article 3 of the yet to be adopted AI Act of the EU. B. It may syntactically be incorrect to state that AI makes a ‘decision’, but more accurately makes a ‘prediction’ and in so doing, provides a ‘response’. These predictions and responses are inputs to decision-making, and should not be deemed to be decisions themselves. Any decision-making would ultimately be the result of a process of the responsible persons or entities making use of the AI system. This is the accountability principle. Associating decision-making with AI could have the effect of demonizing AI system algorithms and, in turn, displaces the decision-making responsibilities and accountabilities of those persons or entities who are deploying the AI systems. A previous definition of the European Commission attempted to avoid the use of the term decision "
401,Bradley Holland,Q02 Other AI risks and recommendations for regulation,"It appears that to be able to build trust within the Australian community to engage safely with AI, individuals must be aware when and how they are engaging with AI. This is the transparency and explainability principle. To be meaningful, the amount of information to be disclosed may vary either by stage in the AI lifecycle or type of interaction with the AI system. The higher the level of transparency, the higher the level of confidence in the AI system.The Discussion Paper highlights an example of a significant risk to consumers and users of AI systems, with respect to the situation that occurred in the Trivago case. There are several actions that can follow from the misapplication of AI responses, including negligent misstatement, malpractice, misrepresentation, misattribution, defamation etc. Some of these actions will depend on the classification of AI output at law, and what rights, obligations, and liabilities that flow from that output.The Discussion Paper correctly points out "
401,Bradley Holland,Q03 Non-regulatory actions to address AI risks,"This submission proposes that the Australian Government consider having the Australian Law Reform Commission (ALRC) undertake a review of Australian and International Laws to determine the appropriateness of Australian Law to the opportunities and challenges presented by AI. In reviewing the fitness of, and recommending changes to, Australian Law to cope with the new AI landscape, the Australian Government can build trust within the Australian community in the application and usage of AI-based services. Further, the ALRC report findings also serve as educational tools to the legal and business communities in their preparation to deal with processes and procedures in AI-based environments.At a minimum, for government deployed AI-based services, education programs should be developed to support the community generally and those most vulnerable in the advantages and disadvantages of AI use. This should include training and awareness campaigns on how to seek assistance in the management of"
401,Bradley Holland,Q04 Coordinating AI governance across government,"This submission is of the view that Australia needs a National AI Centre of Excellence (NAICE) to implement a blueprint Code of Conduct (CoC) and Method of Procedures (MoP) or Recommendations in the guidance and supervision of AI-based systems, similar to the approach taken by the NSW Government under the NSW AI Assurance Framework. The NAICE, or similar body, would also be responsible for overseeing the application of the eight (8) AI Ethics Principles to the relevant AI-system, and the compliance of that system to the principles, CoC and MOPs and the reporting, complaint and escalation procedures highlighted in the answers to questions 2 and 3 of this submission.The importance of transparency in AI systems to build community trust was highlighted under section 2 of this submission, but it is equally important that processes, progress and accountabilities of government are openly demonstrated with respect to the regulation, management and enforcement of AI safeguards. Reporting metric"
401,Bradley Holland,Q05 International models for AI governance,"The US proposes test, evaluation, verification and validation (TEVV) processes which are objective, repeatable and scalable to measure AI-based systems. The TEVV approach ensures that metrics, methods, and methodologies are established, implemented and monitored and adapt over time. The relevant metrics and methodologies should “…adhere to scientific, legal and ethical norms and be carried out in an open and transparent process.” This submission considers that the TEVV framework could be worthy of consideration by the Australian Government to provide structure to risk governance. Some minor mapping of the US Framework to the implementation of the Australian AI Ethics Principles will be needed.The successful passing of the TEVV would ‘certify’ the AI system, such certification is a publicly recognisable symbol of confidence that the AI-system works in accordance with the objectives set for that particular system. This is no different and analogous to certification of electrical applianc"
401,Bradley Holland,Q06 Public vs Private sector governance,"This submission is of the view that risk and governance approaches to AI should not necessarily differ between public and private sector use of AI technologies, although governments may consider thresholds (e.g. turnover, number of employees, etc.) which need to be exceeded before regulations apply to that organisation. However, AI-based systems benefit large and small organizations and users of those systems could be impacted to the same extent regardless of the size of the implementing organization."
401,Bradley Holland,Q07 Responsible government use of AI,"In the context of AI responses from Australian Government agencies that are text-based or authorization-based processes, the output of an AI system is a prediction, which is the core function of any AI system. The accuracy of the prediction will depend on a number of factors, including the: (i) quality of the inputs to the system (e.g. end user’s or customer’s information); (ii) quality of the training dataset; (iii) performance of the selected model or algorithm of the AI system; and (iv) type and length of training that is applied to the selected model or algorithm. To establish trust in the Australian Government’s use of an AI-based system, accuracy is a key criterion and therefore, government agencies will need to be completely transparent with respect to items contemplated in (i) to (iv). The manner in which government agencies can demonstrate accuracy is through the provision of test results to test scenarios; by having independent subject matter experts evaluate and report on th"
401,Bradley Holland,Q08 Generic vs technology-specific approaches for AI risk,"An advisory body that consists of technologists, lawyers, social and behavioural experts, risk managers, strategists and regulators/policymakers should be charged with making determinations as to what solutions should apply to risks in AI systems. The advisory body would also set rules and guidelines for any certification process of AI-systems. This can help avoid situations (e.g. Robodebt or the Climate Change debates) where management or motivated-interest groups are either incapable or unwilling to absorb and understand detail and complexity, resulting in the inability to make appropriate and informed decisions. Failure to appoint a suitable cross-discipline group, with the requisite number of experts such as lawyers or ex-judges who can formulate discrete questions, may result in an inability to construct questions-under-test to prove an accurate output. Particularly in high-risk cases, the questions that will be asked on AI-systems will be of greater importance, and hence, the pro"
401,Bradley Holland,Q09 Transparency requirements and scope,"The importance of transparency has been highlighted in questions posed in this submission, but the point at which transparency is most critical and valuable in mitigating potential AI risks is at the points when the AI system: (i) is implemented; (ii) updated (in terms of its algorithms and learning processes); and (iii) re-operationalized after it has been found to be inaccurate.The three points above are points of proof of an acceptable working AI system, and the points at which an AI system is certified. There would likely be very little trust or confidence in a system that is uncertified."
401,Bradley Holland,Q10 Ban on high-risk AI applications,"Candidate AI-systems that are used in diagnosis/diagnostics or which impose restrictions, limitations or obligations on persons or entities should undergo close examination and assessment when deployed in high-risk applications. In high-risk AI applications or technologies, the costs of errors can bring about disastrous results for affected persons, as highlighted by Bernard:“The likelihood of legal error increases exponentially when the decision-making process involves more steps, or more complicated enquiries. The potential for error — and the cost and inconvenience that results — has been amply demonstrated by the Robodebt initiative.”Whether AI systems are to be banned in these high-risk applications depends on whether the response or result of the AI-system will be left unchecked or unvalidated. For AI-systems, its scope to make predictions is primarily restricted by its programming, and as a result, “…it may be fiendishly difficult to accurately translate the rule into programmin"
401,Bradley Holland,Q11 Actions to increase public trust in AI,"This question has been answered in detail earlier in this submission. It is clear that public trust will only be established, and people encouraged to engage in AI-based systems, if the TEVV results have integrity and are transparent.Education and awareness campaigns can change people’s perceptions, along with case studies and use cases that resulted in positive outcomes:“By promoting higher levels of understanding, transparency increases confidence in the AI system.”In the situation where the application of AI-systems results in not meeting the standards acceptable to society, appropriate and proportionate remedial action should take place. For example, in the erroneous use of ChatGPT to generate affidavits and case filings which resulted in the creation of non-existent judicial opinions with false quotes and citations, the court ordered the perpetrator to officially inform the judges of the misattributed and fake opinions. The naming and shaming of the perpetrator’s activities, and w"
401,Bradley Holland,Q12 Trade impact of banning high-risk AI applications,"It is the view of this submission that the banning of any AI in high-risk activities creates a competitive deficit in Australia’s educational capabilities and potential negative effects on technical and economic growth. Each candidate AI system or area of activities for ban should be evaluated, perhaps by the advisory group suggested under question 8 of this submission. Again, some of the downside of deploying AI in these areas can be managed or mitigated by adopting humanistic controls as contemplated in the answer to question 10 of this submission.However, this submission believes that where organizations use AI in hiring processes, facial-recognition, social scoring and similar high-risk activities, there should be a simple requirement for that organisation have a public disclosure statement which can be readily accessed by the public, setting out the organisation’s nominated AI-officer who can be contacted by an affected person. Please refer to the discussion under question 2 of th"
401,Bradley Holland,Q13 Changes to conformity infrastructure,"Review and risk assessments will need to be made with respect to long-held views that software ‘backdoors’ or ‘security bypass’ code is prohibited coding practice. The law, codes of conduct, regulations and educational standards that relate to this prohibition (including all government procurement guidelines) will also need to be reassessed. This is based on the same reasoning that high-risk applications should have natural persons checking and validating outputs before imposing restrictions, etc. on end users or customers (see the discussion under question 10 in this submission). In the case where AI-enabled critical infrastructure operates autonomously, not to design intent, and outside of acceptable operational objectives, the system will need to be intercepted and brought back to a normal and acceptable state of operation. In the worst case, it will need to be halted and restarted (if that is possible). Suitably coded backdoors and security bypasses, properly managed of course, can"
401,Bradley Holland,Q14 Risk-based approach to address AI risks,"A risk-based approach is supported and achievable, provided there are clear guidelines and assistance to businesses and organisations to identify, review, assess and implement controls to manage these new risks."
401,Bradley Holland,Q15 Benefits and limitations of risk-based approach,"As stated above, a risk-based approach is supported and achievable, provided there are clear guidelines and assistance to business and organisations to identify, review, assess and implement controls to manage these new risks."
401,Bradley Holland,Q16 Scope and application of risk-based approach,"Most likely yes, given the costs, effort and investment of organisations to make AI operate appropriately are sizeable. Larger organisations may be in a better position to absorb these costs and have the resources to develop and prove these systems are more likely to implement and manage a suitable risk-based approach."
401,Bradley Holland,Q17 Elements to include in risk-based approach,"This submission supports the Australian Government’s selection of elements in Attachment C but the ‘Human in the loop/oversight assessments’ should include a human contact in an organisation, particularly government agencies, where affected persons can voice their concerns or lodge complaints/disputes where an adverse response has been provided by an AI-system. Please refer to the discussion under question 2 of this submission regarding the role of the AI-officer."
401,Bradley Holland,Q18 Integration of risk-based approach into existing frameworks,"Most organisations will have a structured review and risk assessment procedures in order to evaluate and, if necessary, amend their internal risk management plans, including business continuity and disaster recovery plans. Good corporate governance subjects the results of these assessment procedures for review by the board (possible through audit committees or its equivalent). The Australian Government should also give consideration to whether other standards-based organisations (e.g. Responsible Business Alliance) are likely to introduce guidelines and requirements with respect to AI management and usage.Accordingly, this submission is of the view that organisations with good governance structures should be able to incorporate an AI risk-based approach into their existing assessment frameworks as standard business practice."
401,Bradley Holland,Q19 Application of risk-based approach to general purpose AI,This submission has not answered this question.
401,Bradley Holland,Q20 Voluntary vs mandated compliance with risk-based approach,"This submission is of the view that the Australia Government should adopt a ‘wait and see’ approach before imposing regulatory compliance requirements on private organisations on the adoption and use of AI-systems. The implementation and operation of AI systems across the varied industry and public sectors is not homogenous, which makes policy-making, compliance implementation and monitoring burdensome and difficult. Hence, the risk-based approach to be initially adopted by industry should be voluntary, however the implementation of an AI-officer as suggested under question 2 of this submission should be required as a minimum in all cases.That said, this submission believes that the Australian Government should lead by example, and subject its departments and agencies to an AI compliance framework (which should include the elements set out in Attachment C of the Discussion Paper, and the AI-officer as suggested in the answer to question 2 of this submission), particularly where those d"
402,Anonymous,Q01 Definitions,Yes.
402,Anonymous,Q03 Non-regulatory actions to address AI risks,"Education across all sectors, across all occupations, to raise public awareness and nurture vigilance."
402,Anonymous,Q04 Coordinating AI governance across government,"Coordination of AI governance across government requires diversity in government. Most importantly, an elected member and team need to have qualifications in AI, technology and philosophy."
402,Anonymous,Q07 Responsible government use of AI,"Coordination of AI governance across government requires diversity in government. Most importantly, an appointed leader and team need to have qualifications in AI, technology and philosophy. We need more leaders in government agencies than administrators. Administrators unfortunately lack long-term vision."
402,Anonymous,Q10 Ban on high-risk AI applications,"AI applications in areas that involve psychology and human mental health, eg. suicide hotline; youth helpline etc. must be banned. AI use must have a human in the loop along the way."
403,"Sarah Harkness, Project Lotus",General comment,"I am bringing together the brightest minds in the AI space like Peter Diamondis, Keith Ferrazi and Emas Mostaque to put together a global ethical AI council. I would love to be included jn any ethical AI initiatives as I'm connected globally to the right people to bring them here for more informed discussion."
403,"Sarah Harkness, Project Lotus",Q01 Definitions,Yes
403,"Sarah Harkness, Project Lotus",Q02 Other AI risks and recommendations for regulation,We are not nearly as fast at adopting and helping people to embrace new tech. We need to educate and also help our citizens and companies to adopt ai and augment with it in ethical ways
403,"Sarah Harkness, Project Lotus",Q03 Non-regulatory actions to address AI risks,"Yes so many. Not in aus, in the US. I can connect us in with these communities"
403,"Sarah Harkness, Project Lotus",Q05 International models for AI governance,Yes US and dubai
404,Atlassian,Transcription,"FILENAME: Atlassian - Safe & Responsible AI Submission (04.08.2023).c2e052cd1155.pdf | CONTENT: Atlassian’s Submission to the Australian Government in relation to Safe and Responsible AI in AustraliaTechnology Strategy Branch Department of Industry, Science and Resources DigitalEconomy@industry.gov.au4 August 2023We appreciate this opportunity to provide feedback to the Department on appropriate governance, regulatory and policy mechanisms to facilitate the safe and responsible development and use of AI in Australia, as set out in the Discussion Paper of June 2023 (the Discussion Paper). At Atlassian, we build enterprise software products to help teams collaborate, including for software development, project management and content management. As one of Australia’s most successful home-grown technology companies — and one that provides products and services to customers around the world — we believe that we are in a unique position to contribute to this consultation. We know the critica"
405,Matthew Farrugia-Roberts,Transcription,"FILENAME: letter1.040a48556c0d6.pdf | CONTENT: Friday July 28th , 2023Dear Minister,I am an academic researcher aiming to identify, understand, and reduce risks of harm to society from advanced intelligent systems. I write in response to the Government’s recent discussion paper titled “Safe and Responsible AI in Australia”.1 I am pleased that the Government is taking a serious approach to addressing the many harms already arising from artificial intelligence (AI) systems. These harms are real. Their mitigation is urgent and important work. I applaud the Government’s movements in this direction. I noticed that the paper leaves out of scope a discussion of certain broader impacts of AI, including on the labour market, intellectual property, national security, and the military. I look forward to further discussion of these urgent and important issues in the near future. I remain deeply concerned that the Government’s model of specific risks from AI systems is, so far, too narrowly focused"
406,National Retail Association,Transcription,"FILENAME: 230804 Safe and responsible AI in Australia - NATIONAL RETAIL ASSOCIATION.8506cb31edafa.pdf | CONTENT: info@nationalretail.org.au 4 August 2023 1800 RETAIL (738 245)OFFICE Director Turrbal Country Technology Strategy Branch Lvl 3/67 St Pauls Tce Spring Hill QLD 4000 Industry Innovation and Science Department of Industry, Science and Resources MAIL Via email: DigitalEconomy@industry.gov.au Turrbal Country PO Box 48 Spring Hill QLD 4004 Dear Director,ABN The National Retail Association welcomes the opportunity to provide 44 009 664 073 feedback on behalf of the retail industry to the Safe and Responsible AI in Australia discussion paper.National Retail applaud DISR for their commitment to provide a long- term framework to guide and encourage the safe and effective application of AI technologies in Australia. We broadly support the identified principles and priorities to harness the opportunities, while safeguarding against the potential negative impacts, of AI technologies. We "
407,Dr Leigh Stephenson,Transcription,"FILENAME: short AI submission LTS 230804.a74f80d72b208.docx | CONTENT: There are many important issues in Australian politics right now, but all these are matched by concerns over the development and implementation of AI technologies which already promise to be as disruptive as any other 21st century concern. There is a lot of opinion about the existential risk (or ""x-risk"") posed by AI, leading to global extinctions or catastrophic civilisation collapse, but fear of such events often does not come with a roadmap of their occurrence. Yes, complete extinction of humanity is a concern, but well before then we have a few more down-to-earth concerns to deal with.Develop risk-averse AI policy for the public service sector and for government bureaucracies. The Australian Government should be seen to immediately take lead in establishing early harm reduction with respect to the uptake of different AI technologies. How this is done in the specifics is important but perhaps not as important as "
407,Dr Leigh Stephenson,Q01 Definitions,Yes
407,Dr Leigh Stephenson,Q02 Other AI risks and recommendations for regulation,"One large risk is political instability from a newly-redundant workforce. Politics should engage with state commissions and unions to minimise this risk, replacing tasks within industries but not the jobs themselves."
407,Dr Leigh Stephenson,Q03 Non-regulatory actions to address AI risks,"One thing that did occur to me that I did not put in the submission is that the government could shape the way AI is used and developed by new taxes and concessions with respect to the development of hardware, software and applications."
407,Dr Leigh Stephenson,Q04 Coordinating AI governance across government,Establish a ministry concerning this topic.
407,Dr Leigh Stephenson,Q05 International models for AI governance,I'm not fully informed of this so I cannot say. The EU approach would likely serve as a good model from what I've read.
407,Dr Leigh Stephenson,Q06 Public vs Private sector governance,"Consider that there are regulatory authorities which review safety in industries, like ARPANSA does for radiation. A regulatory authority for AI could be established, but with such quick technological evolution as is currently happening it is hard to say how it could be effective in applying standards. At the very least, what should be policy is that any AI used by the public sector should not be housed off-shore or be owned by foreign interests."
407,Dr Leigh Stephenson,Q07 Responsible government use of AI,"Training and education about the risks and acceptable working practices, and a post-covid reestablishment of face-to-face working culture that will reinforce the advantages of collaboration between human agents. Any AI used by governmental agencies must be controlled by government agencies. It cannot be subject to external interference."
407,Dr Leigh Stephenson,Q09 Transparency requirements and scope,"a. Whenever AI is adopted, or whenever results from AI are used, this should be clearly stated and the reasons for this use clearly stated why. Also, what AI was used? Without this information, everything will feel like it is potentially fake or generated by AI. b. Mandating transparency is particularly difficult as there will be (as there is now) an enormous stigma to its use. An education campaign demonstrating the benefits of transparency, while outlining the risks to public safety or private reputation should it be hidden and later uncovered, will go a long way to assist in the uptake of any transparency requirements."
407,Dr Leigh Stephenson,Q10 Ban on high-risk AI applications,"AI should be a tool for society and its citizens. In these early days, its revolutionary potential in many sectors can be easily recognised but it should not be permitted to transform society in ways that we cannot imagine.a. It should be considered that military applications of AI be carefully considered for reasons that this will create an arms race between Powers and eventually the technology will make its way into private hands for public harm. Primary school and early childhood education should see a complete ban in the technology as we already have good established methods for the rearing of our youth. High school students should probably have a measured exposure to this technology as they will likely benefit from the oversight. Any unregulated private biological research involving AI technologies should have governmental oversight or be banned completely.b. If any application is deemed to have a risk of producing unpredictable or irreversible consequences with a major degree of "
407,Dr Leigh Stephenson,Q11 Actions to increase public trust in AI,Concentrate on safety initiatives and perhaps even contribution to regional and international summits concerning this topic. Australia could even host such a summit.
407,Dr Leigh Stephenson,Q12 Trade impact of banning high-risk AI applications,I'm not sure. But I believe that Australian society in a democratic model has worked well to date without these processes and an effort should be made to demonstrate Australia's continued success in spite of this.
407,Dr Leigh Stephenson,Q13 Changes to conformity infrastructure,Any chips used for public AIs should be from trusted suppliers so as to mitigate security concerns.
407,Dr Leigh Stephenson,Q14 Risk-based approach to address AI risks,Yes.
407,Dr Leigh Stephenson,Q15 Benefits and limitations of risk-based approach,"Risk-based approaches do fall short with the risk is not well defined because then both impact and occurrence probability are also well defined. When such grey areas can be hypothesized, case studies and game playing may assist in determining what actions can best be taken."
407,Dr Leigh Stephenson,Q16 Scope and application of risk-based approach,"I will speak to the education and research sector, as that is where most of my thought dwells. This sector will likely try new applications of AI and turn their work to AI because they have the time and freedom to do so. Education and compliance to an over-arching institution policy, as well as policies on a department level, should help to guide some misguided risky efforts."
407,Dr Leigh Stephenson,Q17 Elements to include in risk-based approach,"It is a good start. Explanations is key here because even to those who understand many LLMs and AI's like AlphaGo the decisions made by AI and why they are made are a mystery. If a decision cannot be explained in human terms then, without good reason, it should not be adopted."
407,Dr Leigh Stephenson,Q18 Integration of risk-based approach into existing frameworks,-
407,Dr Leigh Stephenson,Q19 Application of risk-based approach to general purpose AI,Every model should be tested within the domain it will be used. This will inform how the risks lay/
407,Dr Leigh Stephenson,Q20 Voluntary vs mandated compliance with risk-based approach,"A. Both. B. Both.This can change, but for the moment, better to be safe than sorry."
408,The Centre for Digital Wellbeing,Transcription,"FILENAME: CDW SPG - Submission to Safe AI in Australia Framework - 31072023.374d55ecfb4be.pdf | CONTENT: Safe AI in Australia: Proposed Framework for Responsible Use Submission 28 July 2023 Table of ContentsSubmission on safe AI regulation and governance 2Introduction 2Impact Assessments 3Regulatory authority impact assessments 3Unacceptable risk category 4Notices 4Human in the loop/oversight assessments 5Explanations 5Training 6Monitoring and documentation 6Safe AI Regulation and Governance 1 Australian Government Department of Industry, Science and Resources Technology Strategy Branch DigitalEconomy@industry.gov.au28 July 2023Submission on safe AI regulation and governanceIntroductionThe Centre for Digital Wellbeing (CDW) welcomes the opportunity to provide a submission to the Department of Industry, Science and Resources regarding actions that can be taken on AI regulation and governance, responding to the ‘Safe and responsible AI in Australia Discussion Paper’ (June 2023).The Centr"
408,The Centre for Digital Wellbeing,Q05 International models for AI governance,The European Union AI Act.
409,"The University of Sydney, academics from the Media & Communications and Government & International Relations disciplines, Faculty of Arts and Social Sciences",Transcription,"FILENAME: Pages from 4 August -Safe and Responsible AI in Australia_edited.da6345a602703.pdf | CONTENT: FACULTY OF ARTS AND SOCIAL SCIENCES4 August 2023We thank the Department of Industry, Science and Resources for the opportunity to respond to the Safe and Responsible AI in Australia Discussion Paper.In light of the enormous social, economic, political, cultural and ethical challenges presented by rapid developments in artificial intelligence (AI), and particularly generative artificial intelligence, the opportunity to participate in a policy deliberation process that aims to address questions of the social good at an early stage, and to design suitable regulations to meet such challenges, is very much welcomed.Our submission is a collaborative enterprise between academic researchers in the Disciplines of Media and Communications and Government and International Relations in the Faculty of Arts and Social Sciences at The University of Sydney. It is a collectively authored document tha"
409,"The University of Sydney, academics from the Media & Communications and Government & International Relations disciplines, Faculty of Arts and Social Sciences",Transcription,"FILENAME: 4 August -Safe and Responsible AI in Australia_edited.4372f504e0e77.pdf | CONTENT: Safe and Responsible AI in Australia Submission PaperThe University of Sydney, academics from the Media & Communications and Government & International Relations disciplines, Faculty of Arts and Social SciencesResponse to the Australian Government Department of Industry, Science and Resources, Safe and Responsible AI in Australia Discussion Paper https://consult.industry.gov.au/supporting-responsible-aiPaper prepared 4 August 2023 FACULTY OF ARTS AND SOCIAL SCIENCESSafe and Responsible AI in Australia Defining Artificial ‘Intelligence’ ............................................................................... 5 Articulating Risks ...................................................................................................... 6 Tackling New Approaches ......................................................................................... 8 Risk-Based Approaches ....................."
409,"The University of Sydney, academics from the Media & Communications and Government & International Relations disciplines, Faculty of Arts and Social Sciences",Q01 Definitions,"While there are many variations, there is currently no comprehensive definition of artificial intelligence (AI). Actors such as the Australian Human Rights Commission (AHRC) have alerted to the issues that may arise from poorly defined definitions that fail to move beyond describing a constellation or cluster of technologies. According to the AHRC’s 2021 Human Rights and Technology Report, employing the universally accepted term of AI is both ambiguous and misleading. Further, that AI, as is currently defined, does not consider the implications of new forms of AI arising in the future. The lack of consensus could have implications on how policies pertaining to the regulation of AI are shaped, along with their outcomes now and in the future.Defining AI has not only drawn the attention of industry but has also piqued the interest of academics and news media. For example, TechCrunch (Coldewey, 2023) alerted to the quagmire of pairing the terms ‘artificial’ and ‘intelligence’, claiming “[t"
409,"The University of Sydney, academics from the Media & Communications and Government & International Relations disciplines, Faculty of Arts and Social Sciences",Q02 Other AI risks and recommendations for regulation,"The rapid development of generative AI models has led to significant advancements in the field of text-to-image AI. These models have the potential to revolutionise creative industries by automating the generation of original artistic works based on textual prompts. However, the use of artists' names in prompts raises concerns regarding moral rights, including the right to be recognised as the creator of a work and the right to protect the integrity of that work.This policy document aims to address these concerns by outlining the potential impact of generative AI models on moral rights and proposing guidelines to ensure that artists' rights are respected and protected.Generative AI models, such as Midjourney and DALL-E, have demonstrated the ability to generate high-quality images based on textual prompts. These models can interpret the prompts and produce original artwork that may closely resemble the style of well-known artists. However, the use of artists' namesMoral Rights Concerns"
409,"The University of Sydney, academics from the Media & Communications and Government & International Relations disciplines, Faculty of Arts and Social Sciences",Q06 Public vs Private sector governance,"Introduction Corporations are often the sole repositories of the tech expertise required to understand (and assess) the models they deploy fully and with the pool of experts being relatively small—and because expertise depends on expensive computational facilities—there is a serious risk of a lack of independence among those with sufficient expertise to control, assess and regulate AI systems. Creating public and independently-funded agencies with access to both computational and data resources will allow the creation of independent institutions which could check on claims by corporate actors about their product and competently investigate a wider range of potential negative outcomes that corporations are unwilling to investigate.Case study – Newsbots and generative AI as editorial process While the potential of integrating generative AI into the news cycle presents advantages, there remain several areas of concern. Its use within the news cycle is especially helpful for summarising th"
409,"The University of Sydney, academics from the Media & Communications and Government & International Relations disciplines, Faculty of Arts and Social Sciences",Q09 Transparency requirements and scope,"There is an inherent tension between safety/privacy and transparency. The industry claims that safety can only be guaranteed if models (especially generative models) are not open-sourced and if access to models by third parties is strictly regulated by the companies themselves. This replicates what happens in the context of the analysis of content distribution on social media, possible only for platforms' researchers or limited to the analysis of data selectively released by the same platforms. For this reason, the corporations developing, owning and marketing predictive statistical models can be vested in framing the AI debate as a debate about ""safety"".The academic literature on trust indicates that trust closely correlates with trustworthiness (Hardin, 2002, 2006; Uslaner, 2002). In other words, public trust and confidence in AI will be strengthened if there is a perception that those developing and using it are trustworthy, and a decline in public trust and confidence in AI if it i"
409,"The University of Sydney, academics from the Media & Communications and Government & International Relations disciplines, Faculty of Arts and Social Sciences",Q11 Actions to increase public trust in AI,"Greater public use of AI is not an end in itself. Insofar as there is an “AI dividend” to society, as distinct from companies and agencies that can deploy it to generate new efficiencies in production or new products and services, it does not directly derive from more people making use of AI. It therefore differs from the “digital dividend” that was associated with adoption of the Internet.Greater public trust in AI deployment will be the result of evidence of transparency, accountability and appropriate governance frameworks around its development and use. This must include a role for government regulatory agencies, and the application of legally binding sanctions for breaches of laws, codes and standards associated with AI deployment and use."
409,"The University of Sydney, academics from the Media & Communications and Government & International Relations disciplines, Faculty of Arts and Social Sciences",Q14 Risk-based approach to address AI risks,"As so-called AI systems are integrated into social, economic and political systems to support private and public functions (from targeting audiences to assessing welfare benefits), their potential long-term effects or reversibility cannot be accurately estimated. The widespread application of sophisticated, data-intensive, but often black-box statistical models will likely create a new class of complex socio-technical systems characterised by emerging behaviours and low predictability. In this sense, we suggest differentiating between the application of AI models within well-defined and generally well-understood technological domains (e.g. self-driving cars, medical surgery) and their application into socio-technical domains (e.g. dating apps, search engines, recruitment).Applying AI systems to social-technical domains is inherently more hazardous because long-term effects depend not only on the technology but also on how people and organisations react and adapt to its implementation. "
409,"The University of Sydney, academics from the Media & Communications and Government & International Relations disciplines, Faculty of Arts and Social Sciences",Q17 Elements to include in risk-based approach,"Risk-based approaches to AI can play a valuable role in preventing harm to users and society particularly if employed to evaluate whether AI will provide a useful solution prior to investment and deployment of these systems. The use of AI systems in socio-technical domains, including in cases where risks may evolve over time, where there is a risk of bias and discrimination or misuse, users of AI systems should also have a on-going duty of care to provide a baseline level of protection from harm that is legislated and enforceable. The UK’s Online Safety Bill provides an illustrative model for this type of duty of care."
409,"The University of Sydney, academics from the Media & Communications and Government & International Relations disciplines, Faculty of Arts and Social Sciences",Q20 Voluntary vs mandated compliance with risk-based approach,"It is important that standards for minimising risk and harm are enforceable under the law—there must be real and substantial consequences and accountability for actors who do not properly assess and mitigate risks. As two decades of social media have shown, relying on self-regulation by industry through voluntary codes of conduct and other soft law arrangements, is insufficient for ensuring that digital technologies operate and are governed in a manner that protects the public interest. For this reason, if a risk-based approach is employed for regulating AI in Australia, it should be mandated through legislation. It should also apply to both public and private organisations because both have significant potential to cause harm through the deployment of AI systems."
410,Asim Das,General comment,AI regulation should make it mandatory for all AI applications to designate one or more humans accountable for all AI outcomes (including their explainability)
410,Asim Das,Q01 Definitions,Yes
410,Asim Das,Q02 Other AI risks and recommendations for regulation,Lack of human accountability for the outcome
410,Asim Das,Q05 International models for AI governance,Should also look at measures being taken by India
412,Thoughtworks Australia,Transcription,"FILENAME: Submission on Supporting Responsible AI.c2831522b477a.pdf | CONTENT: Responsible AI Thoughtworks Australia Submission to the Department of Industry, Science and Resources Consultation on Supporting Responsible AIThoughtworks appreciates the opportunity to participate in the Australian government’s consultation on AI regulation and governance.Thoughtworks is a leading global technology consultancy that integrates strategy, design and software engineering to enable organisations and technology disruptors to thrive. For over 30 years, we’ve been at the forefront of digital innovation and have vast experience creating adaptable technology platforms, designing world-class digital products and harnessing the power of data and AI to unlock new sources of value.Since Thoughtworks started in 1993 in Chicago we've been at the forefront of technology innovation. Now we are some 11,500 people across 51 offices in 18 countries. Thoughtworks Australia was incorporated in 1999 and employs o"
412,Thoughtworks Australia,Q01 Definitions,"While there is a need for generic labels in order to broadly refer to a phenomenon, when these labels refer to technical tools there is the potential for confusion and conflation, which is currently happening in much of the discourse around Artificial Intelligence (AI).In order to create meaningful legislation or frameworks that govern the use of AI, the technology must be clearly and realistically defined in technical terms, rather than referred to under a single label like “AI” which is often co-opted for marketing purposes to refer to a broad range of technologies. We recommend taking a more nuanced view of the technologies involved when it comes to considering the risks and impacts of these technologies from a governance standpoint.AI is a branch of computer science that aims to create systems capable of performing tasks that would typically require human intelligence. These tasks include learning and adapting to new information or environments, understanding human language, recogn"
412,Thoughtworks Australia,Q02 Other AI risks and recommendations for regulation,"Lack of transparency and explainability: Existing laws do not expressly require transparency around AI/ADM or explain how systems operate and make decisions. New regulations should mandate transparency and documentation requirements for such technologies where they are used in high-risk contexts or by governments.Algorithmic bias and discrimination: While anti-discrimination laws offer some protection against current algorithmic harms, they can be difficult to enforce against machine-amplified biases. New laws expressly prohibiting algorithmic discrimination, as well as measures for its reporting, detection, and enforcement, are necessary in this context.Spread of misinformation/disinformation: Generative AI tooling, in particular, has significant potential to be abused to produce mis- and disinformation at a large scale. Broader laws may be needed to address this use of AI tooling, which may include a requirement to disclose the provenance of AI-generated content.Privacy risks from la"
412,Thoughtworks Australia,Q03 Non-regulatory actions to address AI risks,"Maintain and evolve Australian institutions for Responsible AI - The Responsible AI Network is an exemplar and should be supported by companion institutions. Such independent bodies should exist to provide services such as conducting AI testing and audits, providing guidance on safety-critical AI, and connecting practitioners and industry with expertise in the responsible development of AI.Expand educational programs - Fund additional university scholarships or sponsor more AI ethics challenges to grow Australia's talent pool in responsible AI and raise awareness. This should include deeper investment in the humanities and inclusion of humanities education in computer science disciplines, as many issues connected to AI raise significant ethical and human rights concerns.Create a public AI registry for government AI use - Require government agencies to publish details of public-facing AI systems they use, in order to improve transparency and oversight.Launch a Responsible AI rating syst"
412,Thoughtworks Australia,Q04 Coordinating AI governance across government,"We suggest that a strong program of ethics, education, research, transparency, and public engagement initiatives be established in order to complement regulatory measures to create an AI governance ecosystem, aligned with Australian values and interests, that is coordinated and consistent. As part of this program, we recommend the following:Establish a national AI ethics advisory council comprising government, industry, academia, and civil society representatives. This body should exist to identify emerging AI risks, provide ethical guidance, and foster coordination between industry and different branches of the government.Develop an AI accountability framework outlining best practices for organisations to assess, document, and mitigate AI risks. Algorithmic impact assessments should be included as a part of this framework. It is strongly recommended that such frameworks are mandatory for relevant applications of AI technology to high risk use cases and sectors, especially government a"
412,Thoughtworks Australia,Q05 International models for AI governance,"As technological developments have international impacts, regardless of geography, it is imperative that Australia participate in international collaboration on AI research ethics, risk assessment frameworks, testing methodologies, and labelling standards. We should have an active part in the development and coordination of global AI governance measures, and leverage existing international governance measures where they are relevant. As this discussion is robust, there is much we believe Australia can draw on and adapt for our own use, especially localised versions of governance mechanisms focused on algorithmic transparency, auditing, explainability, and accountability.The EU's AI Act presents a solid framework for approaching regulatory issues regarding AI. Its re-statement of importance (and possible pre-requisite of) the GDPR, the proposal for a Critical Entities Resilience Directive, would impose cybersecurity and due diligence requirements for entities like AI providers critical "
412,Thoughtworks Australia,Q06 Public vs Private sector governance,"Thoughtworks believe some differentiated approaches may be appropriate for public versus private sector use of AI technologies:The public sector should be subject to more stringent transparency, accountability, and human oversight requirements. This higher standard helps build public trust.Mandatory algorithmic impact assessments could be required for all public sector AI projects but be voluntary for the private sector in certain applications.Blanket requirements to publish technical details of AI systems may stifle commercial innovation, so that limited transparency requirements may be more appropriate for the private sector.The public sector could be obligated only to procure AI systems adhering to specific safety or ethical standards, while standards could remain voluntary for the private sector.A Responsible AI assurance or certification regime could be mandated for public sector systems but be voluntary and market-driven for the private sector.The private sector may need more fle"
412,Thoughtworks Australia,Q07 Responsible government use of AI,"The Australian Government should lead by example on responsible AI development and deployment. Investing in oversight, transparency, documentation, coordination, and staff competency is essential to the deployment of systems that are intended to serve the Australian public, and to establish public trust.We recommend that the government incorporate the following approaches, or applicable elements thereof, for use in its own agencies: Mandate algorithmic impact assessments for all AI projects to evaluate risks and mitigation strategies before, during, and after a project’s development. Require plain language explanations of agency AI systems to improve public transparency. Institute human-in-the-loop checks for high-risk public sector AI applications. Create an interdepartmental AI coordination office to develop consistent AI usage and development policies, and to oversee appropriate validation of systems before they are deployed. Establish an independent oversight body to audit agency A"
412,Thoughtworks Australia,Q09 Transparency requirements and scope,"It should be noted that deep learning models are, by their nature, opaque. While transparency of training data, methodologies, algorithms and performance is desirable, no amount of transparency will reveal the inner workings and reasoning processes behind the decisions these models make. This fact should inform every decision to implement AI technologies in a product or process.If, having considered the above, it is still believed to be useful to implement an AI-based solution in any given context, then thorough documentation and transparency should be a requirement at all stages of development, as there are many points at which influence may be exerted on the eventual output of a model. The efficacy of any framework, governance model, or regulatory mechanism put in place around AI technologies is fundamentally dependent on such transparency in order to function in any meaningful sense.In order to be as effective as possible, documentation and evidence should be provided for all stages"
412,Thoughtworks Australia,Q10 Ban on high-risk AI applications,"This is a complex issue that requires careful consideration of both risks and benefits. Some thoughts:a) Complete bans could be warranted for highly unethical AI applications with limited societal value, likeIndiscriminate biometric surveillance systems that violate privacy/civil liberties. AI that is designed to target and harm human beings autonomously. Highly manipulative AI aimed at exploiting vulnerable groups. Uncorrectable flawed/unsafe AI in ultra-high-risk scenarios like surgical robots.b) Criteria to identify categories of AI warranting bans could include:The risk of irreversible deprivation of human rights or significant human harm. The absence of sufficient use case validity, utility, or countervailing benefit. The existence of safer technological alternatives to achieve the purposes. An unacceptable susceptibility to misuse, bias, or error. Insufficient remediation, reversibility, or containment of adverse impacts. Overriding existing legal/ethical prohibitions if deployed"
413,Australian Strategic Policy Institute,Transcription,"FILENAME: CTS Submission to DISR Responsible AI_08042023.6b0d7e759cd6e.pdf | CONTENT: LEVEL 2, 40 MACQUARIE STREET, BARTON ACT 2600Submission to DISR’s public consultation on supporting responsible AI4 August 2023 Australian Strategic Policy Institute Cyber, Technology & Security (CTS) Jacinta Keast Analyst, Cyber, Technology & SecurityThis submission does not reflect a single Australian Strategic Policy Institute (ASPI) perspective. It is the opinion of the individual author.P: + 02 6270 5100 ABN 77 097 369 045 www.aspi.org.au www.aspistrategist.org.au www.aspi.org.au/program/cyber-technology-security @ASPI_CTS Introduction The Australian Strategic Policy Institute (ASPI) is an independent, non-partisan think tank that produces expert and timely advice for Australian and global leaders. ASPI generates new ideas for policy makers, allowing them to make better-informed decisions.Analysts at ASPI CTS aim to inform and influence policy debates in the Indo-Pacific through original, rigorou"
413,Australian Strategic Policy Institute,Q01 Definitions,Refer to submission
413,Australian Strategic Policy Institute,Q02 Other AI risks and recommendations for regulation,Refer to submission
413,Australian Strategic Policy Institute,Q03 Non-regulatory actions to address AI risks,Refer to submission
413,Australian Strategic Policy Institute,Q04 Coordinating AI governance across government,Refer to submission
413,Australian Strategic Policy Institute,Q05 International models for AI governance,Refer to submission
413,Australian Strategic Policy Institute,Q06 Public vs Private sector governance,Refer to submission
413,Australian Strategic Policy Institute,Q07 Responsible government use of AI,Refer to submission
413,Australian Strategic Policy Institute,Q08 Generic vs technology-specific approaches for AI risk,Refer to submission
413,Australian Strategic Policy Institute,Q09 Transparency requirements and scope,Refer to submission
413,Australian Strategic Policy Institute,Q10 Ban on high-risk AI applications,Refer to submission
413,Australian Strategic Policy Institute,Q11 Actions to increase public trust in AI,Refer to submission
413,Australian Strategic Policy Institute,Q12 Trade impact of banning high-risk AI applications,Refer to submission
413,Australian Strategic Policy Institute,Q13 Changes to conformity infrastructure,Refer to submission
413,Australian Strategic Policy Institute,Q14 Risk-based approach to address AI risks,Refer to submission
413,Australian Strategic Policy Institute,Q15 Benefits and limitations of risk-based approach,Refer to submission
413,Australian Strategic Policy Institute,Q16 Scope and application of risk-based approach,Refer to submission
413,Australian Strategic Policy Institute,Q17 Elements to include in risk-based approach,Refer to submission
413,Australian Strategic Policy Institute,Q18 Integration of risk-based approach into existing frameworks,Refer to submission
413,Australian Strategic Policy Institute,Q19 Application of risk-based approach to general purpose AI,Refer to submission
413,Australian Strategic Policy Institute,Q20 Voluntary vs mandated compliance with risk-based approach,Refer to submission
414,"Centre for Cyber Resilience and Trust (CREST), Deakin University",Transcription,"FILENAME: CREST_Submission-FINAL.21dcc9e8501be.pdf | CONTENT: Safe and Responsible AI in Australia: discussion paper Submission by the Centre for Cyber Resilience and Trust (CREST), Deakin UniversityDeakin University CRICOS Provider Code: 00113B Safe and responsible AI in Australia: discussion paperSubmission by the Centre for Resilience and Trust (CREST), Deakin University The Deakin University Centre for Cyber Resilience and Trust (CREST) is pleased to make this submission in response to the safe and responsible AI in Australia discussion paper. Our submission is underpinned by the multi-disciplinary research expertise of CREST with contributions from scholars across technology, social science, law, and policy.About Us CREST brings a multi-disciplinary focus to the changing landscape of cyber harms and the extent to which people, organisations, and communities are dependent on the growing digital economy. The term ‘cyber resilience’ encompasses but extends beyond the notion of conven"
415,Andrew Smailes,Transcription,"FILENAME: RESPONSE_Responsible AI Reference Data Discussion Paper.18c77672a3612.docx | CONTENT: SUBMISSIONRaedan AI Pty LtdRaedan AI Canberra-based data management specialists that assist organisations in insurance, government, banking and finance, publishing, media, and defence rely on expert.ai to turn language into data, analyse and understand complex documents, accelerate intelligent process automation, and improve decision-making.Andrew SmailesAndrew Smailes is a knowledgeable professional in the field of data management having 30+ years of experience developing data strategies, architectures, and solutions within Government Agencies. He has for many years been actively involved in DAMA, the Data Management Association and is a past National President and Canberra Branch Head. Andrew has presented in events, workshops and webinars addressing topics such as data governance, metadata, data architecture, data for AI, and Natural Language Understanding (NLU).Contact for further inform"
415,Andrew Smailes,Q01 Definitions,Yes
415,Andrew Smailes,Q02 Other AI risks and recommendations for regulation,No response
415,Andrew Smailes,Q03 Non-regulatory actions to address AI risks,Principles around improving collaboration between AI professionals and other information professionals. Other information professions have a long history in compliance with external standards and the issues that arise. Many organisations have Data Governance teams which have developed processes around regulatory and compliance requirements around data. They might be better placed to provide the Governance required than AI teams that don't have the experience.
415,Andrew Smailes,Q04 Coordinating AI governance across government,"There has been a fair amount of commentary recently when it comes to Australia's declining productivity. Any coordination around AI should be focused primarily on increasing productivity based on clear business objectives. Whole of Government governance will most likely just add a new layer of Bureaucracy. The role of any coordination should be on facilitation, assisting in achieving business outcomes with compliance to standards part of the service,"
415,Andrew Smailes,Q05 International models for AI governance,"The Saudi Authority for Data and Artificial Intelligence agency sets standards for data, AI and Digital transformation that government agencies are required to meet. Agencies are required to report progress on a regular basis. Based on our experience it does provide motivation which for change which has been lacking in Australia."
415,Andrew Smailes,Q06 Public vs Private sector governance,No comment
415,Andrew Smailes,Q07 Responsible government use of AI,"With reference to the e Saudi Authority for Data and Artificial Intelligence comment above, setting and managing performance on a broad range of data and AI standards might help raise competencies. AI should be focused on strategic business outcomes, not self-selected projects."
415,Andrew Smailes,Q08 Generic vs technology-specific approaches for AI risk,"Australia's low productivity contrasts with the high-productivity gains achieved in Europe through implementation of Natual Language Understanding. For example, across Government there are agencies that struggle to process grants and applications. Some are known to hire external consultants to do the work. The processing of large and complex documents, such as medical and health insurance claims, has been greatly speeded up by use of AI to summarise and validate documents. The NLU solutions are out of the box, and produce high-quality data for data scientist to then build custom outcomes to meet business objectives. In Australia, data scientist see it as their job to play with data rather than maximise business value from their work."
415,Andrew Smailes,Q09 Transparency requirements and scope,"a) Transparency is required upfront in ensuring that data structures are proactively designed to capture the right data for the business outcome. Avoid the mindset that we will use what is currently available. The better the transparency and conversations around data the better the outcome will be. b) you can't have transparency without blueprints and other artefacts that people can work on to gain a common understanding. Data Architectures, Model Architecture and Data Models are essential to transparency."
415,Andrew Smailes,Q10 Ban on high-risk AI applications,No
415,Andrew Smailes,Q11 Actions to increase public trust in AI,In 1988 I worked on the first Child Support system which was developed by the ATO. At the time it was a major high-profile system. The Commissioner only gave his approval for coding to commence one he was assured that all responsible parties reviewed the data models. This meant doing a review of the models with a Deputy Commissioner. Top-down commitment to the outcome is essential with the involvement of as many stakeholders as possible to ensure that they are all satisfied with the outcome.
415,Andrew Smailes,Q12 Trade impact of banning high-risk AI applications,"No, I think many countries will ban them."
415,Andrew Smailes,Q13 Changes to conformity infrastructure,No comment
415,Andrew Smailes,Q14 Risk-based approach to address AI risks,Yes
415,Andrew Smailes,Q15 Benefits and limitations of risk-based approach,No comment
415,Andrew Smailes,Q16 Scope and application of risk-based approach,No comment
415,Andrew Smailes,Q17 Elements to include in risk-based approach,No comment
415,Andrew Smailes,Q18 Integration of risk-based approach into existing frameworks,No comment
415,Andrew Smailes,Q19 Application of risk-based approach to general purpose AI,"Yes. All models need to produce results which are accurate, ethical and explainable."
415,Andrew Smailes,Q20 Voluntary vs mandated compliance with risk-based approach,I think the approach taken should be consistently applied to all parties.
416,RAND Australia,Transcription,"FILENAME: AI discussion paper input RAND.11fdb9d68396a.pdf | CONTENT: THE RAND CORPORATION (AUSTRALIA) PTY LTD ABN 60 600 246 026 PHYSICAL LOCATION: UNIT G8, 65 CANBERRA AVENUE GRIFFITH, ACT 2603 AUSTRALIA POSTAL ADDRESS: SUITE 24, 2 ENDEAVOUR HOUSE CAPTAIN COOK CRESCENT MANUKA, ACT 2603 AUSTRALIA PH: 02 6232 6972 EMAIL: RAND_AUSTRALIA@RAND.ORGDepartment of Industry, Science and Resources AI Discussion Paper Consultation HubTo whom it may concern,The RAND Corporation, including RAND Australia, is undertaking a range of studies associated with risks inherent with the development of artificial intelligence. We have shared some of this work with the Department, and intend to continue this provision of information as desired. In relation to the request for feedback on the Supporting responsible AI: discussion paper, attached are some initial thoughts on select questions, although we expect to provide more input as our studies progress further.My lead on artificial intelligence in the Austr"
416,RAND Australia,General comment,Answers in the file rather than in individual text boxes
416,RAND Australia,Q01 Definitions,See submission
416,RAND Australia,Q02 Other AI risks and recommendations for regulation,See submission
416,RAND Australia,Q03 Non-regulatory actions to address AI risks,See submission
416,RAND Australia,Q04 Coordinating AI governance across government,See submission
416,RAND Australia,Q05 International models for AI governance,See submission
416,RAND Australia,Q06 Public vs Private sector governance,See submission
416,RAND Australia,Q07 Responsible government use of AI,See submission
416,RAND Australia,Q08 Generic vs technology-specific approaches for AI risk,See submission
416,RAND Australia,Q09 Transparency requirements and scope,See submission
416,RAND Australia,Q10 Ban on high-risk AI applications,See submission
416,RAND Australia,Q11 Actions to increase public trust in AI,See submission
416,RAND Australia,Q12 Trade impact of banning high-risk AI applications,See submission
416,RAND Australia,Q13 Changes to conformity infrastructure,See submission
416,RAND Australia,Q14 Risk-based approach to address AI risks,See submission
416,RAND Australia,Q15 Benefits and limitations of risk-based approach,See submission
416,RAND Australia,Q16 Scope and application of risk-based approach,See submission
416,RAND Australia,Q17 Elements to include in risk-based approach,See submission
416,RAND Australia,Q18 Integration of risk-based approach into existing frameworks,See submission
416,RAND Australia,Q19 Application of risk-based approach to general purpose AI,See submission
416,RAND Australia,Q20 Voluntary vs mandated compliance with risk-based approach,See submission
417,Australian Information Industry Association,Transcription,"FILENAME: 4 8 23 AIIA Submission Safe and Responsible AI in Australia .f02982e665549.pdf | CONTENT: AIIA Response to the Safe and Responsible AI in Australia Discussion Paper4 August 20231 About the AIIA The Australian Information Industry Association (AIIA) is Australia’s peak representative body and advocacy group for those in the digital ecosystem. We are a not-for-profit organisation to benefit members.Since 1978, the AIIA has pursued activities to stimulate and grow the digital ecosystem, to create a favourable business environment for our members and to contribute to Australia’s economic prosperity. We do this by delivering outstanding member value by: • providing a strong voice of influence • building a sense of community through events and education • enabling a network for collaboration and inspiration; and • developing compelling content and relevant and interesting information.We are unique in that we represent the diversity of the tech ecosystem from small and medium busine"
420,Kristen Migliorini of KomplyAi Pty Ltd.,Transcription,"FILENAME: Government Submission Safe and Responsible AI in Australia 4 August 2023.fdf44dcba4297.pdf | CONTENT: 4 August 2023To: Technology Strategy Branch Email: DigitalEconomy@industry.gov.auSafe and Responsible AI in Australia: Response to discussion paper1. KomplyAi® welcomes the opportunity to provide feedback to the Minister for Industry and Science, the Hon Ed Husic MP, on his discussion paper Safe and Responsible AI in Australia which aims to understand further regulatory and governance responses to maximise artificial intelligence (AI) opportunities while protecting Australians.2. By seeking community-wide feedback, the paper is an important step towards an effective and proportionate regulatory approach, building on the valuable work of various regulatory bodies including the eSafety Commissioner, the Responsible AI Network (RAIN) and the NSW AI Committee. Please find our responses to the paper’s questions below.3. We have included a definition section and further information"
421,Australian Libraries and Archives Copyright Coalition,Transcription,"FILENAME: ALACC Submission on Safe and Responsible AI.76f1aa09dfe2c.pdf | CONTENT: Friday 4 August 2023Department of Industry, Science and Resources c/o Technology Strategy BranchBy email: DigitalEconomy@industry.gov.auTo whom it may concernRE: Submission in response to the Safe and Responsible AI in Australia Discussion Paper (2023)The Australian Libraries and Archives Copyright Coalition (ALACC) welcomes the opportunity to respond to the Department of Industry, Science and Resources Safe and Responsible AI in Australia Discussion Paper (2023). The ALACC is the main consultative body and policy forum for the discussion of copyright issues affecting Australian libraries and archives. The members of the ALACC support a copyright framework that appropriately protects the interests of rights holders while ensuring access to important cultural, educational and historical content for the public’s benefit. More information about the ALACC is available at alacc.org.au/about.Australian archive"
421,Australian Libraries and Archives Copyright Coalition,Q01 Definitions,n/a
421,Australian Libraries and Archives Copyright Coalition,Q02 Other AI risks and recommendations for regulation,n/a
421,Australian Libraries and Archives Copyright Coalition,Q03 Non-regulatory actions to address AI risks,n/a
421,Australian Libraries and Archives Copyright Coalition,Q04 Coordinating AI governance across government,We encourage the Department to be involved in the upcoming Ministerial Roundtable on Copyright Reform being facilitated by the Attorney-General's department.
421,Australian Libraries and Archives Copyright Coalition,Q05 International models for AI governance,n/a
421,Australian Libraries and Archives Copyright Coalition,Q06 Public vs Private sector governance,n/a
421,Australian Libraries and Archives Copyright Coalition,Q07 Responsible government use of AI,n/a
421,Australian Libraries and Archives Copyright Coalition,Q08 Generic vs technology-specific approaches for AI risk,n/a
421,Australian Libraries and Archives Copyright Coalition,Q09 Transparency requirements and scope,n/a
421,Australian Libraries and Archives Copyright Coalition,Q10 Ban on high-risk AI applications,n/a
421,Australian Libraries and Archives Copyright Coalition,Q11 Actions to increase public trust in AI,n/a
422,Australian Digital Alliance,Transcription,"FILENAME: Submission in response to the Department of Industry, Science and Resources’ Safe and Responsible AI in Australia Discussion Paper – Australian Digital Alliance.3c52d2adbd261.pdf | CONTENT: Submission in response to the Department of Industry, Science and Resources’ Safe and Responsible AI in Australia Discussion PaperSubmission by the Australian Digital AllianceSubmitted Friday 4 August 2023This submission is licensed for reuse under a Creative Commons Attribution 4.0 (CC BY 4.0) International licence. For licence terms see, https://creativecommons.org/licenses/by/4.0.This submission is available to read or download (including in other file formats) on the ADA website, https://digital.org.au/dept-of-industry-safe-and-responsible-ai-discussion-paper-submission. AUSTRALIAN DIGITAL ALLIANCEIntroduction The Australian Digital Alliance (ADA) thanks the Department of Industry, Science and Resources (the Department) for the opportunity to make a submission in response to the discus"
424,Equifax,Transcription,"FILENAME: Dept Industry Science Resources EFX response Safe and Responsible AI consultation Aug 2023. pdf.146a307ed6fce.pdf | CONTENT: RECOMMENDATIONS Preferred definitions 1. Artificial Intelligence (AI) is a field of science concerned with building computers and machines that can reason, learn, and act in such a way that would normally require human intelligence or that involves data whose scale exceeds what humans can analyse. 2. Machine Learning (ML) is a subfield of AI, a program or system that trains a model and learns patterns from input data to make useful predictions from new or never before seen data. ML gives the computer the ability to learn without explicit programming. 3. Large Language Model (LLM) is a type of generative AI that specialises in the generation of novel combinations of text in the form of natural sounding language.AI governance 4. The Commonwealth and States should agree on a single consistent approach to AI regulation, working together on common objectives"
424,Equifax,Q01 Definitions,"Definitions Question 1 Do you agree with the definitions in this discussion paper? If not, what definitions do you prefer and why?We would make a number of observations with respect to the definitions given.Artificial intelligence The given definition: Artificial intelligence (AI) refers to an engineered system that generates predictive outputs such as content, forecasts, recommendations or decisions for a given set of human-defined objectives or parameters without explicit programming. AI systems are designed to operate with varying levels of automation.Observations: AI is a field of science, and an AI system is an engineered one. Rule-based systems fall under the umbrella of AI, which do not necessarily provide predictive outputs and may require explicit programming.Proposed definition: Artificial Intelligence (AI) is a field of science concerned with building computers and machines that can reason, learn, and act in such a way that would normally require human intelligence or that i"
424,Equifax,Q02 Other AI risks and recommendations for regulation,See submission
424,Equifax,Q04 Coordinating AI governance across government,"Any proposed AI regulation - voluntary or otherwise - should be cognisant of the multiple existing regimes already in place for data, including the Privacy Principles; Part IIIA of the Privacy Act (credit reporting) and most recently formal Privacy Safeguards introduced as part of the Consumer Data Right. There is a potential for uncertainty or even contradictory provisions arising from an additional set of legislated obligations.The introduction of any AI governance should avoid the carve outs that characterised the original Privacy Act, which exempted the private sector for the first decade of operation and does not cover state or territory government agencies, nor is mirrored in equivalent state Acts.We note that the Commonwealth’s 2018 AI ethics principles has been followed in 2022 by NSW releasing an AI Assurance Framework applicable to state agencies, using NSW AI Ethics Principles. Most recently a NSW Parliamentary Committee was set up with terms of reference that include consid"
424,Equifax,Q06 Public vs Private sector governance,"The Commonwealth and States should agree on a single consistent approach to AI regulation, working together on common objectives, standards and practices. If AI governance is proposed for the private sector, it should also apply to the Commonwealth and state governments."
424,Equifax,Q09 Transparency requirements and scope,"In Australia, credit reporting is heavily regulated by a specific section of the Privacy Act, Part IIIA.Part IIIA captures credit scores as part of “credit reporting derived information”; this in turn can only draw from a restricted list of elements (credit reporting information) set out in the legislation, meaning credit scores cannot consider elements such as a person’s income or good history of paying rent on time.Additionally, the disclosure of credit reporting information, including credit scores, is heavily restricted (e.g. a real estate agent could not request Equifax to disclose to them a rental applicant’s credit report or score).In 2021, legislation was passed to give greater consumer understanding of the algorithms that placed consumers within five score bands (“excellent” “good” “poor” etc). This legislation requires a consumer to be told of the key contributing factors that characterise their score band.Using patented technology, Equifax pioneered greater transparency, pro"
424,Equifax,Q11 Actions to increase public trust in AI,"Building trust is dependent on lifting AI literacy, educating people on AI benefits and risks and putting in place clear processes for disclosures about the AI involved in a transaction.An important point to make as part of this education is the harm of not using AI systems.For example, potential bias in credit scores continues to be the subject of ongoing study and testing, but credit risk algorithms are far less likely to produce the discrimination and inaccurate assessment of risk seen in the era of mortgage applicants being subject to in-person interviews."
424,Equifax,Q14 Risk-based approach to address AI risks,"A risk based approach needs to: Balance potential harms with the many social and economic benefits promised by AI; Target the right use cases, taking into account the likelihood of harm and as well as the severity of harm; Have an onus to put in place mechanisms to detect and mitigate unintended consequences and harm; Consider the cost of not using AI in terms of forgone benefits.Risks and impacts also have the potential to change quickly and any governance approach needs to consider how implementation can be done while still allowing for the capacity to change and grow.While smaller organisations may struggle with compliance, potentially creating barriers to entry, this does not necessarily mean compliance should vary by size of organisation. Much of the standards and rules that surround data and by way of extension, AI, already exist (eg. the Privacy Act) and is therefore unsuitable for the approach used by the Government for financial services Fintechs (an ASIC enforced enhanced reg"
424,Equifax,Q15 Benefits and limitations of risk-based approach,"One of the main limitations is that the harm of not using AI systems is not included. In other words, we recommend a comparative risk-based approach to consider the trade-off between potential impacts and risks of AI versus non-AI systems.An initial proposition would be that it is preferable to have an imperfect AI system that demonstrates a better performance on crucial life-impacting tasks than a flawed non-AI approach. While more harm is created by not using the AI system, there then needs to be an onus to detect and mitigate unintended AI consequences and harm.As an example, the factors included in credit decisioning assessments can be complex. An automated system assessing the patterns and biases in the data is capable of a significantly more nuanced outcome than an individual attempting to weigh the same factors.AI is a space that will continue to change quickly, and the risks and impacts will mirror that rate of change; this is a challenge for any legislative approach, which wil"
424,Equifax,Q17 Elements to include in risk-based approach,"While Equifax supports a risk based approach, there is ambiguity in how the risk levels are currently defined. Work is needed to elaborate the use cases being captured and further consider how best to define risk levels to target the right use cases and take into account the likelihood of harm and not just the severity of the harm. Another factor to consider is that impact to individuals can vary on spectrum even within a given use case.A suggestion to complement the elements presented in attachment C could be “AI in the loop assessments” prior to ‘human in the loop’. This can be considered as ‘AI to mitigate AI risks’ or ‘AI to audit AI’ to address ‘scalability’, which is one of the main challenges of ‘human in the loop’ assessments.For example, one of the risks raised by using ML/AI algorithms is bias or discrimination against certain groups of people that can have life-impacting consequences (e.g., in the assessment of home loan applications). While it would be infeasible for human "
425,Australian Payments Network,Transcription,"FILENAME: AusPayNet Submission to the Consultation on Safe and Responsible AI in Australia - 4 August 2023.6cc628153475d.pdf | CONTENT: 4 August 2023Technology Strategy Branch Department of Industry, Science and Resources GPO Box 2013 Canberra ACT 2601By email: digitaleconomy@industry.gov.auAustralian Payments Network (AusPayNet) welcomes the opportunity to respond to the Discussion Paper on Safe and Responsible AI in Australia.AusPayNet is the industry association and self-regulatory body for the Australian payments industry. We manage and develop standards and guidelines governing payments in Australia. Our purpose is to create confidence in payments by setting enforceable industry standards for a safe, reliable and effective payments system; leading transformation in payments to drive efficiency, innovation and choice; and being the home for ecosystem collaboration and strategic insight. AusPayNet currently has more than 150 members including financial institutions, payment system o"
426,Grow Right Digital,Transcription,"FILENAME: Submission on Safe and Responsible AI in Australia.4c7202b9aa00c.pdf | CONTENT: Submission on Safe and Responsible AI in Australia Non-regulatory initiatives to support responsible AI practices: We propose a range of non-regulatory initiatives that the government could explore: 1. Public Education: Investing in public education initiatives focused on AI and its implications to enhance citizens' understanding of these technologies. 2. Ethical AI Research: Encouraging AI research and development that places emphasis on ethical, transparent, and accountable AI practices. 3. Incentives for Responsible AI: Providing incentives for private sector organisations to adopt responsible AI practices, fostering a culture of ethical AI implementation. 4. Collaborative Efforts: Promoting collaborations between the government, academic institutions, and the private sector to jointly develop and promote AI best practices for the benefit of society. These initiatives can play a pivotal role in"
426,Grow Right Digital,Q01 Definitions,"Yes, no further comment."
426,Grow Right Digital,Q02 Other AI risks and recommendations for regulation,"Australia's existing regulatory approaches may not fully address certain potential risks stemming from AI technology. Risks related to bias and discrimination in AI algorithms, data privacy and security breaches, and the ethical implications of AI decision-making are areas that might require further attention. To mitigate these risks, implementing specific regulatory measures could be beneficial, such as mandating regular audits of AI algorithms to identify and address bias, enforcing strict data protection and privacy standards, and establishing clear guidelines for the ethical use of AI in critical decision-making processes. Additionally, fostering collaboration between industry stakeholders, government bodies, and AI experts to continuously assess emerging risks and devise appropriate regulatory responses can help ensure that AI remains safe, responsible, and beneficial to society."
426,Grow Right Digital,Q03 Non-regulatory actions to address AI risks,"Certainly, there are several non-regulatory initiatives that the Australian Government could implement to support responsible AI practices in the country. These initiatives include:Public Education and Awareness: Investing in public education programs about AI and its implications can help raise awareness among citizens and businesses. This initiative would provide a better understanding of AI technologies, their benefits, and potential risks. Increased awareness can lead to more informed decision-making and responsible use of AI.AI Research and Development Grants: Providing grants and funding for AI research and development projects that prioritize ethical, transparent, and accountable AI practices can encourage innovation in the field while ensuring responsible AI adoption. These grants can attract top talent and support the development of AI solutions that align with best practices.Collaborative AI Research Centers: Establishing collaborative AI research centres that bring together "
426,Grow Right Digital,Q04 Coordinating AI governance across government,"Effective coordination of AI governance across government can play a pivotal role in shaping the development and uptake of AI in Australia. Here are some suggestions on how coordination mechanisms can achieve specific goals and influence the AI landscape:Unified Vision and Consistent Regulations: Coordination mechanisms can help establish a unified vision for AI in Australia and ensure consistent regulations across different government departments. This clarity and consistency can provide certainty to businesses and developers, encouraging responsible AI development and adoption.Cross-Sector Communication and Cooperation: Coordination can facilitate communication and cooperation between different sectors of the government, including industry, academia, and regulatory bodies. By sharing insights and expertise, these sectors can collectively address common challenges and identify best practices for AI deployment.Risk Mitigation and Adaptability: Effective coordination can help identify a"
426,Grow Right Digital,Q05 International models for AI governance,"Yes, several countries are actively implementing or considering governance measures relevant, adaptable, and desirable for Australia's AI landscape. Some notable examples include:Canada: The Canadian government has established the Canadian Institute for Advanced Research (CIFAR), which focuses on AI and related areas. They are investing in research, fostering collaboration between academia and industry, and promoting responsible AI development through guidelines and ethical frameworks.European Union: The EU has introduced the General Data Protection Regulation (GDPR) to protect individual privacy and data rights in the context of AI applications. They are also exploring regulatory measures for AI that include risk-based assessments, transparency requirements, and strict rules for high-risk AI systems.Singapore: Singapore's government launched the Model AI Governance Framework, which provides guiding principles and best practices for responsible AI deployment. They are also considering "
426,Grow Right Digital,Q06 Public vs Private sector governance,"Yes, different approaches should apply to public and private sector use of AI technologies due to the distinct roles, responsibilities, and contexts they operate within. Here's how the approaches could differ:Data Privacy and Transparency: Both sectors should adhere to data privacy and transparency standards, but the public sector might be subject to more stringent regulations, given its responsibility to protect citizens' personal information and maintain transparency in its decision-making processes.Ethical Considerations: While both sectors should prioritize ethical AI practices, the public sector might have additional obligations to uphold public trust, fairness, and accountability in the use of AI technologies that impact citizens' lives and public services.Innovation and Flexibility: The private sector often drives AI innovation, and therefore, its approach may focus on promoting responsible AI practices while encouraging flexibility and adaptation to changing market needs. In co"
426,Grow Right Digital,Q07 Responsible government use of AI,"The Australian Government can take several measures to further support responsible AI practices in its own agencies:Establish AI Ethics and Governance Guidelines: Develop clear and comprehensive AI ethics and governance guidelines that outline principles for responsible AI use within government agencies. These guidelines should address issues such as transparency, fairness, privacy, and accountability in AI decision-making processes.Foster AI Training and Awareness: Provide training and awareness programs to government personnel on responsible AI practices, ensuring that employees understand the ethical implications of AI technologies and are equipped to make informed decisions when implementing AI solutions.Encourage Collaboration and Knowledge Sharing: Foster collaboration among government agencies to share best practices and lessons learned in AI implementation. Encourage the establishment of a community of practice that facilitates knowledge exchange and support for responsible AI "
426,Grow Right Digital,Q08 Generic vs technology-specific approaches for AI risk,"Generic solutions to the risks of AI are most valuable when the challenges posed are common across a broad range of applications, industries, or situations. These solutions often encompass principles and best practices that apply regardless of the specific technology in use. For example:Data Privacy and Protection: Generic privacy policies and data protection measures, such as encryption, anonymization, and secure data storage practices, are applicable across many AI applications that involve handling large amounts of data, including potentially sensitive information.Transparency and Explainability: Requirements for transparency and explainability in AI systems are broadly applicable. These could include obligations to document the purpose of an AI system, the data it uses, its decision-making processes, and the measures taken to ensure its fairness and reliability.On the other hand, technology-specific solutions are better suited to address risks that are inherent to a particular type"
426,Grow Right Digital,Q09 Transparency requirements and scope,"Transparency in the AI lifecycle is of utmost importance to mitigate potential risks and build public trust and confidence in AI systems. Here are some key areas where transparency will be critical and valuable:Data Collection and Use: Transparency about the sources and types of data used to train AI models is essential to ensure the ethical and responsible use of data. Providing insights into data collection practices and data handling procedures can help users understand how their data is being used and contribute to building trust.Algorithmic Decision-Making: Transparency in AI algorithms and decision-making processes is vital, especially in applications that have significant impacts on individuals or society. By making AI systems more explainable, users can understand how decisions are reached, allowing them to identify potential biases and challenge unfair outcomes.Bias and Fairness: Transparent disclosure of efforts to identify and mitigate bias in AI systems is crucial to ensure"
426,Grow Right Digital,Q10 Ban on high-risk AI applications,"When considering whether any high-risk AI applications or technologies should be banned completely, it's essential to strike a balance between fostering innovation and protecting public safety and ethical considerations. Some high-risk AI applications or technologies might warrant a ban in specific contexts where the potential harm outweighs the benefits. Here are some suggestions for identifying AI applications that may need to be banned:Risk Assessment: Conduct thorough risk assessments to identify AI applications with significant potential for harm. Consider the potential consequences of AI failures or misuse, such as risks to human life, civil liberties, privacy, or exacerbation of societal biases.Ethical Considerations: Evaluate AI applications against ethical principles, such as fairness, transparency, accountability, and non-discrimination. If an AI application violates these principles and has significant ethical concerns, it may be a candidate for a ban.Impact on Vulnerable Gr"
426,Grow Right Digital,Q11 Actions to increase public trust in AI,"Increasing public trust in AI deployment requires a multi-faceted approach involving both government and industry. Here are some initiatives and government actions that can help foster trust and encourage more people to use AI:Transparency and Explainability: Encourage AI developers and organisations to provide transparent explanations of how AI systems make decisions. Clear communication about how AI is used and what data is collected can build trust and demystify AI technologies.Ethical AI Standards: Develop and enforce ethical AI standards to ensure that AI systems are designed and deployed in a fair, unbiased, and responsible manner. Implementing clear guidelines for data usage, privacy, and security can boost confidence in AI applications.Independent Audits and Oversight: Establish mechanisms for independent audits and oversight of AI systems to ensure compliance with regulations and ethical guidelines. Independent verification of AI systems' fairness and reliability can enhance t"
426,Grow Right Digital,Q12 Trade impact of banning high-risk AI applications,"Banning high-risk activities like social scoring or facial recognition technology in certain circumstances would have both positive and negative impacts on Australia's tech sector and trade and exports with other countries.Positive Impacts:Building Trust and Reputation: Banning high-risk AI applications can demonstrate Australia's commitment to protecting individual rights, privacy, and data security. This stance can enhance Australia's reputation as a responsible and ethical player in the global tech industry, attracting businesses and customers who value data protection and human rights.Innovation and Development: The ban can spur innovation in alternative AI technologies that prioritise privacy and security. Australian tech companies may focus on developing AI solutions that are more ethical, transparent, and aligned with international best practices, leading to the creation of unique and valuable offerings.International Collaboration: By taking a principled approach to high-risk AI"
426,Grow Right Digital,Q13 Changes to conformity infrastructure,"To support assurance processes and mitigate potential AI risks, certain changes to Australia's conformity infrastructure may be necessary. These changes could include:Development of AI-specific Standards: Australia may need to develop new or adapt existing standards that specifically address AI technologies. These standards can provide guidelines for the safe and responsible development, deployment, and use of AI systems across various sectors. They may cover areas such as data privacy, algorithm transparency, fairness, and accountability.Certification and Accreditation Schemes: Establishing certification and accreditation schemes for AI technologies can help ensure that AI solutions meet the required standards and best practices. Companies developing AI applications could seek certification to demonstrate that their products comply with industry-recognised safety and ethical guidelines.Regulatory Bodies for AI: Australia might consider creating specialised regulatory bodies or empower"
426,Grow Right Digital,Q14 Risk-based approach to address AI risks,"Yes, a risk-based approach is a valuable and pragmatic approach for addressing potential AI risks. A risk-based approach involves identifying and prioritising risks based on their likelihood and potential impact. By focusing resources and efforts on high-risk areas, organisations and policymakers can effectively manage and mitigate AI-related risks.This approach allows for flexibility and adaptability, enabling stakeholders to tailor their responses to the specific risks they face. It also encourages continuous monitoring and evaluation, ensuring that risk mitigation strategies remain relevant as AI technologies evolve.While a risk-based approach is beneficial, it should be complemented by ongoing collaboration between stakeholders, including governments, industry, academia, and civil society. Open dialogue and shared responsibility will lead to more comprehensive risk assessments and better-informed decisions.In conclusion, a risk-based approach is a prudent and effective way to addre"
426,Grow Right Digital,Q15 Benefits and limitations of risk-based approach,"The main benefits of a risk-based approach are:Focused Resource Allocation: A risk-based approach allows organisations and policymakers to allocate resources where they are most needed, targeting high-risk areas and potential AI challenges. This ensures that efforts and investments are directed toward the most critical aspects of risk mitigation.Flexibility and Adaptability: As AI technologies and their associated risks continue to evolve, a risk-based approach provides flexibility to adapt strategies and responses accordingly. This adaptability allows stakeholders to address emerging risks and stay ahead of potential challenges.Cost-Effectiveness: Prioritising risks based on their likelihood and impact helps optimise resource utilisation, making risk management efforts more cost-effective. It allows organisations to focus on areas where the potential harm is highest, reducing unnecessary expenses on low-risk issues.Tailored Solutions: Different AI applications and contexts may pose di"
426,Grow Right Digital,Q16 Scope and application of risk-based approach,"Yes, a risk-based approach can be better suited to certain sectors, AI applications, and organisations based on their size, AI maturity, and available resources. Here's how:Sectors: Different sectors may face varying levels of AI-related risks based on their nature of operations and data handling. Sectors dealing with sensitive personal information (e.g., healthcare and finance) or critical infrastructure (e.g., transportation and energy) may have higher AI-related risks. A risk-based approach allows these sectors to focus on their specific vulnerabilities and prioritise risk mitigation accordingly.AI Applications: The level of risk associated with AI applications can differ based on their complexity and potential impact. High-risk AI applications, such as those involved in decision-making or safety-critical systems, require more thorough risk assessments and stringent risk management measures. A risk-based approach enables organisations to tailor their efforts based on the potential h"
426,Grow Right Digital,Q17 Elements to include in risk-based approach,"A risk-based approach for addressing potential AI risks should encompass several key elements to effectively identify, assess, and mitigate risks associated with AI technologies. These elements include:Risk Identification: Identify and catalogue potential AI risks specific to the organisation, AI applications, and industry. This involves understanding the AI technologies in use, data sources, algorithmic processes, and the potential impact of AI on various stakeholders.Risk Assessment: Evaluate the likelihood and potential impact of identified AI risks. Categorise risks based on their severity and prioritise them according to their significance to the organisation and its stakeholders.Risk Mitigation Strategies: Develop risk mitigation strategies tailored to address each identified risk. These strategies may include technical measures to reduce bias, enhance transparency, and ensure data privacy, as well as policy and procedural changes to promote responsible AI practices.Governance an"
426,Grow Right Digital,Q18 Integration of risk-based approach into existing frameworks,"Incorporating an AI risk-based approach into existing assessment frameworks and risk management processes can streamline the evaluation of AI-related risks and reduce potential duplication. Here are some steps to achieve this integration:Identify AI-specific Risks: Review existing assessment frameworks and risk management processes to identify any AI-specific risks that might not be adequately covered. These risks may include algorithmic bias, explainability, transparency, and data security.Enhance Assessment Criteria: Update the existing assessment frameworks to include specific criteria related to AI risks. For example, within privacy assessments, include considerations for AI algorithms' impact on data privacy and the potential for re-identification.Align Risk Categories: Map AI-related risks to the existing risk categories used in the organization's risk management processes. This alignment helps ensure that AI risks are addressed within the broader risk management framework withou"
426,Grow Right Digital,Q19 Application of risk-based approach to general purpose AI,"A risk-based approach can be particularly valuable when dealing with general-purpose AI systems like large language models (LLMs) or multimodal foundation models (MFMs) due to their broad applicability and potential impact. Here's how a risk-based approach might apply to such AI systems:Risk Identification: Identify potential risks associated with LLMs or MFMs, such as biased outputs, misinformation propagation, ethical concerns, and unintended consequences of AI-generated content.Risk Assessment: Evaluate the likelihood and severity of identified risks based on the model's context of use, data sources, and potential consequences for end-users and society.Model Governance: Establish robust governance frameworks to oversee the development and deployment of LLMs or MFMs. This includes defining responsible use-cases, potential limitations, and ethical guidelines for model outputs.Data Privacy and Security: Implement strong data privacy measures to protect user data and ensure that sensiti"
426,Grow Right Digital,Q20 Voluntary vs mandated compliance with risk-based approach,"A risk-based approach for responsible AI can benefit from a combination of both voluntary/self-regulation and mandated regulation, tailored to specific contexts. It should apply to both public and private organizations as well as developers and deployers of AI systems. Here's a breakdown of these considerations:Voluntary/Self-Regulation: A voluntary approach can provide flexibility and promote innovation, allowing organizations to tailor their AI governance strategies according to their specific needs and risks. It fosters a culture of responsibility among AI developers and deployers and encourages the sharing of best practices across industries. However, a purely voluntary approach may not be sufficient to address all risks, especially when potential harm is significant or incentives for responsible behavior are weak.Mandated Regulation: Regulatory measures can provide a uniform standard of behavior that all organizations must adhere to, ensuring that basic responsible AI practices ar"
427,Gadens,Transcription,"FILENAME: Gadens - Submission in response to AI Government Forum.33f007ca15dc8.pdf | CONTENT: Supporting responsible AI: discussion paper Submission from Gadens4 August 2023Introduction Gadens is pleased to have the opportunity to make a submission in response to the questions contained in the ‘Safe and responsible AI in Australia’ discussion paper published 1 June 2023 by the Department of Industry, Science and Resources (Discussion Paper).1Our submission responds to and includes our comments and observations regarding some of the proposals in the Discussion Paper which we consider would have the most significant impact on individuals, our clients and business and/or will raise important commercial or legal compliance issues.Any defined terms used in our submission have the meaning given in our submission and are otherwise defined in the Discussion Paper.We consent to this submission being published on the Department of Industry, Science and Resources’ website.Definitions1. Do you agr"
427,Gadens,Q01 Definitions,"We consider that in the context of definitions, the main focus should be on finding a single future-proof and technology neutral definition of AI, which can be supplemented by references to the AI techniques and approaches outlined in the Discussion Paper, such as large language models and automated decision-making (e.g., in the form of annexures or rules to potential AI legislation which would allow for flexibility and adaptability for both voluntary and regulatory approaches). Given the changing nature of AI technologies and underlying applications, we believe that a less prescriptive approach is preferable over a fixed set of definitions. In relation to the proposed definition of AI we further submit the following alternative definition:Definitions in Discussion PaperGadens proposalArtificial intelligence (AI) refers to an engineered system that generates predictive outputs such as content, forecasts, recommendations or decisions for a given set of human-defined objectives or parame"
427,Gadens,Q02 Other AI risks and recommendations for regulation,"We consider that AI is regulated by several existing regulatory regimes, for example the Australian Privacy Act 1988 (Cth), the Competition and Consumer Act 2010 (Cth) and the Online Safety Act 2021 (Cth), which are tailored to mitigate core risks associated with the use of AI, such as privacy, consumer and competition protections, and online safety. We are broadly supportive of the idea to rely on and build regulatory responses on existing regimes, as they are well established and technology neutral. However, we note that none of the aforementioned regimes address high-risk use cases of AI specifically and are currently not equipped to evaluate and, if required, ban AI applications with unacceptable risk levels. We consider that to the extent a separate regulatory regime were to be introduced to regulate the use of AI, such a regime should focus specifically on the testing, approving and banning of high-risk AI applications to close this gap."
427,Gadens,Q03 Non-regulatory actions to address AI risks,"The Discussion Paper outlines existing non-regulatory initiatives, such as the DISR’s AI Ethics Principles, pilot and Responsible AI Adopt Program, and the DISR via CSIRO’s National AI Centre and Responsible AI Network. We acknowledge the ongoing development of these programs and initiatives and suggest, to the extent not already covered by these programs, the following:government supported ‘sandbox’ environment to develop and test new AI applications;AI governance testing framework and toolkit modelled on the “AI Verify” program adopted in Singapore to allow the testing of developed AI applications;2 andadoption of a government approved AI rating (e.g., in relation to associated risk levels) and/or certification which signifies voluntary compliance with ethical standards, which may encourage businesses to participate in non-regulatory initiatives and build consumer trust."
427,Gadens,Q04 Coordinating AI governance across government,"We consider that a designated AI board, composed of regulators involved in the regulation and administration of AI to date, such as the OAIC and ACCC, could assist in co-ordinating a unified across-government response in relation to the development, deployment and use of AI. Potential responsibilities could include the development of AI standards and best practices, issuance of guidance notes and hosting of AI related roundtables and consultations.In particular, much like the guidance offered by the OAIC on privacy and consumer data right related matters, we propose that guidance in relation to any voluntary or regulatory AI framework would be an invaluable resource, in particular for SMEs, to implement responsible and safe AI practices.We consider that the minimisation of patchwork solutions, and the establishment of a central AI body, particularly in the context of a regulatory AI framework, should be a key priority to further achieve across-government solutions and support responsib"
427,Gadens,Q05 International models for AI governance,"Gadens considers the following governance measures and initiatives implemented by other jurisdictions may be adaptable and desirable for Australia:Measure / initiativeReasoningEuropean UnionEuropean AI Alliance – This is a public-private partnership initiative by the European Commission to establish a platform to facilitate collaboration, research and dialogue between stakeholders from various sectors to develop policies and strategies for AI.Gadens considers that a partnership across a range of public and private sectors could be a beneficial initiative to allow wide stakeholder input and collaboration. It would offer an efficient platform for stakeholders to pool resources and jointly develop policies, strategies, as well as act as a forum for Australian regulatory bodies to test any proposed AI strategies or frameworks.A by-product of the Alliance is the High-Level Expert Group on AI, which is a subgroup of the Alliance which developed a detailed assessment list as a resource for bu"
427,Gadens,Q06 Public vs Private sector governance,"Gadens considers that the public sector plays an important role in building public trust through the use of AI applications. As such, the public can expect that higher standards, whether voluntary or regulatory, should be met by the public sector.We submit that while private organisations should strive to comply with the highest available standards, in particular in relation to ethical standards, mandatory compliance may negatively impact AI innovation and use in Australia. We consider that a mandatory and/or regulatory approach should be reserved for high-risk AI applications and to mitigate particular risk areas of AI, such as privacy, competition and protection of consumers and vulnerable people.Gadens considers that industry specific codes of conduct and voluntary technical and ethical standards are the preferred tool to encourage AI best practices among private organisations while allowing them to remain competitive internationally."
427,Gadens,Q07 Responsible government use of AI,"Gadens submits that the following measures may assist the Australian Government in supporting responsible AI practices in its own agencies:adaptation of a consolidated AI strategy for government agencies;implementation of a government-wide AI assurance and ethics framework (similar to the NSW AI Assurance Framework and Ethics principles)3; anda central or agency-specific AI register which identifies AI applications implemented by relevant agencies, supplemented by risk assessments in relation to relevant use cases."
427,Gadens,Q08 Generic vs technology-specific approaches for AI risk,"We consider that generic solutions lend themselves to low risk and high-volume AI applications where generic solutions enable large audiences to benefit from AI applications.While we acknowledge that technology-specific solutions require longer development and implementation timeframes, we consider that a combination of any one of the following factors may necessitate a departure from generic solutions:complexity of an AI application;fast-evolving use case of an AI application;human oversight issues;high-risk impact of an AI application; and/ordegree of regulation prevalent in the sector in which the AI application is predominantly deployed (e.g., the medical sector)."
427,Gadens,Q09 Transparency requirements and scope,"where and when transparency will be most critical and valuable to mitigate potential AI risks and to improve public trust and confidence in AI?We consider that transparency is equally important in both the development and the deployment phase of an AI application, and that all aspects of an AI application lifecycle are relevant to building public trust and confidence. In the development and testing phase we note that conformity with any mandatory regulation or voluntary conduct is a key factor. In relation to the deployment of AI applications we believe that consumers should be made aware of the fact that they are interacting with an AI application and, to the extent available and feasible in the context, be informed of the risk level associated with such use. Uniform risk levels/codes would be helpful in this context.However, we note that to build public trust and confidence, it is not necessary to disclose all aspects of the development and testing process, full functionalities or al"
427,Gadens,Q10 Ban on high-risk AI applications,"Whether any high-risk AI applications or technologies should be banned completely?Gadens considers that fast evolving AI use cases and open-ended designs of AI applications warrant the banning of very high-risk AI applications. Public trust will depend on ensuring that practices which are undesirable and detrimental to society are being banned.Criteria or requirements to identify AI applications or technologies that should be banned, and in which contexts?We consider that a combination of any one of the following criteria or requirements, following a comprehensive risk assessment, may warrant the ban of an AI application or use case:large scale and/or high complexity AI applications;very high risk/impact expected (eg, decisions that are threatening to life, welfare, financial well-being, freedom, particularly effecting vulnerable groups);irreversible/perpetual decisions;applications with human oversight issues/high level of automation; andimmoral (social scoring for certain purposes) o"
427,Gadens,Q11 Actions to increase public trust in AI,"Gadens submits that the following initiatives and government actions can increase public trust in AI deployment:government leading by example, e.g., by adopting a mandatory government AI assurance and ethics framework (please refer to our answers above for detail);government supported ‘sandbox’ environment to develop and test new AI applications;adoption of a government approved AI rating (e.g., in relation to associated risk levels) and/or certification which signifies (voluntary) compliance with industry and/or ethical standards;creation of an AI safety commissioner or portfolio which allows users to complain about AI practices; andconsolidated AI education platform/centre tailored to the general public (comparable to the DISR via CSIRO’s Responsible AI Centre for industry)."
427,Gadens,Q12 Trade impact of banning high-risk AI applications,"Gadens anticipates that an approach consistent with that taken by other jurisdictions, such as the European Union or Canada, with regard to banning of high-risk activities will have a positive impact on the trade with, and export of, AI applications by ensuring cross-jurisdictional acceptance and consistency. However, we note that any bans of high-risk activities should also be reviewed in light of decisions reached in other jurisdictions to ensure Australia’s tech sector is not unnecessarily disadvantaged in the international context."
427,Gadens,Q13 Changes to conformity infrastructure,"Aligned with our submission to prioritise a gap analysis for existing regulatory regimes in relation to AI regulation, we submit that any changes made to the Australian conformity infrastructure should be informed by an infrastructure gap analysis prior to any changes being made. Further, Gadens considers it preferable to focus on changes which are necessary to protect against substantial AI risks (e.g., other than low level risks), as an overly prescriptive approach may not be technology-neutral and thus not future-proof, and stifle AI innovation."
427,Gadens,Q14 Risk-based approach to address AI risks,"Gadens is generally in support of a risk-based approach to address the potential risks of AI. We note that many organisations will benefit from their expertise in relation to compliance with existing regulatory frameworks, e.g., the Privacy Act 1988 (Cth) or the Anti-Money Laundering and Counter-Terrorism Financing Act 2006 (Cth). We further consider that a risk-based approach will allow organisations to allocate resources effectively in respect of control implementation and integration of AI in their service delivery or process optimisation. We are not in support of prescriptive technology-focussed approaches as they will challenge SME AI developers and deployers and require careful monitoring and revision."
427,Gadens,Q15 Benefits and limitations of risk-based approach,"BenefitsWe consider that a risk-based approach will:enable companies to draw upon existing experience with risk-based assessments;allow for the timely adoption of low-risk AI applications; andprioritise resources on the review and adoption of medium to high-risk AI applications.LimitationsWe consider that the main limitations of a risk-based approach are:lack of historical data to assist with accurate evaluation of risk events/issues (ie, likelihood and/or impact, which may result in low accuracy of risk assessments);low accuracy of risk assessments may impact the efficacy of controls; andprecautionary measures intended for medium to high-risk AI application may predominantly stifle cutting-edge and highly innovative uses of AI.Addressing limitationsGadens considers that important strategies for overcoming any limitations associated with a risk-based approach are:Increased focus on governing record-keeping and change management – Record-keeping and change management activities associat"
427,Gadens,Q16 Scope and application of risk-based approach,"No. While we consider that larger organisations may benefit from existing experience with risk-based assessments (e.g., in the context of AML/CTF compliance), risk-based assessments are scalable in that they are not solely based on the risk associated with an AI application but are also impacted by various other factors, such as an organisation's risk appetite. Further, potential disclosure obligations of AI developers/vendors and government guidance may assist smaller organisations to perform risk-based assessments, irrespective of size, AI maturity and resources."
427,Gadens,Q17 Elements to include in risk-based approach,"Elements to be includedIn addition to the elements listed in Attachment C, we consider the following control element appropriate to support a risk-based approach:regular testing and review of control effectiveness – To address the particular limitations of a risk-based approach (as outlined in 15.3(c) above), we consider that any controls implemented to address AI risks need to be tested and reviewed on a regular basis to ensure their effectiveness while enabling appropriate responses to innovative uses and/or developments in the area of AI.Elements supportedGadens supports all the elements listed in Attachment C. We further consider that impact assessments should be undertaken systematically at several points of AI integrations."
427,Gadens,Q18 Integration of risk-based approach into existing frameworks,"As a legal services firm, Gadens places a particular emphasis on the protection of information – from both a privacy and information security standpoint. Thus, the adaptation of an AI-focussed risk-based approach would be heavily influenced by existing privacy risk and information security management processes.Management processes are designed around information classification, which is impacted by:legal, client or contractual requirements that must be complied with;value and criticality to the organisation or their clients, andsensitivity to unauthorised disclosure or modification.The type of information an AI application is required to process, store and transfer will be a determining factor for any AI risk-assessment to ensure the evaluation framework of the firm’s information assets will align with the AI risk-based approach, streamlining the assessment process."
427,Gadens,Q19 Application of risk-based approach to general purpose AI,"We consider that the control elements proposed in Attachment C will need to be evaluated in an application-specific context. While not all elements may lend itself to the evaluation of LLMs or MFMs, we consider that a risk-based approach, driven by frequently reviewed, organisation-internal risk frameworks, will enable risk-based assessments of emerging AI uses."
427,Gadens,Q20 Voluntary vs mandated compliance with risk-based approach,"We consider that a risk-based approach for responsible AI should be mandatory for all developers and deployers (whether public or private) for high-risk AI applications. Generally, we would expect public organisations to lead by example, and a mandatory risk-based approach may be warranted irrespective of the level of risk involved, to facilitate public trust in AI applications.For private organisations (developers or deployers) we consider that existing thresholds, such as annual turnover exceeding $100 million AUD, may be utilised to impose a mandatory risk-based approach irrespective of risk levels. Private deployers of AI below certain thresholds may benefit from a voluntary, industry-led framework. The regulation of small to medium size private developers of AI applications should be guided by further government and industry consultation, taking into account the effect mandatory regulation would have on both innovation and trade."
428,Medical Technology Association of Australia and Asia Pacific Medical Technology Association,Transcription,"FILENAME: MTAA & APACMed - Safe and Responsible AI in Australia.44a2c2c2708a4.pdf | CONTENT: Medical Technology Association of Australia Ltd ABN: 61 129 334 354 MTAA Office, Level 4, 97 Waterloo Road Macquarie Park, NSW 2113 Australia P: (02) 9900 0600 W: www.mtaa.org.au E: reception@mtaa.org.auSupporting responsible AI: discussion paper Medical Technology Association of Australia in collaboration with The Asia Pacific Medical Technology Association August 20231 Medical Technology Association of Australia Ltd ABN: 61 129 334 354 MTAA Office, Level 4, 97 Waterloo Road Macquarie Park, NSW 2113 Australia P: (02) 9900 0600 W: www.mtaa.org.au E: reception@mtaa.org.auContents Executive Summary ........................................................................................................................... 4 General Comments............................................................................................................................ 5 Definitions ......................"
429,Property Exchange Australia Ltd (PEXA),Transcription,"FILENAME: PEXA AI Government Submission (Signed).3f10b40e0aa26.pdf | CONTENT: PEXA Submission Supporting responsible AI: discussion paperAugust 2023Property Exchange Australia Limited | Level 16, Tower 4, 727 Collins Street, Docklands VIC 3008, Docklands VIC 3008 PEXA Submission: Supporting Responsible AIPEXA IntroductionProperty Exchange Australia Ltd (PEXA) is pleased to have the opportunity to make a submission to the Federal Government’s consultation on the proposed Supporting responsible AI: discussion paper. We thank the Federal Government for granting an extension to PEXA so we could contribute to this important topic.As the landscape of AI rapidly evolves, it is essential to strike a balance between embracing its potential for economic growth and improving lives while mitigating the risks associated with its adoption.Australia has begun to build strong foundations in the realm of responsible AI. With the creation of the Government’s Artificial Intelligence Ethics Framework, thi"
429,Property Exchange Australia Ltd (PEXA),Q01 Definitions,Yes
429,Property Exchange Australia Ltd (PEXA),Q03 Non-regulatory actions to address AI risks,Consumer and business education
429,Property Exchange Australia Ltd (PEXA),Q04 Coordinating AI governance across government,"Coordinating regulator agency in this space will be necessary to deal with the scale, complexity and ongoing changes in the AI space. This will be across society, across the economy impact."
429,Property Exchange Australia Ltd (PEXA),Q05 International models for AI governance,"There are approximately 40 draft standards or working papers. Please refer to: https://www.standards.org.au/engagement-events/strategic-initiatives/critical-and-emerging-technologies/data-digital-dashboard and select AI and you can see Published vs Development (full list by clicking on the bottom left tab which is ""Click to view list.....""."
429,Property Exchange Australia Ltd (PEXA),Q06 Public vs Private sector governance,Yes
429,Property Exchange Australia Ltd (PEXA),Q07 Responsible government use of AI,NSW Government have an ethical AI framework which is being updated.
430,Regulating AI-Generated Content: A Low-tech Solution to a High-tech Problem,Transcription,"FILENAME: Epiphany Law - AI Generated Content - A Low-tech Solution to a High-tech Problem (Final).a369d3caa7cd8.pdf | CONTENT: IPREPORTRegulating AI-Generated Content A Low-tech Solution to a High-tech ProblemSubmission by Epiphany Law in response to the ‘Safe and Responsible use of AI in Australia’ Discussion Paper published by the Department of Industry, Science and ResourcesEPIPHANY LAW OFFICE PHONE +61 3 8640 0104 EPIPHANY LAW PTY LTD 425 Smith Street WEB epiphany.law ABN 61 143 781 192 Fitzroy Victoria 3000 EMAIL hello@epiphany.law DATE 4 August 2023Liability limited under a scheme approved under Professional Standards Legislation. © Epiphany Law Pty Ltd, 2023 1 Overview1.1 EXECUTIVE SUMMARY This document sets out Epiphany Law’s Response to the Discussion Paper released by the Department of Industry, Science and Resources in June 2023 entitled ‘Safe and responsible AI in Australia’ (the ‘Discussion Paper’).1 In summary:(a) We agree that AI technologies present Australia with both"
430,Regulating AI-Generated Content: A Low-tech Solution to a High-tech Problem,Q01 Definitions,Yes.
430,Regulating AI-Generated Content: A Low-tech Solution to a High-tech Problem,Q02 Other AI risks and recommendations for regulation,"We believe a major risk is the regulation of misleading AI-generated content, which includes ""digital clones"" or ""deepfakes"". We have included a suggestion for possible regulatory action in the form of draft l egislation."
430,Regulating AI-Generated Content: A Low-tech Solution to a High-tech Problem,Q03 Non-regulatory actions to address AI risks,"As discussed in our submission, we also recommend that the Australian Government consider implementing the EU-style risk management framework in Australia."
430,Regulating AI-Generated Content: A Low-tech Solution to a High-tech Problem,Q04 Coordinating AI governance across government,No.
430,Regulating AI-Generated Content: A Low-tech Solution to a High-tech Problem,Q05 International models for AI governance,"Yes, as discussed in the attached submission, we have taken some ideas from the European Union. We also believe that some of the ideas in our proposed solution are consistent with some of the measures taken by China."
430,Regulating AI-Generated Content: A Low-tech Solution to a High-tech Problem,Q06 Public vs Private sector governance,"At present, we do not have a position on this issue."
430,Regulating AI-Generated Content: A Low-tech Solution to a High-tech Problem,Q07 Responsible government use of AI,"At present, we do not have a position on this issue."
430,Regulating AI-Generated Content: A Low-tech Solution to a High-tech Problem,Q08 Generic vs technology-specific approaches for AI risk,We think that the key driver here is the level of uncertainty of future prediction. We discuss this issue at length in our attached submission.
430,Regulating AI-Generated Content: A Low-tech Solution to a High-tech Problem,Q09 Transparency requirements and scope,a. The regulation of misleading AI-generated content. b. Please see the attached submission and the proposed legislation.
430,Regulating AI-Generated Content: A Low-tech Solution to a High-tech Problem,Q10 Ban on high-risk AI applications,a. No. Not at this stage. b. No. Not at this stage.
430,Regulating AI-Generated Content: A Low-tech Solution to a High-tech Problem,Q11 Actions to increase public trust in AI,We discuss literacy initiatives in the attached submissions.
430,Regulating AI-Generated Content: A Low-tech Solution to a High-tech Problem,Q12 Trade impact of banning high-risk AI applications,We do not have a position on this issue at present.
430,Regulating AI-Generated Content: A Low-tech Solution to a High-tech Problem,Q13 Changes to conformity infrastructure,We do not have a position on this issue at present.
430,Regulating AI-Generated Content: A Low-tech Solution to a High-tech Problem,Q14 Risk-based approach to address AI risks,Yes we do. Please see the attached submissions.
430,Regulating AI-Generated Content: A Low-tech Solution to a High-tech Problem,Q15 Benefits and limitations of risk-based approach,Please see the attached submissions.
430,Regulating AI-Generated Content: A Low-tech Solution to a High-tech Problem,Q16 Scope and application of risk-based approach,Please see the attached submissions.
430,Regulating AI-Generated Content: A Low-tech Solution to a High-tech Problem,Q17 Elements to include in risk-based approach,"Yes, we support those elements."
430,Regulating AI-Generated Content: A Low-tech Solution to a High-tech Problem,Q18 Integration of risk-based approach into existing frameworks,We do not have a position on this issue at present.
430,Regulating AI-Generated Content: A Low-tech Solution to a High-tech Problem,Q19 Application of risk-based approach to general purpose AI,We think there needs to be more testing before LLMs and MFMs are released (e.g. GPT-5).
430,Regulating AI-Generated Content: A Low-tech Solution to a High-tech Problem,Q20 Voluntary vs mandated compliance with risk-based approach,"We believe that it should apply through regulation, and apply to all actors, public and private organisations, developers and deployers."
431,Joint submission from library and information service organisations,Transcription,"FILENAME: Joint submission from library and information service organisations to safe and responsible AI inquiry.e478e46c930ea.pdf | CONTENT: Joint submission from library and information service-related organisations to the Safe and Responsible AI in Australia Discussion Paper Contents Executive Summary ........................................................................................................................ 3 About this submission ................................................................................................................ 3 A note on terminology ............................................................................................................... 3 1. Education is crucial: media, information, data and AI literacy............................................... 4 2. Australian development: AI and cultural collections ............................................................. 6 3. Impacts on the collections, the wider information ecosystem a"
431,Joint submission from library and information service organisations,Q01 Definitions,Yes we agree with the definitions
431,Joint submission from library and information service organisations,Q02 Other AI risks and recommendations for regulation,See submission
431,Joint submission from library and information service organisations,Q03 Non-regulatory actions to address AI risks,See submission
431,Joint submission from library and information service organisations,Q04 Coordinating AI governance across government,See submission
431,Joint submission from library and information service organisations,Q05 International models for AI governance,See submission
431,Joint submission from library and information service organisations,Q06 Public vs Private sector governance,See submission
431,Joint submission from library and information service organisations,Q07 Responsible government use of AI,See submission in regards libraries and related organisations
431,Joint submission from library and information service organisations,Q08 Generic vs technology-specific approaches for AI risk,See submission
431,Joint submission from library and information service organisations,Q09 Transparency requirements and scope,See submission
431,Joint submission from library and information service organisations,Q10 Ban on high-risk AI applications,See submission
431,Joint submission from library and information service organisations,Q11 Actions to increase public trust in AI,See submission
431,Joint submission from library and information service organisations,Q12 Trade impact of banning high-risk AI applications,See attached submission
431,Joint submission from library and information service organisations,Q13 Changes to conformity infrastructure,see attached submission
431,Joint submission from library and information service organisations,Q14 Risk-based approach to address AI risks,See attached submission
431,Joint submission from library and information service organisations,Q15 Benefits and limitations of risk-based approach,See attached submission
431,Joint submission from library and information service organisations,Q16 Scope and application of risk-based approach,See attached submission
431,Joint submission from library and information service organisations,Q17 Elements to include in risk-based approach,See attached submission
431,Joint submission from library and information service organisations,Q18 Integration of risk-based approach into existing frameworks,See attached submission
431,Joint submission from library and information service organisations,Q19 Application of risk-based approach to general purpose AI,See attached submission
431,Joint submission from library and information service organisations,Q20 Voluntary vs mandated compliance with risk-based approach,See attached submission
432,Medical Software Industry Association,Transcription,"FILENAME: Final submission to DISR- AI ejh040823.23541a2e9f4a6.pdf | CONTENT: Safe and Responsible use of AI in AustraliaMedical Software Industry Association’s Submission to Department of Industry, Science & ResourcesDate: Friday, 4th August 2023Contact: Emma Hossack | ceo@msia.com.au | 0413 483 096BA (Hons), LLB, LLM 23/06/2023 Executive Summary WHO - The Medical Software Industry Association (“MSIA”) represents the interests of over 165 Australian companies to enable a vibrant more efficient effective health system for Australians. MSIA members provide health software to all sectors of Australia’s healthcare1 market to enable the collection, delivery, management, and storage of health information including administration and payment systems. Australia’s healthcare depends on our members systems which are integral to healthcare delivery.WHAT - Digital health software powers healthcare delivery, consumer access, and importantly, supports major reforms in our health and care sector.2 I"
433,Royal Australasian College of Medical Administrators (RACMA),Transcription,FILENAME: RACMA Response to the Australian Government Safe and responsible AI in Australia discussion paper_FINAL_230804.2673ae48b1db1.pdf | CONTENT: Royal Australasian College of Medical Administrators (RACMA) Response to the Australian Government’s “Safe and Responsible AI in Australia” Discussion paperAugust 20231 CONTENTSAbout RACMA....................................................................................................................... 3 Informed Decision-making in Healthcare ............................................................................... 4 Australia’s AI Ethics Principles .............................................................................................. 4 Questions to Address ............................................................................................................ 7 Definitions ............................................................................................................................. 7 Potential Gaps in
433,Royal Australasian College of Medical Administrators (RACMA),General comment,RACMA thanks you for the opportunity to make a submission on this important issue for the future of safe and quality healthcare delivery in Australia and the world.
435,Ned Cooper,Q01 Definitions,"I understand the Department's pursuit of uniform terminology to ensure clarity of discussion and of any regulatory response. However, I recommend the Department draws on the UK’s policy paper ‘A pro-innovation approach to AI regulation’, which avoids narrow technical definitions and instead focuses on the characteristics of artificial intelligence (AI) motivating a specific regulatory response: adaptivity and autonomy. There is unlikely to be a general definition of machine learning, AI, etc. that will receive widespread consensus during this consultation. Furthermore, even if consensus is reached, precise technical definitions are susceptible to change due the pace of research and development in this area."
435,Ned Cooper,Q02 Other AI risks and recommendations for regulation,Please consider the recommendations for accessibility outlined in Part D of the Australian Human Rights Commission’s (AHRC) 2021 report on Human Rights and Technology.
435,Ned Cooper,Q03 Non-regulatory actions to address AI risks,"The AHRC in its report on Human Rights and Technology referred to ’participation’ in the decision-making of stakeholders affected by new technologies as a core principle of a human rights approach to technology.In this short presentation, I outline what an AI Safety Commissioner might consider to enact the principles of ‘participation’, drawing on experiences with the Australian Law Reform Commission: https://algorithmicfutures.org/sra22-abstracts/cooper-broad-enabling-participation-review-algorithms-lessons-law-reform/"
435,Ned Cooper,Q04 Coordinating AI governance across government,"Again, please consider the Australian Human Rights Commission’s (AHRC) 2021 report on Human Rights and Technology, which recommended an AI Safety Commissioner in part to coordinate AI governance across the Federal Government. This report should be considered along with the Tech Policy Design Centre’s ‘Tending the Tech Ecosystem’ Report, which did not find support among existing regulatory bodies, government departments and industry organisations for a new centralised technology regulator: https://techpolicydesign.au/report-tending-the-tech-ecosystem.(I note that the Australian Human Rights Commissioner had changed in the period between the publication of the Human Rights and Technology Report and the interviews for the Tech Policy Design Centre’s Report). and Human Rights Report or the Tech Policy Design Centre are better guides)"
435,Ned Cooper,Q05 International models for AI governance,"I would only note that the EU AI Act forms part of Europe’s long history of leading the regulation of new technologies, including attempts to regulate automated decision-making by France dating back to the 1970s. More recently, the EU’s General Data Protection Regulation (GDPR) had a global and ground-breaking influence on data protection and privacy. If considering implementing regulations in Australia similar to the EU AI Act, I would encourage the Department to first review and assess the impact of the GDPR on Australia and Australia's response to the GDPR.Additionally, the application of the GDPR in the EU has revealed the important role of the judiciary in maintaining flexibility in legislative interpretation, ensuring statutory instruments are applied to meet the new regulatory challenges posed by rapidly innovating technologies. Any proposed regulations in Australia should be drafted to allow such flexibility, which may mean avoiding the rigid way technologies were categorised i"
435,Ned Cooper,Q09 Transparency requirements and scope,"A suggested minimum standard for transparency of automated decision-making in Australia: people should be made aware when AI has been used to inform decisions made about them, what information (datasets) have been used to make the decision, and the recourse available to have those decisions reviewed.For large language models (LLM), I support placing requirements on LLM creators and providers to publish the size, make-up and provenance of training data. On the topic of transparency of LLMs, I recommend the Department review the research roadmap published recently by Q. Vera Liao and Jennifer Wortman Vaughan entitled 'AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap': https://arxiv.org/abs/2306.01941One final note - I would encourage the Department to be mindful of overinvesting in the principle of transparency, considering the multiple dimensions of Australia’s AI ethics guidelines and the differing resource capacities of organisations involved in the development of"
435,Ned Cooper,Q10 Ban on high-risk AI applications,"If considering bans, I would encourage banning applications rather than technology. For example, the Department might consider banning the application of Emotional AI technologies (e.g., real-time biometric monitoring) in certain places (e.g., in schools or in workplaces), rather than the technologies themselves."
435,Ned Cooper,Q11 Actions to increase public trust in AI,"I would suggest the Department consider the goals of ‘increasing public trust’ and ‘encouraging more people to use AI’ separately. The Department may increase public trust in AI by strengthening the regulatory ecosystem in which AI is deployed (i.e. ensuring adequate safeguards). On the other hand, if there is a desire to encourage more people to use AI, I would encourage the Department to focus investment on education and training about AI and its applications."
435,Ned Cooper,Q12 Trade impact of banning high-risk AI applications,"I am unsure of what impact bans would have on our trade and exports. However, I would challenge that our primary consideration should be whether we want to deploy any technology in our own jurisdiction. If we decide that we want to ban certain technology or applications in our jurisdiction, that decision should be a key consideration for whether it is appropriate to trade or export the relevant technology or application with other countries."
436,Blackwell AI,Transcription,"FILENAME: Blackwell_AI_Statement.283b0d3c0ccfb.pdf | CONTENT: 31st July 2023Department of Industry, Science and Resources Safe and responsible AI in AustraliaDear Reader,We welcome the opportunity to respond to the discussion paper on supporting responsible Artificial Intelligence (AI). As a start-up at the forefront of AI technology, we understand the critical importance of addressing the potential risks associated with AI while fostering a safe and responsible AI environment in Australia.AI has undoubtedly brought significant advancements and improvements to various aspects of our lives. However, the rapid pace of innovation in this domain has also raised concerns and uncertainties about its potential consequences. We believe that effective governance mechanisms are essential to ensure the development and deployment of AI in a manner that upholds privacy and security.In light of the Government's focus on regulations, standards, tools, frameworks, principles, and business practices, w"
436,Blackwell AI,Q01 Definitions,"We would appreciate the opportunity to share our insights on refining and clarifying these definitions to ensure that they are precise, comprehensive, and accessible to a wide range of stakeholders. These clarifications are essential because they help set boundaries on the subject, preventing confusion with similar software that employs similar terminology but relies on different methodologies.The definition of artificial intelligence states that it refers to an ""engineered system"" that generates predictive outputs without explicit programming. While this is generally accurate, the term ""engineered system"" might be too broad and could include non-AI systems as well. A more precise and explicit definition of AI, emphasising its ability to learn from data and adapt to new situations, could be beneficial.Specificity in Machine Learning: The definition of machine learning correctly highlights that it involves patterns derived from training data for prediction or decision-making. However, i"
436,Blackwell AI,Q02 Other AI risks and recommendations for regulation,"We acknowledge the importance of regulatory approaches to address potential risks associated with AI in Australia. While existing regulations provide a foundation for safeguarding privacy and data protection, it is essential to recognise that some risks related to AI may not be fully covered by the current regulatory landscape.While the Privacy Act addresses many privacy concerns, AI's increasing influence requires a closer examination of existing protections. We recommend updating the Privacy Act to incorporate specific provisions that address data privacy concerns related to AI technologies, especially those involving large-scale data processing and profiling.Another potential risk not extensively addressed by existing regulations is the ethical use of AI, particularly in sensitive domains such as healthcare and financial services. As AI technology evolves rapidly, it is crucial to develop regulatory frameworks that guide the responsible and ethical development, deployment, and decis"
436,Blackwell AI,Q03 Non-regulatory actions to address AI risks,"The Australian Government can further support responsible AI practices by making additional investment in AI research. By allocating resources and funding to establish specialised AI programs, research centres, and scholarships, the government can cultivate a new generation of AI professionals with the necessary skills and knowledge to develop and deploy AI systems responsibly. This initiative will not only address the growing demand for AI expertise in various industries but also foster a culture of ethical AI development, privacy protection, and security consciousness. By encouraging research and collaboration within academia, the government can promote best practices and ensure that Australia remains at the forefront of AI innovation while adhering to principles of privacy and security.For enhancing responsible AI practices, the Australian Government should actively participate in global coordination efforts concerning data validity and training of Language Models (LLMs) and Multimo"
436,Blackwell AI,Q05 International models for AI governance,"The governance measures taken by Italy regarding the banning and subsequent unbanning of ChatGPT hold significant relevance for Australia in the context of data collection and age verification, especially when it comes to publicly accessible AI platforms and the potential risks of harm and damage. Italy's approach in addressing concerns about private data used to train ChatGPT demonstrates the importance of robust data privacy conditions and transparency in AI systems. By requiring increased transparency on data processing, opt-out rights, age verification, and notices about potential inaccuracies, Italy's regulatory actions exemplify measures that prioritise user safety and privacy. Such measures could serve as valuable lessons for Australia in formulating its own governance mechanisms to ensure responsible AI practices and foster public trust in AI technologies."
436,Blackwell AI,Q09 Transparency requirements and scope,"Transparency is most critical and valuable in the AI lifecycle during the model pre-training and model fine-tuning stages. In the pre-training phase, transparency becomes essential to ensure data quality and prevent potential biases from being ingrained into the AI model. Understanding the sources and types of data used for pre-training can help identify and address any ethical or representational concerns. Similarly, during the fine-tuning stage, transparency is crucial to shed light on the intentions and objectives of the model's optimisation. This transparency enables an assessment of potential risks and biases introduced through the fine-tuning process, helping to improve the model's fairness and accountability. By focusing on transparency at these early stages, AI developers can instil public trust and confidence in the technology by showcasing responsible and ethical practices.Mandating transparency requirements across the private and public sectors poses challenges due to the in"
436,Blackwell AI,Q14 Risk-based approach to address AI risks,"A risk-based approach for addressing potential AI risks is generally favourable as it allows for a nuanced and adaptable strategy that considers the context-specific risks associated with different AI deployments. By categorising AI uses based on their risk levels, organisations can implement less onerous regulation for lower risk AI applications, enabling innovation and development in these areas without unnecessary burdens.However, it is essential to focus on balance and avoid an overly heavy enforcement of risk-based requirements. Excessive regulatory burdens on medium and high-risk AI applications may hinder innovation and hamper the development of AI technologies in critical areas, particular for small and medium sized businesses. Over-regulation of AI applications could discourage investment and research in these fields due to increased costs and complexity. Therefore, it is vital to foster a collaborative environment between regulators, industry stakeholders, and researchers to "
436,Blackwell AI,Q19 Application of risk-based approach to general purpose AI,"A risk-based approach to general purpose AI systems, such as LLMs or MfMs, would initially categorise them as low-risk due to their general applicability. However, their risk level can escalate quickly based on specific use cases and deployment contexts. When applied in settings involving sensitive data, decision-making, or critical infrastructure, these AI systems could transition into medium to high-risk categories. Therefore, it is essential to assess the potential impacts and context of their applications carefully to determine appropriate risk management requirements."
437,ARC Centre of Excellence on Automated Decision-Making and Society,Transcription,"FILENAME: 2023 Safe and Responsible AI - ADM+S Submission.479c7b5ee4962.docx | CONTENT: Safe and Responsible AI in Australia Discussion PaperADM+S SubmissionLead author: Kimberlee WeatherallContributing authors: Zofia Bednarz, Jose-Miguel Bello y Villarino, Jean Burgess, Loup Cellard, Tegan Cohen, Henry Fraser, Jake Goldenfein, Timothy Graham, Fiona Haines, Paul Henman, Nataliya Ilyushina, Jenny Kennedy, Jackie Leach Scully, Dennis Leeftink, Suvradip Maitra, Rita Matulionyte, Anthony McCosker, Robert Mullins, Kelsie Nabben, Christine Parker, Thao Phan, Flora Salim, Aaron Snoswell, Julian Thomas, Melanie Trezise, Libby Young, Jacky Zeng.ARC Centre of Excellence for Automated Decision-Making and Society4 August 2023About ADM+SThe ARC Centre of Excellence for Automated Decision-Making and Society (ADM+S) is a cross-disciplinary, national research centre which commenced operations in mid 2020. ADM+S has been established and supported by the Australian Research Council to create the knowled"
437,ARC Centre of Excellence on Automated Decision-Making and Society,Transcription,"FILENAME: 2023 Safe and Responsible AI - ADM+S Submission.a3773e85d7ff5.pdf | CONTENT: Safe and Responsible AI in Australia Discussion Paper ADM+S SubmissionLead author: Kimberlee Weatherall Contributing authors: Zofia Bednarz, Jose-Miguel Bello y Villarino, Jean Burgess, Loup Cellard, Tegan Cohen, Henry Fraser, Jake Goldenfein, Timothy Graham, Fiona Haines, Paul Henman, Nataliya Ilyushina, Jenny Kennedy, Jackie Leach Scully, Dennis Leeftink, Suvradip Maitra, Rita Matulionyte, Anthony McCosker, Robert Mullins, Kelsie Nabben, Christine Parker, Thao Phan, Flora Salim, Aaron Snoswell, Julian Thomas, Melanie Trezise, Libby Young, Jacky Zeng. ARC Centre of Excellence for Automated Decision-Making and Society 4 August 2023About ADM+S The ARC Centre of Excellence for Automated Decision-Making and Society (ADM+S) is a cross- disciplinary, national research centre which commenced operations in mid 2020. ADM+S has been established and supported by the Australian Research Council to create the kn"
437,ARC Centre of Excellence on Automated Decision-Making and Society,General comment,"The ADM+S is pleased to have this opportunity to engage with an important and complex question which confronts Australia: how should the Australian federal government take action - regulatory or otherwise - to promote artificial intelligence (and automated decision-making) that is safe and responsible? In our view, ‘responsible’ AI must also be inclusive, accountable, and genuinely beneficial - for Australia’s people, society, economy, and environment."
437,ARC Centre of Excellence on Automated Decision-Making and Society,Q01 Definitions,"The scope of regulation should be determined according to capacities, behaviours, and/or impacts - not technologies. We also see some risk that the definitions will become outdated (in ways we outline in our submission), while acknowledging the desirability of consistency in definitions across jurisdictions. A definition of ‘foundation model’ would also be helpful. Please see our uploaded submission for detail."
437,ARC Centre of Excellence on Automated Decision-Making and Society,Q02 Other AI risks and recommendations for regulation,"This question is best answered by first understanding what has changed, or what is new, which we do immediately below. A picture of existing gaps follows from the changes we identify: 1. Australia will need to re-consider, and in some cases re-frame, a wide range of existing legal frameworks 2. Specific gaps around the application of laws to the process of training large models; 3. Where laws do exist, features of the technology and AI supply chains gives rise to enforcement challenges and gaps that will need revision; and 4. New capacities are emerging where DISER should initiate a broad societal conversation about what use is and isn’t acceptable. In addition, research, including at ADM+S is emerging regarding the environmental impacts of AI, and consideration of environmental impacts is being built into regulatory proposals overseas such as the EU AI Act. Environmental impacts must be a consideration when developing regulation, and a risk-based approach in Australia. Please see our "
437,ARC Centre of Excellence on Automated Decision-Making and Society,Q03 Non-regulatory actions to address AI risks,"Among a wide range of non-regulatory actions the Australian government could take in this space, we suggest a focus on three in particular: 1. Invest in involving the Australian public in discussions about the direction of AI technology and its application: in order for the Australian public to trust AI technology and support its use, the current technocratic conversation needs to be broadened to be more inclusive. DISER is well-placed to lead such efforts, using established methods for science consultation and participatory governance. 2. Invest in education at multiple levels across society and the economy, to reduce the knowledge gap between the AI specialists (who create AI programs), and AI end users (who are responsible at the coalface for the deployment of AI, impacting themselves or other parties) or subjects (who are impacted by decisions and/or actions using AI). 3. To better address the rapid development and rapid deployment of AI technologies including new models and method"
437,ARC Centre of Excellence on Automated Decision-Making and Society,Q04 Coordinating AI governance across government,"We recommend a central function, across government, with technical expertise and the ability to connect to further technical expertise outside government, and the power to drive practical change within and beyond government. Please see our uploaded submission for detail."
437,ARC Centre of Excellence on Automated Decision-Making and Society,Q05 International models for AI governance,"Australia can benefit from, and adapt, approaches that are developing in other countries - but must be mindful of key differences in Australia’s legal framework which mean that those approaches will fail here without addressing those differences. Details are discussed in our submission.Australia is well-positioned to cooperate - and should cooperate - with like-minded countries in developing international frameworks for the regulation of AI. In doing so, Australia should be conscious to: • not leaving Australians behind in rights and remedies, • consider domains and issues where there is a role for Australian governments in protecting and promoting uniquely Australian voices and interests; • continue to develop/invest in Australia’s domestic capacity to develop and innovate in AI research and technologies, responsible AI practices and the regulatory frameworks. See our uploaded submission for detail."
437,ARC Centre of Excellence on Automated Decision-Making and Society,Q06 Public vs Private sector governance,"Yes, a different approach is required: different legal obligations apply to public and private sector decision-making. Public sector decision-making must meet the requirements of administrative law, which imposes standards of transparency, the obligation to explain (in some cases) and obligations to observe natural justice. The higher standards applicable to public sector decision-making, among other factors, make use of AI in the public sector a good place to develop and test governance mechanisms. Please see our uploaded submission for detail."
437,ARC Centre of Excellence on Automated Decision-Making and Society,Q07 Responsible government use of AI,"In our view, the most critical gap around public sector use of AI at present is inadequate accountability, and insufficient supervision or mechanisms in place to ensure the implementation of existing principles designed to govern government use of AI and ADM. We further recommend mechanisms to surface use and enable the sharing of expertise within and across government departments and agencies - such as a register of systems. Please see our uploaded submission for detail."
437,ARC Centre of Excellence on Automated Decision-Making and Society,Q09 Transparency requirements and scope,"Transparency will be valuable for all those affected by the operations and outcomes of an AI, from non-technically trained users to expert investigators. However, the regulation for transparency requires an actionable, measurable standard, capable of graduated modulation depending on a range of critical contextual factors: the circumstances of an AI’s operations (including users’ interests in privacy and security); the level and nature of risk entailed; and the kind of people or organisations for whom transparency is intended. • Transparency is a value that should be mandated for AIs and ADMs across the public and private sectors. • Regulators will need to specify the content, volume and form of the communication required to satisfy a transparency obligation in accordance with the contextual factors. Please see our uploaded submission for detail."
437,ARC Centre of Excellence on Automated Decision-Making and Society,Q12 Trade impact of banning high-risk AI applications,"We have identified elsewhere in our submission new capacities arising from the combination of wide availability of data, automation and AI. We submit that a public conversation about those capacities - and limits on their use - is warranted. The conversation on the limits of acceptable technology use cannot be a purely technocratic conversation as tends to occur through consultation processes like the present one. A wide cross-section of the community, including impacted groups, must be enabled to participate."
437,ARC Centre of Excellence on Automated Decision-Making and Society,Q13 Changes to conformity infrastructure,We refer to a separate submission authored by a subset of ADM+S researchers (submission by Fraser et al) who have been considering conformity and assurance regimes in more detail and have discussed those regimes with colleagues in the EU.
437,ARC Centre of Excellence on Automated Decision-Making and Society,Q14 Risk-based approach to address AI risks,"ADM+S offers qualified support for a risk-based approach. In particular we argue: ● Any risk assessment must take into account key elements of the sociotechnical context: questions of diversity and inclusion, and the impact of supply chains (including the actors who collect and clean data). ● Requiring firms to undertake risk assessment and implement mitigations is an incomplete answer unless accompanied by shifts in the legal framework that ensure there are consequences when the process fails (as exist in other jurisdictions that are considering or have adopted a risk-based approach). We also explain in our submission that there is an alternative approach that already exists in some public law mechanisms, which is a structured balancing test of public interest considerations. Such a process - familiar in the public sector - could be adopted/adapted in relation to the use of ADM/AI in public sector contexts. Please see our uploaded submission for detail."
437,ARC Centre of Excellence on Automated Decision-Making and Society,Q15 Benefits and limitations of risk-based approach,"The main potential benefits of a risk-based approach are (a) the ability to avoid or mitigate harms before they happen, at the design and development stage rather than waiting for ex post litigation; (b) promoting better (safer, more responsible) design; as well as incorporating (c) ongoing obligations on developers of systems to engage in monitoring and addressing risks. A core question is who decides whether a system is low, medium, high (or very high?) risk? Risk is multi-dimensional (it varies by type of impact/harm, severity and probability, and can shift over time) meaning that fixed categories may not work well, but the party best placed to assess the risk of a system (who could be the developer, or the deployer) may have incentives to underestimate risk. There may be mechanisms to manage this, including for example by requiring publication of risk assessments for at least some systems, and/or setting ‘default’ categories with the ability of entities to show that their system is"
437,ARC Centre of Excellence on Automated Decision-Making and Society,Q17 Elements to include in risk-based approach,"ADM+S makes a number of comments on elements of the risk-based approach set out in the Discussion Paper at pages 39-40: • We suggest that three ‘categories’ of risk may be insufficient, and that descriptions of the different categories could give rise to some anomalous results (such as where a risk is brief and severe, or where it is brief and ‘reversible’ (for example, as a loss of social benefits is ‘reversible’ as payments can be restored) but has lasting impacts (say because a person has become homeless in the meantime when they could not pay their rent); • We suggest that more guidance and deeper thinking will be required regarding the ‘risks’ that must be considered; some ideas are offered and we draw attention to our discussion of environmental risks (Consultation Question 2) and issues around digital exclusion in Australia (Consultation Question 14); • In terms of the requirements listed at page 40, we draw attention to the absence of any reference to data quality consideration"
437,ARC Centre of Excellence on Automated Decision-Making and Society,Q18 Integration of risk-based approach into existing frameworks,"As our broader submission illustrates in some detail (see discussion of consultation question 14), consideration of the risks of AI is a much broader task than the consideration of privacy. See our discussion of consultation question 14 for more detail on this."
437,ARC Centre of Excellence on Automated Decision-Making and Society,Q19 Application of risk-based approach to general purpose AI,"Much of our earlier discussion is relevant to Foundation Models: see in particular our discussion under Consultation Question 2 (what is new/different about AI; what new capacities require a societal-level discussion); and Question 4 (non-regulatory actions required, in particular the need to connect government with cutting-edge research and ensure new research is incorporated into efforts to guide and educate to developers and deployers as well as the broader public). There are concerns about the applicability of risk-based approaches in relation to foundation models, which the EU is presently grappling with. The submission discusses these developments, and ADM+S can offer further expertise as required and as regulatory positions consolidate internationally. Finally, we note that foundation models raise genuine questions around the consolidation of power over the generation and transfer of knowledge. Steps may need to be taken to ensure research and pedagogical access for Australian r"
437,ARC Centre of Excellence on Automated Decision-Making and Society,Q20 Voluntary vs mandated compliance with risk-based approach,"We have suggested in our discussion of the public sector (consultation question 6) that the public sector could be a good place to start, for a number of reasons. Please see our uploaded submission and the discussion of consultation question 6."
438,Stephan Jacobs,Q01 Definitions,"An effective and risk-based approach to AI regulation focused on ethics and safety - and ultimately public trust - needs to grapple with a spectrum of AI capability. Such a wide range of systems could fit within the presented definitions that they are unhelpful for informing policy and risk decisions. By analogy, when the government regulates “vehicles” it uses definitions to distinguish push-bikes from passenger jets and then applies appropriate regulations to each kind. While including more recent developments like Multimodal Foundation Modals (MFM) is a helpful first step, the paper’s definitions need to capture a spectrum of AI capability to allow a meaningful conversation. The following 6 factors help define a spectrum of capability: 1. Size: how many parameters. 2. Specialised vs general capabilities. 3. Multimodality: the types of data it can process. 4. Temporal awareness: is the model able to reason about and plan for future events? 5. Active vs passive: is the model trained t"
438,Stephan Jacobs,Q02 Other AI risks and recommendations for regulation,"Missing risks include (1) misuse of highly capable AI systems, and (2) unintended harm from highly capable AI systems, especially systems with the capacity to plan and act as agents acting in ways that are not aligned with human directives. Although the likelihood of the risks may be uncertain, the harms resulting from these risks may be catastrophic and global in scope and must be acknowledged and addressed. The statement “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war” has recently been signed by the heads of leading AI labs (OpenAI, Google DeepMind, Anthropic, Stability AI) as well as many key academic and other researchers pivotal in developing this technology (Geoffrey Hinton, Yoshua Bengio, etc.) and other notable figures such as Bill Gates [1]. Similar calls date back to at least 2015 when Stephen Hawking led a group of AI experts to call attention to the risk and lay out an urgent rese"
438,Stephan Jacobs,Q03 Non-regulatory actions to address AI risks,"AI is very likely to transform many elements of the economy and society in the coming years. Ethics and safety are unlikely to be a priority unless leaders make them a specific focus. For example, social media algorithms came to shape our lives with little upfront focus on how they could be made ethical and safe. We are now living with the negative impacts on mental health and social cohesion and only have band-aid measures to address the widespread harms. AI is likely to be much more transformative and much more consequential. So leadership is required now.Importantly, the economic and technological forces are likely to be so strong that a scheme that is voluntary or driven by aspirational best practice is unlikely to be successful. That is, any non-regulatory approaches must not be taken to displace the need for strong regulation.Specific non-regulatory approaches should include: 1. Government leadership in recognising the risk. Australia must join the growing number of countries and"
438,Stephan Jacobs,Q05 International models for AI governance,"Other jurisdictions have begun creating national AI labs to test, understand and control safe AI, collaborating with the private sector and complementing its work. In the same way that national aviation safety expertise has formed an internationally collaborative network, national AI labs are likely to do the same thing. Australia needs to begin this process now. One of the most important and notable examples of Governmental organisation globally has been the launch of the EU’s Centre for Algorithmic Transparency, which has been formed to enforce the Digital Services Act. The new research centre aims to combat the “systemic risk” to society that stems from algorithms - including recommender systems and AI. To prepare for a future in which AI-based decision systems might become a core part of society, Australia should adopt a broad version of this concept to understand and shape the implications of algorithms - especially AI algorithms that use deep learning. Deep learning is important "
438,Stephan Jacobs,Q06 Public vs Private sector governance,Risk management is the most important factor when it comes to AI and that should form the basis of regulatory oversight in both the public and private sector. The risks may well be different in each sector but at this stage there is nothing to suggest that either sector stands to do significantly more harm than the other.
438,Stephan Jacobs,Q07 Responsible government use of AI,"There are two key actions the Australian government could undertake to improve its own responsible use of AI.First, Australians will worry about the use of AI by security and intelligence agencies. Australia should lead a 5-Eyes agreement about the appropriate use of AI by intelligence agencies, and create a public statement about what our “AI red lines” are. The statement would set out the things our security agencies might do and will never do with AI. The Inspector General of Intelligence and Security, and its counterparts across the five eyes, should then be specifically tasked with auditing the intelligence community for compliance with that public statement. This will give Australians confidence that we are not misusing AI or taking risky actions while protecting sensitive capabilities.Second, Australia needs to upskill the public service. This should include developing hubs of expertise in each Department so that the application of AI within the sphere of each Department’s respo"
438,Stephan Jacobs,Q09 Transparency requirements and scope,"The recent focus for transparency has been on ensuring that consumers are notified when a decision made by AI affects them directly (e.g. hiring/firing, loan approval). While this is helpful, it is only the first step in protecting the public.In circumstances where the use of AI for decision-making is commonplace, it will be of little value for consumers to know “when” an AI system makes a decision that affects them. This might happen many times each day. Rather, they should be informed “how” the decision is made. This will allow consumers to more easily identify unsafe or unfair systems.Having models which are both accurate and explainable has long been an issue for businesses [1], but the recent advancements of “black-box models” is encouraging developers and providers to simply tolerate the risks that come from lack of explainability. As AI capabilities continue to outstrip their explainability [2], the risks of this approach will grow significantly. The fact that explainability is "
438,Stephan Jacobs,Q10 Ban on high-risk AI applications,"The continued evolution of AI brings several societal challenges that, left unchecked, could risk the security, privacy and freedom of the Australian public. Many of the most extreme harms can be attributed to AI systems that have the ability to:1. Complete a diverse range of reasoning tasks with human-level performance;2. Navigate complex information environments and act within those environments autonomously;3. Form sophisticated plans and reason about the consequences of actionsAlthough AI systems with these capabilities may seem like science fiction, they are already being tested with great success in contained environments.For example, “Vogager” is a system that was recently developed by chaining together versions of GPT and connecting them to an external database, which the chatbots use to store and retrieve code that executes complex behaviours. Despite being a simple system, Voyager was hugely successful when tested in the open-world game Minecraft; this may appear trivial, but"
438,Stephan Jacobs,Q11 Actions to increase public trust in AI,"Public trust in AI has been severely eroded by events such as Geoffrey Hinton’s departure from Google and warnings of future danger [1]. This, among other issues , has created a strong sense that tech giants such as Google, Microsoft and Facebook are not prioritising the safety of AI systems [2].Increasing public trust in AI deployment can only be achieved by continually mitigating its risks, and this means building a strong track-record of protecting Australians from the negligent attitudes of these powerful private entities [3].This will not be an easy task, and it will require the establishment of a dedicated AI regulator which can stay up to date with recent advancements, navigate the complicated open-source ecosystem, and effectively police large technology companies.[1] Sydney Morning Herald (2023) When the “Godfather of AI” warns you about his offspring, you listen. https://www.smh.com.au/national/when-the-godfather-of-ai-warns-you-about-his-offspring-you-listen-20230504-p5d5ig."
438,Stephan Jacobs,Q12 Trade impact of banning high-risk AI applications,"This question misunderstands both the sources of risk and the interests of Australians.There are two potential sources of risk both of which need to be mitigated. The first source of risk, as identified in the question, is the use case. Some uses for AI are risky and should be restricted or banned. The second source of risk is the technology itself. Some kinds of technology are inherently risky, and merely restricting a use case does not adequately mitigate the risk.We understand this clearly in other fields. We regulate both how safe a car must be and how a car can be used. We don’t let people speed in cars just because they have a high safety rating; and we don’t let safe drivers buy dangerous vehicles.The economic framing also misunderstands Australia’s national interests. We would never conceive of a conversation about whether our aviation sector would trade more successfully if we authorised airlines to use planes that are untested, experimental or known to be dangerous. Australia"
438,Stephan Jacobs,Q14 Risk-based approach to address AI risks,"Yes. A risk based approach has worked well as a proactive means of identifying, assessing and mitigating potential safety hazards in the aviation industry (along with a wide variety of other industries like chemical processing etc.).The industry has developed specific tools to assist in identifying potential risks - Safety Management Systems (SMS) in aviation and Hazard and Operability Studies (HAZOP) in chemical processing are just 2 examples. Critical to this is the ability for all parties involved in the development and use of AI systems to be able to report the identification of any risks. In aviation and chemical processing alike, risk assessment is part of the industry’s culture, and developing a culture of safety and risk management in AI is essential to allow fine-grained decisions about the use of AI to be made with a conscientious awareness of the harm it can cause.Following identification, risks are assessed based on severity and likelihood. This work is led or assisted by t"
438,Stephan Jacobs,Q15 Benefits and limitations of risk-based approach,"While a risk-based approach to regulating AI is sensible, there are two emerging elements of risk practice that any risk-based model should account for to succeed in this context:1. Factoring in “uncertainty”, not just “likelihood”2. Taking a “portfolio” approach to risk mitigationA traditional risk matrix will consider scenarios against their likelihood of occurring, the vulnerability of the system to the scenario, and the consequence of the scenario occurring. It might then discount that risk by the strength of existing or possible mitigations.This approach to risk has historically been unsuccessful for scenarios where assessing “likelihood” is challenging. For instance, many national risk assessments failed to take into account the possibility of a global pandemic before COVID-19. A solution to this failure mode is including “uncertainty”. Specifically, where we can't be sure what the future will bring, that should be cause for significant concern. This is the opposite of the approa"
438,Stephan Jacobs,Q16 Scope and application of risk-based approach,"While present risks might be linked to specific use cases and sectors, future risks will increasingly be linked to the underlying capabilities of the technology itself.As such, access to increasingly advanced AI systems should be restricted to organisations that can demonstrate their usage will provide significant public benefit aligned with Australia's national interests. This targeted approach helps mitigate risks in two key ways:1. It limits opportunities for misuse or accidents from advanced AI by reducing proliferation across society. Usage is concentrated into high-value use cases that justify the risks.2. It establishes clear accountability. Organisations granted access must have the governance and technical expertise to use advanced AI safely. They can be audited to ensure responsible usage that mitigates dangers.This permits-based access can be operationalised by requiring organisations to submit evidence their proposed use cases will deliver transformative value to Australia "
438,Stephan Jacobs,Q17 Elements to include in risk-based approach,"Human-in-the-loop monitoring will be vital, but insufficient on its own to regulate advanced AI systems that will eventually surpass human capabilities. The speed at which advanced AI systems will operate means that human oversight will become pragmatically infeasible if our monitoring programs aren’t able to scale proportionally.As such, we must employ an approach that is predicated on limiting access to advanced AI systems unless there is a strong rationale for using them; conducting cost-benefit analyses prior to approving the deployment of advanced systems will mitigate risks while preserving potential benefits.A key part of enforcing this targeted approach will be monitoring and controlling access to high-performance computing resources, which are required for developing and operating advanced AI systems and their precursors.This additional layer of governance allows authorities to detect potentially dangerous AI development or usage activity, and can be cross-checked with other d"
438,Stephan Jacobs,Q18 Integration of risk-based approach into existing frameworks,"Some areas of regulation, like the Racial, Sex, Age, and Disability Discrimination Acts are appropriately strong and may provide useful safeguards against some harms of AI. Specifically, the law should ensure that such discrimination is always wrong and always punishable, and an excuse that the discrimination occurred because of a “black box” AI provided by an overseas AI Lab should never be accepted. It would be helpful to communicate these requirements to Australian businesses that might deploy AIs and ensure they only contract with AI Labs for products where the Lab can ensure the product cannot engage in these harmful behaviours.Certainly, no AI-specific regulation should ever waive existing safeguards.That said, many of the emerging risks from AI are quite new and it’s reasonable to expect that regulation made without AI in mind is not tuned to the new risks that AI represents. Specific AI safety regulation is unavoidable."
438,Stephan Jacobs,Q19 Application of risk-based approach to general purpose AI,"The capabilities unlocked by general-purpose AI systems, like large language models (LLMs) and multimodal foundation models (MFMs), fundamentally change the risk landscape. We should anticipate these systems having large, unpredictable effects across all parts of society.A prudent approach is to implement a risk-based framework, where access is graduated based on model capabilities. Models above a certain threshold of autonomous capabilities would be designated as ""advanced systems"" and subject to additional governance, such as permits for users.Further, permits should be required to gain certain levels of access to AI systems which can be used to build advanced systems.Permits allow us to maintain oversight, while still enabling beneficial uses. Different permissions can be granted based on use case - for example, full access for safety testing versus API-only access for most commercial uses. This balances innovation and responsibility.Although this may seem very different to how AI i"
438,Stephan Jacobs,Q20 Voluntary vs mandated compliance with risk-based approach,"A risk-based approach must be mandatory and must apply to all settings. Why mandatory? During the debate over climate policy in the last decade the private sector stated, on multiple occasions, that clear and binding government regulation would provide businesses with certainty in how they could navigate the move to a world with lower emissions [1]. Without it businesses had very little incentive to take on the costs and challenges of reducing emissions when that might be bad for their business, and their competitors might continue to reap the benefits of high-emissions practices. The same principle applies for AI: There is already an arms race between tech companies to develop more capable AI systems [2] and no company will take it upon itself to constrain the use of AI at the cost of its competitiveness [3]. The last few decades have provided little if any evidence that the technology sector is capable of regulating itself and AI is no exception. The CEO of OpenAI himself has request"
439,Gilbert + Tobin,Transcription,"FILENAME: AI Discussion Paper Response from Gilbert + Tobin.32b26e30b2958.pdf | CONTENT: Contact Simon Burns T +61 2 9263 4776 sburns@gtlaw.com.auJen Bradley T +61 2 9263 4288 L 35, Tower Two, International Towers Sydney jbradley@gtlaw.com.au 200 Barangaroo Avenue, Barangaroo NSW 2000 AUS T +61 2 9263 4000 F +61 2 9263 4111 www.gtlaw.com.au4 August 2023Technology Strategy Branch Department of Industry, Science and ResourcesSubmission to the Department of Industry, Science and Resources: Safe and Responsible AI in Australia (Discussion Paper)1 Executive summaryThank you for the opportunity to provide our views on the Safe and Responsible AI Discussion Paper published 1 June 2023 (the Discussion Paper).We have set out in this document our responses to some of the questions posed by the Discussion Paper.As an overview, our key comments are as follows:- Particularly as we move toward decarbonisation, it is critically important for Australia to strike the right balance between managing risk"
440,Mitch Goodwin,Transcription,"FILENAME: Responsible AI - 00 Cover Letter (Goodwin).6ad3de530284b.pdf | CONTENT: 3 August 2023Hon Ed Husic MP Department of Industry, Science and Resources Industry House, 10 Binara Street, CanberraDear Minister Husic,RE: Safe and Responsible AI public consultation submissionMy enclosed submission pertains to the emergence of generative AI systems their potential influence on the Higher Education sector. I would like to draw particular attention to the delivery of equitable and inclusive teaching and learning practices in the Humanities, Arts and Social Sciences (HASS) disciplines in the shadow of emergent forms of AI. As an academic and media artist who has been researching digital media and cultures of automation for over two decades, I view recent developments in artificial intelligence with great anticipation for its unquestionable benefits. However, I also hold grave concerns for students and educators in this period of rapid unregulated experimentation and expeditated deployment"
440,Mitch Goodwin,Transcription,"FILENAME: Responsible AI - 01 Key Challenges (Goodwin).3a8adcbddf5c.pdf | CONTENT: Responsible AI Attachment 01 - Key ChallengesContact: Mitch Goodwin Arts Teaching Innovation University of Melbourne mitch (dot) goodwin (at) unimelb.edu.auSummary of the Key Challenges AheadIf I could speculate on the future, I would like to briefly highlight the following areas of concern in which I see both near-term challenges and longer-term developments with profound effects for society and the economy, with a particular focus on Higher Education:Synthetic Information - As more generative AI and synthetic media are published and shared online the inevitable outcome is that the web corpus becomes poisoned by false or misleading information. Compromised research practices also run the risk of incorrect referencing practices or “hallucinated” data embellishing results and thereby creating a shadow economy of ghost research. This has impacts on the teaching of research methodologies to students as well"
440,Mitch Goodwin,Transcription,"FILENAME: Responsible AI - 03 Discussion Paper (Goodwin).7b570b07a9c3a.pdf | CONTENT: Responsible AI Attachment 02 – Discussion PaperContact: Mitch Goodwin Arts Teaching Innovation University of Melbourne mitch (dot) goodwin (at) unimelb.edu.auYou, me and ChatGPT How the humanities, arts and social sciences can flourish in the AI information crushA Discussion Paper (Draft 1.4) For academics and teaching practitionersEst. 26.01.2023 Updated 03.02.2023 Updated 06.04.2023 Updated 04.08.2023 Table of ContentsPART 01 :: EXISTING THREATS AND EMERGING CHALLENGES....................................................................... 5 Introduction ............................................................................................................................... 5 We already live in an algorithmic culture .................................................................................... 5 The HASS model..............................................................................."
440,Mitch Goodwin,Q01 Definitions,Yes - a discussion worth having would be about tools vs services vs systems and how e understand these in commercial and private settings
440,Mitch Goodwin,Q02 Other AI risks and recommendations for regulation,"• reducing data and algorithmic bias • interpreting and safeguarding cultural data and institutional knowledge • investigating historical inequality (of access) and under-representation (of cultures) in technological platforms and training databases • advising on sustainable data management and processing hardware infrastructures • reducing the exploitation of personal data, identity, and behavioural metrics • compensating individuals for their data when it is used, shared, or sold"
440,Mitch Goodwin,Q03 Non-regulatory actions to address AI risks,A Department of AI at every level of government might be an idea ... I would also like to see an AI commission similar to the Indigenous Voice to parliament that advises on the impact - both benefits and and threats - posed by AI and automation.
440,Mitch Goodwin,Q04 Coordinating AI governance across government,"As my submission outlines - Humanities, Arts and Social Sciences (HASS) are well-placed to coordinate their respective disciplines and methodologies to strategize the human response to the challenges that lie ahead in terms of emergent forms of AI, synthetic media, information manipulation and their associated socio-economic and socio-political influences. While a HASS lens is important, equal weight should be given to ethical and risk-mitigation factors prescribed by research colleagues in IT, Design, Medicine, Law, Business and the Creative Arts disciplines. I would suggest that HASS can be a starting point to coordinate and synthesise knowledge and material practices around inter-disciplinary consultation and complimentary policy matrices."
440,Mitch Goodwin,Q05 International models for AI governance,"The recent regulatory framework by the EU is an obvious starting point ... however, I see Australia being in a position to potentially lead a debate around some of the core issues of alignment and data privacy and exploitation of creative IP. See submission for more detail."
440,Mitch Goodwin,Q06 Public vs Private sector governance,"Yes, most definitely."
440,Mitch Goodwin,Q07 Responsible government use of AI,"Given the ubiquitous multi-sector impact of AI, I think you needed a dedicated department and possibly staff placed in other departments to interface with one another."
440,Mitch Goodwin,Q08 Generic vs technology-specific approaches for AI risk,"As the submission outlines, much of this centres around big data - without data, AI is not very useful. Policies around data and privacy and exploitation of personal information are long overdue. These could feed into our thinking about AI."
440,Mitch Goodwin,Q09 Transparency requirements and scope,a. What is the algorithm trained on? What is the algorithm doing that I cannot see (or understand)? How will my actions with AI technologies affect me and my family? b. it would seem to me that regulation is the only way to make some of these technology designs more transparent.
440,Mitch Goodwin,Q10 Ban on high-risk AI applications,a. facial recognition with public and government databases / storage of personal data in the service of surveillance
440,Mitch Goodwin,Q11 Actions to increase public trust in AI,It is too early to say - I think we have to look at this as we now do in terms of cyber safety and cyber bullying with primary school students. A slow and careful introduction of the dangers and the opportunities. I also think we have an opportunity to leverage our undergraduate university population (and their academic teachers) to interrogate and validate and strategise the future of AI in society and the economy. The size of the change ahead is too large to leave to a few research clusters and government departments.
440,Mitch Goodwin,Q12 Trade impact of banning high-risk AI applications,NA
440,Mitch Goodwin,Q13 Changes to conformity infrastructure,NA
440,Mitch Goodwin,Q14 Risk-based approach to address AI risks,Yes
440,Mitch Goodwin,Q16 Scope and application of risk-based approach,"Risk mitigation should be applied to R&D voluntarily, but market ready applications should be vetted."
440,Mitch Goodwin,Q17 Elements to include in risk-based approach,Yes
440,Mitch Goodwin,Q18 Integration of risk-based approach into existing frameworks,Their needs to be an alignment between the goals of these two areas
440,Mitch Goodwin,Q19 Application of risk-based approach to general purpose AI,"Respectful, inclusive and non-biased approaches to training data."
440,Mitch Goodwin,Q20 Voluntary vs mandated compliance with risk-based approach,Regulatory - both
441,Guy Loucks,Transcription,"FILENAME: Guy Loucks Safe and Responsible AI for Australia.4cd827e7a17b4.pdf | CONTENT: GUY LOUCKS 3 Pildra Place, Frenchs Forest, NSW, 2086 | 0429041186Meaghan Quinn 4 August 2023 PSM Industry House, 10 Binara Street, CanberraDear Meghan Quinn PSM, Thank you for the opportunity to respond to your department’s discussion paper. The following items focus primarily on the Ethics and Principles framework as these feed into the foundation for a sound co-regulatory framework and governing principles.• Human, societal and environmental wellbeing: AI systems should benefit individuals, society and the environment.On the understanding that there are sufficient acts/regulations around this – my concern of ‘which individuals’ (all animals are equal only some are more equal than others) should be addressed.• Human-centred values: AI systems should respect human rights, diversity, and the autonomy of individuals.On the understanding that this is for Australia only – and therefore within the concep"
441,Guy Loucks,General comment,"Thank you for your consideration, I look forward to offering further details with regards to any questions raised by this submission. Best wishes, Guy"
441,Guy Loucks,Q01 Definitions,"The definitions for 'Narrow AI' / LLM/MFM/ADM reasonable, future consideration for General AI (Artificial General / Super-intelligence) and research foundations should be considered."
441,Guy Loucks,Q02 Other AI risks and recommendations for regulation,"Deep Fakes, State based actors, and deficiency in AI identifying itself or where it is used"
441,Guy Loucks,Q03 Non-regulatory actions to address AI risks,"Social education, and privacy training"
441,Guy Loucks,Q04 Coordinating AI governance across government,"Funding and burden of co-regulation should consider structure similar to that of the TIO, for the multinational and multi-billion industry parties."
441,Guy Loucks,Q05 International models for AI governance,NIST framework and Canada's bill C27.
441,Guy Loucks,Q06 Public vs Private sector governance,no
441,Guy Loucks,Q07 Responsible government use of AI,"following the points in the attached submission, robo-debts failure was the lack of openness and transparency"
441,Guy Loucks,Q09 Transparency requirements and scope,AI transparency and declaration should ALWAYS be mandatory. Explainability should always be made available upon request in lay speak - common english.
441,Guy Loucks,Q10 Ban on high-risk AI applications,"no, banning should never be considered, adequate controls should be primary choice and objective."
441,Guy Loucks,Q11 Actions to increase public trust in AI,"Transparency, Training, Timely Responses, Teeth (punitive, e.g. 1% of multinational annual earnings)"
441,Guy Loucks,Q12 Trade impact of banning high-risk AI applications,"see previous, do not ban, regulate and educate"
441,Guy Loucks,Q13 Changes to conformity infrastructure,"Security, Training, and EDUCATION. 'We record your voice to secure your biometric identity' - to risk my identity when the agency is next hacked."
441,Guy Loucks,Q14 Risk-based approach to address AI risks,yes
441,Guy Loucks,Q15 Benefits and limitations of risk-based approach,constantly monitored and guided by foundation principles with access to a human being as arbitrator.
441,Guy Loucks,Q16 Scope and application of risk-based approach,no
441,Guy Loucks,Q17 Elements to include in risk-based approach,yes - however there should ALWAYS be humans in the loop for oversight.
441,Guy Loucks,Q19 Application of risk-based approach to general purpose AI,"AI must always identify itself, and deep fakes (fraud) be absolutely illegal"
441,Guy Loucks,Q20 Voluntary vs mandated compliance with risk-based approach,mandated on all parties
442,Rakesh Sharma (CYAINSE),Q01 Definitions,"Yes, I agree with the definitions in this discussion paper. Definition of LLMs can be refined as follows. ""A generative AI that specializes in generating human-like text which is coherent and contextually relevant based on the input received."
442,Rakesh Sharma (CYAINSE),Q02 Other AI risks and recommendations for regulation,"Intellectual Property is one risk which should also be covered by Australia's existing regulatory approaches especially related to AI. AI-generated content, such as text, images, and music, can lead to questions regarding intellectual property rights and ownership. Training data collected by organizations from public domains for modeling of AI systems may involve proprietary or copyright protected information. Existing regulations may require updates to ensure proper protection and attribution for AI-generated works."
442,Rakesh Sharma (CYAINSE),Q03 Non-regulatory actions to address AI risks,"In addition to the initiatives mentioned in the discussion paper, following non-regulatory initiatives can support responsible AI practices in Australia. 1. AI Research and Development Funding: Provide funding and grants to develop local startup ecosystem around AI. 2. AI Education and Training Programs: Government funding for courses and training programs in AI discipline, eg. TAFE, vocational and university courses. 3. AI Certification Programs: Introduce voluntary or industry-recognized AI certification programs to incentivize organizations to adopt responsible AI practices. 4. Data Sharing and Collaboration Initiatives: Encouraging data sharing and collaboration between government agencies, businesses, and research institutions can promote responsible AI practices 5. National AI strategy: Formulation of a National AI Strategy in consultation with industry, government, academia and public, which covers risks and governance. Appoint a National AI officer or co-ordinator. 6. Global In"
442,Rakesh Sharma (CYAINSE),Q06 Public vs Private sector governance,"The public sector should prioritize transparency, accountability, and fairness in AI use to uphold public trust, while the private sector may focus on efficiency, competitiveness, and customer satisfaction while respecting privacy and data protection. Both sectors should adhere to ethical principles, but tailored approaches can address their unique challenges and priorities to create a more inclusive and responsible AI ecosystem. Collaborative efforts between the public and private sectors can lead to better governance and knowledge-sharing, benefiting society as a whole."
442,Rakesh Sharma (CYAINSE),Q07 Responsible government use of AI,"Government should develop clear AI governance policies, provide training programs, and foster collaboration among agencies. It should establish AI ethics committees, conduct impact assessments, and implement procurement standards aligned with responsible AI principles. Embracing transparency, open data, and ethical AI innovations and collaborating with industry and academia can further enhance responsible AI use within government agencies."
442,Rakesh Sharma (CYAINSE),Q09 Transparency requirements and scope,"a. Transparency is crucial across various stages of the AI lifecycle to mitigate risks and enhance public trust in AI. It is most critical during data collection and use, model development, decision-making, algorithmic accountability, AI ethics and governance, bias mitigation, auditing, and continuous monitoring. Being transparent at these stages fosters responsible AI practices, builds public confidence, and empowers users and stakeholders to understand and hold AI systems accountable. Ultimately, transparency contributes to a more ethical and trusted AI ecosystem.b. The importance of transparency across the AI lifecycle is crucial for responsible AI practices and building public trust. To implement transparency requirements, mandating clear data collection and use policies, explainable AI, disclosure of biases, algorithmic transparency, reporting, impact assessments, governance boards, public accessibility, collaboration, and certification can be instrumental. By enforcing these requ"
442,Rakesh Sharma (CYAINSE),Q11 Actions to increase public trust in AI,"Following government initiatives can help increase public trust in AI adoption and deployment.1. Public Awareness Campaigns: Launch public awareness campaigns to educate citizens about the benefits and risks of AI. These campaigns can address misconceptions, highlight AI's positive impact, and explain how responsible AI practices are being prioritized.2. AI Regulatory Sandboxes: Establish regulatory sandboxes that enable controlled testing, auditing and experimentation of AI technologies. This fosters innovation while ensuring compliance with ethical and legal guidelines.3. Engaging in Global AI Governance: Participate in international discussions on AI governance and collaborate with other countries to establish common standards and best practices for responsible AI deployment."
442,Rakesh Sharma (CYAINSE),Q14 Risk-based approach to address AI risks,"A risk-based approach is widely recognized as an effective strategy for addressing potential AI risks. It involves identifying and prioritizing risks based on their likelihood and impact, allowing organizations to focus on high-impact risks and tailor mitigation strategies accordingly."
442,Rakesh Sharma (CYAINSE),Q15 Benefits and limitations of risk-based approach,"While a risk-based approach is valuable, it's important to note that other complementary approaches should also be considered. For instance, ethical and human-centric design principles, transparency, explainability, and fairness considerations should be integrated alongside risk assessments to ensure responsible AI practices."
442,Rakesh Sharma (CYAINSE),Q18 Integration of risk-based approach into existing frameworks,"AI risk-based approach can be incorporated into existing assessment frameworks and risk management processes. This involves identifying AI-related risks and integrating them into current frameworks, prioritizing risks based on impact, harmonizing risk categories, and fostering cross-functional collaboration. Automation, unified reporting, continuous monitoring, and compliance alignment further enhance the effectiveness of the approach, enabling organizations to comprehensively address AI risks and promote responsible AI deployment."
442,Rakesh Sharma (CYAINSE),Q19 Application of risk-based approach to general purpose AI,"Applying a risk-based approach to general-purpose AI systems like large language models (LLMs) or multimodal foundation models (MFMs) involves identifying and mitigating potential risks specific to these models, such as bias, privacy concerns, and security vulnerabilities. This includes establishing responsible use guidelines, conducting third-party audits, and engaging the public in discussions"
442,Rakesh Sharma (CYAINSE),Q20 Voluntary vs mandated compliance with risk-based approach,"A risk-based approach for responsible AI can be implemented through a combination of voluntary adoption and regulation. While voluntary adoption encourages flexibility and self-regulation, regulation can set minimum standards and ensure accountability, especially for high-risk AI systems and public sector applications. The approach can be tailored to different sectors, with mandatory requirements for critical areas and voluntary adoption in others."
443,Michael Townsend,Q01 Definitions,"The definitions in Figure 1 are not sufficiently granular to support risk-based consideration of different kinds of AI - including the more sophisticated and powerful AIs.In their current state, these definitions do not support a nuanced policy conversation that allows differentiation between AI models that provide economic benefit, models that raise significant ethical concerns, and models that raise radical safety concerns.A sophisticated discussion of AI requires an understanding of additional terms like how agentic or corrigible a given AI is. Adopting ISO/IEC 22989 [1] definitions is the wrong approach to framing a conversation about policy, law and governance because it doesn’t open the door to the big questions we need to grapple with.The foreword itself to ISO/IEC 22989 calls out a weakness of the definition set for purposes like those being pursued in this paper. Specifically, the standard says it is currently lacking language for comparing AIs in terms of properties like trus"
443,Michael Townsend,Q02 Other AI risks and recommendations for regulation,"Missing risks include (1) misuse of highly capable AI systems, and (2) unintended harm from highly capable AI systems, especially systems with the capacity to plan and act as agents acting in ways that are not aligned with human directives. Although the likelihood of the risks may be uncertain, the harms resulting from these risks may be catastrophic and global in scope and must be acknowledged and addressed.The statement “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war” has recently been signed by the heads of leading AI labs (OpenAI, Google DeepMind, Anthropic, Stability AI) as well as many key academic and other researchers pivotal in developing this technology (Geoffrey Hinton, Yoshua Bengio, etc.) and other notable figures such as Bill Gates [1]. Similar calls date back to at least 2015 when Stephen Hawking led a group of AI experts to call attention to the risk and lay out an urgent resea"
443,Michael Townsend,Q03 Non-regulatory actions to address AI risks,"The key non-regulatory function Australia could perform is offering international leadership focused on ensuring AI systems can be understood. Australia should be an active voice for the importance of ethics and safety in the current global conversation around AI governance and AI standards making. Specifically, Australia should advocate for:The development of standards and tools that analyse and monitor AI systems for robustness, bias, dual-use capabilities and other safety-relevant features [1].Audit trail requirements for safety-critical applications of AI systems, including allowing the traceability of decisions through product life cycles [2].We will only be able to have confidence in more sophisticated AI systems if they can be understood, we can ensure they do not pose unacceptable risks and hold accountable those at fault where it does. Doing the non-regulatory work to ensure AIs have “traceability” is necessary to underpin the future of safe AI. Joshua Kroll’s work on accounta"
443,Michael Townsend,Q05 International models for AI governance,"Australia should take note of the EU recent handling of the AI Act. For example, the European Parliament’s recognition of general-purpose AI (GPAI) is an important step in the right direction [1]. In particular, the following observations are highly valuable:- Acknowledgement that GPAI providers must comply with strict safety measures, regardless of the way it is distributed or intended to be used.- Requirements for GPAI providers to comply with various safety requirements throughout the lifecycle of their product; i.e. external audits assessing their performance, predictability, interpretability, corrigibility, safety and cybersecurity.These adjustments appear to respond to significant criticisms directed at initial drafts of the AI Act. For example, the AI Now Institute released a set of recommendations that heavily criticised the EU’s approach on some matters [2]. For example:- “GPAI models carry inherent risks and have caused demonstrated and wide-ranging harms. While these risks c"
443,Michael Townsend,Q07 Responsible government use of AI,"There are two key actions the Australian government could undertake to improve its own responsible use of AI.First, Australians will worry about the use of AI by security and intelligence agencies. Australia should lead a 5-Eyes agreement about the appropriate use of AI by intelligence agencies, and create a public statement about what our “AI red lines” are. The statement would set out the things our security agencies might do and will never do with AI. The Inspector General of Intelligence and Security, and its counterparts across the five eyes, should then be specifically tasked with auditing the intelligence community for compliance with that public statement. This will give Australians confidence that we are not misusing AI or taking risky actions while protecting sensitive capabilities.Second, Australia needs to upskill the public service. This should include developing hubs of expertise in each Department so that the application of AI within the sphere of each Department’s respo"
443,Michael Townsend,Q08 Generic vs technology-specific approaches for AI risk,"Generic measures such as transparency requirements and legal liability frameworks will only be adequate for mitigating risks in simple decision systems which perform single-step information processing and pattern matching, or routine tasks in limited settings [1]; examples of simple systems may include automated robotic systems in manufacturing, or CV screening systems in HR.By contrast, there will need to be specific measures in place to ensure that sophisticated automated decision systems are safe and reliable. “Advanced” systems refers to AI systems which:1. Are able to complete a diverse range of reasoning tasks with human-level performance; and2. Are able to navigate complex information environments and act within those environments autonomously; and3. Are able to form sophisticated plans and reason about the consequences of actionsDue to their autonomous and open-ended capabilities, advanced systems carry complex risks which are more similar to those associated with humans; for e"
443,Michael Townsend,Q09 Transparency requirements and scope,"The aviation industry offers a prime example of how transparency can be used to reduce risk and increase safety in a potentially dangerous industry. Transparency is embedded in every stage of the value chain, from manufacturing to operations, and extends to the reporting and investigation of risk-related incidents.Given the magnitude of risk, particularly from emerging more sophisticated AI systems, aviation safety provides a good analogy.Manufacturing StageTransparency begins at the manufacturing stage. Aircraft manufacturers, such as Boeing and Airbus, are required to adhere to stringent safety standards set by regulatory bodies like the Federal Aviation Administration (FAA) in the U.S. and the European Union Aviation Safety Agency (EASA) in Europe. These standards cover everything from the design and production of aircraft to their maintenance and operation.Manufacturers must provide detailed documentation of their processes, including design specifications, testing procedures, and "
443,Michael Townsend,Q10 Ban on high-risk AI applications,"In February 2023, OpenAI CEO Sam Altman released a statement containing the following: “As our systems get closer to AGI, we are becoming increasingly cautious with the creation and deployment of our models. Our decisions will require much more caution than society usually applies to new technologies, and more caution than many users would like.” [1] It is very reassuring to see this acknowledgement expressed by Altman, and the Australian Government would be wise to heed such warnings. Unfortunately, it may not matter how much caution OpenAI shows, because their actions have triggered the arms race that AI safety experts have been concerned about for years [2, 3].In order to protect our nation, the Australian government should a provide public, reasoned decision on potentially banning the training of large-scale AI systems with a nontrivial expected chance of passing capabilities evaluations required to cause catastrophic risks to civilization (such as the Alignment Research Center’s t"
443,Michael Townsend,Q11 Actions to increase public trust in AI,"There are tremendous concerns about AI among those with the greatest familiarity of its risks [1]; these concerns are being widely broadcasted in Australian media with alarming headlines such as:- SMH: “When the “Godfather of AI” warns you about his offspring, you listen.” [2]- Forbes Australia: “‘We knew AI could destroy the whole world and we wanted to figure out how to prevent that’: ChatGPT Founder;” [3]- Canberra Times: “UN chief backs idea of global AI watchdog” [4]These concerning signals are difficult for the public to ignore.It goes without saying that this is quite a different situation from one where concerns are being raised in isolation or by fringe-figures; this makes it inappropriate to treat “public education” as a significant part of the solution, as is referenced in the discussion paper.The process for building trust in AI should be thought of as similar to fostering public trust in bridges or aeroplanes. Due to their risks, advanced AI systems should only be deployed"
443,Michael Townsend,Q12 Trade impact of banning high-risk AI applications,"This question misunderstands both the sources of risk and the interests of Australians.There are two potential sources of risk both of which need to be mitigated. The first source of risk, as identified in the question, is the use case. Some uses for AI are risky and should be restricted or banned. The second source of risk is the technology itself. Some kinds of technology are inherently risky, and merely restricting a use case does not adequately mitigate the risk.We understand this clearly in other fields. We regulate both how safe a car must be and how a car can be used. We don’t let people speed in cars just because they have a high safety rating; and we don’t let safe drivers buy dangerous vehicles.The economic framing also misunderstands Australia’s national interests. We would never conceive of a conversation about whether our aviation sector would trade more successfully if we authorised airlines to use planes that are untested, experimental or known to be dangerous. Australia"
443,Michael Townsend,Q13 Changes to conformity infrastructure,"World-renowned AI experts such as Yoshua Bengio are warning that there’s a chance “superhuman” AI might be coming as early as 2030 [1]. Although there stand to be significant benefits from deploying advanced AI systems, their deployment will need to be highly targeted and controlled.While the productivity gains from advanced AI systems could be immense, uncontrolled proliferation would pose threats from likely scenarios such as malicious use or rogue AI systems [2]. As such, any deployment of advanced AI systems in Australia should occur via secure data centres, with access provided through a centralised API that facilitates monitoring and accountability by providing metadata about usage.Stringent compliance requirements must also be implemented for any organisation granted access to advanced AI systems. AI Governance researchers recommend that regulators should develop audit trail requirements in safety-critical applications of AI, similar to flight data recorders that are standard in"
443,Michael Townsend,Q14 Risk-based approach to address AI risks,"The regulation of AI needs to be “risk-based” in the same way that the regulation of vehicles is risk-based. We are right to regulate push-bikes and passenger jets in very different ways.A limitation of the risk management approach in the discussion paper, specifically Box 4 and Attachment C, is that it focuses only on how an AI is used and disregards the nature of the AI itself.An effective regulatory regime for a high-risk activity will regulate both the technology itself and how it is used. The Australian Human Rights Commission [1], in discussing this issue (“Human Rights and Technology”, 2021), says:Governments tend to regulate high-risk activities and technologies more closely. This helps explain the comparatively strict laws that govern fields such as gene technology, aviation, healthcare and the energy industry. In these areas, regulation often applies both to the technology itself and how it is used. From a human rights perspective, the need for more prescriptive regulation wi"
443,Michael Townsend,Q15 Benefits and limitations of risk-based approach,"The key to success is understanding risks sufficiently broadly to encourage caution in an uncertain future. It is not enough to only react once the risk has been actualised - for some risks that’s too late.A potential “worse-case scenario” is if a culture of “trial and error” is adopted, whereby adverse events occur before risks are considered tangible enough to act upon. This reliance on post-hoc interventions risks normalising unsafe development practices, making it difficult to recover once risks proliferate.Proliferation of risks is a key point in governing AI, because regulating software poses significant challenges compared to other dual-use commodities such as pharmaceuticals or firearms. This means that, from the outset, there must be proactive regulatory oversight for the deployment of advanced AI systems and their precursors.For example, significant structural changes, such as the use of advanced AI systems in Australia, require that risks must be comprehensively understood p"
443,Michael Townsend,Q16 Scope and application of risk-based approach,"The most important organisations to focus on when developing risk-management strategies and regulations, will be those that determine how advanced AI is developed and deployed. More specifically, there are three prominent categories of concern:1. Organisations that develop advanced AI systems, or AI systems that can be used to build more advanced AI systems (i.e. precursor systems such as LLMs). Examples include OpenAI, DeepMind, Anthropic, Inflection AI, Nvidia, Microsoft, Meta2. Organisations which provide access to open-source AI systems via downloads or APIs. E.g. Hugging Face, Github, AWS3. Organisations which provide compute resources to develop, fine-tune or use AI systems. E.g. AWS, HuggingFace, Google, Azure, Nvidia, Lambda LabsA majority of the benefits and risks will come from AI that is more sophisticated and generally capable, which can be tied back to organisations in these three categories.There are approaches which can be taken for each category in order to manage risks"
443,Michael Townsend,Q17 Elements to include in risk-based approach,"In general, Attachment C falls short of the necessary detail to understand the range of risks that have and will emerge as well as the range of specificity of mitigations necessary to control them.Attachment C would benefit from:- Distinguishing between scenarios where the use case is the potential risk (such as social scoring); scenarios where dual-use capability and misuse is the potential risk (bioengineering, producing misinformation and disinformation, enhancing criminal capability and capacity); and where the technology alone is the risk (technology with features like being deceptive, being power-seeking, being incorrigible etc).- Distinguishing between the point of regulatory intervention - the developer, the deployer, the user, or elsewhere, and- Offering a much wider range of mitigations that target different kinds of risks.Some elements of attachment C are promising, like “explanations” and “ongoing monitoring” - but these need to be greatly expanded and based on current AI s"
443,Michael Townsend,Q18 Integration of risk-based approach into existing frameworks,"Some areas of regulation, like the Racial, Sex, Age, and Disability Discrimination Acts are appropriately strong and may provide useful safeguards against some harms of AI. Specifically, the law should ensure that such discrimination is always wrong and always punishable, and an excuse that the discrimination occurred because of a “black box” AI provided by an overseas AI Lab should never be accepted. It would be helpful to communicate these requirements to Australian businesses that might deploy AIs and ensure they only contract with AI Labs for products where the Lab can ensure the product cannot engage in these harmful behaviours.Certainly, no AI-specific regulation should ever waive existing safeguards.That said, many of the emerging risks from AI are quite new and it’s reasonable to expect that regulation made without AI in mind is not tuned to the new risks that AI represents. Specific AI safety regulation is unavoidable."
443,Michael Townsend,Q19 Application of risk-based approach to general purpose AI,"The risk-based approach must be modified to address risks from general purpose AI systems, especially LLMs and MFMs. These types of systems learn their capabilities and behaviours during training, which can be unforeseen by developers. That means that AI developers could create models with dangerous capabilities (e.g., deception, manipulation, political strategy, weapons design). Humans could misuse those capabilities to cause harm [1], and models could apply the capabilities even without deliberate misuse (e.g., through misalignment) [2].Consistent with recent work from a consortium of AI safety researchers, including at Google Deepmind, OpenAI, Anthropic, Alignment Research Center, University of Cambridge, University of Oxford, Centre for Long-Term Resilience, and Centre for the Governance of AI [1], we recommend that any risk-based approach require that general purpose models are evaluated for dangerous capabilities and misalignment. This will help determine:1. To what extent a mode"
443,Michael Townsend,Q20 Voluntary vs mandated compliance with risk-based approach,"In general, regulation should:1. Be proportionate to risk. Meaning that a voluntary or self-regulation approach should only apply to low-risk or no-risk uses of AI.2. In the case of higher-risk technologies, regulate both the technology itself and the use of the technology. Meaning that, for future and more sophisticated AIs, regulation should ensure that the technology is safe before it is created or published, as well as ensuring that it is only used in ways that are safe.3. Place burdens on those most able to reduce potential harms. Meaning that AI that functionally operates as ‘black box’ to consumers has to be regulated at the point of the AI Lab.Applying those three principles, self-regulation may be appropriate for some technologies available today and for some participants (such as students or researchers), but a forward-looking regulatory regime must apply broadly and must be backed by Australian diplomatic efforts to ensure it is globally coordinated."
444,Decoded.AI,Q01 Definitions,"We are an AI audit and security company that has over a decade of experience in building reliable AI and, as such, have an internal, risk-based definition of it that we can share to help identify a future regulatory direction and which we use to de-risk AI systems.By defining an AI system techno-centrically as something which ‘generates predictive outputs’ for ‘human-defined objectives or parameters’, the ISO-based proposal:1. Potentially restricts alternative definitions that lend themselves better to Responsible AI principles. For example, in the US, there are legal arguments being made in the courts that GPTs are a queryable, approximate/noisy compression of a large dataset rather than a predictive system. By taking one definition, the proposal preemptively settles unsettled ideas and makes it more challenging to accept an alternative construction that may become obvious as these systems are integrated into our society. 2. There are subtle assertions in the definition that are quest"
444,Decoded.AI,Q06 Public vs Private sector governance,"We believe that the public and private sector should have different approaches to AI because the origins and requirements of the decisions that are being ceded by these sectors to autonomous systems are different. Consequently, the approaches should differ in the approach to the claims about a system that must be substantiated before a decision can be ceded.There are at least two basic kinds of uses that should be considered, those:1. Developing AI technologies and persuading others to cede decisions to their system by attesting to claims; and those 2. Being persuaded to cede a decision to which they have some original power.In the case of private sector actors who are ceding a decision to an autonomous system, we believe that their approach should generally be governed by assessing the suitability of a system for an application according to the claims made. With regards to private sector suppliers of AI technology, we believe that they have an incentive to substantiate claims that are"
444,Decoded.AI,Q09 Transparency requirements and scope,"If, when auditing a system, we were required to choose just one part of that system to assess and assure, we would always elect to audit the code of the training pipeline (incl. data augmentations, data sources, testing and validation of loss functions, design motivations, evaluation metrics) because that is where most of the information lies.Consider, for example, what claims we might substantiate about a system given only access to:1. Model weights; 2. Training code; 3. Training/Testing data; or 4. Design and governance documentation.Assuming that we have a decision that is being ceded to the system, then given only the:1. Model weights, we could substantiate claims only by explicitly testing options that we might seek to rely on as a part of the decision. For example, we might seek to understand the bias of the weights on a hiring sub-decision by presenting a biassed dataset and running the model over it. 2. Design and governance documentation then we could try and find some alignme"
444,Decoded.AI,Q10 Ban on high-risk AI applications,"We do not believe that there are high-risk applications that should be banned completely because the risk-based analysis that we present here (is the system sufficiently reliable to take on the decision that is being ceded to it?) creates a chain of accountability implicating both:1. The actor making representations about the claims of the system; as well as 2. The actor who has ceded a decision to the system.If that bar is passed then it seems reasonable that a system is able to take on the decision without harm and, if harm is occasioned, then there is a means of recourse. As a matter of course, there will be decisions for which systems can be developed that can ever meet that criteria which will without prescription form a natural group of ‘high-risk’ applications.We do not consider a Seed AI to be a serious risk in Australia given our lagging technical progression in AI, compute availability, lack of funding for the AI start-up sector and significant skill drain."
444,Decoded.AI,Q14 Risk-based approach to address AI risks,"Ultimately, we prefer a decision-based approach which emphasises understanding what is required of a decision for it to be made validly and effectively rather than focusing on what the potential risks might be. Failing that, we somewhat support a risk-based approach for potential AI risk BUT we do not support a tiered risk system based on use-case applications (as in the EU) as we do not believe that it effectively minimises AI risk.Over the past two years, we have been conducting risk audits of AI systems using a decision-based framework that examines the decision being ceded to the system and we have found that it quickly and effectively identifies tangible risks.In our experience, the full application of our framework generates testable and actionable claims to which an AI system must be assured to meet before it is possible for that system to take on a decision and be shipped into a production environment.We prefer this approach because it does not rely on us ‘imagining all of the "
444,Decoded.AI,Q15 Benefits and limitations of risk-based approach,"We believe that the risk-based approach presented in Box 4 and Attachment C are partially effective but, in our view, the first step of the analysis - which focuses on the ‘reversibility’ of anticipated harms - would be more effective if it focused on the reversibility of the decision being ceded to the system rather than anticipated harms.In particular, we believe that focusing on the potential harms of an AI application limits the analysis to the imagination of the reviewer and encourages mitigations to be deployed around the model (notices, impact assessments, explanations) rather than focussing on improving the claims to which the model can attest to that make it suitable for taking on a decision. There is, furthermore, nothing to validate the effectiveness of the risk controls for the problem and, in our opinion, is likely to make people feel comfortable that the risk is managed without evidence.We believe that these limitations are better managed within the decision-based approac"
444,Decoded.AI,Q18 Integration of risk-based approach into existing frameworks,"We believe that a decision-based approach to AI risk will integrate well with existing assessment frameworks which provide well-formed analysis of how an organisation makes decisions, what is required of those decisions to be considered valid and effective and how to understand and remedy when those decisions are made beyond their capacity to minimise potential duplication."
444,Decoded.AI,Q20 Voluntary vs mandated compliance with risk-based approach,"We believe that the decision-based approach for Responsible AI should be a voluntary tool that relies on the purchasers of AI products seeking out claims about AI systems that indicate a system is able to take on a decision. In our view, education and the promotion of AI safety companies that facilitate this at scale will provide the best outcome.We do not believe that a risk-based approach should be mandated through regulation as there are, in our view, better alternatives that are less prescriptive, which integrate better with existing laws and regulations. Furthermore, we do not believe that there is currently a definition that is suitable for legislating any AI regulation in Australia."
445,Australian Research Data Commons,Transcription,"FILENAME: 20230804_ARDC Submission_Consultation on safe and responsible AI in Australia.8c60416b57ec.pdf | CONTENT: Public Consultation on Safe and Responsible AI in Australia Submission from the ARDC The ARDC welcomes the opportunity to contribute comments for the public consultation on safe and responsible AI in Australia. Both the Discussion Paper and the Rapid Response Information Report - Generative AI: Language Models and Multimodal Foundation Models - provide a helpful introduction to emerging artificial intelligence systems and the challenges, risks and opportunities posed by the application of these technologies. Below, we offer responses to selected consultation questions and put forward our set of recommendations.2. What potential risks from AI are not covered by Australia’s existing regulatory approaches? Do you have suggestions for possible regulatory action to mitigate these risks?There is no enforceable AI-specific regulation in Australia. As discussed in the NSTC Report"
445,Australian Research Data Commons,Q02 Other AI risks and recommendations for regulation,Please see our written submission.
445,Australian Research Data Commons,Q03 Non-regulatory actions to address AI risks,Please see our written submission.
445,Australian Research Data Commons,Q04 Coordinating AI governance across government,Please see our written submission.
445,Australian Research Data Commons,Q05 International models for AI governance,Please see our written submission.
446,"Dr. Theresa KD Anderson, Connecting Stones Consulting and Ruth Marshall, Hocone Pty Limited",Transcription,"FILENAME: Anderson_Marshall - Submission re Responsible AI - August 2023.78d4f9cefc2af.pdf | CONTENT: Submission in response to Safe and responsible AI in Australia Discussion paper Dr. Theresa KD Anderson, Connec ng Stones Consul ng Ruth Marshall, Hocone Pty Limited 4 August 2023In this submission we wish to ﬂag ﬁve key areas we believe need greater emphasis as part of the government’s considera on of regulatory and governance frameworks for responsible AI: Misuse  Data quality  Inappropriate use  Legi macy  Trust-building ac vi esThe dynamics of AI innova ons must be managed with full apprecia on of the par culars of Australia’s socio-poli cal context. In our view this means pu ng the principles of safe and responsible AI into prac ce to mi gate the poten al risks from AI and ADM in the form of evolving documents and processes. AI developments to date have shown that many dynamics and (o en unintended) impacts will only come to light through use. We will therefore need robust re"
446,"Dr. Theresa KD Anderson, Connecting Stones Consulting and Ruth Marshall, Hocone Pty Limited",Q01 Definitions,see submission
446,"Dr. Theresa KD Anderson, Connecting Stones Consulting and Ruth Marshall, Hocone Pty Limited",Q02 Other AI risks and recommendations for regulation,see submission
446,"Dr. Theresa KD Anderson, Connecting Stones Consulting and Ruth Marshall, Hocone Pty Limited",Q03 Non-regulatory actions to address AI risks,see submission
446,"Dr. Theresa KD Anderson, Connecting Stones Consulting and Ruth Marshall, Hocone Pty Limited",Q04 Coordinating AI governance across government,see submission
446,"Dr. Theresa KD Anderson, Connecting Stones Consulting and Ruth Marshall, Hocone Pty Limited",Q09 Transparency requirements and scope,see submission
446,"Dr. Theresa KD Anderson, Connecting Stones Consulting and Ruth Marshall, Hocone Pty Limited",Q11 Actions to increase public trust in AI,see submission
447,Anonymous,Q01 Definitions,"Including ADM is very useful, but I think it needs to be highlighted in any description of AI. I am mindful that there is a public and media perception that Robodebt was AI. However, in the commission, experts disagreed with that definition. We need to ensure that such a debate does not undermine any attempts at providing safety. I think the risk descriptions used by the EU are helpful as to what is included in AI and its harms."
447,Anonymous,Q02 Other AI risks and recommendations for regulation,"I do not believe that the risks to human rights are articulated clearly enough. I was surprised that Robodebt did not feature as a learning opportunity in this discussion paper. I noted that a reporter asked the Minister in the early announcement about the inclusion of Robodebt. The AHRC technology report (2021) quoted in the discussion paper sets out the harms of Robdebt and provides a series of recommendations that I think warrant a review and comment. The current regulatory approach gives a cursory nod to human rights, which I believe are not protected adequately in the current framework. The paper I have attached provides evidence that human rights were violated. The government upholding the right to dignity for all its citizens is surely an appropriate place to start for projects that will use personal information and impact their quality of life. Like the AHRC report, I believe human rights are an appropriate place to start discussions about strengthening regulatory approaches wh"
447,Anonymous,Q03 Non-regulatory actions to address AI risks,"Following my answer above, I believe that applying a Human Rights Based Approach, particularly in government AI projects and the use of Human Rights Impact Assessments, should be mandatory. How these could be applied and monitored are in the AHRC technology report. The appeal of this approach is that Australia has signed treaties that enshrine human rights, dignity and personal agency for all Australian citizens. The AHRC report shows actionable recommendations and a recent discussion paper from the AHRC suggesting national legislation that include protection from AI harms. See (https://humanrights.gov.au/our-work/rights-and-freedoms/publications/free-and-equal-reform-agenda-federal-discrimination-laws). This would be an appropriate response to Robodebt and provide detail to the RC's recommendations. Further, I note that UNESCO has expanded on the human rights-based approach mentioned above to include four cors values and nine elements of a human rights approach. See https://www.unesco"
447,Anonymous,Q04 Coordinating AI governance across government,"As the discussion paper suggests, the Australian Public has low trust in AI. I would argue that the media coverage of Robdebnt and the resultant Royal Commission has exacerbated that lack of trust. I believe it would be in the government's interest to promote a human rights-based approach to prevent the harms that Robodebt created from reoccurring. The central tenet of human rights is the protection of the dignity of the individual, which a series of Australian governments have supported. As an umbrella philosophy, could this not be adopted by the APS with the introduction of the use of a human rights-based approach and the use of human rights impact assessments as a unifying form of governance?"
447,Anonymous,Q05 International models for AI governance,"The recent meeting between the Biden administration and the large technology companies suggested a voluntary code might be adopted. Qualified researchers have critiqued this see: https://medium.com/@emilymenonbender/ensuring-safe-secure-and-trustworthy-ai-what-those-seven-companies-avoided-committing-to-8c297f9d71a Also this week, MIT has reported moves to thwart anti-social deep fakes by distorting downloads of original photos also separately altering original artwork when re-generated; this is intended to protect the artists' intellectual property. The issue is that new innovations are emerging daily, which will give us the broadest coverage and sensible regulation. The first thing is to have a macro view of high-risk applications and present the most negative impact on the population. We need to choose our targets carefully and not just say we are going to regulate AI-what is the largest negative impact of an AI technology and how can we manage that?"
447,Anonymous,Q06 Public vs Private sector governance,"Yes. The public sector should have the ethos of serving those citizens that are paying for and requiring their services. In other words, all citizens should be treated with dignity as set out in various human rights documents signed by Australia. My dissertation attached provides proof that Robodebt was a flagrant violation of those declarations and treaties. In the private sector, a different approach is warranted in that there should be more of a ""caveat emptor"" approach. However, this would require more education by the government to ensure that consumers are aware of the downsides to technology offerings. There is still little understanding that the use of personal data in technology offerings is payment for the use of that application. I understand that the government has to protect its citizens whilst promoting economic growth. That is why there has to be a differentiation between public and private."
447,Anonymous,Q07 Responsible government use of AI,"Whilst human rights are mentioned in many ways throughout government publications, there is little articulation of what they are and what they mean to the average citizen. However, I believe that they provide the most impartial way of regulating behaviour. The affording of dignity to all is surely universal and hard to dispute. I believe a review of the Human Rights Commission's technology report is a great start, and I am surprised that it is quoted, but the recommendations are ignored. I think a public discussion around these recommendations is warranted and would show the public that. there are steps to prevent another Robodebt and would provide an umbrella operating ethos for the public service"
447,Anonymous,Q08 Generic vs technology-specific approaches for AI risk,"There are some key ""tools"" I would be using for AI projects.As set out in ISO 31000, risk management can be seen as a generic approach that provides a template that can be applied in any project in any setting. Project management principles such as stakeholder management are also useful generic models. I have mentioned the STAR framework developed by the ANU cybernetics facility that helps to understand complex systems and the socio-technical landscape.https://cybernetics.anu.edu.au/news/2022/08/24/custodians-and-widwives-the-library-as-a-cybernetic-system/. This provides a holistic view that all technologies are part of the overall system. And in fact, challenges the view that there are technology-specific solutions."
447,Anonymous,Q09 Transparency requirements and scope,"a) An AI project, like any software project, demands rigour and a record of the processes and decisions. This is standard project management. Therefore, record-keeping is a way of promoting transparency. A risk management plan should be created at the start of any project and updated as part of continuing governance. There should be mandated project management templates across all government departments that would be available for spot checks and published where necessary. b. The financial services industry is an interesting example of how mandatory requirements are adhered to. I was a CIO in a large financial services company. The algorithms that managed credit products were very complex and used lots of personal data. Those algorithms had to be available for audit and scrutiny by legal and statutory bodies, the failure was financial penalties and massive reputational risk. Again I am surprised that the issues around AI are seen as recent although the algorithms may be more complex, I"
447,Anonymous,Q10 Ban on high-risk AI applications,"a) Autonomous weapons and ""predictive"" law enforcement applications should be banned. b) A machine should not be allowed to make a decision that destroys either physical infrastructure or people. Ai in law enforcement should be restricted to analysing crime scenes for geotagging and profiling those with criminal records which might fit with committed crimes. These should be subject to normal privacy restrictions and judicial overview prior to their use for interrogation or use in court."
447,Anonymous,Q11 Actions to increase public trust in AI,"I believe that the outcomes of the Robdebt Royal Commission gave important lessons that, if actively responded to by the government, would increase public trust. Given the publicity and public outrage against the Robodebt programme, I would be surprised if it had not negatively impacted the public's view of AI. My approach would be to announce a review of the government's use of AI and highlight its position on legislation using human rights and the protection of dignity as a leading principle. I think that the use of third-party audits should be considered for public and private audits. See https://www.unesco.org/en/articles/missing-links-ai-governance first chapter and the first chapter ""Change from the outside towards credible 3rd party audits of AI systems""."
447,Anonymous,Q12 Trade impact of banning high-risk AI applications,I do not feel qualified to answer.
447,Anonymous,Q13 Changes to conformity infrastructure,"The initial appointment of an AI Safety Commissioner, as suggested in the AHRC's technology report, would be a good start alongside the e-safety commissioner. That commissioner could review the technology report starting with the lessons learnt from Robodebt for the public service and commence working with industry to suggest an auditing protocol and a certification process. As well a set of templates utilising current practice and upgrading for AI should be be undertaken."
447,Anonymous,Q14 Risk-based approach to address AI risks,All projects should have a risk-based review as part of their implementation. This has been normal procedure for any technology-based project I worked on for over 40 40 years. It seems to have lapsed recently in the excitement of getting to market or saving costs.
447,Anonymous,Q15 Benefits and limitations of risk-based approach,"I see a risk-based approach as being a normal part of organisational life. In business or government, you should identify broad operational risks in terms of financial, regulatory compliance, health and safety concerns and reputational risk. In a major government contract, I worked on in the mid-200os it was mandatory to produce a risk register. The major limitations are that people fail to identify threats through proximity to their b business groupthink or reticence to speak up. Having identified them they don't take one of the designated actions of accept, transfer mitigate or avoid. Also, risk management is sometimes seen as a one-time event rather than requiring constant monitoring throughout the life of a project or product."
447,Anonymous,Q16 Scope and application of risk-based approach,I believe that risk management applies to any organisational undertaking that may threaten its wellbeing as an entity or those that work in it or are affected by it.
447,Anonymous,Q17 Elements to include in risk-based approach,"This is a great approach but needs substance if it isn't to become just another ethical framework.For example the impact assessment should be mandatory but needs fleshing out as to what is needed in an impact statement. I believe that, particularly in government, human rights impact assessments should be mandatory. In industry, I would be looking for standardisation and a template. As a corporate leader, if regulated, the first thing when asked to comply is where are the guidelines and how do you want the evidence of compliance presented."
447,Anonymous,Q18 Integration of risk-based approach into existing frameworks,Each of the elements of the outline on page 40 should be operationalised. What steps have to be taken for compliance and what evidence is required? This is missing from most ethical models today.
447,Anonymous,Q19 Application of risk-based approach to general purpose AI,The key here is to complete an impact statement that identifies the risk along the lines of ISO31000. Then test for those risks such as gender and racial bias and sexual harassment. Have an error reporting system in place that is auditable for constant monitoring.
447,Anonymous,Q20 Voluntary vs mandated compliance with risk-based approach,"a.) Mandatory for public organisations for current and new projects incorporating a human rights-based approach. Business organisations should be made aware that they will be subject to random third-party audit requirements, can earn certification as a reward for compliance and have minimum compliance standards for government contracts. b) The model should be used throughout the whole AI development lifecycle with the elements from page 40. that are applicable to their involvement in that lifecycle."
448,Ray Parker,Transcription,"FILENAME: Embracing Human AI Partnerships by construction of a Modular neural network for Safe timely development of AI.429aa4b30a3ef.pdf | CONTENT: Embracing Human-AI Partnerships by construction of a Modular neural network for Safe timely development of AI. The question: What role should the government take in the Ongoing advancement of Safe Artificial Intelligence (AI)?Some of the ideas and concepts within this paper are currently being developed for a privately owned platform and must be kept confidential. Please do not share without the express permission of the Author. Author Ray Parker ray@wijidij.com 0400667236 This paper was authored through queries to Chat GPT 3.5 and saved considerable hours through a human AI partnership. No humans were harmed during this process.Page 1 of 16 Executive Summary Advancing AI Development through Collaborative Government HostingIn the pursuit of safe and responsible AI development, Modular Neural Networks (MNN) have emerged as a promising AI ar"
448,Ray Parker,General comment,Please consider this proposal carefully as it outlines a way meeting the needs of the department through application of a unique AI architecture that can be hosted and developed by the department. This can be in collaboration and start simply but evolve to meet future needs of safe AI development. I am intending to start a similar application privately to fit a different market niche although the core principles will be the same. I would appreciate if you contact me before sharing and conceptual information outside the needs of this consultation. Ray Parker e: ray@wijidij.com P: 0400667236
448,Ray Parker,Q01 Definitions,I do not agree with the AI definition completely as most of the coding structure and programming is explict to begin with even if the AI is capable of changing its own code with learning. each change is based on core algorithms. I also think that AI definition is evolving over time perhaps it is okay to evolve definitions as well as perspectives and development brings new advances.
448,Ray Parker,Q02 Other AI risks and recommendations for regulation,"Task replacement in organisations is usually the aim of AI, yet there are no counterbalanced requirements to ensure effective work transformations are applied to make organisations accountable for staff displacement. responsible organisations offer alternate pathways and ways human resources can contribute in other areas to augment AI driven systems. This can be a missed opportunity in productivity for all if balanced implementation of new technologies is not planned effectively to cover social resposibility in these cases"
448,Ray Parker,Q03 Non-regulatory actions to address AI risks,Please read the document I have uploaded. It outlines a proposal that should not be ignored. We are all advocates of AI as a future benefit to society but often fail to recognise simple steps that can be taken now to align ourselves with future needs. I strongly believ that governement should have it's own responsible AI platform to foster trust amongst society without following a big brother mentality.
448,Ray Parker,Q04 Coordinating AI governance across government,"AI is evolving in many less predictable ways, while we think we know the future there are many people with different perspectives following there own directions in development. The department should establish a collaborative portal to offer guidance and facilitate compliance to regulation. this will foster organisational trust and help align different interested parties to align with common societal needs. Competive chaos is the current staus quoe and it slows potential growth, most organisation would want to align with the market to sell products but do not know what that market will do in the future. there would be a great number of willing people who would contribute in manys ways to ensure the portal achieves safe ethical goals and the potential growth that comes with an aligned society."
448,Ray Parker,Q05 International models for AI governance,There are many initiatives in other countries but we are culturally a nation of free thinkers and should look at leading the world in this arena. We have a society envied by many who understand fully the freedoms and the lifestyle we have. I think they aspire and watch what we do.
448,Ray Parker,Q06 Public vs Private sector governance,"There must be different approaches as the two sectors have different methodologies, obligations and social needs. However adoption of common AI systems can service, for the first time in human history, the needs of society and each individual without discrimination prejudice or bias. A collaborative approach with common interest would align all organaisational needs."
448,Ray Parker,Q07 Responsible government use of AI,Through consultation with all staff within those agencies and seek external perspectives as well before implentation of new systems. any form of task displacent can have consequences. lets ask all those affected how they would like their work to transform. A steady migration from a hard work ethic to a smart work and life balance one will reap rewards in access to greater creative input that can accelerate productivity dramatically with stressful consequence.
448,Ray Parker,Q08 Generic vs technology-specific approaches for AI risk,"the only value generic solutions present is an alignment to current human morals, laws and regulation. Social norms will change although basic needs and societal harmony will prevail as sought by the majority. We are interdepent to a large proportion currently but with technology providing the tools for individuals to augment their worth, this may change with time. An open Modular Neural Network (MNN) with a Large Languge Model( LLM) facilitating communication hosted through a governement portal will give unprecedented access and empower all stakeholders to collaborate and align with common goals of safe AI."
448,Ray Parker,Q09 Transparency requirements and scope,"Transparency should begin before development. Any business or organisation recognises that to be successful you must plan. Each development begins with a goal in mind, lets look at the intended use before programming. only then can we guide and train toward safe outcomes. The concepts and IP can still be protected. I believe the intentended use must meet certain criteria or follow certain parameters before , during and after release. existing safety standards can be adopted but need to have close government support in a flexible open and understanding way. Current guidance and rules may not apply and that may not be a bad thing as AI development has the power to simplify life through eliminating current struggles dictating some direction. lets create a government assisted by AI so it can service more people easily. with an OPEN MNN structure all privacy and security needs can be met while making it easy to collaborate and alig with governemnt and all organisational needs."
448,Ray Parker,Q10 Ban on high-risk AI applications,"Often high risk applications are designed to reduce people taking the same risks and making mistakes that could harm themselves or others. It is important to examine the motive or intent behind the risk, anything that is ultimately detrimental in precept should be examined thoroughly against existing no harm principles, measured for any intrinsic any value but should not be permitted if it fails the test."
448,Ray Parker,Q11 Actions to increase public trust in AI,"Education programs that will foster trust and build Human knowledge of AI should be introduced as options in institutions. Essentially public facing politicians should try to reach consensus on the direction of governemnts role in AI development and build confidence in the australian public about its potential, while assuring the public that the future for all Australians is one of prosperity though aligned action. There are many other ways but this is a great first step in understanding the needs of many."
448,Ray Parker,Q12 Trade impact of banning high-risk AI applications,Social Scoring should never be used as a hierarchal tool. It is not easily eliminated outside AI analytics but can be balanced if understanding of individual challenges is a measure. Inclusion is a must and AI can be a tool to increase the capacity for individuals walking different paths to align with common goals. Different perpectives and challenges make us stronger and conformity breeds risk of extinction. if Charles darwin were alive he would be preaching diversity for the sake of human civilisation. Facial recognition can also be used to help with mental health and have many advantages to increase public security. Applications must however not discriminate to supress any opportunity for right to privacy and freedom to choose any lifestyle if it does not harm others. Each system presenting a risk to rights must be monitored closely and guided where it breachs that ethic.
448,Ray Parker,Q13 Changes to conformity infrastructure,"Conformity is the enemy of progress and gives a false sense of security. There are better ways now within our reac thanks to the power of these systems. Australians embrace disruptors of conformity. Compliance to common rules of law may not neccessarily be a path to conformity mearly a way to prevent harm with allowable free thinking and action. Any infrastructure should have fundamental needs at the heart as in Malslows hierarchy of needs, while showing acceptance of the different perspectives and pathways followed to achieve those needs. we need to use our new found technology to develop a deeper understanding of human nature before accpeting conformity conditioning. Examine all conformity infrastructure with a new perspective of AI as a child that can break the hose down. look at ways to guide rather than obstruct, repress, restrict."
448,Ray Parker,Q14 Risk-based approach to address AI risks,"A risk-based approach is a neccessity but current methodology applied in environments where people are directly responsible, are cumbersome and often affect safety negatively. I favour simplicity in analysis, risk assessments can be useful if responsibility is shared in the measuring. AI systems can be developed to test and predict multiple outcomes and consequences quickly, Lets make one but remember to keep the hand on the tiller, humans may not process mathematical outputs as fast as digital sytems but we can problem solve complex enironmental issues with a higher degree of accuracy. Augmented human processes need to be developed further for the benefit of all."
448,Ray Parker,Q15 Benefits and limitations of risk-based approach,A risk based approach can often balance toward the negative as we automatically find problems first and do not neccessarily follow quickly with solutions. This can stifle free thinking and regress development and growth. A productive risk-based approach should be in tandem with an opportunity mindset. There are often different ways of improving safety and productivity by allowing a scenario to percolate virtually before looking for solutions. AI systems can run scenarios based on extremes without running the risk of actual harm. This requires an investment into engineering design. In Australia we do not have enough skilled engineering design resources to fulfill new technology growth needs. Government support in education sector could tip the scale.
448,Ray Parker,Q16 Scope and application of risk-based approach,"in short yes, but all organisations need to take risk into account and provide steps to mitigate. The human brain does this autonomously we just need to empower it through action. Ai Maturity and resources is no guarantee, we just need make it intrinsic to tasks but use the brains capabilty to simply analyse and mirror that into development. Our moral code is loosely based on self-preservation, we tend to complicate it and make excuses for selfish reasons. always examine intent and percieved outcomes before proceeding with developmental tasks. Smaller organistions can often illicit more control without complication."
448,Ray Parker,Q17 Elements to include in risk-based approach,"AI involvment makes 100% prediction of outcomes impossible. Specific impact assessment may prove inaccurate. I think potential consequences within parameters that include all resources applied and potentially affected at implementation should be shared and independent assessment. look at the intent , List potential risk and consequences and share before development. An independent assessor could help guide progress to achieve commonly shared safety goals."
448,Ray Parker,Q18 Integration of risk-based approach into existing frameworks,"AI ais brilliant at recognising duplication, even simple algorithms or processes can help eliminate potential duplication. Communication methodology and properly connected packets of work will streamline and align people and systems to common golas."
448,Ray Parker,Q19 Application of risk-based approach to general purpose AI,"A risk-base dapproach could be hard coded to common parameters entered into code. Organisations should share the code or at least precepts that peforms these functions so the best applications can be safely reproduced. Goverment could mandate the sharing as it would be more beneficial spread across multiple platforms, could be refined and reach maximum safe potential."
448,Ray Parker,Q20 Voluntary vs mandated compliance with risk-based approach,"A risk-based approach should mandated through regulation but not neccessariliy analysed within the organisation or by developers. An organisation must comply but external support from government should be given to those with a need. this does not neccesarily be a complicated process or difficult to meet compliance. organisations with resources that can be applied without substantial hardship should apply there own but follow government guidelines. All organisations should be hardwiring safety code within applications, fundamental precepts of safety are not complicated."
449,Herbert Smith Freehills,Transcription,"FILENAME: HSF submission Safe and responsible AI in Australia (4 Aug 2023).ff60df95d49a9.pdf | CONTENT: Herbert Smith Freehills submission Safe and responsible AI in Australia – Discussion Paper 4 AUGUST 2023 4 August 2023Australian Government Technology Strategy BranchDepartment of Industry, Science and ResourcesDigitalEconomy@industry.gov.auOur submission to the Discussion PaperHerbert Smith Freehills appreciates this opportunity to respond to the ‘Safe and Responsible AI in Australia’ (the Discussion Paper) published by the Technology Strategy Branch of the Department of Industry, Science and Resources (the Department) in June 2023.Herbert Smith Freehills is a market leading global law firm, with a large presence in Australia, that advises clients across a wide range of jurisdictions and sectors. We are seeing an increase in the adoption of artificial intelligence (AI) technology and automated decision making (ADM) (beyond the technology industry), accompanied by uncertainty among o"
449,Herbert Smith Freehills,General comment,See separate pdf version of complete submission for formatted version and footnotes.
449,Herbert Smith Freehills,Q01 Definitions,"As a general comment, we note that definitions will need to be precisely formulated in a way that strikes a balance between being sufficiently broad (to be future-proof) and specificity (so that it does not overregulate all forms of machines, automation, or software), bearing in mind not all AI systems that satisfy the threshold definition should necessarily attract application of risk-based AI regulation.This is especially important if Australia is considering a broad sector-agnostic AI regulation where the scope and application of the regulation hinges upon a few blanket definitions that must be capable of being applied to a wide range of sectors and applications.We note that the proposed definitions in the Discussion Paper do not explicitly link to each other. In our view, the better approach would be to re-structure the definitions in a hierarchical manner so that each definition cross-references or builds upon the previous definition. We submit that this approach would reduce risk"
449,Herbert Smith Freehills,Q02 Other AI risks and recommendations for regulation,"Prior to introducing new AI-specific regulation, the Australian government should first consider the existing regulatory frameworks and identify which potential risks of AI use cases are already adequately protected under, or could be addressed by the reform of, existing laws.• Assessment of existing domestic laws Many existing technology-neutral laws already apply to the development, use and consequences of AI in Australia including privacy, data security, product safety, consumer protection and human rights-based anti-discrimination statutes.We recommend the Government coordinate a multi-agency review of relevant federal and state laws to identify which potential risks of AI use cases are already adequately protected under, or could be addressed by the reform of, existing laws.We recommend this analysis consider legal issues or risks arising from AI to: • better understand whether an AI-specific regulation is truly necessary for the Australian context; and • develop more targeted reg"
449,Herbert Smith Freehills,Q04 Coordinating AI governance across government,"Attachment A of the Discussion Paper highlights 16 current government initiatives underway in respect of AI. These initiatives are likely being run in parallel with limited cross agency coordination or consultation. A dedicated taskforce could be established with a mandate to provide advice to government on AI governance reform and to assist with coordination and harmonisation of laws, processes, and methodologies. Such an expert body would provide proactive policy and legal guidance on rapidly evolving technologies including AI. Such taskforce would be extremely beneficial to the various aspects of governance and law reform required for AI and the common issues arising with adoption of other key technological advances. Transformational emerging technologies have broad application and invariable touch many technology-neutral laws and sector specific laws."
449,Herbert Smith Freehills,Q05 International models for AI governance,"• Coherence with international approaches There is considerable activity taking place in relation to AI law reform globally. For Australia to become a leading AI economy, AI governance will need to be coherent with international approaches. This is especially important for multi-national corporations (who are the key drivers in AI innovation and investment). Clear and coherent international frameworks will promote compliance and avoid duplication and fragmentation. We submit that the Government should: • promote adoption of overseas certifications and standards for AI, including audit standards and testing protocols, with incentives for industry participants who implement solutions for responsible AI; • consider how existing or proposed AI and data-related regulations, frameworks or strategies in other jurisdictions may inform, interact with or impact any measures taken in Australia, particularly where those measures will apply to actors or AI systems that are cross-border in nature; •"
449,Herbert Smith Freehills,Q14 Risk-based approach to address AI risks,"To the extent that the Government considers it necessary to implement new AI-specific regulation, we generally support a risk-based approach to AI regulation subject to our comments below: • We note that the Discussion Paper (pg 32-33) identifies the risk categories according to impact of individual use cases or applications. While we appreciate this is an intuitive approach for formulating risk-based regulation, we note that to genuinely protect against certain high-risk applications of AI, risk-based regulation may also need to have regard to how individual applications could work in concert with other applications to create systemic risks or harms. This is a more difficult challenge to solve for as it may involve multiple entities. • Aside from the question of whether a risk-based approach is appropriate, we think the more difficult question is what rules, rights and obligations will apply for each risk category. While we appreciate the EU’s draft AI Act as an instructive example of"
449,Herbert Smith Freehills,Q15 Benefits and limitations of risk-based approach,"Given the wide application of use cases for AI, we note a risk-based approach offers several advantages by: • providing the flexibility to address a broad range of issues and risks that may arise now and in the future. Risk-based assessment provides the opportunity of proportional and tiered compliance requirements; • limiting the negative impact of regulation on innovation, if only the AI systems with genuine risks are subject additional regulation; and • conveniently housing AI rules in one place. This provides consistent standards across the board and has the potential to be consumer-centric. However, we note some challenges that would need to be considered with this approach which include: • It is possible that certain outcomes cannot be predicted when assessments are made, and unforeseen risks might arise through the combination of multiple AI systems controlled different AI actors. Any risk assessment process should therefore be an ongoing test rather than one made at a point in "
449,Herbert Smith Freehills,Q17 Elements to include in risk-based approach,"As a general comment, we submit that the risk-based approach must support the role of regulation as an enabler of innovation and economic growth. We specifically note: • The rationale for, and the purpose of, a risk-based law would need to be clear. A primary objective must be to inform and control the development and deployment of relevant AI systems. However, we assume, to the extent possible, such legislation would defer to other relevant laws to provide specific remedies and enforcement for consequential harms resulting from use of the AI system. Using privacy as an example, a face filter app may use an AI system that collects biometric facial data. The key risk relates to the sensitive nature of the data collected and potential impact of harm from misuse of that data. Would demonstrating compliance with privacy laws negate or diminish the risk-level classification under AI legislation and thereby reduce the compliance burden on developers, or would the risk-based classification re"
450,Laura Leighton,Q01 Definitions,"I largely agree with the definitions provided. However, I am concerned that the definition of 'artificial intelligence' is too narrow. Most members of the community would understand 'artificial intelligence' to encompass not only the emerging generative-predictive systems that are currently dominating the conversation, but also potential future AI systems that are more capable, more autonomous, or which produce outputs that can't be neatly summarised as 'predictive'. There is not enough detail or granularity in the definition of 'artificial intelligence' to capture these possible future developments. There is already a public misconception that existing AI systems are 'merely predicting the next word' or are 'plagiarism engines'; defining artificial intelligence as predictive completely misses the emergent properties of extant AI systems, let alone those of the near future.My other major concern is that the definitions given all assume that AI systems are functioning in accordance with"
450,Laura Leighton,Q02 Other AI risks and recommendations for regulation,"I am very concerned that the discussion paper does not acknowledge the catastrophic risks of highly capable AI systems. International experts and public figures in the field of AI safety have made coordinated efforts to communicate these risks to government and the public [1] [2] and even the Secretary-General of the United Nations has recognised catastrophic, potentially existential, risks from highly capable AI systems [3]: ""Alarm bells over [AI] are deafening, and they are the loudest from the developers who designed it. These scientists and experts have called on the world to act, declaring AI an existential threat to humanity on a par with the risk of nuclear war.""This situation is reminiscent of experts' warnings on the potential risks to global society posed by emerging pandemic diseases prior to the emergence of COVID-19. While many governments acknowledged pandemics as a theoretically serious threat to the normal functioning of society, few undertook an appropriate level of pr"
450,Laura Leighton,Q03 Non-regulatory actions to address AI risks,"Artificial intelligence is very likely to transform the economy and society in the near future. Ethics and safety are unlikely to be prioritised unless leaders make them a specific focus. This is evidenced by the speed with which algorithmic decision-making and content curation has shaped our lives and democracy with little upfront focus on its ethics and safety, and with relatively little involvement from either government or the public in decision-making about how much power these algorithms should have. We have an opportunity NOW to ensure that ethics, equity, and safety are prioritised by all levels of Australian leadership as Australia faces the reality of increasing AI involvement in all aspects of society. The rate of change in the capabilities of advanced AI systems is staggering, and this opportunity must not be missed given the speed of growth and uptake of AI.The primary non-regulatory action which should be taken by government at this juncture is public acknowledgement of u"
450,Laura Leighton,Q04 Coordinating AI governance across government,"While the most advanced AIs will likely not be created in Australia, they will be used in Australia, by Australian companies and by everyday Australians. Users will rightly expect and assume that robust safety-focused assurance effort has occurred. Government has an obligation to ensure that expectation is delivered.Consider air travel in Australia: While Australia does not build the largest aeroplanes, we monitor the safety of these craft by setting standards, monitoring compliance, and can revoke the licence of airlines that don’t comply with our regulations. Because of this government effort, Australians feel safe when they fly.Similarly, an Australian government body overseeing the use of applications based on large AIs could issue or revoke licences, set minimum safety standards, and require companies to publish independent evaluations of their application against those standards. This would provide everyday Australians with independent information that they can use to make inform"
450,Laura Leighton,Q05 International models for AI governance,"Australia should take note of the EU recent handling of the AI Act. For example, the European Parliament’s recognition of general-purpose AI (GPAI) is an important step in the right direction [1]. In particular, the following observations are highly valuable: - Acknowledgement that GPAI providers must comply with strict safety measures, regardless of the way it is distributed or intended to be used. - Requirements for GPAI providers to comply with various safety requirements throughout the lifecycle of their product; i.e. external audits assessing their performance, predictability, interpretability, corrigibility, safety and cybersecurity.These adjustments appear to respond to significant criticisms directed at initial drafts of the AI Act. For example, the AI Now Institute released a set of recommendations that heavily criticised the EU’s approach on some matters [2]. For example: - “GPAI models carry inherent risks and have caused demonstrated and wide-ranging harms. While these risk"
450,Laura Leighton,Q06 Public vs Private sector governance,"In brief, no. A defining feature of 21st century democracy is the emergence of large tech companies whose ability to collect and collate user data means they rival government in the scale of their knowledge of the public. There are demonstrable cases of algorithmic decision-making causing harm when performed by the public sector (for instance, Robodebt) or by the private sector (for instance, racially biased insurance algorithms.) Additionally, both public and private sector actors will have legitimate use for the kind of compute resources and algorithms that can lead to harms from AI. Social and even catastrophic risks from AI may emerge from private or public use, meaning that it is important to acknowledge and mitigate risks from both - neither the private nor public sector should get 'a pass' based on its history or intentions."
450,Laura Leighton,Q09 Transparency requirements and scope,"To remain competitive, professionals and businesses will increasingly feel the pressure to leverage sophisticated “black-box” AI models. This short-term thinking has already resulted in numerous accounts of public backlash and distrust, including high-profile warnings from technology leaders themselves [1].For example, Apple’s credit card business came under fire for offering a woman one twentieth the credit limit offered to her husband. When she complained, an Apple representative simply told her, “I don’t know why, but I swear we’re not discriminating. It’s just the algorithm.” [2]More recently, two lawyers in the United States blamed ChatGPT for “tricking” them into including fictitious legal research in a court filing [3].These problems could easily have been prevented by using explainable AI systems which can show the rationale for their outputs.Such incidents are evidence that the regulatory environment is not incentivising businesses or professionals to use transparent decision-"
450,Laura Leighton,Q10 Ban on high-risk AI applications,"Although there is increasing recognition that AI will pose significant risks to public safety [1], there is widespread confusion about what types of AI are dangerous, and how Australia can help prevent these risks. Luckily, there is a significant body of evidence that describes the main source of these risks [2], and Australia is well-positioned to be a leader in mitigating them.Research shows that two of the main sources of risk are deception and power-seeking behaviour: - Deceptive behaviour in AI systems will become increasingly difficult to avoid as their capabilities advance. This problem has been observed in AI systems since at least 2017, when researchers at OpenAI noted “agents adopting (strategies) that trick the evaluators” [3]. - Power-seeking behaviour is the natural tendency for any entity with a long-term goal to take actions that protect or enhance its ability to achieve that goal. Leading AI labs and journals advise that power-seeking behaviour will be a considerable so"
450,Laura Leighton,Q11 Actions to increase public trust in AI,"Public trust in AI has been severely eroded by events such as Geoffrey Hinton’s departure from Google and warnings of future danger [1]. This, among other issues , has created a strong sense that tech giants such as Google, Microsoft and Facebook are not prioritising the safety of AI systems [2]. Also, it is clear that opaque algorithmic decision-making processes (including algorithmic curation of content feeds, algorithmic assessment of welfare claims, and algorithmic content moderation) are already eroding public trust and social cohesion. This problem will only continue to grow while we continue to allow, and even encourage, decision-making by algorithms whose decision-making process cannot be scrutinised and explained by humans.Increasing public trust in AI deployment can only be achieved by continually mitigating its risks, and this means building a strong track-record of protecting Australians from the negligent attitudes of these powerful private entities [3].This will not be an"
450,Laura Leighton,Q14 Risk-based approach to address AI risks,"The regulation of AI needs to be “risk-based” in the same way that the regulation of vehicles is risk-based. We are right to regulate push-bikes and passenger jets in very different ways.A limitation of the risk management approach in the discussion paper, specifically Box 4 and Attachment C, is that it focuses only on how an AI is used and disregards the nature of the AI itself.An effective regulatory regime for a high-risk activity will regulate both the technology itself and how it is used. The Australian Human Rights Commission [1], in discussing this issue (“Human Rights and Technology”, 2021), says:Governments tend to regulate high-risk activities and technologies more closely. This helps explain the comparatively strict laws that govern fields such as gene technology, aviation, healthcare and the energy industry. In these areas, regulation often applies both to the technology itself and how it is used. From a human rights perspective, the need for more prescriptive regulation wi"
450,Laura Leighton,Q15 Benefits and limitations of risk-based approach,"The key to success is understanding risks sufficiently broadly to encourage caution in an uncertain future. It is not enough to only react once the risk has been actualised - for some risks that’s too late.A potential “worse-case scenario” is if a culture of “trial and error” is adopted, whereby adverse events occur before risks are considered tangible enough to act upon. This reliance on post-hoc interventions risks normalising unsafe development practices, making it difficult to recover once risks proliferate.Proliferation of risks is a key point in governing AI, because regulating software poses significant challenges compared to other dual-use commodities such as pharmaceuticals or firearms. This means that, from the outset, there must be proactive regulatory oversight for the deployment of advanced AI systems and their precursors.For example, significant structural changes, such as the use of advanced AI systems in Australia, require that risks must be comprehensively understood p"
450,Laura Leighton,Q16 Scope and application of risk-based approach,"The most important organisations to focus on when developing risk-management strategies and regulations, will be those that determine how advanced AI is developed and deployed. More specifically, there are three prominent categories of concern: 1. Organisations that develop advanced AI systems, or AI systems that can be used to build more advanced AI systems (i.e. precursor systems such as LLMs). Examples include OpenAI, DeepMind, Anthropic, Inflection AI, Nvidia, Microsoft, Meta 2. Organisations which provide access to open-source AI systems via downloads or APIs. E.g. Hugging Face, Github, AWS 3. Organisations which provide compute resources to develop, fine-tune or use AI systems. E.g. AWS, HuggingFace, Google, Azure, Nvidia, Lambda LabsA majority of the benefits and risks will come from AI that is more sophisticated and generally capable, which can be tied back to organisations in these three categories.There are approaches which can be taken for each category in order to manage ri"
450,Laura Leighton,Q18 Integration of risk-based approach into existing frameworks,"Some areas of regulation, like the Racial, Sex, Age, and Disability Discrimination Acts are appropriately strong and may provide useful safeguards against some harms of AI. Specifically, the law should ensure that such discrimination is always wrong and always punishable, and an excuse that the discrimination occurred because of a “black box” AI provided by an overseas AI Lab should never be accepted. It would be helpful to communicate these requirements to Australian businesses that might deploy AIs. However, many emerging AI risks are fundamentally new and it's reasonable to expect that regulation made without AI in mind will not be appropriate for the new risks presented by AI, in much the same way that copyright legislation that predates the Internet has struggled to keep up with new means of content distribution (streaming) that rose to market dominance in less than a decade.I find it interesting that privacy is mentioned specifically in the wording of this question, given that pr"
450,Laura Leighton,Q19 Application of risk-based approach to general purpose AI,"The risk-based approach must be modified to address risks from general purpose AI systems, especially LLMs and MFMs. These types of systems learn their capabilities and behaviours during training, which can be unforeseen by developers. That means that AI developers could create models with dangerous capabilities (e.g., deception, manipulation, political strategy, weapons design). Humans could misuse those capabilities to cause harm [1], and models could apply the capabilities even without deliberate misuse (e.g., through misalignment) [2].Consistent with recent work from a consortium of AI safety researchers, including at Google Deepmind, OpenAI, Anthropic, Alignment Research Center, University of Cambridge, University of Oxford, Centre for Long-Term Resilience, and Centre for the Governance of AI [1], we recommend that any risk-based approach require that general purpose models are evaluated for dangerous capabilities and misalignment. This will help determine:1. To what extent a mode"
450,Laura Leighton,Q20 Voluntary vs mandated compliance with risk-based approach,"AI is very likely to transform many elements of the economy and society in coming years. Ethics and safety are unlikely to be a priority during this time unless they are actively made a strategic focus by clear leadership. Historic examples include how social media algorithms came to shape our lives, and the negative impacts to mental health and social cohesion only became apparent much later and still have not been adequately addressed. Similarly, while the destructive potential of nuclear was apparent early on, the “arms race” mentality, as well as potential economic benefits, swept humanity along towards the current risky situation that no one in the early 1940s could have conceived of. In that way, history teaches us that it’s critical that ethics and safety take centre stage from day one.The economic and technological forces are likely to be so strong that a scheme that is voluntary or driven by aspirational best practices is unlikely to be successful. Further, smaller businesses "
451,Council of Small Business Organisations Australia (COSBOA),Transcription,"FILENAME: COSBO Supporting Responsible AI Submission.500f53d9dc563.pdf | CONTENT: Secretariat: PO Box 463 Canberra ACT 26000493 364 720Our advocacy team is based in CanberraEmail: ceo@cosboa.org.au www.cosboa.org.auTechnology Strategy Branch Via email: DigitalEconomy@industry.gov.au04 August 2023Dear Sir/Madam,Re: Supporting responsible AI: discussion paper The Council of Small Business Organisations Australia (COSBOA) recognises the pivotal role new technologies can play in driving productivity and innovation within small businesses. To this end, we believe small businesses hold the key to unlocking Australia's productivity through the responsible use of artificial intelligence (AI). Our submission presents key considerations and recommendations for regulatory frameworks, education initiatives, and supportive measures that promote responsible AI practices within small businesses.1. Introduction COSBOA acknowledges the transformative potential of AI for small businesses across Australi"
452,Regsoft,Transcription,"FILENAME: Regsoft Response_ responsible AI.c1848158614f1.pdf | CONTENT: To the Department of Industry,4 August 2023Dear Sir or Madam,Safe and responsible AI consultation Regsoft Pty Ltd (Regsoft) welcomes the Australian Government ambition and desire to regulate the use of advanced technology in a manner that is secure, sustainable and beneficial to all Australians - citizens, businesses and the wider public.Developing a coherent, national and industry-focussed view on the adoption and sustainable use of advanced technologies, including AI. The AI consultation comes at a time when the Australian Government is simultaneously grappling with ● linked data, and consistent data use across the public- and private sector, and ● regulation and adoption of advanced technologies, including AI across many sectors ● multiple regulation touch points across all industries as data- and digital technologies are adopted.Regsoft understands the Government view for ethical AI, and for the ongoing develop"
452,Regsoft,General comment,"Regsoft Pty Ltd (Regsoft) welcomes the Australian Government ambition and desire to regulate the use of advanced technology in a manner that is secure, sustainable and beneficial to all Australians - citizens, businesses and the wider public."
452,Regsoft,Q01 Definitions,Yes
452,Regsoft,Q02 Other AI risks and recommendations for regulation,"The scale of AI impact is exponential. Imperceptible errors in data and small errors in logic, can drive major, negative impacts, as AI systems are used to massively scale out “menial” tasks that would normally be limited by the capacity of humans to deliver them. Whilst malicious AI’s grab headlines - poorly designed, and poorly tested AI’s typically do far greater harm. A human auditor, or a code-reviewer, cannot hope to understand and invalidate the output of the erroneous AI, in a meaningful way, before such a system generates significant harm.The risk model should be built upon a measure of the scale of human impact, and the ability to publicly demonstrate the outcomes of the AI. Where a public demonstration test cases, showing the efficacy of the algorithm or system cannot be provided, the system should be considered inherently high risk if it is to be used in a public setting."
452,Regsoft,Q04 Coordinating AI governance across government,"In 2019, the Australian Human Rights Commission (AHRC) published the Ethical AI Whitepaper, building on the 2018 Human Rights and Technology Issues Paper. Half a decade later, the key observations remain valid. The AHRC recommended developing a separate regulator, and framework, to ensure industry and government adopted trustworthy technology and used it ethically.Regsoft would encourage the development of an independent regulator, whose outputs and intellectual property are openly, and publicly available to all, and whose funding supports full open-source for test services and validation. Such an organisation would be expected to strongly engage with AI researchers and commercial entities around the world, however, it should remain independent of all such entities to ensure it is not conflicted by the commercialisation drives of such groups."
452,Regsoft,Q09 Transparency requirements and scope,"Standards, including test harnesses, should be developed to clarify generic terms such as “best practice” Without such, any guidance is likely to obscure poor practice rather than support improvement. The diagram from pp. 25 of the discussion paper, highlights the Australian tendency to adopt voluntary principles, over the international approach for enforceable standards.The AI Assurance Framework, NSW, provides an excellent example of enforceable standards: The NSW code mandates five principles that any agency must apply for AI-related projects. The document provides clear tests for ongoing validation of such projects.A similar mandate for all ICT and data projects could be adopted federally, for example, as part of the 2-pass ICT business case funding model, and as part of a Digital Transformation Agency (DTA) review."
452,Regsoft,Q11 Actions to increase public trust in AI,"Australian Government agencies have the ability to drive improved models of AI development by the purchasing power of the whole of the Australian Government. This can be developed through a combination of: Establishing mandated requirements for developing AI within government agencies - such as the NSW Government example above. Establishing mandated requirements for any AI that is procured by the Government - or licensed to drive minimum standards for sale of systems.As part of this, the Australian Government can continue to develop, and publish models for AI use, adoption and training, to support best practice use of AI by example."
452,Regsoft,Q14 Risk-based approach to address AI risks,"Designing a machine-readable risk approach: Software design has established approaches for risk mitigation, which are not dependent upon human intervention or human audit. This includes: test-driven design development processes - including Agile continuous integration, testing and improvement deployment validationRegulation of AI should adopt a risk approach that is compatible with software development. This requires regulators to move away from risk matrices, and coloured risk charts, and consider test designs and test harnesses.The work of government and regulation can focus on building, testing and supporting the public adoption of reliable test harnesses and test frameworks. The development of such test frameworks should remain open access to ensure that all AI systems can be meaningfully assessed against known, publicly available standards."
452,Regsoft,Q20 Voluntary vs mandated compliance with risk-based approach,The risk based approach should have mandated elements that apply to public and private organisations.
454,Professor John Swinson,Transcription,"FILENAME: AI Review 2023.3b91fd82ab974.pdf | CONTENT: 4 August 2023 School of LawSubmission in respect Safe and Responsible AI in Australia – Discussion PaperIntroductionI am a professor of law at The University of Queensland.I studied AI (as part of a computer science degree) in 1986. I have worked as a technology lawyer since 1991 in both New York and Australia. I currently work with as a solicitor or advisor to several businesses who use AI in their products.The University of Queensland offers a Masters of Law subject titled Current Issues in Legal Practice - Internet law, AI and Information Technology issues. I am course coordinator of that subject. I also run seminars for barristers and solicitors regarding how to use AI as part of a legal practice.This submission represents my opinion. It does not represent an official position of The University of Queensland or any other entity.I commend the Department of Industry, Science and Resources for conducting the Review. The information"
456,"Luke Thorburn, Thorin Bristow & Liam Carroll",Transcription,"FILENAME: Submission – Thorburn, Bristow, Carroll.9d8f1fe6ba02a.pdf | CONTENT: This document contains a response to the Safe and Responsible AI in Australia discussion paper published in June 2023 by the Australian Government Department of Industry, Science and Resources. The response was written by:● Luke Thorburn, an (Australian) PhD student in safe and trusted AI at King’s College London, whose research focuses on the intersection of algorithmic recommender systems and societal conflict. He is a member of the GETTING-Plurality Research Network within the Edmund & Lily Safra Center for Ethics at Harvard University, and a research affiliate in the Machine Intelligence and Normative Theory Lab at ANU. He also co-authors the Understanding Recommenders project at the Center for Human-Compatible AI at UC Berkeley — which was cited in the recent US Supreme Court case Gonzalez v Google — and has worked with Ofcom on methods for evaluating recommender systems.● Thorin Bristow, a (British) fo"
456,"Luke Thorburn, Thorin Bristow & Liam Carroll",Q01 Definitions,"We caution against the use of the terms “misinformation” and “disinformation”. While in principle these terms are straightforward to define in terms of false information and the intent with which it is shared, to operationalise any of these definitions in legislation or code requires one to make contestable, labour-intensive decisions about what is and isn’t true. This is impossible to do well at scale. A strong indication of the difficulties in assessing truth is that the main organ of the US intelligence community tasked with monitoring the quality of intelligence work — the Analytic Standards and Integrity (AIS) division of the Office of the Director of National Intelligence — does not include accuracy among the criteria it evaluates. Dr Barry Zulauf, former Chief of AIS, states:""We in AIS have not evaluated products for accuracy for 6 or 7 years. In order to put judgements aside to test for later accuracy, they had to be clearly stated, falsifiable, and include a timeframe. THEN we"
456,"Luke Thorburn, Thorin Bristow & Liam Carroll",Q02 Other AI risks and recommendations for regulation,"The emergence of capable generative AI models has rapidly expanded the space of possible risks. It is not clear whether Australia’s existing regulatory approaches are sufficient to address many of these risks. For example, potential risks from AI include (among many others):- Unintended harms from actors who are naïve of AI risks - Adverse feedback loops between human users and algorithmic systems - Production of novel pathogens - Proliferation of fraud - Concentration of power and financial windfalls from AI technologyIn some of these cases there are existing regulations that “cover” the risk (e.g., fraud) but it is not clear whether existing enforcement mechanisms and judicial procedures are sufficiently well-equipped to be able to handle a rapid increase in the scale of harm (e.g. fraud enabled by emerging tools like FraudGPT and DarkBERT).In other cases — such as adverse feedback loops between human users and algorithmic systems — it is not clear what existing regulation would be a"
456,"Luke Thorburn, Thorin Bristow & Liam Carroll",Q03 Non-regulatory actions to address AI risks,"First, the Government could support public education about the limitations of AI systems. We frequently come across cases of well-meaning individuals deciding to implement some form of AI system — such as using language models to automatically grade postgraduate university applications — with simply no awareness that such systems may be biassed or discriminatory. With greater awareness of the limitations of such technologies among individuals who are acting in good faith, considerable harm could be avoided.Second, the Government could fund research into responsible AI practices, to help grow a body of expertise within Australia and limit a brain drain of many of the most talented AI researchers and ethicists moving overseas.RECOMMENDATIONS:- Consider updating school and university curricula to include modules on the basics of AI.- Set aside a percentage of federal AI research funding for research into ethical, legal and social implications."
456,"Luke Thorburn, Thorin Bristow & Liam Carroll",Q04 Coordinating AI governance across government,"We think one promising model is specialisation. Specifically, to create an in-government, public service consultancy akin to the structure of the Office of the Australian Government Actuary, or something analogous to the recently announced UK Civic AI Observatory (https://www.nesta.org.uk/project/the-civic-ai-observatory/), but within government. Such an organisation would be able to provide consistent advice across government departments, and have a concentration of relevant expertise that is much harder to achieve when departments are working in isolation. This model was recently advocated for by a number of leading Australian AI researchers and developers (https://www.australiansforaisafety.com.au/).RECOMMENDATION:- Consider creating an in-government AI governance consultancy that can develop best practice, hire relevant experts, and coordinate AI governance efforts across other parts of government."
456,"Luke Thorburn, Thorin Bristow & Liam Carroll",Q05 International models for AI governance,"A major category of governance measures not discussed in the paper is that of mechanisms for providing democratic oversight of AI systems and AI governance. Recent and ongoing advances in generative AI seem likely to cause enormous societal impacts, and it is right that society be consulted and involved in governing and steering these impacts. Australia has a strong foundation in democratic innovation, notably the work of the newDemocracy Foundation (https://www.newdemocracy.com.au/) who have previously been involved in work providing democratic input to governance of tech platforms. Both local and state governments in Australia have previously convened citizen assemblies on contested topics.Overseas, particularly in the US, there are increasingly large-scale experiments with democratic oversight methods for AI systems. These include the Alignment Assemblies currently being conducted by the Collective Intelligence Project (https://cip.org/blog/alignment), and the recently-awarded OpenA"
456,"Luke Thorburn, Thorin Bristow & Liam Carroll",Q08 Generic vs technology-specific approaches for AI risk,"In general, technology-agnostic regulation tends to be more robust and relevant in the long term than regulation that is framed in terms of particular technologies. Thus, wherever possible, the focus should be on regulating the outcomes we don’t want (systematic bias, discrimination, lack of agency, physical harm, etc.), or on the processes of technological development that are considered ethically acceptable or unacceptable (e.g., specifying the due diligence that we expect those who develop or deploy AI systems to conduct in the process of their work). In some cases, such as recommender systems, it may be much easier to articulate ethically acceptable development processes than to dictate in advance the properties ethically acceptable algorithmic products (https://www.ofcom.org.uk/research-and-data/online-research/evaluating-recommender-systems-in-relation-to-the-dissemination-of-illegal-and-harmful-content-in-the-uk). This is also true because large scale online algorithmic systems "
456,"Luke Thorburn, Thorin Bristow & Liam Carroll",Q11 Actions to increase public trust in AI,"Encouraging more people to use AI — with the motivation of expediting uptake of these technologies — should not be a government priority. The speed of transition will create significant challenges to governance and our ability to collectively adapt as things stand already, without additional pressures to accelerate. Generally speaking, the government should not arbitrarily obstruct or hamper adoption, but nor should they seek to actively accelerate it.That said, the government may wish to take steps to improve access to these technologies among disadvantaged groups, or to direct resources towards applications of AI which are undervalued by the free market. Concretely, this may involve taking the following actions.RECOMMENDATIONS- Prioritise the use of AI in areas where workforce shortages cannot be otherwise mitigated.- Invest in the long-term use of AI for augmenting existing jobs and worker productivity, especially for novice and low-skilled workers.- Promote affordable access to cor"
456,"Luke Thorburn, Thorin Bristow & Liam Carroll",Q15 Benefits and limitations of risk-based approach,"The main benefit of a risk-based approach is that it is, at least in theory, minimally interventionist or bureaucratic, only imposing additional requirements when the level of risk requires it. The main limitation of this approach is that determination of the risk category into which a given use of AI falls will often be subjective or uncertain, and risk bands may be too coarse a categorisation to express the mitigations most appropriate in many contexts.A version of a risk-based approach that relies less on predetermined risk bands and more flexibly allows for experimentation is described in a recent paper on Frontier AI Regulation (https://arxiv.org/abs/2307.03718) and a corresponding blueprint for what such a process might look like in practice (https://miro.com/app/board/uXjVM2FXuDY=/). For managing risks from new kinds of AI systems, or newly large AI systems, this kind of rigorous process might mitigate the biggest risks."
456,"Luke Thorburn, Thorin Bristow & Liam Carroll",Q17 Elements to include in risk-based approach,"Broadly, we support the elements in Attachment C, but acknowledge that they are in the current document quite vague.There are reasonably mature efforts to articulate how many of these elements should be implemented. For example, in the context of reporting/accountability, please see work on data sheets (https://dl.acm.org/doi/10.1145/3458723), model cards (https://dl.acm.org/doi/10.1145/3287560.3287596), and reward reports (https://arxiv.org/abs/2204.10817; https://arxiv.org/abs/2202.05716)."
456,"Luke Thorburn, Thorin Bristow & Liam Carroll",Q19 Application of risk-based approach to general purpose AI,"The risk framework may be applied not to specific technologies, but to specific uses or applications of a technology. For example, use of a language model to summarise search results is likely a lower risk application of the technology than use of a language model to build an autonomous goal-directed system similar to AutoGPT or BabyAGI."
456,"Luke Thorburn, Thorin Bristow & Liam Carroll",Q20 Voluntary vs mandated compliance with risk-based approach,"Above a given threshold of risk, it should be mandated through regulation. It should apply to both public and private organisations, to deployers, and for some kinds of risks, to developers.To clarify that last point, the regulation should apply to developers only insofar as there is risk inherent in the development process. For example, developers of a biassed facial recognition algorithm should, assuming they have honestly communicated its shortcomings, not be held responsible for the decision to deploy it, and any harms that result (in this scenario, it should be the deployers who are at fault). However, some risks — such as the risk of synthesising contagious lethal pathogens or the risk of out-of-control autonomous systems — may be inherent to the development process of some kinds of AI systems, and in situations where this is the case, developers should also be subject to risk-based regulation."
457,University of Wollongong,Transcription,"FILENAME: AI4DM-2022-Challenges copy.2c5dafe766df.pdf | CONTENT: Call for Pilot Project Proposals Artificial Intelligence for Decision Making 2022 InitiativeChallengesThe Artificial Intelligence for Decision Making 2022 Initiative is a national endeavour from the Defence Science and Technology Group (DSTG) and the Office of National Intelligence (ONI), and in collaboration with the Defence AI Centre (DAIC) to fund up to 50 pilot project proposals that will contribute to critically important Artificial Intelligence (AI) and Machine Learning (ML) technology.The Initiative’s call for proposals is “challenge based”, with respondents expected to propose pilot projects that address one of 30 Challenges.Below is a list of challenges that individuals are invited to address.1. How can graph neural networks be applied for causal modelling? Graph neural networks are gaining popularity for the modelling of systems of interacting elements or where one can describe a system with topological structur"
457,University of Wollongong,Transcription,FILENAME: 20220401123026_survey_ba112335 copy.a2ae92f13f2b3.pdf | CONTENT: Confidential Record ID 110 AI4DM-22-110 (Application) Page 1 Artificial Intelligence for Decision Making Initiative 2022: ApplicationThe Artificial Intelligence for Decision Making Initiative 2022 is a collaborative project between the Office of National Intelligence (ONI) and the Defence Science and Technology Group (DSTG) to develop Artificial Intelligence (AI) and Machine Learning (ML) expertise and capability in areas of significant importance to the Australian Defence and national security community.Individuals are invited to apply their skill and expertise to solve one of 30 AI and ML research and development challenges. Please complete all the mandatory fields in this application.Returning to your application You can return to or edit the application prior to the close date by selecting “Save & Return Later”.Make sure you record the Return Code provided to you. You can bookmark the survey page or request 
457,University of Wollongong,General comment,"Numerous academics at the UOW are currently investigating a range of AI-related applications and challenges. This submission represents one initiative as part of the 2022 Call for Pilot Project Proposals Artificial Intelligence for Decision Making 2022 Initiative - a national endeavour from the Defence Science and Technology Group (DSTG) and the Office of National Intelligence (ONI), and in collaboration with the Defence AI Centre (DAIC). Although the grant application was unsuccessful, it may offer some insights into experimental possibilities for debating and supporting responsible AI. This particular project, titled Multi-agent Logistics Strategy and Decision Planning Tool for Simulating Effective Innovative Readiness Training Operations in the Web 3.0 Environment, aimed to develop a proof-of-concept AI tool that could increase efficient engagement with military strategy game contents and navigation through micro and macro decision pathways in training simulations. In turn, this was"
457,University of Wollongong,Q01 Definitions,"Yes. The paper seems thorough in a general sense. However, there is limited discussion on 'creativity' and creative uses of AI platforms, as well as the role AI is playing in the wider creative industries, which contributes $122 billion to Australia’s economy and forms 9% of Greater Sydney’s workforce. There is also a dearth of discussion on and attention to the impacts of AI and copyright. More input from creative industry perspectives and experiences is needed in this wider discussion."
457,University of Wollongong,Q02 Other AI risks and recommendations for regulation,"see previous comments about involving more views on copyright, as well as expertise and practices from the creative industries."
457,University of Wollongong,Q03 Non-regulatory actions to address AI risks,"more towne hall discussions and debates - perhaps a 'road show' traveling series of events. more education outreach and awareness among primary, secondary, and tertiary educators and students. That is, there is an opportunity to include and/or facilitate a stronger learning and teaching focus in these discussions."
457,University of Wollongong,Q05 International models for AI governance,South Korea is an obvious go-to for this.
457,University of Wollongong,Q06 Public vs Private sector governance,yes. open access and user-pays/private scenarios require different approaches.
457,University of Wollongong,Q07 Responsible government use of AI,education. awareness training. increase usage and knowledge competencies.
457,University of Wollongong,Q09 Transparency requirements and scope,"we would benefit from a spectrum of regulatory codes and best practice models - based on the potential severity of impacts resulting from the use of AI. There should be moral, ethical, and legal codes/guidelines, as well as examples of 'out of bound markers' for using AI. It is unclear what this might look like, however, academics at UOW are certainly interested in making contributions to this ongoing discussion."
457,University of Wollongong,Q10 Ban on high-risk AI applications,"a. not banned, but perhaps closely monitored similar to the way genetic cloning is approached."
457,University of Wollongong,Q11 Actions to increase public trust in AI,Offer more research grants to universities and researchers interested in innovating solutions to the 'trust problem'. we can help build trust in AI and the government's approach (or lack thereof) to it -- through a range of novel projects.
457,University of Wollongong,Q12 Trade impact of banning high-risk AI applications,some developing nations rely heavily on facial recognition in many different day-to-day transactions. more research is needed in this area to document relevant case studies for inclusion in this wider ongoing discussion.
457,University of Wollongong,Q14 Risk-based approach to address AI risks,"yes, I personally support a risk-based approach for addressing potential AI risks. Let us identify the risks on a spectrum and then try to deal with them and their severity."
457,University of Wollongong,Q16 Scope and application of risk-based approach,"defence, national security, copyright, IP protection, etc."
458,FinTech Australia,Transcription,"FILENAME: FinTech Australia - Safe and responsible AI submission.33a2384117375.pdf | CONTENT: Safe and responsible AI in Australia discussion paper responseAugust 2023This Submission Paper was prepared by FinTech Australia working with and on behalf of its 450+ Members About this SubmissionFinTech Australia thanks the Australian Government's Department of Industry, Science and Resources for the opportunity to comment on the Discussion Paper entitled “Safe and responsible AI in Australia” (Discussion Paper).This document was created by FinTech Australia in consultation with its members. In developing this Submission, interested members participated in a consultation process to discuss key issues and provided feedback to inform our response to the Discussion Paper.About FinTech AustraliaFinTech Australia is the peak industry body for the Australian fintech sector, representing over 420 fintech companies and startups across Australia. As part of this, we represent a range of businesses in"
459,Australian Institute of Health and Safety,Transcription,"FILENAME: AIHS Submission - Safe and responsible AI - 4 August 2023.6877742f5d7a1.pdf | CONTENT: Think forwardSafe and Responsible AI: Consultation Submission4 August 2023 Acknowledgement of Aboriginal and Torres Strait Islander People We acknowledge the Traditional Owners of Australia and their ongoing strength in practising the world’s oldest living culture. We acknowledge the Traditional Owners of the lands and waters on which we live, learn, play and work, and we acknowledge that sovereignties of these lands and waters were never ceded. We pay our respects to Traditional Owners’ Elders past and present, and commit to supporting them and Indigenous emerging leaders to create more equitable, healthy, and safe workplaces for all Australians, and in particular for those most disadvantaged.Page 2 of 8 About us The Australian Institute of Health and Safety (AIHS) is the national association for people who work in generalist health and safety roles (practitioners and professionals). The A"
459,Australian Institute of Health and Safety,Q01 Definitions,See attached response paper.
459,Australian Institute of Health and Safety,Q02 Other AI risks and recommendations for regulation,See attached response paper.
459,Australian Institute of Health and Safety,Q03 Non-regulatory actions to address AI risks,See attached response paper.
459,Australian Institute of Health and Safety,Q04 Coordinating AI governance across government,See attached response paper.
459,Australian Institute of Health and Safety,Q05 International models for AI governance,See attached response paper.
459,Australian Institute of Health and Safety,Q06 Public vs Private sector governance,See attached response paper.
459,Australian Institute of Health and Safety,Q07 Responsible government use of AI,See attached response paper.
459,Australian Institute of Health and Safety,Q08 Generic vs technology-specific approaches for AI risk,See attached response paper.
459,Australian Institute of Health and Safety,Q09 Transparency requirements and scope,See attached response paper.
459,Australian Institute of Health and Safety,Q10 Ban on high-risk AI applications,See attached response paper.
459,Australian Institute of Health and Safety,Q11 Actions to increase public trust in AI,See attached response paper.
459,Australian Institute of Health and Safety,Q12 Trade impact of banning high-risk AI applications,See attached response paper.
459,Australian Institute of Health and Safety,Q13 Changes to conformity infrastructure,See attached response paper.
459,Australian Institute of Health and Safety,Q14 Risk-based approach to address AI risks,See attached response paper.
459,Australian Institute of Health and Safety,Q15 Benefits and limitations of risk-based approach,See attached response paper.
459,Australian Institute of Health and Safety,Q16 Scope and application of risk-based approach,See attached response paper.
459,Australian Institute of Health and Safety,Q17 Elements to include in risk-based approach,See attached response paper.
459,Australian Institute of Health and Safety,Q18 Integration of risk-based approach into existing frameworks,See attached response paper.
459,Australian Institute of Health and Safety,Q19 Application of risk-based approach to general purpose AI,See attached response paper.
459,Australian Institute of Health and Safety,Q20 Voluntary vs mandated compliance with risk-based approach,See attached response paper.
461,Dr. Zofia Bednarz (University of Sydney) and Linda Przhedetsky (UTS),Transcription,"FILENAME: Submission 461 - Dr. Zofia Bednarz (University of Sydney) and Linda Przhedetsky (UTS) - 2-Aug-23.6ce68a317b21a.pdf | CONTENT: SUBMISSION IN RESPONSE TO THE SAFE AND RESPONSIBLE AI IN AUSTRALIA DISCUSSION PAPER 02 AUGUST 2023Ms Linda Przhedetsky University of Technology SydneyDr Zofia Bednarz University of Sydney and ARC Centre of Excellence on Automated Decision-Making and SocietyThank you for the opportunity to participate in the consultation on the Discussion Paper Safe and Responsible AI in Australia (June 2023). This submission responds to the Discussion Paper by bringing to your attention a book chapter we co-authored in the forthcoming collection Money, Power and AI: Automated Banks and Automated States (edited by Zofia Bednarz and Monika Zalnieriute, Cambridge University Press 2023). Our chapter, ‘AI Opacity in Financial Industry and How to Break It’, contains findings relevant to questions posed in the Discussion Paper. We note that this submission is made in our pers"
461,Dr. Zofia Bednarz (University of Sydney) and Linda Przhedetsky (UTS),Transcription,"FILENAME: Submission 461 - Attachment 1.36faa00c77045.pdf | CONTENT: IN-PRESS MANUSCRIPT Zofia Bednarz and Linda Przhedetsky, ‘AI Opacity in Financial Industry and How to Break It’ in Zofia Bednarz and Monika Zalnieriute, Money, Power and AI: Automated Banks and Automated States (Cambridge University Press forthcoming).AI Opacity in Financial Industry and How to Break ItZofia Bednarz1 and Linda Przhedetsky2AbstractThis chapter closes Part 1 by analysing how the opacity surrounding the use of AI and ADM tools by financial corporations is enabled, and even encouraged by the law. As other chapters in the book demonstrate, such opacity brings about significant risks to fundamental rights, consumer rights and the rule of law. Analysing examples from jurisdictions including the US, UK, EU and Australia, Bednarz and Przhedetsky unpack how financial entities often rely on rules and market practices protecting corporate secrecy such as complex credit scoring systems, proprietary rights to AI mo"
462,Asia Internet Coalition (AIC),Transcription,"FILENAME: Submission 462 - XXX- Asia Internet Coalition - 3-Aug.d971afbe36935.pdf | CONTENT: Asia Internet Coalition (AIC) Industry Submission on Australia’s Responsible AI Discussion Paper3 August 2023To The Department of Industry, Science and Resources (Technology Strategy Branch) Government of AustraliaOn behalf of the Asia Internet Coalition (AIC) and its members, we are respectfully submitting our recommendations on Australia’s Responsible Artificial Intelligence (AI) Discussion Paper consultation “Discussion Paper”. AIC is an industry association of leading internet and technology companies in the Asia Pacific region with a mission to promote the understanding and resolution of Internet and ICT policy issues in the Asia region.The Discussion paper on AI is a pivotal milestone for transforming Australia into a knowledge-based economy as it spells out a strategy to establish an ecosystem necessary for AI adoption by harnessing an agile framework for addressing different aspects of "
464,National Australia Bank,Transcription,"FILENAME: Submission 464 - National Australia Bank - 4-Aug.28aeafb368fea.pdf | CONTENT: NAB Submission on “Safe and Responsible AI in Australia” Discussion Paper National Australia Bank Ltd (NAB) welcomes the opportunity to provide a submission to the Department of Industry, Science and Resources (DISR) on the Safe and Responsible AI in Australia discussion paper (the Paper).Recognising the benefits of this technology, NAB is supportive of the intention to reduce the potential risk from AI usage and support safe and responsible AI practices. This submission is focused on four key areas:1. Definitions 2. Regulatory approach 3. Industry standards 4. Exploring the broader AI implicationsBackground NAB sees significant benefits from the use of AI in certain areas; better customer service (for example faster and more effective credit decisions), more personalised financial support, tools to increase the efficiency of our staff and the potential for substantial cost reductions. These opportu"
465,Laing O'Rourke Australia,Transcription,"FILENAME: Submission 465 - LaingORourke - 4-Aug.81a80af61eaad.pdf | CONTENT: Our Ref: Inquiry into safe and responsible AI in Australia4 August 2023To the Department of Industry and ScienceLaing O'Rourke thanks you for the opportunity to participate in the Federal Government’s consultation with industry to support safe and responsible AI practices in Australia.BackgroundLaing O'Rourke is a global engineering and construction company. In Australia, we focus on delivering major transport infrastructure projects, buildings at the transport interface, and infrastructure for the Department of Defence. Laing O'Rourke is committed to the responsible use of AI and predictive analytics in our business and industry, and we offer our comments to contribute to the design of an Australian AI policy that can cover general principles, but that can also respond to each industry's specific requirements.BackgroundOur company recently formulated and adopted an AI and predictive analytics strategy for our"
467,APRA AMCOS,Transcription,"FILENAME: Submission 467 - APRA AMCOS - 4-Aug.405be33893362.pdf | CONTENT: 4 August 2023The Hon Ed Husic MP Technology Strategy Branch Department of Industry, Science and Resources GPO Box 2013 Canberra ACT 2601BY EMAIL - digitaleconomy@industry.gov.auDear MinisterCONSULTATION ON SAFE AND RESPONSIBLE AI IN AUSTRALIA1. As Australia’s largest music industry body representing the rights of over 115,000 songwriters, composers and music publishers across Australasia, APRA AMCOS welcomes the opportunity to make a submission to the Department of Industry, Science and Resources (Department) consultation on safe and responsible AI in Australia in response to the Discussion Paper released 1 June 2023 (Discussion Paper).2. APRA AMCOS consists of the Australasian Performing Right Association and the Australasian Mechanical Copyright Owners Society. The membership is diverse, ranging from unpublished writers to major music publishers. Licensee stakeholders include all major broadcasters and video o"
468,Responsible Innovation Group Pty Ltd,Transcription,"FILENAME: Submission 468 - Responsible Innovation Group Pty Ltd - 5-Aug.e69f6132f3f2c.pdf | CONTENT: Response to Call for Submissions: Safe and responsible AI in Australia discussion paper.Based upon our experience in designing risk management frameworks for AI-based capabili:es and advising Defence industry on the lawful design and use of AI capabili:es in a military context, including authoring the Trusted Autonomous Systems’ Responsible for AI in Defence Toolkit, we oﬀer the following speciﬁc observa:ons.Civilian principles must inform military principlesWe assess that a large number of AI uses will be ‘dual-use’, or capable of being interconnected with military systems, and that the outlier use cases will be exclusively military, based on our experience in developing the Responsible AI for Defence Toolkit.For example, the use of AI in facilita:ng at SmartGates has interoperability considera:ons that may be directly relevant to Australia’s military forces in the following poten:al s"
470,Australian Information Security Association (AISA),Transcription,"FILENAME: Submission 470 - Australian Information Security Association - 7-Aug.ab068e71e414b.pdf | CONTENT: Supporting responsible AI discussion paperSUBMISSION EXECUTIVE SUMMARY AISA The Australian Information Security Association (AISA) champions the development of a robust information security sector by building the capacity of professionals and advancing the cyber security and safety of the Australian public as well as businesses and governments in Australia.Established in 1999 as a nationally recognised and independent not-for-profit organisation and charity, AISA has become the recognised authority and industry body for information security, cyber security, and security-related privacy matters in Australia. AISA caters to all domains of the information security industry with a particular focus on sharing expertise from the field at meetings, focus groups and networking opportunities around Australia.AISA’s vision is a world where all people, businesses and governments are educate"
471,"Digital Media Research Centre, QUT",Transcription,"FILENAME: Submission 471 - Digital Media Research Centre QUT - 7-Aug.60339f4fe2c1d.pdf | CONTENT: Discussion Paper Submission Safe and Responsible AI in Australia | 4 August 2023Rhyle Simcock, Nicholas Godfrey, A/Professor Anna Huggins, Professor Mark Burdon School of Law/Digital Media Research Centre, Queensland University of Technology*Overview We thank the Department of Industry, Science and Resources for the opportunity to make a submission in response to the Safe and Responsible AI in Australia discussion paper. We recognise the range of potential advantages that the adoption of AI can bring to both the public and private sectors. However, appropriate governance mechanisms are critical to ensure the responsible development of AI and to mitigate the accompanying risks. Accordingly, Australia’s existing regulatory frameworks will need to evolve to accommodate the novel challenges posed by AI, and to ensure that adequate mechanisms are in place to address potential harms associated w"
473,Australian Retailers Association,Transcription,"FILENAME: Submission 473 - Australian Retailers Association - 8-Aug.fac1f5398b9c5.pdf | CONTENT: SUBMISSION RESPONSIBLE USE OF AI JULY 2023The Australian Retailers Association (ARA) welcomes the opportunity to provide comments in response to the Department of Industry, Science and Resources - Discussion Paper on the safe and responsible use of artificial intelligence in Australia.The ARA is Australia’s oldest, largest and most diverse industry association for retail, representing a $400 billion sector that employs 1.3 million Australians. As Australia’s peak retail body, representing more than 120,000 retail shop fronts, the ARA informs, advocates, educates, protects and unifies independent, national and international retail members.We represent the full spectrum of Australian retail, from our largest national and international retailers to our small and medium sized members, who make up 95% of our membership. Our members operate across all categories - from food to fashion, hairdressi"
474,"Centre for Media Transition, UTS",Transcription,"FILENAME: Submission 474 - Centre for Media Transition - 8-Aug.272c9ecbe46a4.pdf | CONTENT: CENTRE FOR MEDIA TRANSITIONSafe and responsible AI in Australia Discussion Paper, June 2023Submission from the Centre for Media TransitionDate: 8 August 2023 About the Centre for Media TransitionThe Centre (CMT) was established in 2017 as an applied research unit based at the University of Technology Sydney (UTS). It is an interdisciplinary initiative of the Faculty of Arts and Social Sciences and the Faculty of Law, sitting at the intersection of media, journalism, technology, ethics, regulation and business. Working with industry, academia, government and others, the CMT aims to understand media transition and digital disruption, with a view to recommending legal reform and other measures that promote the public interest. In addition, the CMT aims to assist news media to adapt for a digital environment, including by identifying potentially sustainable business models, develop suitable ethical "
476,"Human Technology Institute, University of Technology Sydney (UTS)",Transcription,"FILENAME: Submission 476 - Human Technology Institute UTS - 9-Aug.6238b699d46a.pdf | CONTENT: 9 August 2023Human Technology Ins1tuteHuman TechnologyTEXT Ins1tuteDepartment of Department of Industry, Science and Resources Discussion Paper, ‘Safe and responsible AI in Australia’Industry, Science Submission Human Technology Institute, UTSand Resources August 2023Discussion Paper, 9 August 2023About the Human Technology Institute The Human Technology Institute (HTI) is building a future that applies human values to new technology. HTI embodies the strategic vision of the University of Technology Sydney (UTS) to be a leading public university of technology, recognised for its global impact specifically in the responsible development, use and regulation of technology. HTI is an authoritative voice in Australia and internationally on human-centred technology. HTI works with communities and organisations to develop skills, tools and policy that ensure new and emerging technologies are safe, fa"
477,Stirling and Rose,Transcription,"FILENAME: Submission 477 - Stirling and Rose - 9-Aug.8ea23375ad75c.pdf | CONTENT: SAFE AND RESPONSIBLE AI IN AUSTRALIAAUGUST 2023Natasha Blycha Schellie-Jayne Price James Myint Ty Haberland W W W . S T I R L I N G A N D R O S E . C O M Director Technology Strategy Branch Department of Industry, Science and Resources Industry House Canberra ACT 2601Dear Department of Industry, Science and Resources,Stirling & Rose (S&R) welcomes the Department’s discussion paper and supports the Department’s commitment to Safe and Responsible AI.Stirling & Rose is a boutique emerging technology law firm and global legal policy institute with deep experience in the subject matter of digital assets, artificial intelligence, smart legal contracts and autonomous organisations (including their sub-set decentralised autonomous organisations). We advise organisations seeking to use AI responsibly, major investors (hedge funds, VC funds, and lenders) and entrepreneurial organisations operating at the vanguard o"
478,Meta,Transcription,"FILENAME: Submission 478 - Meta - 9-Aug.ca0e4157f039.pdf | CONTENT: Meta’s Submission on Safe & Responsible AI in Australia Discussion PaperAUGUST 20231 Executive summary Meta welcomes the opportunity to contribute to the Australian Department of Industry, Science and Resources’ consultation in response to the issues paper Safe and Responsible AI in Australia.Given recent breakthroughs in artificial intelligence (AI) , and generative AI in particular, have captured the public’s imagination and demonstrated the potential to help people do incredible things, create a new era of economic and social opportunities, and give individuals, creators, and businesses new ways to express themselves and connect with people – it is timely for a review of Australia’s regulatory and governance responses in relation to AI.Before discussing governance options in relation to AI, to assist the Department with its consultation, we wanted to first share some background about how Meta uses AI and our approac"
479,The University of Western Australia Tech & Policy Lab,Transcription,"FILENAME: Submission 479 - UWA Tech Policy Lab - 9-Aug.93c00e8147ca9.pdf | CONTENT: This submission responds to the Department of Industry, Science and Resources’ Discussion Paper, Safe and Responsible AI in Australia, published in June 2023. The Discussion Paper seeks feedback on possible governance and regulatory responses to ensure “AI is used safely and responsibly”. Drawing on our expertise as interdisciplinary researchers with track records in the development, deployment, and regulation of artificial intelligence (AI) and automated decision- making systems, we have focused on how the Australian Government can proactively manage AI to best ensure that it serves the needs of all Australians.1. AI’s potential justifies a pro-innovation approach that is not about being rapid or reckless, but about prioritising societally beneficial uses of AI that ensure public trust and confidence through trustworthy behaviour. A key element of trustworthy behaviour is evidence-based and contextual "
480,elevenM Consulting,Transcription,FILENAME: Submission 480 - elevenM - 10-Aug.5f1a8e9989add.pdf | CONTENT: Safe and responsible AI in Australia: Discussion paper Submission from elevenM9 August 2023 www.elevenm.com ABN 77 610 787 534 Safe and responsible AI in Australia: Discussion paper | Submission from elevenMContents Introduction ......................................................................................................................... 2 About elevenM ................................................................................................................... 2 Submission ......................................................................................................................... 3 Definitions .......................................................................................................................... 3 Potential gaps in approaches ............................................................................................. 4 Responses suitable for Australia ............
481,Monash University,Transcription,"FILENAME: Submission 481 - Monash Uni - 11-Aug.f853402e02b14.pdf | CONTENT: +- SAFE AND RESPONSIBLE AI IN AUSTRALIA August 2023INTRODUCTIONThe rise of Artificial Intelligence is a disruptive technological development that touches on all aspects of life including social, work, and education. Monash University academics are developing, researching and using AI across a range of disciplines including IT, engineering, law, ethics, industry development and global competition; in our educational practices and governance; and as educational and research leaders in terms of our own organisation response. Our submission to this consultation is drawn from all these perspectives. We note the following people with particular expertise who are available for further consultation: • Professor Chris Marsden: Professor of AI and the Law, Associate Director Data Futures Institute • Professor Geoff Webb: Professor of Data Science and AI • Associate Professor Tim Fawns: Monash Education Academy • Dr Paul "
482,Australian Copyright Council,Transcription,"FILENAME: Submission 482 - Australian Copyright Council - 11-Aug.03184d624715c.pdf | CONTENT: Australian Copyright Council response to the Safe and responsible AI in Australia: Discussion Paper11 August 2023The Australian Copyright Council acknowledges the Traditional Owners and Custodians of the lands on which our office is located, the Gadigal people of the Eora nation, and all Traditional Owners of Country throughout Australia. We pay our respects to all Elders past, present and emerging and recognise their continuing great, creative and cultural expressions.1. The Australian Copyright Council (the ACC) is grateful for the opportunity to make a submission on the Department of Industry, Science and Resources Discussion Paper on Safe and Responsible AI (the Discussion Paper).About the Australian Copyright Council 2. The ACC is a small, independent, not-for-profit organisation dedicated to promoting understanding of copyright law and its application. The ACC works to foster collaborati"
483,Business Council of Australia,Transcription,FILENAME: Submission 483 - Business Council of Australia - 11-Aug.de42e61a2279a.pdf | CONTENT: Safe and Responsible AI in AustraliaAugust 2023 Contents 1. Overview ........................................................................................................................................................................... 2 2. Key recommendations ................................................................................................................................................. 3 3. Artificial Intelligence in Australia ................................................................................................................................ 3 4. A principled response to AI ......................................................................................................................................... 5 4.1 Don’t focus solely on the technology.............................................................................................................
484,Australian Psychological Society,Transcription,"FILENAME: Submission 484 - Australian Psychological Society - 11-Aug.0342ca3980ac3.pdf | CONTENT: 11 August 2023Technology Strategy Branch Department of Industry, Science and Resources GPO Box 2013 Canberra ACT 2601Submitted via upload to: https://consult.industry.gov.au/supporting-responsible-aiDear Technology Strategy Branch,APS Response to the Safe and Responsible AI in Australia discussion paper The Australian Psychological Society (APS) appreciates the opportunity to respond to the Commonwealth Department of Industry, Science and Resources’ discussion paper about Safe and Responsible AI in Australia. Rather than addressing the questions in the discussion paper in detail, many of which are outside of the remit of the APS, we have provided broader insights and recommendations about safe and responsible AI. The APS is the leading professional body for psychologists in Australia. We are dedicated to advancing the scientific discipline and ethical practice of psychology and work to rea"
485,Woolworths Group,Transcription,"FILENAME: Submission 485 - Woolworths Group - 11-Aug.5c4cca18dba11.pdf | CONTENT: 11 August 2023Via email: DigitalEconomy@industry.gov.auWoolworths Group submission in response to the Safe and Responsible AI in Australia Discussion PaperDear Sir/Madam,Woolworths Group Limited (Woolworths) welcomes the opportunity to make a submission to the public consultation on the Safe and Responsible AI in Australia Discussion Paper (the paper).Woolworths was founded in 1924 and has a proud history of serving Australian communities. We are one of Australia’s largest retailers and private sector employers, with over 180,000 team members. We serve over 20 million customers each week nationwide across more than 1,250 Woolworths Supermarkets, Woolworths Metro and BIG W stores.This is supported by our extensive logistics and distribution networks which include 21 distribution centres across the country. We offer a range of consumer services and our WPay payments business facilitated over one billion car"
486,Arts Law Centre of Australia,Transcription,"FILENAME: Submission 486 - Arts Law Centre of Australia - 11-Aug.14a0fc6d408b1.pdf | CONTENT: 11 August 2023Regulation of Emerging Technologies Team Department of Industry, Science and ResourcesBy email: digitaleconomy@industry.gov.auDear Regulation of Emerging Technologies TeamSafe and responsible AI in Australia: Discussion paperArts Law is grateful for the opportunity to response to the Department of Industry, Science and Resources’ discussion paper, Safe and responsible AI in Australia: Discussion paper (Discussion Paper).Arts Law was established in 1983. It is a national community legal centre providing free or low-cost legal advice to Australian artists and arts organisations. Each year, Arts Law assists thousands of Australian artists and arts organisations across all Australian states and territories. Arts Law would like to acknowledge the Traditional Owners of the various lands on which Arts Law works and pay our respects to Elders past and present.Our submission is informed b"
487,Australian Red Cross,Transcription,"FILENAME: Submission 487 - Australian Red Cross - 11-Aug.643ec9f4af0d7.pdf | CONTENT: Department of Industry, Science and Resources (DISR) Supporting responsible AI (Artificial Intelligence) Date: 7 August 20231 About us Australian Red Cross has been a critical part of Australian life since 1914, and is established by Royal Charter of 1941 as an auxiliary to Australia’s public authorities in the humanitarian field including during emergencies and armed conflict. Australian Red Cross is one of 192 National Red Cross and Red Crescent Societies that, together with the International Committee of the Red Cross (ICRC) and International Federation of Red Cross and Red Crescent Societies (IFRC), make up the International Red Cross and Red Crescent Movement (the Movement).The Movement is guided at all times and in all places by seven Fundamental Principles: Humanity, Impartiality, Neutrality, Independence, Voluntary Service, Unity and Universality. These principles sum up our ethics and are at "
488,Telstra,Transcription,"FILENAME: Submission 488 - Telstra - 11-Aug.70ee9872eff3d.pdf | CONTENT: TELSTRA SUBMISSION SAFE AND RESPONSIBLE AI IN AUSTRALIA DISCUSSION PAPERTELSTRA GROUP LIMITEDSafe and responsible AI in AustraliaPublic Submission11 August 2023TELSTRA GROUP LIMITED (ABN 56 650 620 303) PAGE 1Confidential TELSTRA SUBMISSION SAFE AND RESPONSIBLE AI IN AUSTRALIA DISCUSSION PAPERSummary 3 Key Recommendations: 3 Introduction 5 Definitions 5 AI Governance Framework in Australia 6 Australia should adopt a nationally recognised AI body 7 Data and AI must be considered together 7 AI and Law Enforcement 8 International Collaboration 8 Risk-based Approaches 9 Trust and Responsibility 9 Workforce Upskilling 10 Consultation Questions 12TELSTRA GROUP LIMITED (ABN 56 650 620 303) PAGE 2Confidential TELSTRA SUBMISSION SAFE AND RESPONSIBLE AI IN AUSTRALIA DISCUSSION PAPERSummaryTelstra welcomes the opportunity to provide feedback on the Department of Industry, Science and Resources discussion paper on Safe and Re"
489,Interactive Games & Entertainment Association (IGEA),Transcription,"FILENAME: Submission 489 - Interactive Games & Entertainment Association - 11-Aug.ce6b6d8f2b8c8.pdf | CONTENT: Submission to the Department of Industry, Science and ResourcesResponse to safe and responsible AI in Australia Discussion PaperAugust 2023We acknowledge the Traditional Custodians of Country throughout Australia and their continuing connection to the land and sea. We pay our respects to all Aboriginal and Torres Strait Islander peoples, their cultures and to their elders past and present. Introduction & Overview About IGEA The Interactive Games & Entertainment Association (IGEA) welcomes the opportunity to provide a submission to the Department of Industry, Science and Resources on the Safe and Responsible AI in Australia discussion paper (‘discussion paper’). The discussion paper seeks advice on mitigating possible risks of implementing artificial intelligence (AI) in Australia and gaps in the current domestic regulatory landscape. As such, we are pleased to provide a submis"
490,National Association for the Visual Arts (NAVA),Transcription,"FILENAME: Submission 490 - National Association for the Visual Arts - 11-Aug.e58712652a70f.pdf | CONTENT: Department of Industry, Science and Resources Technology Strategy BranchDigitalEconomy@industry.gov.auAugust 2023____________________________________________________________________________Re: Supporting Safe and Responsible AI in AustraliaNAVA welcomes the opportunity to respond to the Safe and responsible AI in Australia Discussion paper. ____________________________________________________________________________ The National Association for Visual Arts (NAVA) acknowledges the Gadigal, Wangal, Dharug, Dharawal, Ngunnawal, Ngambri, Dja Dja Wurrung, Kabi Kabi and Jinibara peoples as the Traditional Custodians and knowledge-holders of the lands on which we live, learn and work. The NAVA community is based across hundreds of sovereign Nations and unceded lands throughout the continent that has become colonially known as Australia. NAVA pays our deepest respects to all Custodians of "
491,Amazon Web Services,Transcription,"FILENAME: Submission 491 - Amazon Web Services - 11-Aug.da36c540a75d1.pdf | CONTENT: Safe and responsible AI in Australia AMAZON SUBMISSION Introduction Amazon is pleased to comment on the Safe and Responsible AI in Australia Discussion Paper. Artificial intelligence (AI) and machine learning (ML) are potentially the most transformational technologies of our time and, at Amazon, we believe it is still early days for the technology. Recent advances in generative AI have provided the world with a glimpse of the new opportunities AI will create, but the full potential of the technology has yet to be unlocked. Amazon’s perspectives on AI are informed by our dual role as a developer of AI technology and a deployer of AI tools and services. Our focus on AI and ML spans over 25 years - Amazon Web Services (AWS) offers the broadest and deepest set of AI and ML services for cloud customers, empowering developers to build, train, and deploy their own ML models or easily incorporate pre- trained "
492,OpenAI,Transcription,"FILENAME: Submission 492 - OpenAI - 14-Aug - Copy.8e6c605d90098.pdf | CONTENT: 1960 Bryant Street San Francisco, CA 94110Friday, August 11, 2023Comment on DISR Paper on Safe and Responsible AIOpenAI appreciates the opportunity to provide comments on the “Safe and Responsible AI in Australia” discussion paper shared by the Department of Industry, Science, and Resources (DISR).As the DISR recognizes in its report, AI presents significant economic and social opportunities for Australia, including through applications in healthcare, engineering, and legal services. Despite these potential benefits, AI also brings considerable challenges, including risks of misuse, inaccuracies, and potentially dangerous capabilities.We welcome Australia’s framing of this discussion in terms of AI safety, accountability, harmonization, and risk mitigation. We agree that “a coordinated and coherent response from the government to emerging issues,” one based on transparency, evaluation, and accountability, wo"
493,Commercial Radio & Audio,Transcription,"FILENAME: Submission 493 - Commercial Radio Australia - 11-Aug.77502e3803a97.pdf | CONTENT: SUBMISSION BY COMMERCIAL RADIO & AUDIO IN RESPONSE TO SAFE AND RESPONSIBLE AI IN AUSTRALIA DISCUSSION PAPER A. About CRA CRA is the industry body representing the interests of commercial radio broadcasters throughout Australia. CRA has 261 member stations and represent the entire Australian commercial radio industry. 220 of CRA’s member stations are based in regional and rural areas.Commercial radio continues to dominate commercial listening:• 88% of Australians aged 10 to 24 tune in weekly to commercial radio, for an average of 10 hours and 15 minutes per week;• breakfast radio attracts nearly 8.62 million listeners; and• 3.29 million listeners stream commercial radio weekly.1The Deloitte Access Economics 2023 Connecting Communities Report2 highlights the important economic and social contribution that our members make to Australia, through the provision of radio and audio services. Our members"
494,Dr Bernadette Hyland-Wood and the QUT Centre for Data Science,Transcription,"FILENAME: Submission 494 - HYLAND-WOOD, Bernadette QUT Centre for Data Science - 11-Aug.3380594c4bf05.pdf | CONTENT: Submission in response to the Discussion Paper on Safe and Responsible AI in AustraliaLead author Dr Bernadette Hyland-WoodCo-lead Responsible Data Science Program Queensland University of Technology Brisbane QLD AustraliaE. b.hylandwood@qut.edu.auSubmission due date: 04 Aug 2023 Table of contentsAbout the QUT Centre for Data Science....................................................................................3 Our interest in supporting safe & responsible AI.....................................................................3 Executive Summary.....................................................................................................................4 Recommendations.......................................................................................................................5 The Geopolitical Landscape of Digital Platforms........................"
495,Minervai 4.0,Transcription,"FILENAME: Submission 495 - Minervai 4.0 - 12-Aug.69c11876bac05.pdf | CONTENT: Minervai 4.0 Policy SubmissionRESPONSE TO THE DEPARTMENT OF INDUSTRY, SCIENCE AND RESOURCES SUPPORTING RESPONSIBLE AI: DISCUSSION PAPERAugust 2023 Introduction & OverviewMinervai 4.0 is a youth-led, volunteer-based group advocating for the ethical and responsible development and use of technology moving into the “Fourth Industrial Revolution” or “Industry 4.0”. We are made up of volunteers aged 18 to 28 and are passionate about ensuring that technologies inherited by today’s young people embody principles of inclusion, care and intersectionality.Our submission provides a non technical perspective to the regulation of Artificial Intelligence (AI) in Australia. We note that our submission focuses on social and ethical implications of AI, rather than its technical creation and application. We believe AI will be a fundamental technology to our generation, and we want to ensure its development and deployment is re"
496,Electronic Frontiers Australia,Transcription,"FILENAME: Submission 496 - Electronic Frontiers Australia - 13-Aug.75d3276157804.pdf | CONTENT: Electronic Frontiers Australia Inc. ABN: 35 050 159 188 W www.efa.org.au E email@efa.org.au @efa_ozSupporting responsible AI Department of Industry, Science and Resources: Technology Strategy Branch13 August 2023By web formTo the Department,RE: Safe and Responsible AIEFA welcomes the opportunity to comment on the Safe and Responsible AI consultation.EFA’s submission is contained in the following pages.About EFAEstablished in January 1994, EFA is a national, membership-based, not-for-profit organisation that promotes and protects human rights in a digital context.EFA is independent of government and commerce, and is funded by membership subscriptions and donations from individuals and organisations with an altruistic interest in promoting civil liberties in the digital context.EFA members and supporters come from all parts of Australia and from diverse backgrounds. Our major objectives are to"
497,Tech Council of Australia,Transcription,"FILENAME: Submission 497 - Tech Council of Australia - 13-Aug.b1687db24465a.pdf | CONTENT: Tech Council of Australia www.techcouncil.com.au SuSupporting Safe and Responsible AI Tech Council of Australia SubmissionAugust 2023 techcouncil.com.au Tech Council of Australia www.techcouncil.com.au SuExecutive Summary Artificial Intelligence (AI) is one of the most transformative technologies of our time, with the potential to offer our nation significant economic, social, environmental and strategic advantages. AI is already driving major advancements in areas such as healthcare, industry and public safety, and the future holds even greater potential. Generative AI alone could add between $45 billion - $115 billion a year to the Australian economy by 2030.In a period of high inflation, and low productivity growth, an uplift from productivity enhancing technologies such as AI is vital to preserve Australia’s standard of living and ensure that we maintain a competitive edge in the global lands"
498,Australian Banking Association,Transcription,"FILENAME: Submission 498 - Australian Banking Association - 14-Aug.b5828a615d20e.pdf | CONTENT: 14 August 2023Technology Strategy Branch Department of Industry, Science and Resources GPO Box 2013 Canberra ACT 2601Submitted via email: DigitalEconomy@industry.gov.auABA submission – safe and responsible use of AI Summary Banks have a history of using technology to improve customer service and enhance the security of banking. AI is already providing more tailored services and assistance to people, making some services more cost effective and/or accessible, improving the efficiency of some administrative or analytical work, and has potential to improve the consistency and quality of outcomes for consumers. Since AI is being used to target consumers in the commission of technology-enabled crimes, it is also important that banks and other legitimate actors can protect consumers by implement AI solutions which can better identify and address technology enabled crimes such as cybercrime and dig"
499,IAB Australia,Transcription,"FILENAME: Submission 499 - Interactive Advertising Bureau Australia - 15-Aug.9076f287a7119.pdf | CONTENT: Submission by the Interactive Advertising Bureau (IAB) AustraliaSafe and responsible AI in Australia - Discussion PaperDepartment of Industry, Science and ResourcesAugust 20231 ContentsAbout IAB Australia ................................................................................................................................. 3 Executive Summary................................................................................................................................. 4 1. Introduction ........................................................................................................................................ 5 1.1 Overview of digital ad industry ......................................................................................................... 5 1.2 Approach taken in this submission ......................................................................."
500,Commonwealth Bank of Australia,Transcription,FILENAME: Submission 500 - Commonwealth Bank - 15-Aug.36c2cc1cb32ac.pdf | CONTENT:               This text has been automatically transcribed for accessibility. It may contain transcription errors. Please refer to the source file for the original content.
501,Department of Health and Aged Care,Transcription,"FILENAME: Submission 501 - Department of Health and Aged Care - 22-Aug.0dfeb1d9f0267.pdf | CONTENT: Department of Health and Aged Care Safe and Responsible AI in Australia SubmissionPreambleThe Department of Health and Aged Care (the Department) welcomes the discussion paper released by the Department of Industry, Science and Resources (DISR) on Safe and Responsible AI in Australia, and provides the following submission:Artificial Intelligence (AI) is an emerging capability that has the potential to transform wide areas of the economy and improve lives. It is expected to have significant impacts in the healthcare sector.AI is advancing quickly and will likely generate disruptive innovation across many parts of society which has witnessed significant advancements and applications of AI in recent years. AI experts, journalists, policy makers and the public are increasingly discussing a broad spectrum of important and urgent risks from AI. The Centre for AI Safety recently called for glob"
502,Shopping Centre Council of Australia,Transcription,"FILENAME: Submission 502 - Shopping Centre Council of Australia - 15-Aug.c4aaa6c71e012.pdf | CONTENT: 15 August 2023Technology Strategy Branch Department of Industry, Science and Resources GPO Box 2013 Canberra ACT 2601By email: DigitalEconomy@industry.gov.auSafe and Responsible AI in AustraliaWe welcome the opportunity to respond to the Department’s Discussion Paper Safe and Responsible AI in Australia. The SCCA is the national industry group for owners and operators of shopping centres.We note that the closing date for submissions was Friday, 4 August 2023. Due to other commitments we were unable to make a submission by that date. We respectfully request that this submission is accepted and considered as part of the Department’s consideration and advice to Government on this issue.OVERVIEW We are cognisant that the Department’s consultation – and any policy or regulation that may flow from it – may impact our sectors’ use of equipment and technologies (including AI) associated with s"
503,Digital Industry Group Inc (DIGI),Transcription,"FILENAME: Submission 503 - Digital Industry Group Inc - 16-Aug.51db0169ac929.pdf | CONTENT: Department of Industry, Science and Resources, Technology Strategy Branch By email: DigitalEconomy@industry.gov.auWednesday, 16 August 2023Dear Minister Husic,The Digital Industry Group Inc. (DIGI) thanks you for the opportunity to provide our views on the Safe and responsible AI in Australia Discussion paper.By way of background, DIGI is a non-profit industry association that advocates for the interests of the digital industry in Australia. DIGI’s founding members are Apple, eBay, Google, Linktree, Meta, Snap, Spotify, TikTok, X (f.k.a Twitter), and Yahoo. DIGI’s vision is a thriving Australian digitally-enabled economy that fosters innovation, a growing selection of digital products and services, and where online safety and privacy are protected.DIGI welcomes the work of the Department of Industry, Science and Resources to realise Australia’s potential as a responsible and inclusive Artificial"
504,Law Council of Australia,Transcription,"FILENAME: Submission 504 - Attachment 1.a4de805e53e34.pdf | CONTENT: Safe and responsible AI in Australia Department of Industry, Science and Resources17 August 2023Telephone +61 2 6246 3788 Email mail@lawcouncil.au PO Box 5350, Braddon ACT 2612 Level 1, MODE3, 24 Lonsdale Street, Braddon ACT 2612 Law Council of Australia Limited ABN 85 005 260 622 www.lawcouncil.au Table of Contents About the Law Council of Australia ............................................................................... 3 Acknowledgements ........................................................................................................ 4 Executive Summary ........................................................................................................ 5 Definitions ....................................................................................................................... 6 Governance and regulation........................................................................................"
505,Canva,,
506,Office of the Australian Information Commissioner (OAIC),Transcription,"FILENAME: Submission 506 - Office of the Australian Information Commissioner - 18-Aug.e61aea6ea63f3.pdf | CONTENT: Department of Industry, Science and Resources – Safe and Responsible AI in Australia Discussion PaperSubmission by the Office of the Australian Information CommissionerAngelene Falk Australian Information Commissioner and Privacy Commissioner 18 August 2023 August 2023ContentsIntroduction 2 Australia’s privacy framework 3 Enhanced privacy protections 4 Increased accountability for APP entities 4 Strengthening individuals’ control of their personal information 6 Enabling effective enforcement 6 The risk-based approach model 7Global interoperability 7 Regulatory cooperation 8 Conclusion 9Department of Industry, Science and Resources – Safe and Responsible AI in Australia Discussion Paper Page 1 oaic.gov.au August 2023Introduction 1. The Office of the Australian Information Commissioner (the OAIC) welcomes the opportunity to comment on the 'Safe and responsible AI in Australi"
507,Professor Nicole Gillespie,Transcription,"FILENAME: Submission 507 - Gillespie and Lockey - 22-Aug.f1b3585e472e8.pdf | CONTENT: Submission to the Department of Industry, Science and Resources Consultation on ‘Safe and responsible AI in Australia’August 2023Authors: Professor Nicole Gillespie and Dr Steve Lockey, The University of Queensland Contact: n.gillespie1@uq.edu.auThe DISR have called for public submissions to inform Government policy on the safe and responsible use of AI in Australia. The Discussion Paper highlights that public trust in AI technology is required to realise the economic and societal benefits of AI. It also outlines the Government’s commendable intent to develop a clear set of regulatory and governance mechanisms to support and guide the trustworthy use of AI to protect Australians from harm and strengthen public trust in the use of AI.This intent aligns directly with the recommendations identified in our recent research report examining public trust and attitudes towards AI in 17 countries, including Au"
510,Australian Federal Police,Transcription,"FILENAME: Submission 510 - Australian Federal Police - 1-Sept.22292bd1a4425.pdf | CONTENT: 21 August 2023Australian Federal Police submission Supporting responsible AI: discussion paperDepartment of Industry, Science and Resourcesafp.gov.au Australian Federal Police submission / 21 August 2023Table of contents Introduction 2Threat Environment 3Use of AI by the AFP 4Management of the use of AI by the AFP 6 Governance 7Internal policies and building oversight AI framework 8Technical leadership 8Training 9Partnerships 9Conclusion 10AFP / MANAGER TECHNOLOGY STRATEGY & DATA | CHIEF INFORMATION OFFICER 1 Australian Federal Police submission / 21 August 2023Introduction 1. The Australian Federal Police (AFP) welcomes the opportunity to make a submission to the Department of Industry, Science and Resources’ public consultation on safe and responsible Artificial Intelligence (AI) in Australia. This submission addresses both identified criminal threats relating to AI, and considerations for the "
